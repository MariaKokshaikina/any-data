company_link,company_name,description,employment_type,full_text,key,link,location,position,publish_date,salary,scraping_date,seniority_level,source
https://jobs.dou.ua/companies/dataart/,DataArt,"{""Required skills"": [""5+ years of engineering experience, 2+ years working with data in the cloud, ideally using AzureExperience working with the Azure Data Platform (Azure SQL, Azure Data Factory, Azure Storage, Azure Functions)Experience supporting Business Intelligence teams and productsExpertise in SQL and comfortable designing, writing, and maintaining complex SQL-based ETLExperience building batch and micro-batch data pipelines; ETL design, implementation, and maintenanceExperience with schema design and data modeling, and analytical skills to QA the data and identify gaps and inconsistencies.Understanding of data warehouse layers, approaches to load data into such layers, and relevant algorithmsExperience in designing a data platform architecture with relation to the infrastructure.Knowledge of Python or ScalaExperience in parallel computingExperience working with GitSpoken English""], ""We offer"": [""DataArt offers:"", ""Professional Development:"", ""Experienced colleagues who are ready to share knowledge;"", ""The ability to switch projects, technology stacks, try yourself in different roles;"", ""Over 150 courses for workplace-based training"", ""Study and practice of English: courses and communication with colleagues and clients from different countries;"", ""Support of speakers who make presentations at conferences and meetings of technology communities."", ""The ability to focus on your work: a lack of bureaucracy and micromanagement, and convenient corporate services;"", ""Lack of dress code, friendly atmosphere, concern for the comfort of specialists;"", ""Flexible schedule and the ability to work remotely;"", ""The ability to work in any of our development centers.""], ""Project description"": [""We are assembling a team that will develop and upgrade the systems of the international association of financial industry professionals. The company that unites 200 thousand investment experts uses more than a hundred different applications.""]}",,"Required skills 5+ years of engineering experience, 2+ years working with data in the cloud, ideally using AzureExperience working with the Azure Data Platform (Azure SQL, Azure Data Factory, Azure Storage, Azure Functions)Experience supporting Business Intelligence teams and productsExpertise in SQL and comfortable designing, writing, and maintaining complex SQL-based ETLExperience building batch and micro-batch data pipelines; ETL design, implementation, and maintenanceExperience with schema design and data modeling, and analytical skills to QA the data and identify gaps and inconsistencies.Understanding of data warehouse layers, approaches to load data into such layers, and relevant algorithmsExperience in designing a data platform architecture with relation to the infrastructure.Knowledge of Python or ScalaExperience in parallel computingExperience working with GitSpoken English We offer DataArt offers:‚Ä¢ Professional Development:‚Äî Experienced colleagues who are ready to share knowledge;‚Äî The ability to switch projects, technology stacks, try yourself in different roles;‚Äî Over 150 courses for workplace-based training‚Äî Study and practice of English: courses and communication with colleagues and clients from different countries;‚Äî Support of speakers who make presentations at conferences and meetings of technology communities.‚Ä¢ The ability to focus on your work: a lack of bureaucracy and micromanagement, and convenient corporate services;‚Ä¢ Lack of dress code, friendly atmosphere, concern for the comfort of specialists;‚Ä¢ Flexible schedule and the ability to work remotely;‚Ä¢ The ability to work in any of our development centers. Project description We are assembling a team that will develop and upgrade the systems of the international association of financial industry professionals. The company that unites 200 thousand investment experts uses more than a hundred different applications. Advanced technological solutions, skilled technical specialists, and the importance of human relations is what makes the work on projects of this company comfortable and interesting. Minimum viable bureaucracy and openness to new solutions allow every developer to significantly influence the products created. In the future, the DataArt team that cooperates with the client will be expanded to several dozens of people: engineers, testers, architects.",Senior Data Engineer@DataArt,https://jobs.dou.ua/companies/dataart/vacancies/131590/?from=list_hot," Kyiv, Kharkiv, Lviv, Dnipro, Odesa, Kherson, Lublin (Poland), Sofia (Bulgaria), Wroclaw (Poland)",Senior Data Engineer,11 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/appflame/,appflame,{},,"appflame ‚Äî is a young and fast-growing company. It launches and develops its products in social discovery / social network / dating niches. Our products operate primarily on Tier-1 markets, such as the USA, Great Britain, Canada, and Australia. The speed of project development from the first idea to a ready-to-use product, the absence of bureaucracy, result-oriented work, data-driven development, and decision making are our key advantages. We are fans of the data analysis. We like making hypotheses, coming up with new ideas and implementing them on our products. All the members of our young but already big team love what they do. Responsibilities:‚Ä¢ Forming different hypotheses in the area of product development (with the aim of increasing the metrics of involvement and monetization);‚Ä¢ Running split tests to verify different hypothesis;‚Ä¢ Operational monitoring of key metrics and determining the causes of deviations from the expected values;‚Ä¢ Providing analytical support in other areas of the project (Developers, PM, Marketing);‚Ä¢ Proactive involvement in Product Management process. Requirements:‚Ä¢ Proficiency in SQL (Vertica, Clickhouse);‚Ä¢ Advanced knowledge of statistics;‚Ä¢ Practical experience in big database management;‚Ä¢ Basic skills of Machine Learning (theory and practice);‚Ä¢ Ability to formulate product development hypotheses and lead a data-driven product management process;‚Ä¢ 2+ years of experience in analytics/machine learning;‚Ä¢ Experience BI and data visualization tools (Tableau is a plus);‚Ä¢ Experience in data-driven Product management is a plus. What we offer:For our employees we offer the best working conditions comparable with those modern corporations can provide:üí∏ Competitive salary based on highest market benchmarks, regular performance, and compensation review based on results achieved.üìï Continuous growth and development opportunities: free English courses, fiction and non-fiction literature, courses / seminars / exhibitions / conferences, and everything else you may need for your personal and professional development.üöë Full medical insurance after the probation period, as well as a corporate doctor.üè´ Comfortable, spacious offices with new repair near Taras Shevchenko and Kontraktova Ploshcha metro stations. Free parking for all staff, shower.üíª Modern working equipment, and everything else you may need for productive work.‚úàÔ∏è Team travels. In just two years of working together, we have already visited Cyprus, Egypt, Sri Lanka, and Spain.üèì Games room with ping-pong, sports corner, table games and PlayStation, nap room.üèÉ‚Äç‚ôÇÔ∏èOffice yoga, team participation in different sports events worldwide. Football, basketball, and running trainings.üçî Free meals (breakfast and lunch), drinks/juices/snacks, fresh fruits, sweets.",Middle Data Analyst@appflame,https://jobs.dou.ua/companies/appflame/vacancies/124561/?from=list_hot, Kyiv,Middle Data Analyst,23 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/youscan/,YouScan,"{""Tell me\u00a0about your company"": [""YouScan.io is a social media listening solution that gives businesses the ability to become better by listening to their customers.YouScan was recognized as #1 solution in Visual Analytics in the world in Martech Challenge conducted by independent experts.Our customers are Pepsico, Google, L\u2019Oreal, Nestle and many other well-known brands.We\u2019re a Ukrainian product company with a strong internal culture.Each year we double our social media coverage, so we\u2019re always looking for a professional to join our team in the Kyiv office.""], ""Tell me\u00a0about your team"": [""You may have heard the joke that data is the new oil, believe it or not, but we are fueling the entire company:)""], ""What would be\u00a0my\u00a0responsibilities if\u00a0I\u00a0apply?"": [""You will be a part of the Data Squad, responsible for collecting, structuring, and storing huge amounts of data, ~400M new mentions in social media daily. That\u2019s one of our four product squads, and it consists of 3 back-end engineers and 1 tech support at this moment.""], ""What\u2019s your stack and tools?"": [""You will expand data collection for major social platforms, make it faster and more cost-effective. We also consider you to improve our crawlers, make them smarter, but still predictable. You\u2019ll work on ranking & discovery mechanisms.You will dive deep into various small and large social platforms to understand their mechanics and opportunities to get more value for our clients.You will participate in architectural discussions, write clean code, and deploy it to production (we deploy many times a day).We also expect you to support your code running in production"", ""proper monitoring with metrics, logs, and alerts will be your friend.""], ""Sounds good, what\u2019s your requirements?"": [""We use C# as our main language, always relying on async IO and utilizing TPL DataFlow for complicated processing pipelines.Microservices are hosted on Azure. We extensively use Azure Queues and Tables Services for communication and storage, and Redis when needed.We\u2019re mostly using .NET Core and Docker Swarm, write our Infrastructure as a code, run it with Terraform. We use Jenkins for build and deploy.We love predictability: metrics on Prometeus + ELK for logs, alerts integration to Slack.Oh, and Fiddler and TestDriven code runner are our daily tools as well.""], ""Anything else?"": [""We\u2019re looking for a real teammate"", ""you share your vision, communicate, bring in your own ideas.The person with analytical skills, the ability to manage tons of details easily, and think fast.You also listen and hear other points of view, can look with another person\u2019s eyes.""]}",,"Tell me about your company YouScan.io is a social media listening solution that gives businesses the ability to become better by listening to their customers.YouScan was recognized as #1 solution in Visual Analytics in the world in Martech Challenge conducted by independent experts.Our customers are Pepsico, Google, L‚ÄôOreal, Nestle and many other well-known brands.We‚Äôre a Ukrainian product company with a strong internal culture.Each year we double our social media coverage, so we‚Äôre always looking for a professional to join our team in the Kyiv office. Tell me about your team You may have heard the joke that data is the new oil, believe it or not, but we are fueling the entire company:) You will be a part of the Data Squad, responsible for collecting, structuring, and storing huge amounts of data, ~400M new mentions in social media daily. That‚Äôs one of our four product squads, and it consists of 3 back-end engineers and 1 tech support at this moment. What would be my responsibilities if I apply? You will expand data collection for major social platforms, make it faster and more cost-effective. We also consider you to improve our crawlers, make them smarter, but still predictable. You‚Äôll work on ranking & discovery mechanisms.You will dive deep into various small and large social platforms to understand their mechanics and opportunities to get more value for our clients.You will participate in architectural discussions, write clean code, and deploy it to production (we deploy many times a day).We also expect you to support your code running in production ‚Äî proper monitoring with metrics, logs, and alerts will be your friend. What‚Äôs your stack and tools? We use C# as our main language, always relying on async IO and utilizing TPL DataFlow for complicated processing pipelines.Microservices are hosted on Azure. We extensively use Azure Queues and Tables Services for communication and storage, and Redis when needed.We‚Äôre mostly using .NET Core and Docker Swarm, write our Infrastructure as a code, run it with Terraform. We use Jenkins for build and deploy.We love predictability: metrics on Prometeus + ELK for logs, alerts integration to Slack.Oh, and Fiddler and TestDriven code runner are our daily tools as well. Sounds good, what‚Äôs your requirements? We‚Äôre looking for a real teammate ‚Äî you share your vision, communicate, bring in your own ideas.The person with analytical skills, the ability to manage tons of details easily, and think fast.You also listen and hear other points of view, can look with another person‚Äôs eyes. You have a positive mindset, passion for software development, and get the job done. From a technical perspective, we expect you to know how to process and store a lot of data, use async IO whenever possible, and write human-readable code. Anything else? ‚Äî We will pay for your professional development ‚Äî conferences, courses, books etc.‚Äî You will be able to work remotely occasionally (before & after COVID)‚Äî Unlimited vacation (yeah, really) ‚Äî Self-education Fridays",Senior Backend Engineer¬†/ C# .NET¬†/ Data Team@YouScan,https://jobs.dou.ua/companies/youscan/vacancies/12430/?from=list_hot, Kyiv,Senior Backend Engineer¬†/ C# .NET¬†/ Data Team,23 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/septa-digital-agency/,Perion,"{""Required skills"": [""3+ year of a full-time data analytics experience"", ""Expert SQL coding skills against large data sets"", ""Strong analytical skills, including the ability to mine data in order to draw meaningful conclusions"", ""Strong oral and written communications skills"", ""Knowledge of Business Intelligence tools such as Tableau"", ""Spoken English""], ""As a plus"": [""Experience, ideally in an Ad-tech company""], ""We offer"": [""Competitive salary;"", ""Career and professional growth;"", ""Possibility to work on the interesting international project;"", ""Long-term employment with paid vacation and other social benefits;"", ""Opportunities for professional development and personal growth;"", ""15.00"", ""24.00 working hours;"", ""Convenient modern office in the city center;"", ""Training and mentoring.""], ""Responsibilities"": [""Work with business teams to understand their analytical needs, including identifying critical metrics and KPIs"", ""Use analytical and problem-solving skills to deliver actionable insights to relevant decision-makers"", ""Develop rich interactive visualizations integrating various reporting components from multiple data sources"", ""Use SQL, Tableau, and other technologies to pull data from different backend systems and produce meaningful information and visualizations"", ""Take complicated problems and build simple frameworks"", ""Work directly with users and management to gather requirements, provide status updates, and build good relationships and rapport""], ""Project description"": [""At Undertone, we sit at the intersection of media, creative, and technology. We have formed deep partnerships with the world\u2019s best digital media properties, developed a suite of groundbreaking multi-screen creative formats for brands to leverage, and built technology platforms that underpin every aspect of campaign planning, delivery, optimization, and measurement.Undertone\u2019s Data Management Service (UDMS) is a big data, cloud-based data warehouse, dashboard and reporting environment. Are you a data rock star? Do you want to help enable a data-driven organization? This is your opportunity to join a mission-critical team, at an innovative company in an industry just beginning to harness the power of data.As a member of the Undertone\u2019s UDMS Team, the Data Analyst drives value by providing provocative, differentiating analytics and insights. This position will support a wide variety of business intelligence efforts across Undertone while working in a highly collaborative manner within multiple large, cloud-based data sources to identify insights and spearhead our next-generation product offerings. Most importantly, you should \u201clove\u201d the data, working with data, finding the nuance that leads to key differentiation for the business and our customers. This is high visibility analytics and consulting position requiring daily interaction with business users, data scientists, engineers, and key stakeholders.""]}",,"Required skills ‚Ä¢ 3+ year of a full-time data analytics experience‚Ä¢ Expert SQL coding skills against large data sets‚Ä¢ Strong analytical skills, including the ability to mine data in order to draw meaningful conclusions‚Ä¢ Strong oral and written communications skills‚Ä¢ Knowledge of Business Intelligence tools such as Tableau‚Ä¢ Spoken English As a plus Experience, ideally in an Ad-tech company We offer ‚Ä¢ Competitive salary;‚Ä¢ Career and professional growth;‚Ä¢ Possibility to work on the interesting international project;‚Ä¢ Long-term employment with paid vacation and other social benefits;‚Ä¢ Opportunities for professional development and personal growth;‚Ä¢ 15.00 ‚Äî 24.00 working hours;‚Ä¢ Convenient modern office in the city center;‚Ä¢ Training and mentoring. Responsibilities ‚Ä¢ Work with business teams to understand their analytical needs, including identifying critical metrics and KPIs‚Ä¢ Use analytical and problem-solving skills to deliver actionable insights to relevant decision-makers‚Ä¢ Develop rich interactive visualizations integrating various reporting components from multiple data sources‚Ä¢ Use SQL, Tableau, and other technologies to pull data from different backend systems and produce meaningful information and visualizations‚Ä¢ Take complicated problems and build simple frameworks‚Ä¢ Work directly with users and management to gather requirements, provide status updates, and build good relationships and rapport Project description At Undertone, we sit at the intersection of media, creative, and technology. We have formed deep partnerships with the world‚Äôs best digital media properties, developed a suite of groundbreaking multi-screen creative formats for brands to leverage, and built technology platforms that underpin every aspect of campaign planning, delivery, optimization, and measurement.Undertone‚Äôs Data Management Service (UDMS) is a big data, cloud-based data warehouse, dashboard and reporting environment. Are you a data rock star? Do you want to help enable a data-driven organization? This is your opportunity to join a mission-critical team, at an innovative company in an industry just beginning to harness the power of data.As a member of the Undertone‚Äôs UDMS Team, the Data Analyst drives value by providing provocative, differentiating analytics and insights. This position will support a wide variety of business intelligence efforts across Undertone while working in a highly collaborative manner within multiple large, cloud-based data sources to identify insights and spearhead our next-generation product offerings. Most importantly, you should ‚Äúlove‚Äù the data, working with data, finding the nuance that leads to key differentiation for the business and our customers. This is high visibility analytics and consulting position requiring daily interaction with business users, data scientists, engineers, and key stakeholders.",Data Analyst@Perion,https://jobs.dou.ua/companies/septa-digital-agency/vacancies/130872/, Kyiv,Data Analyst,12 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/septa-digital-agency/,Perion,"{""Required skills"": [""MSc degree in Data Science, Statistics, Math, Physics, Engineering, or a similar relevant subject"", ""3+ years of hands-on experience as Data Scientist"", ""Vast experience in SQL, Python, and spark"", ""statistical modeling experience, analytical skills, and engineering skills"", ""Ability to quickly learn new technologies, frameworks, and algorithms""], ""We offer"": [""Competitive salary;"", ""Career and professional growth;"", ""Possibility to work on the interesting international project;"", ""Long-term employment with paid vacation and other social benefits;"", ""Opportunities for professional development and personal growth;"", ""Convenient modern office in the city center;"", ""Training and mentoring.""], ""Responsibilities"": [""Deploying models and systems to production by applying data-driven Machine Learning and AI solutions end to end, starting from research and design to development and testing."", ""Explain results and insights to product & business throughout the process."", ""Perform research & analysis to solve complex problems using advanced AI technologies (deep learning, re-enforcement learning, time-series, NLP...)"", ""Work closely with product, data engineers, and the R&D teams to deliver the best solution to the business."", ""Run research and POCs for new technologies and tools as part of our continuous improvement life cycle."", ""Make sure the solution is stable, scalable, and provides accurate results at all times.""], ""Project description"": [""Perion is a global technology company that provides digital advertising technology solutions to the biggest brands and publishers around the globe. With our unique data-driven AI/ML based technologies, we deliver and optimize hundreds of terabytes of data and billions of events per day from dozens of sources to provide a superior user experience across screens and platforms, including mobile, video, social and native.""]}",,"Required skills ‚Ä¢ MSc degree in Data Science, Statistics, Math, Physics, Engineering, or a similar relevant subject‚Ä¢ 3+ years of hands-on experience as Data Scientist‚Ä¢ Vast experience in SQL, Python, and spark ‚Ä¢ statistical modeling experience, analytical skills, and engineering skills‚Ä¢ Ability to quickly learn new technologies, frameworks, and algorithms We offer ‚Ä¢ Competitive salary;‚Ä¢ Career and professional growth;‚Ä¢ Possibility to work on the interesting international project;‚Ä¢ Long-term employment with paid vacation and other social benefits;‚Ä¢ Opportunities for professional development and personal growth;‚Ä¢ Convenient modern office in the city center;‚Ä¢ Training and mentoring. Responsibilities ‚Ä¢ Deploying models and systems to production by applying data-driven Machine Learning and AI solutions end to end, starting from research and design to development and testing.‚Ä¢ Explain results and insights to product & business throughout the process.‚Ä¢ Perform research & analysis to solve complex problems using advanced AI technologies (deep learning, re-enforcement learning, time-series, NLP...) ‚Ä¢ Work closely with product, data engineers, and the R&D teams to deliver the best solution to the business.‚Ä¢ Run research and POCs for new technologies and tools as part of our continuous improvement life cycle.‚Ä¢ Make sure the solution is stable, scalable, and provides accurate results at all times. Project description Perion is a global technology company that provides digital advertising technology solutions to the biggest brands and publishers around the globe. With our unique data-driven AI/ML based technologies, we deliver and optimize hundreds of terabytes of data and billions of events per day from dozens of sources to provide a superior user experience across screens and platforms, including mobile, video, social and native. We are looking for a highly skilled Data scientist to join our Data Science Group to the journey of leveraging Big Data to revolutionize our offering, business operations and decision making across the company‚Äôs ecosystem.",Data scientist@Perion,https://jobs.dou.ua/companies/septa-digital-agency/vacancies/131574/, Kyiv,Data scientist,12 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/healthprecision/,healthPrecision,"{""Required skills"": [""Advanced knowledge of English;"", ""Experience at pharmaceutical/biotechnology companies and/or consulting firms a plus but not required;"", ""Strong critical thinking and analytical skills;"", ""Trustworthy"", ""ability to handle confidential information.""], ""Responsibilities"": [""Medical Expert should also have the ability to gather, compile, and analyze research and program evaluation data and present reports and summaries in written, tabular, and graphic form.Requires a blend of technical aptitude, data management experience and the ability to communicate well with internal and external customers.""], ""Project description"": [""Medical Brain is a product company. We made the high-end Medical Decision Support solution that successfully supports hospitals and medical doctors in the USA, Canada and all over the world to provide the best service and save lives.""]}",,"Required skills ‚Äî Advanced knowledge of English;‚Äî Experience at pharmaceutical/biotechnology companies and/or consulting firms a plus but not required;‚Äî Strong critical thinking and analytical skills;‚Äî Trustworthy ‚Äî ability to handle confidential information. Responsibilities Medical Expert should also have the ability to gather, compile, and analyze research and program evaluation data and present reports and summaries in written, tabular, and graphic form.Requires a blend of technical aptitude, data management experience and the ability to communicate well with internal and external customers. Project description Medical Brain is a product company. We made the high-end Medical Decision Support solution that successfully supports hospitals and medical doctors in the USA, Canada and all over the world to provide the best service and save lives. Our offices are located in the USA and Ukraine (Kiev, Lvov and Cherkassy). We are the professional medical doctors, passionate IT developers and bright managers. We are enthusiastic and dedicated to our mission ‚Äî to help bring people to a healthier state and empower them to maintain it. medicalbrain.com",Medical Expert to¬†Data Science team in¬†Cherkassy@healthPrecision,https://jobs.dou.ua/companies/healthprecision/vacancies/131413/, Cherkasy,Medical Expert to¬†Data Science team in¬†Cherkassy,12 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/geozilla/,GeoZilla,"{""Required skills"": [""We are looking for a talented data scientist with strong engineering background to kick start an exciting new direction in our well established consumer app business.You will be responsible for designing and building ML based solution which predict safety hazards before they happen, and work on improving our product quality by optimising the accuracy of our location data.""], ""We offer"": [""Requirements:"", ""Bachelor\u2019s degree in statistics, applied mathematics, or related discipline;"", ""4+ years experience in data science;"", ""Proficiency with data mining, mathematics, and statistical analysis;"", ""Advanced pattern recognition and predictive modeling experience;"", ""Excellent analytical and problemsolving skills;"", ""Experience in database interrogation and analysis tools, such asZeppelin, Anaconda, Pandas, ScikitLearn, Jupyter, SQL, Spark;"", ""Experience in programming languages and Python, Scala;"", ""Communication and presentation skills in order to explain your work to people who don\u2019t understand the mechanics behind data analysis;"", ""Effective listening skills in order to understand the requirements of the business;"", ""Drive and the resilience to try new ideas if the first one doesn\u2019t work you\u2019ll be expected to work with minimal supervision, so it\u2019s important that you\u2019re able to motivate yourself;"", ""Planning, time management and organisational skills;"", ""Great attention to detail;"", ""Teamworking skills and a collaborative approach to sharing ideas and finding solutions.""], ""Responsibilities"": [""Competitive salary"", ""Possibility of remote work"", ""Medical insurance"", ""In-house English courses"", ""Great office location"", ""Fun working environment""], ""Project description"": [""Work with stakeholders throughout the organization to identify opportunities for leveraging company data to drive business solutions;"", ""Mine and analyze data from company databases to drive optimization and improvement of product development, marketing techniques and business strategies;"", ""Assess the effectiveness and accuracy of data sources and data gathering techniques;"", ""Develop custom data models and algorithms to apply to data sets;"", ""Use predictive modeling to increase and optimize customer experiences, revenue generation, ad targeting and other business out comes;"", ""Develop company A/B testing framework and test model quality;"", ""Coordinate with different functional teams to implement models and monitor outcomes;"", ""Develop processes and tools to monitor and analyse model performance and data accuracy.""]}",,"Required skills We are looking for a talented data scientist with strong engineering background to kick start an exciting new direction in our well established consumer app business.You will be responsible for designing and building ML based solution which predict safety hazards before they happen, and work on improving our product quality by optimising the accuracy of our location data. Requirements:‚Äî Bachelor‚Äôs degree in statistics, applied mathematics, or related discipline;‚Äî 4+ years experience in data science;‚Äî Proficiency with data mining, mathematics, and statistical analysis;‚Äî Advanced pattern recognition and predictive modeling experience; ‚Äî Excellent analytical and problemsolving skills;‚Äî Experience in database interrogation and analysis tools, such asZeppelin, Anaconda, Pandas, ScikitLearn, Jupyter, SQL, Spark;‚Äî Experience in programming languages and Python, Scala;‚Äî Communication and presentation skills in order to explain your work to people who don‚Äôt understand the mechanics behind data analysis;‚Äî Effective listening skills in order to understand the requirements of the business;‚Äî Drive and the resilience to try new ideas if the first one doesn‚Äôt work you‚Äôll be expected to work with minimal supervision, so it‚Äôs important that you‚Äôre able to motivate yourself;‚Äî Planning, time management and organisational skills;‚Äî Great attention to detail;‚Äî Teamworking skills and a collaborative approach to sharing ideas and finding solutions. We offer ‚Äî Competitive salary ‚Äî Possibility of remote work‚Äî Medical insurance‚Äî In-house English courses‚Äî Great office location‚Äî Fun working environment Responsibilities ‚Äî Work with stakeholders throughout the organization to identify opportunities for leveraging company data to drive business solutions;‚Äî Mine and analyze data from company databases to drive optimization and improvement of product development, marketing techniques and business strategies;‚Äî Assess the effectiveness and accuracy of data sources and data gathering techniques;‚Äî Develop custom data models and algorithms to apply to data sets;‚Äî Use predictive modeling to increase and optimize customer experiences, revenue generation, ad targeting and other business out comes;‚Äî Develop company A/B testing framework and test model quality;‚Äî Coordinate with different functional teams to implement models and monitor outcomes;‚Äî Develop processes and tools to monitor and analyse model performance and data accuracy. Project description The people here at GeoZilla don‚Äôt just create apps ‚Äî they create the kind of wonder that offers peace of mind to millions of families worldwide. It‚Äôs the team‚Äôs synergy and its vision that inspire the innovation that runs through everything we do, from amazing technology to partnerships with the industry-leading IoT. Join GeoZilla, and help us make families worldwide feel safer. With over four million registered users on iOS and Android, backed by five years of R&D, GeoZilla is working to make millions of families worldwide safer.More information about GeoZilla can be found on the site www.geozilla.com.",Data Scientist@GeoZilla,https://jobs.dou.ua/companies/geozilla/vacancies/128768/, Kyiv,Data Scientist,12 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/parimatch-tech/,Parimatch Tech,{},,"Our product is a fraud-detection platform, specially designed for betting business. We started 1.5 years ago with a team created from scratch, dedicated processes and technology stack.Currently we process, store and analyze over half a billion events per day using ML and graph algorithms in real-time. We are going to reach two billion events in a 6-9 month horizon adding more and more detailed data sources. Unlike the majority of fraud-detection products available on the market, we are not wasting time on no-code/low-code and other marketing staff, focusing on our algorithms and business results instead.As a part of Data Science team you will work with challenging problems using a huge variety of data sources (bets, payments, site behaviour data, etc.) detailed, enriched and collected for a long time. ‚Äî Innovative solution for fraud detection;‚Äî Real-time prediction;‚Äî Data collection process;‚Äî Strategic view and modelling roadmap. ‚Äî Good knowledge of math and statistics;‚Äî Developing model from scratch (problem definition, data collection, feature engineering, model selection, validation, tuning, etc.);‚Äî Deep understanding of classical ML Algorithms;‚Äî Strong knowledge of Python (numpy, pandas, scikit-learn, etc.);‚Äî Good knowledge of PostgreSQL, MS SQL, ClickHouse;‚Äî High level of personal responsibility, readiness to commit on projects instead of tasks;‚Äî Knowledge sharing abilities. ‚Äî Experience with at least one DL frameworks (Tensorflow, PyTorch);‚Äî Programming background;‚Äî Experience with big data technologies;‚Äî Degree in Computer Science, Engineering, Mathematics or a related field. ‚Äî Medical insurance/Sport compensation;‚Äî Sport club participation (football, running, basketball or swimming clubs);‚Äî 100% paid sick leaves;‚Äî 20 working days of paid vacation. ‚Äî Competitive salary and —Åonstant encouragement for your efforts and contribution;‚Äî Bonuses according to company‚Äôs policy;‚Äî Welfare (financial support in critical situations);‚Äî Gifts for significant life events (marriage, childbirth). ‚Äî Individual annual training budget with an opportunity to visit paid conferences, training sessions, workshops, etc.;‚Äî Free corporate library;‚Äî Opportunity to visit our non-stop internal meetups: open talks, IT Pump, etc. as a participant or a speaker and exchange knowledge;‚Äî A world-class team of T-shaped skilled professionals that share knowledge and support each other. ‚Äî Corporate parties and events (Pub Quiz, Carquest, bowling championships, etc.);‚Äî PM Foundation activities (social responsibility events);‚Äî Weekly events aimed at culture, arts, soft skills development.",Senior Data Scientist for FairPlay Team@Parimatch Tech,https://jobs.dou.ua/companies/parimatch-tech/vacancies/131670/, Kyiv,Senior Data Scientist for FairPlay Team,12 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/reface/,Reface,"{""Required skills"": [""Specialized knowledge:\u25cf Understanding theoretical concepts of statistics/probability, data mining, machine learning.\u25cf Understanding how these theoretical concepts could be applied to real-world problems.\u25cf Knowledge and hands-on experience with Python or other relevant programming languages.\u25cf Knowledge and hands-on experience with any recommender engine tools or frameworks.\u25cf Knowledge and hands-on experience with any visualization tools or frameworks.\u25cf Understanding the developing process of data science projects: CRISP-DM or others.""], ""Responsibilities"": [""Skills and abilities:\u25cf Strong English verbal and written communication skills.\u25cf Ability to work independently with limited supervision.""], ""Project description"": [""Experience:\u25cf Track record in data analysis, data science, machine learning.\u25cf Relevant levels of theoretical knowledge in statistics/probability, data mining, and machine learning.""]}",,"Required skills Specialized knowledge:‚óè Understanding theoretical concepts of statistics/probability, data mining, machine learning.‚óè Understanding how these theoretical concepts could be applied to real-world problems.‚óè Knowledge and hands-on experience with Python or other relevant programming languages.‚óè Knowledge and hands-on experience with any recommender engine tools or frameworks.‚óè Knowledge and hands-on experience with any visualization tools or frameworks.‚óè Understanding the developing process of data science projects: CRISP-DM or others. Skills and abilities:‚óè Strong English verbal and written communication skills.‚óè Ability to work independently with limited supervision. Experience:‚óè Track record in data analysis, data science, machine learning.‚óè Relevant levels of theoretical knowledge in statistics/probability, data mining, and machine learning. Responsibilities ‚óè Collecting, transforming, and preprocessing raw data to prepare it for analysis.‚óè Deriving descriptive statistics out of the preprocessed data.‚óè Building statistical and probabilistic models.‚óè Creating visualizations and analytics reports in the form of dashboards.‚óè Designing, developing, training, and testing data mining, statistical, probabilistic models, and algorithms.‚óè Providing comparative research on different algorithms and models.‚óè Implementing the model in a form that can be easily used by engineers, documenting its interfaces.‚óè Delivering the model in a form that can be easily deployable and maintained. Project description Based on collected data and product usage statistics, candidates are able to create models that affect the improvement of the product metrics. Exploratory data analysis, building recommender engines, statistical models, etc.",Data Scientist@Reface,https://jobs.dou.ua/companies/reface/vacancies/134761/, Kyiv,Data Scientist,12 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/reface/,Reface,"{""Required skills"": [""Specialized knowledge:\u25cf Understanding theoretical concepts of machine learning and neural networks.\u25cf Understanding theoretical concepts of deep learning architectures.\u25cf Understanding how these theoretical concepts could be applied to real-world problems.\u25cf Knowledge and hands-on experience with Python or other relevant programming languages.\u25cf Knowledge and hands-on experience with ML/DL frameworks (PyTorch, TensorFlow etc.).\u25cf Knowledge and hands-on experience with tools for data preprocessing and scraping.""], ""Responsibilities"": [""Skills and abilities:\u25cf Strong English verbal and written communication skills.\u25cf Ability to work independently with limited supervision.""], ""Project description"": [""Experience:\u25cf Track record in deep learning, data science, machine learning.\u25cf Relevant levels of theoretical knowledge in data science and machine learning.""]}",,"Required skills Specialized knowledge:‚óè Understanding theoretical concepts of machine learning and neural networks.‚óè Understanding theoretical concepts of deep learning architectures.‚óè Understanding how these theoretical concepts could be applied to real-world problems.‚óè Knowledge and hands-on experience with Python or other relevant programming languages.‚óè Knowledge and hands-on experience with ML/DL frameworks (PyTorch, TensorFlow etc.).‚óè Knowledge and hands-on experience with tools for data preprocessing and scraping. Skills and abilities:‚óè Strong English verbal and written communication skills.‚óè Ability to work independently with limited supervision. Experience:‚óè Track record in deep learning, data science, machine learning.‚óè Relevant levels of theoretical knowledge in data science and machine learning. Responsibilities ‚óè Collecting, transforming, and preprocessing raw data to prepare it for modeling.‚óè Building machine learning and deep learning models.‚óè Designing, developing, training, and testing models and algorithms.‚óè Providing comparative research on different algorithms and models.‚óè Implementing the model in a form that can be easily used by engineers, documenting its interfaces.‚óè Delivering the model in a form that can be easily deployable and maintained. Project description Based on different types of data, candidates can create machine learning / deep learning models. Ability to work in short iteration cycles (up to 2 weeks) from initial research to PoC prototype. Ability to work simultaneously on multiple tasks. Ability to work in a solo or a small team and be responsible for the final results. Lots of educational and self-educational activities.",Machine Learning Engineer@Reface,https://jobs.dou.ua/companies/reface/vacancies/134760/, Kyiv,Machine Learning Engineer,12 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/eos-data-analytics/,EOS Data Analytics,{},,"EOS Data Analytics is developing cloud-based GIS analysis service. Company uses a combination of satellite imagery, geospatial data, customer workflow information, and consumer behaviour principles to make the deepest and the most comprehensive GIS analysis. Aspectum, as a part of EOS Data Analytics, is a Cloud Platform for Location Business Intelligence.Aspectum allows transforming business data into geospatial data, providing high-quality visualization and powerful analytics for converting it into valuable business outcomes. Aspectum is looking for a talented and productive COO to join the team and help improve a product. Responsibilities:‚Ä¢ company strategy execution and review, planning operational activities to achieve business goals;‚Ä¢ product launch on different markets (WENA focus);‚Ä¢ lead marketing activities and sync marketing with sales needs;‚Ä¢ combining usage data, market and user research, and industry best practises to understand and identify user problems and breakthrough opportunities;‚Ä¢ P&L planning and execution;‚Ä¢ measuring the results of changes introduced, deciding and communicating on success or failure, identifying key learnings and integrating them in overall understanding of the owned area. You need to have:‚Ä¢ 3+ years of experience in Product Management, CMO, Head Of Sales;‚Ä¢ SaaS products launch experience (preferable US market);‚Ä¢ leadership skills ‚Äî you are able to come up with a vision and execute it while leading people who you usually have no authority over;‚Ä¢ analytical skills to understand usage data, market and user researches;‚Ä¢ proven high level of stress resistance. Company offers:‚Ä¢ work with a global brand in dynamic international company;‚Ä¢ social guarantees, health insurance and other benefits;‚Ä¢ 24-day paid vacation, paid sick leave, medical insurance;‚Ä¢ training, conferences, courses;‚Ä¢ compensation of foreign language courses and sport sections.",Chief Operating Officer@EOS Data Analytics,https://jobs.dou.ua/companies/eos-data-analytics/vacancies/131066/, Kyiv,Chief Operating Officer,12 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/ciklum/,Ciklum,{},,"On behalf of Ciklum Digital, Ciklum is looking for a Senior Databricks Developer to join the Kyiv team on a full-time basis. Project description: You can name examples of use in different contexts. Are guided by best-practices and specifications of such skills:",Senior Databricks Developer for Ciklum Digital (200004CY)@Ciklum,https://jobs.dou.ua/companies/ciklum/vacancies/134754/, Kyiv,Senior Databricks Developer for Ciklum Digital (200004CY),12 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/mgid/,MGID,{},,"MGID ‚Äî –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω–∞—è –∫–æ–º–ø–∞–Ω–∏—è-–ª–∏–¥–µ—Ä –Ω–∞ —Ä—ã–Ω–∫–µ –Ω–∞—Ç–∏–≤–Ω–æ–π —Ä–µ–∫–ª–∞–º—ã, –æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è –≤ 2008 –≥–æ–¥—É. –ü–ª–∞—Ç—Ñ–æ—Ä–º–∞ MGID –ø–æ–º–æ–≥–∞–µ—Ç –º–µ–¥–∏–∞ –º–æ–Ω–µ—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∞—É–¥–∏—Ç–æ—Ä–∏—é, –∞ –±—Ä–µ–Ω–¥–∞–º ‚Äî –¥–æ–Ω–µ—Å—Ç–∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ —Ä–µ–∫–ª–∞–º–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ —Å–≤–æ–∏–º –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—è–º. MGID –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è: –æ—Ç –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ —Ä–µ–∫–ª–∞–º–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏, –¥–æ –µ–µ –≤–Ω–µ–¥—Ä–µ–Ω–∏—è –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏. –ù–∞—à–∏ –∫–ª–∏–µ–Ω—Ç—ã ‚Äî –∫—Ä—É–ø–Ω—ã–µ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –±—Ä–µ–Ω–¥—ã: Renault, Domino‚Äôs, airbnb, PizzaHut, Qatar Airlines –∏ –º–Ω–æ–≥–æ –¥—Ä—É–≥–∏—Ö –∫–æ–º–ø–∞–Ω–∏–π, –≤–∫–ª—é—á–∞—è –º–µ–¥–∏–∞—Ö–æ–ª–¥–∏–Ω–≥–∏ –∏ —Å–µ—Ç–µ–≤—ã–µ –∞–≥–µ–Ω—Ç—Å—Ç–≤–∞. MGID ‚Äî —ç—Ç–æ:‚Äî–û–¥–Ω–∞ –∏–∑ —Å–∞–º—ã—Ö –±–æ–ª—å—à–∏—Ö martech-–∫–æ–º–ø–∞–Ω–∏–π –Ω–∞ —É–∫—Ä–∞–∏–Ω—Å–∫–æ–º —Ä—ã–Ω–∫–µ;‚Äî–°–æ–±—Å—Ç–≤–µ–Ω–Ω–∞—è Highload –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç 185 –º–ª—Ä–¥ –∫–æ–Ω—Ç–µ–Ω—Ç–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –¥–ª—è 850 –º–ª–Ω —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–æ—Å–µ—Ç–∏—Ç–µ–ª–µ–π –Ω–∞ –±–æ–ª–µ–µ —á–µ–º 70 —è–∑—ã–∫–∞—Ö;‚Äî–ü–æ–±–µ–¥–∏—Ç–µ–ª—å –º–Ω–æ–≥–æ—á–∏—Å–ª–µ–Ω–Ω—ã—Ö –ø—Ä–µ–º–∏–π –∑–∞ –∏–Ω–Ω–æ–≤–∞—Ü–∏–∏ –∏ –∫–∞—á–µ—Å—Ç–≤–æ –ø—Ä–æ–¥—É–∫—Ç–∞ –≤ —Å—Ñ–µ—Ä–µ AdTech;‚Äî–®—Ç–∞—Ç –±–æ–ª–µ–µ 600 —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–æ–≤, —Ä–∞–±–æ—Ç–∞—é—â–∏—Ö –≤ –æ—Ñ–∏—Å–∞—Ö –≤ –°–®–ê, –ï–≤—Ä–æ–ø–µ –∏ –ê–∑–∏–∏;‚Äî–ù–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–µ –∏–Ω–Ω–æ–≤–∞—Ü–∏–∏, –æ–±–º–µ–Ω –æ–ø—ã—Ç–æ–º –∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞–º–∏ –º–µ–∂–¥—É —Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã–º–∏ –∫–æ–º–∞–Ω–¥–∞–º–∏. –ó–Ω–∞–Ω–∏—è –∏ –Ω–∞–≤—ã–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω—ã: ‚Äî–û–ø—ã—Ç —Ä–∞–±–æ—Ç—ã –Ω–∞ –¥–æ–ª–∂–Ω–æ—Å—Ç–∏ –ë–∏–∑–Ω–µ—Å-–∞–Ω–∞–ª–∏—Ç–∏–∫–∞ –≤ –≤–µ–±-–ø—Ä–æ–µ–∫—Ç–∞—Ö –æ—Ç 3-—Ö –ª–µ—Ç;‚Äî–ó–Ω–∞–Ω–∏–µ –∏ –Ω–∞–ª–∏—á–∏–µ –æ–ø—ã—Ç–∞ —Ä–∞–±–æ—Ç—ã —Å Google, Facebook. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∑–∞–∫—É–ø–∫–∏ —Ç—Ä–∞—Ñ–∏–∫–∞ —É –Ω–∏—Ö, –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ —Ü–µ–ª–µ–π –∏ —ç—Ç–∞–ø–æ–≤ –∫–æ–Ω–≤–µ—Ä—Å–∏–π;‚Äî–£–º–µ–Ω–∏–µ —Ä–∞–±–æ—Ç–∞—Ç—å —Å –±–∞–∑–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö (MySQL, Clickhouse);‚Äî–ó–Ω–∞–Ω–∏–µ –ø—Ä–∏–Ω—Ü–∏–ø–æ–≤ —Ä–∞–±–æ—Ç—ã Google Analytics, –Ø–Ω–¥–µ–∫—Å.–ú–µ—Ç—Ä–∏–∫–∞;‚Äî–í—ã—Å–æ–∫–∏–π —É—Ä–æ–≤–µ–Ω—å –æ—Ä–≥–∞–Ω–∏–∑–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –∏ –≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∫ –¥–µ—Ç–∞–ª—è–º;‚Äî–ó–Ω–∞–Ω–∏–µ –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ –Ω–µ –Ω–∏–∂–µ Intermediate. –ë—É–¥–µ—Ç –ø–ª—é—Å–æ–º: ‚Äî–û–ø—ã—Ç —Ä–∞–±–æ—Ç—ã —Å e-commerce –ø—Ä–æ–µ–∫—Ç–∞–º–∏;‚Äî–ó–Ω–∞–Ω–∏–µ Python. –ß–µ–º –ø—Ä–µ–¥—Å—Ç–æ–∏—Ç –∑–∞–Ω–∏–º–∞—Ç—å—Å—è: ‚Äî–ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ä–µ–∫–ª–∞–º–Ω—ã—Ö –∫–∞–º–ø–∞–Ω–∏–π —Ç–æ–ø–æ–≤—ã—Ö —Ä–µ–∫–ª–∞–º–æ–¥–∞—Ç–µ–ª–µ–π;‚Äî–ü–æ–∏—Å–∫ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –∏ –Ω–∞ –∏—Ö –æ—Å–Ω–æ–≤–∞–Ω–∏–∏ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –±–∏–∑–Ω–µ—Å –ø–æ–¥—Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è–º –ø–æ —É–ª—É—á—à–µ–Ω–∏—é –º–µ—Ç–æ–¥–æ–≤ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏, –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –Ω–æ–≤—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤;‚Äî–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –ø–æ–¥—Ö–æ–¥–æ–≤ –∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∫–æ–º–±–∏–Ω–∞—Ü–∏–π –Ω–∞—Å—Ç—Ä–æ–µ–∫;‚Äî–¢–µ—Å–Ω–æ–µ —Å–æ—Ç—Ä—É–¥–Ω–∏—á–µ—Å—Ç–≤–æ —Å –ø—Ä–æ–¥—É–∫—Ç–æ–≤–æ–π –∫–æ–º–∞–Ω–¥–æ–π –∏ –∫–æ–º–∞–Ω–¥–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –≤ —Ä–∞–º–∫–∞—Ö —Ä–µ—à–µ–Ω–∏—è –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á;‚Äî–ê–Ω–∞–ª–∏–∑ —Ä—ã–Ω–∫–∞ –≤ —Ü–µ–ª–æ–º –∏ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑. –ú—ã –ø—Ä–µ–¥–ª–∞–≥–∞–µ–º: ‚Äî–ù–æ–≤—ã–π –æ—Ñ–∏—Å –≤ –ë–¶ ¬´–ú–∞—Ä–º–µ–ª–∞–¥¬ª;‚Äî–ö—É—Ä—Å—ã –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ —Å –Ω–æ—Å–∏—Ç–µ–ª–µ–º;‚Äî–ì–∏–±–∫–∏–π –ø–æ–¥—Ö–æ–¥ –∫ –≥—Ä–∞—Ñ–∏–∫—É. –î–ª—è –Ω–∞—Å –≥–ª–∞–≤–Ω–æ–µ ‚Äî —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏ –≤–æ–≤–ª–µ—á–µ–Ω–Ω–æ—Å—Ç—å;‚Äî–ö–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–æ–µ —É—á–∞—Å—Ç–∏–µ –≤ —Å–ø–æ—Ä—Ç–∏–≤–Ω—ã—Ö, —ç–∫–æ- –∏ —Å–æ—Ü–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–æ–µ–∫—Ç–∞—Ö. –ö–∞–∫ —ç—Ç–æ –±—ã–≤–∞–µ—Ç ‚Äî —Ç—É—Ç www.facebook.com/MGID.inside‚Äî–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –ø–æ—Å–µ—â–∞—Ç—å –ª–µ–∫—Ü–∏–∏ –ê–∫–∞–¥–µ–º–∏–∏ MGID;‚Äî–ü–∞–∫–µ—Ç –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π —Å—Ç—Ä–∞—Ö–æ–≤–∫–∏ —É—Ä–æ–≤–Ω—è ¬´–ü—Ä–µ–º–∏—É–º¬ª.",Data analyst@MGID,https://jobs.dou.ua/companies/mgid/vacancies/131432/, Kyiv,Data analyst,12 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/ring-ukraine/,Ring Ukraine,{},,"We are looking for an enthusiastic person to join our Neighbourhood moderation team. On this role you will be responsible for content moderation of mobile app ‚Äî you will work with big amounts of videos and comments, posted by our neighbors. Our goal is to be sure that neighbors see only important safety-related posts in their app. Take your chance to work in a passionate and friendly team and make your impact on people‚Äôs safety! ‚Äî Pre-moderate incoming content shared by our neighbors including: videos, images, text posts and comments‚Äî Collect and analyze data for the other departments‚Äî Identify ways to improve the workflows and suggest solutions‚Äî Deliver instant crime and safety alerts to our neighbors and play your part in reducing crime! ‚Äî Upper-intermediate/Advanced English (written and spoken)‚Äî Willingness to learn and adapt to changes‚Äî Basic understanding of IT terminology ‚Äî Experience with PC based computer systems or junior administrator experience (MacOS is preferable)‚Äî Knowledge of bug tracking systems like Jira and etc. Working schedule: mixed shifts, two night shifts/two days off.",Data Moderator@Ring Ukraine,https://jobs.dou.ua/companies/ring-ukraine/vacancies/76959/, Kyiv,Data Moderator,12 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/thredup-inc/,THREDUP Inc.,"{""Required skills"": [""8+ years of experience in building scalable data platforms and tools.Demonstrated experience in implementing Spark for data processing.Demonstrated experience in implementing Kafka for real-time data processing.Demonstrated experience providing rest endpoints to expose data to other applications.Experience integrating and supporting Databricks.Working with noSQL stores like HBASE/DynamoDB.Experience providing solutions to handle data privacy.Experience in implementing machine learning platforms like MLFlow, Tensorflow, Sagemake.Experience in deploying Airflow as a scheduling tool.Experience in implementing CI/CD pipelines using Jenkins.Expert level using Python and PySpark.Understanding with AWS Data technologies (such as Redshift, S3, Glue)Experience with repositories like Git, maven, jfrog.""], ""As a plus"": [""Experience working with cloud databases like Snowflake/Redshift/BigQuery.Experience migrating legacy data platforms to cloud-native solutions.Experience implementing Delta-Lake.Experience working with machine learning models.Experience with Java and building microservices.""], ""We offer"": [""High competitive top market salaryThe opportunity to make a massive impact & influence outcomes for our business and customers alongside passionate coworkersAutonomy. The ability to make, own, and carry out decisionsWorking within a modern tech stackFlexible working hours (possibility to work from home on Tuesdays and Thursdays)Uncounted paid vacationsFull-coverage medical insurance, free lunches on Wednesdays, English classes, etc.Relocation assistance and cost coverage program for candidates from other countries and cities""], ""Responsibilities"": [""Ideate and build the next generation product roadmap for the data platform team.Own Airflow deployments, integration with Datadog and CI/CD.Own the Mlflow machine learning platform and find alternatives or enhance it to support input data versioning and model deployment in A/B testing mode.Build and evangelize reusable components for data pipelines.Implement a solution to capture metadata and lineage.Own the enterprise event bus solution.Own the databricks environment.Own and shape the feature store infrastructure and roadmap.""], ""Project description"": [""At thredUP, we live a true data-driven culture with an ever-growing appetite for data and a mindset to generate insights and make informed business decisions. In this role, you will shape the roadmap for the next generation of data platform that is easy to use, elastic, and cost-efficient. This platform should make it easy for data engineers to build batch and streaming pipelines as well as make it easy to interact with the data for the end consumers. Build a robust machine-learning platform to develop, test, and deploy machine learning models deployed for both batch and real-time use cases. Additionally, support the current technology stack and evaluate newer tools/technologies that reduce the support cost and provide improved productivity and experience.""]}",,"Required skills 8+ years of experience in building scalable data platforms and tools.Demonstrated experience in implementing Spark for data processing.Demonstrated experience in implementing Kafka for real-time data processing.Demonstrated experience providing rest endpoints to expose data to other applications.Experience integrating and supporting Databricks.Working with noSQL stores like HBASE/DynamoDB.Experience providing solutions to handle data privacy.Experience in implementing machine learning platforms like MLFlow, Tensorflow, Sagemake.Experience in deploying Airflow as a scheduling tool.Experience in implementing CI/CD pipelines using Jenkins.Expert level using Python and PySpark.Understanding with AWS Data technologies (such as Redshift, S3, Glue)Experience with repositories like Git, maven, jfrog. As a plus Experience working with cloud databases like Snowflake/Redshift/BigQuery.Experience migrating legacy data platforms to cloud-native solutions.Experience implementing Delta-Lake.Experience working with machine learning models.Experience with Java and building microservices. We offer High competitive top market salaryThe opportunity to make a massive impact & influence outcomes for our business and customers alongside passionate coworkersAutonomy. The ability to make, own, and carry out decisionsWorking within a modern tech stackFlexible working hours (possibility to work from home on Tuesdays and Thursdays)Uncounted paid vacationsFull-coverage medical insurance, free lunches on Wednesdays, English classes, etc.Relocation assistance and cost coverage program for candidates from other countries and cities Responsibilities Ideate and build the next generation product roadmap for the data platform team.Own Airflow deployments, integration with Datadog and CI/CD.Own the Mlflow machine learning platform and find alternatives or enhance it to support input data versioning and model deployment in A/B testing mode.Build and evangelize reusable components for data pipelines.Implement a solution to capture metadata and lineage.Own the enterprise event bus solution.Own the databricks environment.Own and shape the feature store infrastructure and roadmap. Project description At thredUP, we live a true data-driven culture with an ever-growing appetite for data and a mindset to generate insights and make informed business decisions. In this role, you will shape the roadmap for the next generation of data platform that is easy to use, elastic, and cost-efficient. This platform should make it easy for data engineers to build batch and streaming pipelines as well as make it easy to interact with the data for the end consumers. Build a robust machine-learning platform to develop, test, and deploy machine learning models deployed for both batch and real-time use cases. Additionally, support the current technology stack and evaluate newer tools/technologies that reduce the support cost and provide improved productivity and experience. You will interact in a highly collaborative environment of data engineers, data scientists, product managers, domain experts, and business leaders to deliver data solutions.",Lead Data Platform Engineer@THREDUP Inc.,https://jobs.dou.ua/companies/thredup-inc/vacancies/131333/, Kyiv,Lead Data Platform Engineer,12 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/sigma-software/,Sigma Software,"{""Required skills"": [""3+ years of experience as a DevOps Engineer"", ""Knowledge of AWS cloud and services (EC2, S3, RDS, DynamoDB, CloudWatch, ECR) is a must"", ""Experience in automatization of setting multiple EC2 instances"", ""Understanding of Network domains management"", ""Solid knowledge of monitoring tools""], ""As a plus"": [""DBA skills for DBs optimization""], ""Responsibilities"": [""Manage domains and subdomains, S3s, EC2s, DBs (RDS & DynamoDB), CloudWatch, maybe also cost optimization"", ""Work with EC2 templates/images, Race Graphics cloud rendering fleet creation and various shortcuts/automation tools for AWS"", ""Introduce and support monitoring tools"", ""Work on recalculation of various processes (Python/Node) on existing data and/or restreaming of this data"", ""Work with track mapping tool from Google Maps or recorded lap"", ""Evaluate migration from RDS to DynamoDB""], ""Project description"": [""Our project provides the solution for monitoring of motorsports events, which includes remote live streaming units, streaming services, telemetry sensors, telemetry analysis services, and intermediate databases with corresponding access services.We create a unique system that helps monitor all the processes of the race, as well as meet the needs of the most demanding spectators.""]}",,"Required skills ‚Äî 3+ years of experience as a DevOps Engineer‚Äî Knowledge of AWS cloud and services (EC2, S3, RDS, DynamoDB, CloudWatch, ECR) is a must‚Äî Experience in automatization of setting multiple EC2 instances‚Äî Understanding of Network domains management‚Äî Solid knowledge of monitoring tools As a plus ‚Äî DBA skills for DBs optimization Responsibilities ‚Äî Manage domains and subdomains, S3s, EC2s, DBs (RDS & DynamoDB), CloudWatch, maybe also cost optimization‚Äî Work with EC2 templates/images, Race Graphics cloud rendering fleet creation and various shortcuts/automation tools for AWS‚Äî Introduce and support monitoring tools‚Äî Work on recalculation of various processes (Python/Node) on existing data and/or restreaming of this data‚Äî Work with track mapping tool from Google Maps or recorded lap‚Äî Evaluate migration from RDS to DynamoDB Project description Our project provides the solution for monitoring of motorsports events, which includes remote live streaming units, streaming services, telemetry sensors, telemetry analysis services, and intermediate databases with corresponding access services.We create a unique system that helps monitor all the processes of the race, as well as meet the needs of the most demanding spectators.",DevOps Engineer (Real-Time Data Analysis)@Sigma Software,https://jobs.dou.ua/companies/sigma-software/vacancies/134708/," Kyiv, Kharkiv, Lviv, Dnipro, Odesa, Vinnytsia",DevOps Engineer (Real-Time Data Analysis),10 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/proxet/,Proxet,"{""Required skills"": [""Job SummaryWe are looking for a Senior Data Engineer who will be a significant part of creating a data platform for a new product.This is your chance to join one of the fastest-growing and most exciting companies in the USA.We are looking for people who look at the world differently, people who are passionate about making sense out of huge amounts of data, who explore, analyze, and lead the solutions they envision. If you are an engineer that is passionate about developing an innovative product, making an impact, and strive to work with the industry\u2019s best talent as part of one of the fastest-growing and successful companies around"", ""join Thras.io.""], ""As a plus"": [""Requirements:"", ""A clear understanding of ETL and data curation"", ""Strong experience and understanding of relational and distributed databases (including internals) (e.g. PostgreSQL, MySQL, Cassandra, etc.)"", ""Experience with Spark/PySpark, AirFlow"", ""Strong experience with AWS"", ""Strong expertise in Python, SQL"", ""Experience with continuous integration, test automation, and deployment"", ""Experience with data monitoring and tracing in distributed and service-oriented systems"", ""Experience in distributed systems design and best practices"", ""Understanding of integration with BI tools"", ""Advanced / fluent written and spoken English""], ""We offer"": [""Will be a plus:"", ""Proficiency with Kubernetes""], ""Responsibilities"": [""Challenging work in an international professional environment"", ""Mastering English language with a native speaker"", ""40-hour working week with flexible working hours"", ""Flexible work-from-home policy"", ""Competitive salary"", ""PE accounting and support"", ""20 paid vacation days per year"", ""14 paid sick leaves per year"", ""Medical insurance"", ""Annual 250$ deposit for attending external events (conferences, workshops, etc.)"", ""Long-term employment and real opportunities to change roles and projects within the company"", ""Yoga classes, workout corner"", ""Collaborative friendly team environment"", ""Cozy fully equipped office space in the city center""], ""Project description"": [""Responsibilities:"", ""Work with the data engineering team and to design the data platform"", ""Manage and guide software engineers during the development"", ""Systematically work on formalizing business processes, customer acquisition lifecycles, and the overall data framework"", ""Actively participate in the development"", ""Write code/tests in Python"", ""Create ETL pipeline in Airflow"", ""Design ETL flows""]}",,"Required skills Job SummaryWe are looking for a Senior Data Engineer who will be a significant part of creating a data platform for a new product.This is your chance to join one of the fastest-growing and most exciting companies in the USA.We are looking for people who look at the world differently, people who are passionate about making sense out of huge amounts of data, who explore, analyze, and lead the solutions they envision. If you are an engineer that is passionate about developing an innovative product, making an impact, and strive to work with the industry‚Äôs best talent as part of one of the fastest-growing and successful companies around ‚Äî join Thras.io. Requirements:‚Äî A clear understanding of ETL and data curation‚Äî Strong experience and understanding of relational and distributed databases (including internals) (e.g. PostgreSQL, MySQL, Cassandra, etc.)‚Äî Experience with Spark/PySpark, AirFlow‚Äî Strong experience with AWS‚Äî Strong expertise in Python, SQL‚Äî Experience with continuous integration, test automation, and deployment‚Äî Experience with data monitoring and tracing in distributed and service-oriented systems‚Äî Experience in distributed systems design and best practices‚Äî Understanding of integration with BI tools‚Äî Advanced / fluent written and spoken English As a plus Will be a plus:‚Äî Proficiency with Kubernetes We offer ‚Äî Challenging work in an international professional environment‚Äî Mastering English language with a native speaker‚Äî 40-hour working week with flexible working hours‚Äî Flexible work-from-home policy‚Äî Competitive salary‚Äî PE accounting and support‚Äî 20 paid vacation days per year‚Äî 14 paid sick leaves per year‚Äî Medical insurance‚Äî Annual 250$ deposit for attending external events (conferences, workshops, etc.)‚Äî Long-term employment and real opportunities to change roles and projects within the company‚Äî Yoga classes, workout corner‚Äî Collaborative friendly team environment‚Äî Cozy fully equipped office space in the city center Responsibilities Responsibilities:‚Äî Work with the data engineering team and to design the data platform‚Äî Manage and guide software engineers during the development‚Äî Systematically work on formalizing business processes, customer acquisition lifecycles, and the overall data framework‚Äî Actively participate in the development‚Äî Write code/tests in Python‚Äî Create ETL pipeline in Airflow‚Äî Design ETL flows As a Data Engineer you get:‚Äî A job in a highly innovative company in a rapidly growing market where you can make adifference‚Äî The opportunity to be a driving force in the global organization‚Äî The chance to join a group of highly professional and dedicated colleagues who share yourpassion‚Äî Exposure to a broad set of technologies, platforms, and tools‚Äî To collaborate with experienced management and work on challenging tasks‚Äî An informal environment with the spirit of staying ahead of the competition Project description Thrasio buys and invests in developing FBA brands on Amazon. We are becoming one of the largest and most profitable consumer product companies in the Amazon ecosystem while building an amazing place to work, full of people who love to come in every day. Thrasio was established in 2018 with a clear mission: to become the largest, most profitable seller on Amazon. Since then, our growth has been dramatic; we have over $100MM in committed capital, with an impressive group of investors supporting our success.",Senior Data Engineer with ETL@Proxet,https://jobs.dou.ua/companies/proxet/vacancies/126774/," Kyiv, remote",Senior Data Engineer with ETL,09 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/devart/,Devart,"{""Required skills"": [""2+ \u0433\u043e\u0434\u0430 \u043e\u043f\u044b\u0442\u0430 \u0432 BI/Data Analytics, Product Management;\u0430\u043d\u0433\u043b\u0438\u0439\u0441\u043a\u0438\u0439"", ""Intermediate \u0438 \u0432\u044b\u0448\u0435;\u043d\u0430\u0432\u044b\u043a\u0438 \u043d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0438 \u043f\u043e\u0442\u043e\u043a\u043e\u0432 \u0434\u0430\u043d\u043d\u044b\u0445 \u0438 \u043d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0438 \u0430\u0432\u0442\u043e\u043c\u0430\u0442\u0438\u0437\u0430\u0446\u0438\u0438 \u043c\u0435\u0436\u0434\u0443 \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u043c\u0438 \u0438\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442\u0430\u043c\u0438;\u043e\u0442\u043b\u0438\u0447\u043d\u043e\u0435 \u0432\u043b\u0430\u0434\u0435\u043d\u0438\u0435 SQL;\u0432\u043b\u0430\u0434\u0435\u043d\u0438\u0435 Excel/Google Sheets \u043d\u0430 \u0432\u044b\u0441\u043e\u043a\u043e\u043c \u0443\u0440\u043e\u0432\u043d\u0435;\u043e\u0442\u043b\u0438\u0447\u043d\u043e\u0435 \u0432\u043b\u0430\u0434\u0435\u043d\u0438\u0435 \u0438\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442\u0430\u043c\u0438 \u0430\u043d\u0430\u043b\u0438\u0442\u0438\u043a\u0438 (Power BI, Google Analytics, Data Studio \u0438 \u0430\u043d\u0430\u043b\u043e\u0433\u0438);\u043e\u043f\u044b\u0442 \u0440\u0430\u0431\u043e\u0442\u044b \u0441 REST API;\u0440\u0430\u0431\u043e\u0442\u044b SaaS- \u0438 CRM-\u0441\u0438\u0441\u0442\u0435\u043c (HubSpot, Zoho);\u0437\u043d\u0430\u043d\u0438\u0435 R/Python \u0438 JavaScript \u0438\u043b\u0438 \u0430\u043d\u0430\u043b\u043e\u0433\u043e\u0432;\u0437\u043d\u0430\u043d\u0438\u0435 \u043e\u0441\u043d\u043e\u0432 UX.""], ""As a plus"": [""\u0441\u0435\u0440\u0442\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u044f \u043f\u043e \u043a\u043b\u044e\u0447\u0435\u0432\u044b\u043c \u043d\u0430\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u044f\u043c \u0434\u0435\u044f\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438;\u0432\u044b\u0441\u0448\u0435\u0435 \u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u0435 \u0432 \u043e\u0431\u043b\u0430\u0441\u0442\u0438 \u043a\u043e\u043c\u043f\u044c\u044e\u0442\u0435\u0440\u043d\u044b\u0445 \u043d\u0430\u0443\u043a, \u0443\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u044f \u0431\u0438\u0437\u043d\u0435\u0441\u043e\u043c, \u043c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u043a\u0438 \u0438\u043b\u0438 \u0444\u0438\u043d\u0430\u043d\u0441\u043e\u0432.""], ""We offer"": [""\u043a\u043e\u043c\u0444\u043e\u0440\u0442\u043d\u044b\u0435 \u0443\u0441\u043b\u043e\u0432\u0438\u044f \u0440\u0430\u0431\u043e\u0442\u044b: \u0441\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0439 \u0438 \u0441\u0442\u0438\u043b\u044c\u043d\u044b\u0439 \u043e\u0444\u0438\u0441, \u043c\u0435\u0441\u0442\u0430 \u0434\u043b\u044f \u043e\u0442\u0434\u044b\u0445\u0430 (\u043c\u0438\u043d\u0438 \u0441\u043f\u043e\u0440\u0442\u0437\u0430\u043b, \u043d\u0430\u0441\u0442\u043e\u043b\u044c\u043d\u044b\u0439 \u0442\u0435\u043d\u043d\u0438\u0441);\u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e\u0441\u0442\u0438 \u0434\u043b\u044f \u043f\u0440\u043e\u0444\u0435\u0441\u0441\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e\u0433\u043e \u0440\u0430\u0437\u0432\u0438\u0442\u0438\u044f \u0438 \u043b\u0438\u0447\u043d\u043e\u0441\u0442\u043d\u043e\u0433\u043e \u0440\u043e\u0441\u0442\u0430;\u0433\u0438\u0431\u043a\u0438\u0439 \u0433\u0440\u0430\u0444\u0438\u043a \u0440\u0430\u0431\u043e\u0442\u044b;\u043e\u043f\u043b\u0430\u0447\u0438\u0432\u0430\u0435\u043c\u044b\u0439 \u043e\u0442\u043f\u0443\u0441\u043a \u0438 \u0431\u043e\u043b\u044c\u043d\u0438\u0447\u043d\u044b\u0435;\u043a\u0443\u0440\u0441\u044b \u0430\u043d\u0433\u043b\u0438\u0439\u0441\u043a\u043e\u0433\u043e \u044f\u0437\u044b\u043a\u0430 \u0432 \u043e\u0444\u0438\u0441\u0435;\u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u0435 \u0430\u043a\u0442\u0438\u0432\u043d\u043e\u0441\u0442\u0438: \u043a\u043e\u0440\u043f\u043e\u0440\u0430\u0442\u0438\u0432\u044b, \u0442\u0438\u043c\u0431\u0438\u043b\u0434\u0438\u043d\u0433\u0438 (\u044f\u0445\u0442\u0438\u043d\u0433, \u0431\u0430\u0439\u0434\u0430\u0440\u043a\u0438, \u043a\u0432\u0430\u0434\u0440\u043e\u0446\u0438\u043a\u043b\u044b \u0438 \u0442.\u0434.).""], ""Responsibilities"": [""\u0430\u0443\u0434\u0438\u0442 \u0441\u0443\u0449\u0435\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0445 \u0432 \u043a\u043e\u043c\u043f\u0430\u043d\u0438\u0438 CRM, SaaS \u0438 \u0430\u043d\u0430\u043b\u0438\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u0441\u0438\u0441\u0442\u0435\u043c \u0438 \u0441\u0432\u044f\u0437\u0430\u043d\u043d\u044b\u0445 \u0441 \u043d\u0438\u043c\u0438 \u0431\u0438\u0437\u043d\u0435\u0441-\u043f\u0440\u043e\u0446\u0435\u0441\u0441\u043e\u0432;\u043f\u043e\u0438\u0441\u043a, \u0438\u0441\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u043d\u0438\u0435, \u0443\u0442\u0432\u0435\u0440\u0436\u0434\u0435\u043d\u0438\u0435 \u0438 \u0432\u043d\u0435\u0434\u0440\u0435\u043d\u0438\u0435 \u0435\u0434\u0438\u043d\u043e\u0433\u043e \u0438\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442\u0430 \u0432\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 \u0434\u0430\u043d\u043d\u044b\u0445;\u0438\u043d\u0442\u0435\u0433\u0440\u0430\u0446\u0438\u044f \u0441\u0443\u0449\u0435\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0445 \u0438\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442\u043e\u0432 \u0432 \u0441\u0438\u0441\u0442\u0435\u043c\u0443 \u0441\u043a\u0432\u043e\u0437\u043d\u043e\u0439 \u0430\u043d\u0430\u043b\u0438\u0442\u0438\u043a\u0438;\u0432\u044b\u044f\u0432\u043b\u0435\u043d\u0438\u0435 \u0438 \u043f\u0440\u0435\u0434\u043e\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438 \u043e \u043d\u043e\u0432\u044b\u0445 \u0442\u0435\u0445\u043d\u043e\u043b\u043e\u0433\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e\u0441\u0442\u044f\u0445, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043e\u043a\u0430\u0436\u0443\u0442 \u0432\u043b\u0438\u044f\u043d\u0438\u0435 \u043d\u0430 \u0441\u0438\u0441\u0442\u0435\u043c\u044b \u0431\u0438\u0437\u043d\u0435\u0441-\u0430\u043d\u0430\u043b\u0438\u0442\u0438\u043a\u0438 \u043a\u043e\u043c\u043f\u0430\u043d\u0438\u0438;\u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u0435 \u0441\u0438\u0441\u0442\u0435\u043c\u044b \u0430\u043d\u0430\u043b\u0438\u0437\u0430 \u043f\u043e\u0432\u0435\u0434\u0435\u043d\u0438\u044f \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \u0438 \u0443\u0432\u0435\u043b\u0438\u0447\u0435\u043d\u0438\u0435 \u044d\u0444\u0444\u0435\u043a\u0442\u0438\u0432\u043d\u043e\u0441\u0442\u0438 \u0440\u0430\u0431\u043e\u0442\u044b \u0441 \u043d\u0438\u043c\u0438;\u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0435 \u0441 Finance, HR, Marketing, \u043f\u0440\u043e\u0434\u0430\u043a\u0448\u0435\u043d\u043e\u043c \u0434\u043b\u044f \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u044f \u0441\u0438\u0441\u0442\u0435\u043c \u0444\u0438\u043d\u0430\u043d\u0441\u043e\u0432\u043e\u0433\u043e \u0443\u0447\u0451\u0442\u0430 \u0438 \u0440\u0430\u0441\u0447\u0451\u0442\u0430 \u0440\u0435\u043d\u0442\u0430\u0431\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438 \u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u043a\u0438/\u043f\u0440\u043e\u0434\u0432\u0438\u0436\u0435\u043d\u0438\u044f \u043f\u0440\u043e\u0434\u0443\u043a\u0442\u043e\u0432 \u043a\u043e\u043c\u043f\u0430\u043d\u0438\u0438;\u0430\u043d\u0430\u043b\u0438\u0437 \u0438\u0441\u0442\u043e\u0447\u043d\u0438\u043a\u0430 \u043f\u0440\u043e\u0434\u0430\u0436, \u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0438 \u0432\u043d\u0435\u0434\u0440\u0435\u043d\u0438\u0435 \u0440\u0435\u0448\u0435\u043d\u0438\u0439 \u0434\u043b\u044f \u043f\u043e\u0432\u044b\u0448\u0435\u043d\u0438\u044f \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0430 \u043b\u0438\u0434\u043e\u0432 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u0430\u043d\u0430\u043b\u0438\u0437\u0430; \u043f\u043e\u043c\u043e\u0449\u044c \u0432 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438 \u0434\u0430\u043d\u043d\u044b\u0445, \u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u043a\u0435 \u0438 \u043e\u0442\u0441\u043b\u0435\u0436\u0438\u0432\u0430\u043d\u0438\u0438 KPI;\u043f\u0440\u043e\u0432\u0435\u0434\u0435\u043d\u0438\u0435 \u0430\u043d\u0430\u043b\u0438\u0437\u0430 \u0442\u0435\u043a\u0443\u0449\u0438\u0445 \u0441\u0435\u0439\u043b\u0437 \u043f\u0440\u043e\u0446\u0435\u0441\u0441\u043e\u0432 \u0438 \u0432\u044b\u0434\u0432\u0438\u0436\u0435\u043d\u0438\u0435 \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u0439 \u043f\u043e \u0438\u0445 \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u0438;\u0441\u043e\u0433\u043b\u0430\u0441\u043e\u0432\u0430\u043d\u0438\u0435 \u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0439 \u0438 \u0440\u0435\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 \u043f\u0440\u043e\u0435\u043a\u0442\u043e\u0432 \u0441 \u0442\u043e\u043f-\u043c\u0435\u043d\u0435\u0434\u0436\u043c\u0435\u043d\u0442\u043e\u043c \u0438 \u0441\u0442\u0435\u0439\u043a\u0445\u043e\u043b\u0434\u0435\u0440\u0430\u043c\u0438;\u043a\u043e\u043c\u043c\u0443\u043d\u0438\u043a\u0430\u0446\u0438\u043e\u043d\u043d\u043e\u0435 \u0441\u043e\u043f\u0440\u043e\u0432\u043e\u0436\u0434\u0435\u043d\u0438\u0435 \u0432\u043d\u0435\u0434\u0440\u0435\u043d\u0438\u044f \u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u0430\u043d\u043d\u044b\u0445 \u0440\u0435\u0448\u0435\u043d\u0438\u0439 \u0438 \u0438\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442\u043e\u0432;\u0444\u043e\u0440\u043c\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u043f\u0443\u043b\u0430 \u0437\u0430\u0434\u0430\u0447 \u0434\u043b\u044f \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u0445 \u043a\u043e\u043c\u0430\u043d\u0434 \u0432 \u0442\u0430\u0441\u043a-\u0442\u0440\u0435\u043a\u043a\u0435\u0440\u0435 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u0430\u043d\u043d\u044b\u0445 \u043f\u043b\u0430\u043d\u043e\u0432;\u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0438 \u043f\u0440\u0435\u0437\u0435\u043d\u0442\u0430\u0446\u0438\u044f \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438 \u043f\u043e \u0431\u0438\u0437\u043d\u0435\u0441-\u043a\u0435\u0439\u0441\u0430\u043c \u0438 \u0431\u0438\u0437\u043d\u0435\u0441-\u0442\u0440\u0435\u0431\u043e\u0432\u0430\u043d\u0438\u044f\u043c;\u0444\u043e\u0440\u043c\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u0442\u0430\u0431\u043b\u0438\u0446\u044b \u0443\u043b\u0443\u0447\u0448\u0435\u043d\u0438\u0439 \u0432 \u0440\u0430\u043c\u043a\u0430\u0445 OKR;\u0437\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u0438 \u043e\u0431\u043d\u043e\u0432\u043b\u0435\u043d\u0438\u0435 \u043f\u0440\u043e\u0435\u043a\u0442\u043d\u044b\u0445 \u043f\u043b\u0430\u043d\u043e\u0432, \u043a\u0435\u0439\u0441\u043e\u0432, \u043f\u0440\u0435\u0437\u0435\u043d\u0442\u0430\u0446\u0438\u0439 \u0432 Confluence.""], ""Project description"": [""\u041c\u044b \u0438\u0449\u0435\u043c BI/Data Integration Analyst, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0431\u0443\u0434\u0435\u0442 \u043e\u0441\u0443\u0449\u0435\u0441\u0442\u0432\u043b\u044f\u0442\u044c \u0432\u044b\u0431\u043e\u0440, \u0432\u043d\u0435\u0434\u0440\u0435\u043d\u0438\u0435, \u043f\u043e\u0434\u0434\u0435\u0440\u0436\u043a\u0443 \u0438 \u0438\u043d\u0442\u0435\u0433\u0440\u0430\u0446\u0438\u044e \u0441\u0438\u0441\u0442\u0435\u043c \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u0438 \u0445\u0440\u0430\u043d\u0435\u043d\u0438\u044f \u0434\u0430\u043d\u043d\u044b\u0445 (\u0444\u0438\u043d\u0430\u043d\u0441\u043e\u0432\u044b\u0439 \u0443\u0447\u0451\u0442, \u043c\u0430\u0440\u043a\u0435\u0442\u0438\u043d\u0433\u043e\u0432\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435, CRM"", ""\u043a\u043b\u0438\u0435\u043d\u0442\u0441\u043a\u0438\u0435 \u0431\u0430\u0437\u044b \u0434\u0430\u043d\u043d\u044b\u0445, \u0443\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0447\u0435\u0441\u043a\u0438\u0439 \u0443\u0447\u0451\u0442).""]}",,"Required skills 2+ –≥–æ–¥–∞ –æ–ø—ã—Ç–∞ –≤ BI/Data Analytics, Product Management;–∞–Ω–≥–ª–∏–π—Å–∫–∏–π ‚Äî Intermediate –∏ –≤—ã—à–µ;–Ω–∞–≤—ã–∫–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ—Ç–æ–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –º–µ–∂–¥—É —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏;–æ—Ç–ª–∏—á–Ω–æ–µ –≤–ª–∞–¥–µ–Ω–∏–µ SQL;–≤–ª–∞–¥–µ–Ω–∏–µ Excel/Google Sheets –Ω–∞ –≤—ã—Å–æ–∫–æ–º —É—Ä–æ–≤–Ω–µ;–æ—Ç–ª–∏—á–Ω–æ–µ –≤–ª–∞–¥–µ–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏ –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ (Power BI, Google Analytics, Data Studio –∏ –∞–Ω–∞–ª–æ–≥–∏);–æ–ø—ã—Ç —Ä–∞–±–æ—Ç—ã —Å REST API;—Ä–∞–±–æ—Ç—ã SaaS- –∏ CRM-—Å–∏—Å—Ç–µ–º (HubSpot, Zoho);–∑–Ω–∞–Ω–∏–µ R/Python –∏ JavaScript –∏–ª–∏ –∞–Ω–∞–ª–æ–≥–æ–≤;–∑–Ω–∞–Ω–∏–µ –æ—Å–Ω–æ–≤ UX. As a plus —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–æ –∫–ª—é—á–µ–≤—ã–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è–º –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç–∏;–≤—ã—Å—à–µ–µ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ –æ–±–ª–∞—Å—Ç–∏ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã—Ö –Ω–∞—É–∫, —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –±–∏–∑–Ω–µ—Å–æ–º, –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏ –∏–ª–∏ —Ñ–∏–Ω–∞–Ω—Å–æ–≤. We offer –∫–æ–º—Ñ–æ—Ä—Ç–Ω—ã–µ —É—Å–ª–æ–≤–∏—è —Ä–∞–±–æ—Ç—ã: —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π –∏ —Å—Ç–∏–ª—å–Ω—ã–π –æ—Ñ–∏—Å, –º–µ—Å—Ç–∞ –¥–ª—è –æ—Ç–¥—ã—Ö–∞ (–º–∏–Ω–∏ —Å–ø–æ—Ä—Ç–∑–∞–ª, –Ω–∞—Å—Ç–æ–ª—å–Ω—ã–π —Ç–µ–Ω–Ω–∏—Å);–≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–≤–∏—Ç–∏—è –∏ –ª–∏—á–Ω–æ—Å—Ç–Ω–æ–≥–æ —Ä–æ—Å—Ç–∞;–≥–∏–±–∫–∏–π –≥—Ä–∞—Ñ–∏–∫ —Ä–∞–±–æ—Ç—ã;–æ–ø–ª–∞—á–∏–≤–∞–µ–º—ã–π –æ—Ç–ø—É—Å–∫ –∏ –±–æ–ª—å–Ω–∏—á–Ω—ã–µ;–∫—É—Ä—Å—ã –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ –≤ –æ—Ñ–∏—Å–µ;—Ä–∞–∑–ª–∏—á–Ω—ã–µ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏: –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤—ã, —Ç–∏–º–±–∏–ª–¥–∏–Ω–≥–∏ (—è—Ö—Ç–∏–Ω–≥, –±–∞–π–¥–∞—Ä–∫–∏, –∫–≤–∞–¥—Ä–æ—Ü–∏–∫–ª—ã –∏ —Ç.–¥.). Responsibilities –∞—É–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –≤ –∫–æ–º–ø–∞–Ω–∏–∏ CRM, SaaS –∏ –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏—Ö —Å–∏—Å—Ç–µ–º –∏ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –Ω–∏–º–∏ –±–∏–∑–Ω–µ—Å-–ø—Ä–æ—Ü–µ—Å—Å–æ–≤;–ø–æ–∏—Å–∫, –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ, —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –∏ –≤–Ω–µ–¥—Ä–µ–Ω–∏–µ –µ–¥–∏–Ω–æ–≥–æ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö;–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –≤ —Å–∏—Å—Ç–µ–º—É —Å–∫–≤–æ–∑–Ω–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–∏;–≤—ã—è–≤–ª–µ–Ω–∏–µ –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –Ω–æ–≤—ã—Ö —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è—Ö, –∫–æ—Ç–æ—Ä—ã–µ –æ–∫–∞–∂—É—Ç –≤–ª–∏—è–Ω–∏–µ –Ω–∞ —Å–∏—Å—Ç–µ–º—ã –±–∏–∑–Ω–µ—Å-–∞–Ω–∞–ª–∏—Ç–∏–∫–∏ –∫–æ–º–ø–∞–Ω–∏–∏;–ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã –∞–Ω–∞–ª–∏–∑–∞ –ø–æ–≤–µ–¥–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∏ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —Ä–∞–±–æ—Ç—ã —Å –Ω–∏–º–∏;–≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å Finance, HR, Marketing, –ø—Ä–æ–¥–∞–∫—à–µ–Ω–æ–º –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Å–∏—Å—Ç–µ–º —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–≥–æ —É—á—ë—Ç–∞ –∏ —Ä–∞—Å—á—ë—Ç–∞ —Ä–µ–Ω—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏/–ø—Ä–æ–¥–≤–∏–∂–µ–Ω–∏—è –ø—Ä–æ–¥—É–∫—Ç–æ–≤ –∫–æ–º–ø–∞–Ω–∏–∏;–∞–Ω–∞–ª–∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –ø—Ä–æ–¥–∞–∂, —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –∏ –≤–Ω–µ–¥—Ä–µ–Ω–∏–µ —Ä–µ—à–µ–Ω–∏–π –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ª–∏–¥–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞; –ø–æ–º–æ—â—å –≤ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö, —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ –∏ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–∏ KPI;–ø—Ä–æ–≤–µ–¥–µ–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—É—â–∏—Ö —Å–µ–π–ª–∑ –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –∏ –≤—ã–¥–≤–∏–∂–µ–Ω–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –ø–æ –∏—Ö –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏;—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏–π –∏ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–µ–∫—Ç–æ–≤ —Å —Ç–æ–ø-–º–µ–Ω–µ–¥–∂–º–µ–Ω—Ç–æ–º –∏ —Å—Ç–µ–π–∫—Ö–æ–ª–¥–µ—Ä–∞–º–∏;–∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–æ–Ω–Ω–æ–µ —Å–æ–ø—Ä–æ–≤–æ–∂–¥–µ–Ω–∏–µ –≤–Ω–µ–¥—Ä–µ–Ω–∏—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π –∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤;—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—É–ª–∞ –∑–∞–¥–∞—á –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∫–æ–º–∞–Ω–¥ –≤ —Ç–∞—Å–∫-—Ç—Ä–µ–∫–∫–µ—Ä–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –ø–ª–∞–Ω–æ–≤;—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –∏ –ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –ø–æ –±–∏–∑–Ω–µ—Å-–∫–µ–π—Å–∞–º –∏ –±–∏–∑–Ω–µ—Å-—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º;—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã —É–ª—É—á—à–µ–Ω–∏–π –≤ —Ä–∞–º–∫–∞—Ö OKR;–∑–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–Ω—ã—Ö –ø–ª–∞–Ω–æ–≤, –∫–µ–π—Å–æ–≤, –ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏–π –≤ Confluence. Project description –ú—ã –∏—â–µ–º BI/Data Integration Analyst, –∫–æ—Ç–æ—Ä—ã–π –±—É–¥–µ—Ç –æ—Å—É—â–µ—Å—Ç–≤–ª—è—Ç—å –≤—ã–±–æ—Ä, –≤–Ω–µ–¥—Ä–µ–Ω–∏–µ, –ø–æ–¥–¥–µ—Ä–∂–∫—É –∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é —Å–∏—Å—Ç–µ–º –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏ —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö (—Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π —É—á—ë—Ç, –º–∞—Ä–∫–µ—Ç–∏–Ω–≥–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ, CRM ‚Äî –∫–ª–∏–µ–Ω—Ç—Å–∫–∏–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö, —É–ø—Ä–∞–≤–ª–µ–Ω—á–µ—Å–∫–∏–π —É—á—ë—Ç).",BI/Data Analyst@Devart,https://jobs.dou.ua/companies/devart/vacancies/134691/, Kharkiv,BI/Data Analyst,09 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/ciklum/,Ciklum,{},,"On behalf of Seeking Alpha, Ciklum is looking for Senior Data Analyst to join Kyiv team on full-time basis.",Senior Data Analyst for Seeking Alpha (200004CM)@Ciklum,https://jobs.dou.ua/companies/ciklum/vacancies/134664/, Kyiv,Senior Data Analyst for Seeking Alpha (200004CM),09 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/genesis-technology-partners/,Genesis,{},,"Genesis is one of the largest IT companies in Ukraine with more than 1000 people in 9 countries, who create products for 200 million users monthly. We are the most high-loaded company in the country and one of the largest partners of Facebook, Google, Snapchat and Apple in the CEE region. Our team is one of the best high-tech teams in Eastern Europe. Genesis was recognized by DOU.UA as the Best IT Employer in Ukraine in the category 800 ‚Äî 1500 employees in 2018. We received very high ratings across all major evaluation categories such as professional growth, compensation, working conditions, communication with management and colleagues, etc. Now we are looking for a Junior Product/Data Analyst. Ambitious and hardworking professional to join our team. What you‚Äôll be doing:Forming different hypotheses in the area of product development (with the aim of increasing the metrics of involvement and monetization);Primarily work with Marketing & Product team and help to make correct data-informed decisions;Running split tests to verify different hypothesis;Operational monitoring of key metrics and determining the causes of deviations from the expected values. What we expect from you:Experience in analytics, SQL;High mathematical aptitude;Understanding Business Intelligence and Unit Economics;Experience with Excel, BI and data visualization tools;Strong attention to detail as well as good problem solving, time management, and logic skillsAbility to understand the logic of complicated applications. Genesis is a unique place for the development and growth with:Expertise in the development of high-loaded products in international markets;Unique opportunities for learning including various training and seminars within the company, access to a valuable and extensive book library, English courses and participation in key IT industry events worldwide;Perfect working conditions: an excellent office in a 5 minutes‚Äô walk from Taras Shevchenko metro station, free food and drinks in the office, medical insurance. The company has professional coaches in football, basketball and running and supports various sports initiatives (swim races, marathons, etc).",Junior Product/Data Analyst (Mobile Apps)@Genesis,https://jobs.dou.ua/companies/genesis-technology-partners/vacancies/134646/, Kyiv,Junior Product/Data Analyst (Mobile Apps),09 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/8allocate/,8allocate,"{""Required skills"": [""You will fit if you:"", ""have a Product Management working experience in Agile or Kanban methodologies."", ""come from a technical background"", ""you were an engineer or have held a technical role in the past."", ""are accustomed to working with data scientists, data engineers, or AI product developers on complex data architectures and are comfortable diving into the details of how everything works."", ""are familiar with typical cloud-hosted web app architectures and with the services offered by the major public cloud vendors (AWS, Azure, Google Cloud Platform)."", ""always champion the customer, market, and product vision to everyone in the building and constantly gather feedback from everyone."", ""are the type of person who works best when given ownership and responsibility for outcomes (self-motivator). Experience working with senior management stakeholders to get things done.""], ""As a plus"": [""As well as:"", ""Commercial Product collaboration experience with sales, marketing, customer teams.B2B SaaS Product Management experience."", ""Strong knowledge of machine learning and/or predictive modeling."", ""Experience running their own best practice data analysis, such as working experience in quantitative disciplines including data science, data analytics, economics, etc."", ""Working knowledge of SQL and common data analytics solutions such as Tableau or Google Data Studio to run their own data analysis.""], ""We offer"": [""Why choose us?"", ""\u201cFamily and Friends\u201d. We are no longer a start-up, but still, have a family atmosphere in our supportive and spirited team, who are all working together on the same goal."", ""\u201cJust break down all barriers and find a better way\u201d. Every day you\u2019ll meet with interesting and challenging (international) projects that are covering industries from commercial aviation to fintech (different technologies, different products)."", ""\u201cHungry for learning\u201d. You will get a lot of chances for career advancement and the development of new skills, opportunities for mentorship, or learning from more experienced colleagues.""], ""Responsibilities"": [""Benefits from 8allocate:"", ""Corporate events, holidays and team buildings for your joy."", ""Training and development: we have a huge library (about 500 books!) and a budget for your professional development."", ""People-oriented management without bureaucracy."", ""Paid vacation and sick leaves."", ""Relocation program: if you are from another city and want to move to Kyiv, we will be happy to help you.""], ""Project description"": [""Main responsibilities and activities:"", ""Responsible for the collection, quality and storage of our data including managing platform vendor/partner relationships to ensure that our platform meets both client and business expectations."", ""Responsible for the quality and scalability of the data science models that underpin our platform."", ""Define and maintain a vision and strategy for the future evolution of the project\u2019s data platform in collaboration with the CTO and Technical Architects."", ""Define, maintain and deliver a data platform roadmap based on initiatives, OKRs, and the platform strategy by prioritizing the main areas of focus for the data and data science teams based on user and buyer value, business viability, and time to impact."", ""Own and maintain platform KPIs in collaboration with the Data, Data Science, and Ops teams. You\u2019ll be expected to track and report back on the achieved business outcomes and realized customer value to the wider business."", ""Engage in user research pre and post-release when needed to validate the customer impact of data changes. Always represent the voice of the customer."", ""Ideate and collaborate closely with the data and data science teams in agile sprints to come up with and test innovative solutions to problems that work for our customers and our business.""]}",,"Required skills You will fit if you:‚Äî have a Product Management working experience in Agile or Kanban methodologies.‚Äî come from a technical background ‚Äî you were an engineer or have held a technical role in the past. ‚Äî are accustomed to working with data scientists, data engineers, or AI product developers on complex data architectures and are comfortable diving into the details of how everything works.‚Äî are familiar with typical cloud-hosted web app architectures and with the services offered by the major public cloud vendors (AWS, Azure, Google Cloud Platform).‚Äî always champion the customer, market, and product vision to everyone in the building and constantly gather feedback from everyone.‚Äî are the type of person who works best when given ownership and responsibility for outcomes (self-motivator). Experience working with senior management stakeholders to get things done. As a plus As well as:‚Äî Commercial Product collaboration experience with sales, marketing, customer teams.B2B SaaS Product Management experience.‚Äî Strong knowledge of machine learning and/or predictive modeling.‚Äî Experience running their own best practice data analysis, such as working experience in quantitative disciplines including data science, data analytics, economics, etc.‚Äî Working knowledge of SQL and common data analytics solutions such as Tableau or Google Data Studio to run their own data analysis. We offer Why choose us?‚Äî ‚ÄúFamily and Friends‚Äù. We are no longer a start-up, but still, have a family atmosphere in our supportive and spirited team, who are all working together on the same goal.‚Äî ‚ÄúJust break down all barriers and find a better way‚Äù. Every day you‚Äôll meet with interesting and challenging (international) projects that are covering industries from commercial aviation to fintech (different technologies, different products).‚Äî ‚ÄúHungry for learning‚Äù. You will get a lot of chances for career advancement and the development of new skills, opportunities for mentorship, or learning from more experienced colleagues. Benefits from 8allocate:‚Äî Corporate events, holidays and team buildings for your joy.‚Äî Training and development: we have a huge library (about 500 books!) and a budget for your professional development.‚Äî People-oriented management without bureaucracy.‚Äî Paid vacation and sick leaves.‚Äî Relocation program: if you are from another city and want to move to Kyiv, we will be happy to help you. Responsibilities Main responsibilities and activities:‚Äî Responsible for the collection, quality and storage of our data including managing platform vendor/partner relationships to ensure that our platform meets both client and business expectations.‚Äî Responsible for the quality and scalability of the data science models that underpin our platform.‚Äî Define and maintain a vision and strategy for the future evolution of the project‚Äôs data platform in collaboration with the CTO and Technical Architects.‚Äî Define, maintain and deliver a data platform roadmap based on initiatives, OKRs, and the platform strategy by prioritizing the main areas of focus for the data and data science teams based on user and buyer value, business viability, and time to impact.‚Äî Own and maintain platform KPIs in collaboration with the Data, Data Science, and Ops teams. You‚Äôll be expected to track and report back on the achieved business outcomes and realized customer value to the wider business.‚Äî Engage in user research pre and post-release when needed to validate the customer impact of data changes. Always represent the voice of the customer.‚Äî Ideate and collaborate closely with the data and data science teams in agile sprints to come up with and test innovative solutions to problems that work for our customers and our business. Project description 8allocate is a global provider of end-to-end custom software development solutions to companies all over the globe, from North America to the EU to Israel to Australia. Headquartered in Estonia, we run offshore R&D centers in Kyiv and Lviv. Our team is 50% remote and distributed. We specialize in flexible interaction exclusively with international clients (we cover industries from commercial aviation to fintech) thanks to a multinational support group of experts and management. Currently, we are looking for a Senior Data Product Manager on the project, which provides consulting services in Business Intelligence. About the project: a market leader in developing Competitive Intelligence for AdTech Search.Application teams develop unparalleled technologies that help our clients understand their paidand organic search landscape and improve campaign performance. You‚Äôll take responsibility for positive business outcomes, working closely with every discipline to drive the success of their OKRs and product KPIs; collaboration will most directly be with our Data, Data Science, and Web Operations teams.",Senior Data Product Manager@8allocate,https://jobs.dou.ua/companies/8allocate/vacancies/134644/, Kyiv,Senior Data Product Manager,09 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/genesis-technology-partners/,Genesis,{},,"Genesis is one of the largest IT companies in Ukraine with more than 1500 people in 9 countries, who create products for 200 million users monthly. We are the most high-loaded company in the country and one of the largest partner of Facebook, Google, Snapchat and Apple in the CEE region. Our team is one of the best high-tech teams in Eastern Europe. Genesis was recognised by DOU.UA as the Best IT Employer in Ukraine in the category 800 ‚Äî 1500 employees over the last years. We received very high ratings across all major evaluation categories such as professional growth, compensation, working conditions, communication with management and colleagues, etc. We are looking for a structured and experienced Data Security Officer to improve and develop our security systems. You will be responsible for IT security framework creation and implementation, reporting to COO of Genesis and working closely with CTO on projects. Through the framework you will oversee security over 10+ projects inside Genesis group. You will also be working tightly with Head of Legal to ensure that users/employees data is protected. Your primary goal will be to be proactively protect business from external threats, being an owner of security frameworks and principles. Responsibilities:‚Ä¢ Oversee security level of projects inside group, perform security audits of projects;‚Ä¢ Develop and enhance an information security management framework to ensure business sustainability;‚Ä¢ Execute high-level security management framework;‚Ä¢ Understand and interact with related disciplines to ensure the consistent application of policies and standards across projects and services;‚Ä¢ Assess risks and communicate with business stakeholders across the company to raise awareness of risk management concerns;‚Ä¢ Work directly with the business units to facilitate risk assessment and risk management processes. Requirements:‚Ä¢ 7+ years in IT security/DevOps/Compliance/IT management;‚Ä¢ Extensive, practice-based knowledge of security management frameworks, such as ISO/IEC 27001;‚Ä¢ Proven track record of IT security audits/projects implementation;‚Ä¢ Excellent written and verbal communication skills and high level of personal integrity;‚Ä¢ Innovative thinking and leadership with an ability to get right things done;‚Ä¢ Experience with contract and vendor negotiations and management will be an advantage. Genesis is a unique place for the development and growth with:‚Ä¢ Expertise in the development of high-loaded products in international markets;‚Ä¢ Unique opportunities for learning including various training and seminars within the company, access to a valuable and extensive book library, English courses and participation in key IT industry events worldwide;‚Ä¢ Perfect working conditions: an excellent office in a 5 minutes‚Äô walk from Taras Shevchenko metro station, free food and drinks in the office, medical insurance. The company has professional coaches in football, basketball and running and supports various sports initiatives (swim races, marathons, etc).",Data Security Officer@Genesis,https://jobs.dou.ua/companies/genesis-technology-partners/vacancies/134150/, Kyiv,Data Security Officer,09 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/sprout-therapy/,Sprout Therapy,"{""Required skills"": [""We are looking for a Data Engineer to join our Data Analytics team to build efficient data pipelines and maintain and improve our data infrastructure.""], ""We offer"": [""Your background:\u25cf Advanced working SQL knowledge and experience working with relational databases\u25cf Experience with data warehouses systems (e.g. Snowflake or Redshift)\u25cf Experience with workflow systems (e.g. Airflow or Luigi) \u25cf Experience building and maintaining critical reliable ETL pipelines\u25cf Hands-on experience with Python or some other object-oriented programming language\u25cf Experience with Docker\u25cf Hands-on experience with Cloud Solutions (e.g. AWS)\u25cf Experience with version control practices (e.g. Git) and Linux environment \u25cf Familiarity with common API\u2019s: REST, SOAP, etc\u25cf Good understanding of database architecture and best practices\u25cf Good understanding of Back-End development and highly-available infrastructure principles""], ""Responsibilities"": [""What we offer you: \u25cf A competitive salary and flexible hours\u25cf The tools and equipment you\u2019ll need to be successful in your role\u25cf Being part of a team of hard-working, ambitious, and caring individuals who will help you learn and grow professionally""], ""Project description"": [""Responsibilities:\u25cf Define, improve and maintain our data infrastructure\u25cf Develop and maintain robust data pipelines from a wide variety of data sources\u25cf Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.\u25cf Partner with Data Analysts, domain experts, and engineering teams to gather and understand internal data requirements and build reliable data sets aligned with business goals\u25cf Be a champion of the overall strategy for data governance, data security, and data quality\u25cf Keep our data separated and secure across national boundaries through multiple data centers\u25cf Work with data and analytics experts to optimize the performance of business-critical queries and improving functionality in our data systems\u25cf Promote best practices and create and maintain technical documentation""]}",,"Required skills We are looking for a Data Engineer to join our Data Analytics team to build efficient data pipelines and maintain and improve our data infrastructure. Your background:‚óè Advanced working SQL knowledge and experience working with relational databases‚óè Experience with data warehouses systems (e.g. Snowflake or Redshift)‚óè Experience with workflow systems (e.g. Airflow or Luigi) ‚óè Experience building and maintaining critical reliable ETL pipelines‚óè Hands-on experience with Python or some other object-oriented programming language‚óè Experience with Docker‚óè Hands-on experience with Cloud Solutions (e.g. AWS)‚óè Experience with version control practices (e.g. Git) and Linux environment ‚óè Familiarity with common API‚Äôs: REST, SOAP, etc‚óè Good understanding of database architecture and best practices‚óè Good understanding of Back-End development and highly-available infrastructure principles We offer What we offer you: ‚óè A competitive salary and flexible hours‚óè The tools and equipment you‚Äôll need to be successful in your role‚óè Being part of a team of hard-working, ambitious, and caring individuals who will help you learn and grow professionally Responsibilities Responsibilities:‚óè Define, improve and maintain our data infrastructure‚óè Develop and maintain robust data pipelines from a wide variety of data sources‚óè Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.‚óè Partner with Data Analysts, domain experts, and engineering teams to gather and understand internal data requirements and build reliable data sets aligned with business goals‚óè Be a champion of the overall strategy for data governance, data security, and data quality‚óè Keep our data separated and secure across national boundaries through multiple data centers‚óè Work with data and analytics experts to optimize the performance of business-critical queries and improving functionality in our data systems‚óè Promote best practices and create and maintain technical documentation Project description Sprout Therapy is a tech-forward provider of in-home and online Applied Behavior Analysis (ABA) Therapy. Our mission is to provide personalized, technology-empowered care for the needs of children with autism in the environment best suited to help them grow and thrive ‚Äî the home. We‚Äôre dedicated to providing easily accessible, high-quality care to the children and families we serve. We‚Äôre rapidly growing our team ‚Äî and looking for values-driven, diverse, caring professionals to help us improve the availability of autism care.",Data Engineer@Sprout Therapy,https://jobs.dou.ua/companies/sprout-therapy/vacancies/134602/," Kyiv, remote",Data Engineer,09 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/data-virtuality-gmbh/,Data Virtuality GmbH,"{""Required skills"": [""SMART. SQL-NATIVE. MULTI-TALENTED...what you should bring from your professional side""], ""As a plus"": [""Understanding of database technology"", ""Experience with Cloud Platforms (AWS, Azure)"", ""Experience with container platforms"", ""Working experience with Linux"", ""Good SQL skills"", ""Ability to debug the source code and to reproduce customer issues on a local environment"", ""Basic understanding of Business intelligence use cases"", ""Minimum 2-3 years experience in software support/consulting or comparable function""], ""We offer"": [""INNOVATIVE. AMBITIOUS. INTERNATIONAL...what you should bring from your personal side.""], ""Responsibilities"": [""Strong communication skills paired with empathy"", ""Fluent in spoken and written English"", ""Self-organized working method"", ""The highest levels of motivation, responsibility, and ambition to proactively support the growth of our company"", ""Analytical and number-based approach"", ""Problem-solving skills"", ""Inquiring mindset""]}",,"Required skills SMART. SQL-NATIVE. MULTI-TALENTED...what you should bring from your professional side ‚Äî Understanding of database technology‚Äî Experience with Cloud Platforms (AWS, Azure)‚Äî Experience with container platforms‚Äî Working experience with Linux‚Äî Good SQL skills‚Äî Ability to debug the source code and to reproduce customer issues on a local environment‚Äî Basic understanding of Business intelligence use cases‚Äî Minimum 2-3 years experience in software support/consulting or comparable function INNOVATIVE. AMBITIOUS. INTERNATIONAL...what you should bring from your personal side. ‚Äî Strong communication skills paired with empathy‚Äî Fluent in spoken and written English‚Äî Self-organized working method‚Äî The highest levels of motivation, responsibility, and ambition to proactively support the growth of our company‚Äî Analytical and number-based approach‚Äî Problem-solving skills‚Äî Inquiring mindset As a plus NICE-TO-HAVE‚Äî Sales Experience‚Äî University Degree We offer BENEFITS:‚Äî Payments in EURO‚Äî Work from everywhere‚Äî Potential relocation to Germany Responsibilities DIVERSIFIED, CHALLENGING, NEVER BORING! TROUBLESHOOTING ‚Äî You are the primary and secondary technical contact for our customers. You are finding the best solutions for upcoming issues either by only answering small questions or by supporting the operation of our software solutions and server. And you are going one step further: by sharing product feedback you will contribute to our product development andmake it better every day. SAAS MONITORING ‚Äî You use the monitoring systems and tools to proactively monitor, identify, and process any incidents by communicating to the customer or fixing the problem. SETUP AND OPERATIONS ‚Äî You have the responsibility to set up, configure, and use cloud management and monitoring tools, as well as managing the cloud environments (backup, etc.). Additionally, you will support the operation of our software and servers and performing software updates and upgrades on Windows and Linux customer machines remote via RDP and SSH. SQL-NATIVE ‚Äî You have a good understanding of SQL engine operation and are able to write, understand and debug the SQL code of our API connectors, modify code of the connectors on the fly and install updates to the customers. In cases, when heavy SQL reworking is required, you reach out to our SQL department with these tasks.",SQL Support Engineer¬†‚Äî remote@Data Virtuality GmbH,https://jobs.dou.ua/companies/data-virtuality-gmbh/vacancies/126728/, remote,SQL Support Engineer¬†‚Äî remote,09 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/grid-dynamics/,Grid Dynamics,{},,"We develop a proactive approach, based on statistical analyses and predictive modeling, to forecast risks and provide recommendations to mitigate these uncertainties for the Fuel Supply chain. Responsibilities:Pro-active, self-managed, able to work with no clear requirementsDrive the collection, cleaning, processing and analysis of new and existing data sources.Communicate with business stakeholders to clarify their requirements and present the teamwork results Requirements:Data scientist with 7+ years of experienceGood knowledge of the probability theory and statisticsUnderstanding of Bayesian methods, time series analysis, sensor signal processingHands-on experience with probabilistic programming (Pyro, PyMC3, JAGS, or similar)Experience with cloud (Amazon SageMaker) is required Experience with PySpark is requiredHands-on experience with data preparation, cleansing, feature engineering, and visualization Strong working knowledge of SQL, Teradata preferredGood communication skills and interpersonal skills.Experience in working across different global cultures a plus.Ph.D. in Mathematics will be a plus. We offer: Opportunity to work on bleeding-edge projectsWork with a highly motivated and dedicated teamFlexible scheduleMedical insuranceBenefits programCorporate social eventsProfessional development opportunities About us: Grid Dynamics is the engineering services company known for transformative, mission-critical cloud solutions for retail, finance and technology sectors. We architected some of the busiest e-commerce services on the Internet and have never had an outage during the peak season. Founded in 2006 and headquartered in San Ramon, California with offices throughout the US and Eastern Europe, we focus on big data analytics, scalable omnichannel services, DevOps, and cloud enablement.",Senior Data Scientist@Grid Dynamics,https://jobs.dou.ua/companies/grid-dynamics/vacancies/134595/," Kyiv, Kharkiv, Lviv",Senior Data Scientist,08 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/data-science-ua/,Data Science UA,"{""Required skills"": [""2+ years of experience researching and developing Machine Learning algorithms;"", ""Solid mathematical background. Work in the field of topology will be a strong plus;"", ""Practical experience in developing Computer Vision solutions;"", ""Ability to quickly prototype and implement new ideas, finding simple and accurate problem solutions;"", ""Programming skills in Python (numpy, OpenCV);"", ""Solid fundamentals in classical Machine Learning / Deep Learning;"", ""Understanding of main principles of generative models;"", ""Working knowledge of at least one Deep Learning framework (PyTorch preferred).""], ""As a plus"": [""You have experience with GAN"", ""Your own projects on GitHub in your favorite field of machine learning that you can show and tell about.""], ""Responsibilities"": [""Being part of the team, on a typical workday you are going to do either of the following:1. Extract state-of-the-art papers\u2019 core approaches;2. Dissect these cores into pros and cons;3. Reforge approaches to counter weaknesses;4. Implement swiftly demo quality prototypes;5. Convert proven useful prototypes into well-documented production-ready code.""], ""Project description"": [""Our friends is a technology company working with machine learning since 2011. We have been developing B2B and B2C products based on computer vision and natural language processing and have already created several state of the art machine learning systems.In this position, you will work in a team of engineers, researchers and developers focused on developing cutting edge machine learning technologies related to Computer Vision and Generative Neural Networks. The position requires great problem solving and analytical thinking, a deep understanding main machine learning algorithms aimed to solve of computer vision tasks.""]}",,"Required skills ‚Äî 2+ years of experience researching and developing Machine Learning algorithms;‚Äî Solid mathematical background. Work in the field of topology will be a strong plus;‚Äî Practical experience in developing Computer Vision solutions;‚Äî Ability to quickly prototype and implement new ideas, finding simple and accurate problem solutions;‚Äî Programming skills in Python (numpy, OpenCV);‚Äî Solid fundamentals in classical Machine Learning / Deep Learning;‚Äî Understanding of main principles of generative models;‚Äî Working knowledge of at least one Deep Learning framework (PyTorch preferred). As a plus ‚Äî You have experience with GAN‚Äî Your own projects on GitHub in your favorite field of machine learning that you can show and tell about. Responsibilities Being part of the team, on a typical workday you are going to do either of the following:1. Extract state-of-the-art papers‚Äô core approaches;2. Dissect these cores into pros and cons;3. Reforge approaches to counter weaknesses;4. Implement swiftly demo quality prototypes;5. Convert proven useful prototypes into well-documented production-ready code. Project description Our friends is a technology company working with machine learning since 2011. We have been developing B2B and B2C products based on computer vision and natural language processing and have already created several state of the art machine learning systems.In this position, you will work in a team of engineers, researchers and developers focused on developing cutting edge machine learning technologies related to Computer Vision and Generative Neural Networks. The position requires great problem solving and analytical thinking, a deep understanding main machine learning algorithms aimed to solve of computer vision tasks.",Machine Learning Research Engineer@Data Science UA,https://jobs.dou.ua/companies/data-science-ua/vacancies/108681/, Kyiv,Machine Learning Research Engineer,08 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/altexsoft/,AltexSoft,"{""Required skills"": [""Good knowledge of Machine Learning algorithms and Data Analysis approaches"", ""Good knowledge of Python language, pandas, scikit-learn, plotly/seaborn, etc."", ""Understanding and ability to work according to CRISP-DM methodology"", ""Ability to solve complex algorithmic problems"", ""Good knowledge of databases and SQL"", ""Willingness to learn new things and grow professionally"", ""At least 2 years of experience in Data Science area""], ""As a plus"": [""Experience with Flask, Docker"", ""Knowledge of Deep Learning, experience with Keras and TensorFlow"", ""Experience in NLP or CV area"", ""Experience in NoSQL databases (MongoDB, Redis, etc.)"", ""Good knowledge of at least one general-purpose programming language (C++, Java, C#, something else)"", ""Experience in Presales process and project estimation""], ""Project description"": [""As a Data Scientist, you will work in a great team of Data Scientists on different ML problems. The main tasks are Data Analysis, Machine Learning modeling and validation, ML-based algorithms implementation for target platforms and languages.""]}",,"Required skills ‚Äî Good knowledge of Machine Learning algorithms and Data Analysis approaches‚Äî Good knowledge of Python language, pandas, scikit-learn, plotly/seaborn, etc.‚Äî Understanding and ability to work according to CRISP-DM methodology‚Äî Ability to solve complex algorithmic problems‚Äî Good knowledge of databases and SQL‚Äî Willingness to learn new things and grow professionally‚Äî At least 2 years of experience in Data Science area As a plus ‚Äî Experience with Flask, Docker‚Äî Knowledge of Deep Learning, experience with Keras and TensorFlow‚Äî Experience in NLP or CV area‚Äî Experience in NoSQL databases (MongoDB, Redis, etc.)‚Äî Good knowledge of at least one general-purpose programming language (C++, Java, C#, something else)‚Äî Experience in Presales process and project estimation Project description As a Data Scientist, you will work in a great team of Data Scientists on different ML problems. The main tasks are Data Analysis, Machine Learning modeling and validation, ML-based algorithms implementation for target platforms and languages.","Data Scientist, Machine Learning engineer@AltexSoft",https://jobs.dou.ua/companies/altexsoft/vacancies/134548/," Kharkiv, Lviv","Data Scientist, Machine Learning engineer",08 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/3dlook/,3DLOOK,"{""Required skills"": [""Solid mathematical background;10+ years of software engineering experience;Deep tech expertise in CV / ML / 3D would be a big plus;Experience in data science would be a big plus;5+ years of technical team management experience;Experience in hiring, leading, and coaching a multi-functional engineering team to support them to be successful;An engaging leader who has worked with internal and external stakeholders to identify and justify solutions. Demonstrated ability to understand/learn our markets, our clients, and our value chain;Passionate about growth and continuous improvement;Incredible work ethic, drive, and passion;Fluency in English and strong communication skills, both verbal and written.""], ""We offer"": [""Official Stock options;Investment in your growth and self-development;Competitive compensation;20 working days of paid vacations and paid sick leaves;10 remote days each month and one remote month from another country per year;Foreign language classes inhouse and communication with native speakers;Online fitness with a corporate trainer;Modern and conveniently located offices with good working conditions;Corporate, social and cultural events""], ""Responsibilities"": [""Create long-term core technology roadmaps and define operational plans to execute according to them;Understand business requirements and goals to create and adjust roadmaps accordingly;Help R&D team members to solve complex R&D tasks and run different experiments;Develop technical aspects of the company\u2019s strategy to ensure alignment with its business goalsDiscover and implement new technologies that yield competitive advantageEstablish a reporting system about main core tech updates and metrics for the executive team and present it on weekly, monthly and quarterly meetings;Inform executive team members on a regular basis about any external technological trends and ideas, that we need to be aware of;Actively participate in hiring core tech team members together with the HR team;Run 1-1 with the core tech team members on a weekly basis;Actively participate in the performance and salary evaluation of the core tech team members;Monitor key tech trends in our and adjacent segments in order to react fast to the global changes, that might have an impact on what we do;Coach team members and help them with their career development and growth;Ensure that the team unleashes its whole potential of innovation and creativity.""], ""Project description"": [""3DLook is on a mission to bring deeper levels of personalization to the consumer shopping experience by turning body data into business intelligence and remove the challenge of fit communication between consumers and the places they love to shop.We are result-oriented enthusiasts who work tirelessly to build technology that has the potential to transform the fashion industry and how we shop online. We combine expertise and experience and work together to deliver outstanding results for our clients. This is a great opportunity for you to shape the growth, development and culture of an exciting and very fast-growing company in the retail market.""]}",,"Required skills Solid mathematical background;10+ years of software engineering experience;Deep tech expertise in CV / ML / 3D would be a big plus;Experience in data science would be a big plus;5+ years of technical team management experience;Experience in hiring, leading, and coaching a multi-functional engineering team to support them to be successful;An engaging leader who has worked with internal and external stakeholders to identify and justify solutions. Demonstrated ability to understand/learn our markets, our clients, and our value chain;Passionate about growth and continuous improvement;Incredible work ethic, drive, and passion;Fluency in English and strong communication skills, both verbal and written. We offer Official Stock options;Investment in your growth and self-development;Competitive compensation;20 working days of paid vacations and paid sick leaves;10 remote days each month and one remote month from another country per year;Foreign language classes inhouse and communication with native speakers;Online fitness with a corporate trainer;Modern and conveniently located offices with good working conditions;Corporate, social and cultural events Responsibilities Create long-term core technology roadmaps and define operational plans to execute according to them;Understand business requirements and goals to create and adjust roadmaps accordingly;Help R&D team members to solve complex R&D tasks and run different experiments;Develop technical aspects of the company‚Äôs strategy to ensure alignment with its business goalsDiscover and implement new technologies that yield competitive advantageEstablish a reporting system about main core tech updates and metrics for the executive team and present it on weekly, monthly and quarterly meetings;Inform executive team members on a regular basis about any external technological trends and ideas, that we need to be aware of;Actively participate in hiring core tech team members together with the HR team;Run 1-1 with the core tech team members on a weekly basis;Actively participate in the performance and salary evaluation of the core tech team members;Monitor key tech trends in our and adjacent segments in order to react fast to the global changes, that might have an impact on what we do;Coach team members and help them with their career development and growth;Ensure that the team unleashes its whole potential of innovation and creativity. Project description 3DLook is on a mission to bring deeper levels of personalization to the consumer shopping experience by turning body data into business intelligence and remove the challenge of fit communication between consumers and the places they love to shop.We are result-oriented enthusiasts who work tirelessly to build technology that has the potential to transform the fashion industry and how we shop online. We combine expertise and experience and work together to deliver outstanding results for our clients. This is a great opportunity for you to shape the growth, development and culture of an exciting and very fast-growing company in the retail market. As a Chief Technology Officer, you will be responsible for outlining the company‚Äôs technological vision, implementing technology strategies, and ensuring that the technological resources are aligned with the company‚Äôs business needs. Also, you will need to be an excellent leader for our growing R&D team and make sure that every team member is growing professionally, and showing his best level of performance.If you want to be a part of the company that will change the online shopping market, we‚Äôd love to meet you!",Chief Technology Officer@3DLOOK,https://jobs.dou.ua/companies/3dlook/vacancies/134529/," Kyiv, Odesa, relocation, remote",Chief Technology Officer,08 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/parimatch-tech/,Parimatch Tech,{},,"‚Äî Interpret the received data;‚Äî Analyze results using statistical techniques and provide ongoing reports;‚Äî Develop and implement databases, data collection systems, data analytics and other strategies that optimize statistical efficiency and quality;‚Äî Acquire data from primary or secondary data sources and maintain databases/data systems Identify, analyze, and interpret trends or patterns in complex data sets;‚Äî Filter and ‚Äûclean‚Äù data by reviewing computer reports, printouts, and performance indicators to locate and correct code problems;‚Äî Work with management to prioritize business and information needs;‚Äî Locate and define new process improvement opportunities. ‚Äî Proven working experience as a Data Analyst or Business Data Analyst;‚Äî Technical expertise regarding data models, database design development, data mining and segmentation techniques;‚Äî Strong knowledge of and experience with reporting packages (Business Objects etc), databases (SQL etc), programming (XML, Javascript, or ETL frameworks);‚Äî Knowledge of statistics and experience using statistical packages for analyzing datasets (Excel, SPSS, SAS etc);‚Äî Strong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy;‚Äî Adept at queries, report writing and presenting findings;‚Äî BS in Mathematics, Economics, Computer Science, Information Management or Statistics.",Data Analyst@Parimatch Tech,https://jobs.dou.ua/companies/parimatch-tech/vacancies/125054/, Kyiv,Data Analyst,08 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/snap/,Snap Inc.,{},,"Snap Inc. ‚Äî –∞–º–µ—Ä–∏–∫–∞–Ω—Å–∫–∞—è –∫–æ–º–ø–∞–Ω–∏—è —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫ –∏ –≤–ª–∞–¥–µ–ª–µ—Ü –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è Snapchat. –ù–∞—Å—á–∏—Ç—ã–≤–∞–µ—Ç –±–æ–ª–µ–µ 180 –º–∏–ª–ª–∏–æ–Ω–æ–≤ –∞–∫—Ç–∏–≤–Ω—ã—Ö –µ–∂–µ–¥–Ω–µ–≤–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–∏ÃÜ –ø–æ –≤—Å–µ–º—É –º–∏—Ä—É, –∞ —Ç–∞–∫–∂–µ –≤—Ö–æ–¥–∏—Ç –≤ –¢–û–ü-10 –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏ÃÜ AppStore –∏ Google Play –≤ –°–®–ê. –ö—Ä—É–ø–Ω–µ–∏ÃÜ—à–∏–µ –∫–∏–Ω–æ-—Å—Ç—É–¥–∏–∏ –∏ –∏–∑–≤–µ—Å—Ç–Ω—ã–µ –±—Ä–µ–Ω–¥—ã, —Ç–∞–∫–∏–µ –∫–∞–∫, Sony Pictures, 20th Century Fox, PepsiCo, Disney, Warner Brothers –∏ –º–Ω–æ–≥–∏–µ –¥—Ä—É–≥–∏–µ —è–≤–ª—è—é—Ç—Å—è —Ä–µ–∫–ª–∞–º–æ–¥–∞—Ç–µ–ª—è–º–∏ Snapchat. –í –£–∫—Ä–∞–∏–Ω–µ –º—ã –∏—â–µ–º —Ñ—Ä–∏–ª–∞–Ω—Å–µ—Ä–∞, –∫–æ—Ç–æ—Ä—ã–π –±—É–¥–µ—Ç –∑–∞–Ω–∏–º–∞—Ç—å—Å—è —Ä–∞–∑–º–µ—Ç–∫–æ–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –ß—Ç–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –¥–µ–ª–∞—Ç—å::‚Äî –†–∞–∑–º–µ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (–∫ –ø—Ä–∏–º–µ—Ä—É, –µ—Å—Ç—å –º–Ω–æ–∂–µ—Å—Ç–≤–æ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π —Å –ª—é–¥—å–º–∏ –∏ –¥–ª—è –∫–∞–∂–¥–æ–π –Ω—É–∂–Ω–æ –ø–æ–Ω—è—Ç—å, –µ—Å—Ç—å —É —á–µ–ª–æ–≤–µ–∫–∞ —à–ª—è–ø–∞ –Ω–∞ –≥–æ–ª–æ–≤–µ –∏–ª–∏ –Ω–µ—Ç);‚Äî –†–∞–±–æ—Ç–∞ —Å –≥—Ä–∞—Ñ–∏–∫–æ–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, –≤—ã–¥–µ–ª–∏—Ç—å –≥—Ä–∞–Ω–∏—Ü—É –º–µ–∂–¥—É –≤–æ–ª–æ—Å–∞–º–∏ –∏ —Ñ–æ–Ω–æ–º, –≤—ã–¥–µ–ª–∏—Ç—å –±—Ä–æ–≤–∏, –≥–ª–∞–∑–∞);‚Äî –°–±–æ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ;‚Äî –†–∞—Å—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Ç–æ—á–µ–∫ –Ω–∞ –ª–∏—Ü–µ –∏ –¥—Ä—É–≥–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–∞—Ö;‚Äî –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ —Ñ–æ—Ç–æ –ø–æ –∫—Ä–∏—Ç–µ—Ä–∏—è–º. –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è: ‚Äî –ë–∞–∑–æ–≤—ã–µ –∑–Ω–∞–Ω–∏—è –ü–ö: –±—Ä–∞—É–∑–µ—Ä, Excel, —Ä–∞–±–æ—Ç–∞ —Å —Ñ–∞–π–ª–∞–º–∏, Skype –∏ —Ç. –¥.;‚Äî –ë–∞–∑–æ–≤—ã–µ –∑–Ω–∞–Ω–∏—è —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–∞ –∏ –æ–ø—ã—Ç —Ä–∞–±–æ—Ç—ã –≤ –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö —Ä–µ–¥–∞–∫—Ç–æ—Ä–∞—Ö (–±–∞–∑–∞ Photoshop);‚Äî –ó–Ω–∞–Ω–∏–µ –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ (–º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å –≤–ª–∞–¥–µ–Ω–∏—è ‚Äî Intermediate);‚Äî –°–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –æ—Å–≤–æ–∏—Ç—å –ª—é–±—É—é –ø—Ä–æ–≥—Ä–∞–º–º—É –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏;‚Äî –£—Å–∏–¥—á–∏–≤–æ—Å—Ç—å;‚Äî –í–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å;‚Äî –ì–∏–ø–µ—Ä–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å;‚Äî –ë—ã—Å—Ç—Ä–∞—è –æ–±—É—á–∞–µ–º–æ—Å—Ç—å. –£—Å–ª–æ–≤–∏—è —Ä–∞–±–æ—Ç—ã: ‚Äî –ù–µ–ø–æ–ª–Ω—ã–π —Ä–∞–±–æ—á–∏–π –¥–µ–Ω—å;‚Äî –£–¥–∞–ª–µ–Ω–Ω–∞—è —Ä–∞–±–æ—Ç–∞;‚Äî –°–≤–æ–±–æ–¥–Ω—ã–π –≥—Ä–∞—Ñ–∏–∫;‚Äî –û–ø–ª–∞—Ç–∞ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –∫–∞—á–µ—Å—Ç–≤–∞ –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø—Ä–æ–¥–µ–ª–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç—ã. –°–≤–æ–µ —Ä–µ–∑—é–º–µ, —Ç—ã –º–æ–∂–µ—à—å –æ—Ç–ø—Ä–∞–≤–ª—è—Ç—å –º–Ω–µ –Ω–∞ –ø–æ—á—Ç—É: dstrelchuk@c.snap.com",Data Labelling (Contract)@Snap Inc.,https://jobs.dou.ua/companies/snap/vacancies/127717/, remote,Data Labelling (Contract),08 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/allvue/,Allvue,"{""Required skills"": [""WHAT WE NEED"", ""Minimum of 3 years of relevant experience working in Business Intelligence / Report Development or ETL"", ""Experience with SQL 2012 or later, and SQL Server Reporting Services. Clear understanding of Microsoft\u2019s SQL Stack with Integration Services (SSIS), Analysis Services (SSAS), Reporting Services (SSRS) and SQL Agent Jobs; and the role of data movement in data analyticsExperience with Power BI, C# in database Assembly integration, or SharePoint integration a plus"", ""Knowledge and understanding of relational database design and programming"", ""Knowledge and understanding of data warehouse star schema design and structure"", ""In depth SQL querying skills with understanding and experience in optimizing speed of queries"", ""Master\u2019s degree or equivalent experience"", ""Background knowledge in finance, economics, statistics or mathematics a plus"", ""Full professional proficiency in written/spoken English""], ""We offer"": [""WHAT WE OFFER"", ""Fun, fast-paced work environment"", ""Constantly evolving, cutting edge technology"", ""The ability to make a significant impact immediately upon jumping in"", ""An opportunity to work with some of the best firms and the best people in the financial industry"", ""The ability to create change in the product we sell, by using the very same solution (in a very different way) to perform your daily job"", ""Commitment to diversity and inclusion in our workplace and workforceAllvue Academy continuous training and education"", ""Numerous team building activities to promote collaboration""], ""Responsibilities"": [""YOUR RESPONSIBILITIES"", ""Serve in a leadership role in the Power BI and ETL team"", ""welcome and mentor new developers"", ""Lead Power BI and ETL team to develop, test, implement and maintain Allvue data products"", ""Lead, design and develop ETL processes for the data warehouse lifecycle"", ""Collaborate and work alongside with Product Owner and other technical professionals"", ""Design and develop SQL Server stored procedures, functions and views be used and to feed SSRS reports"", ""Troubleshoot data issues and performance tuning of SQL queries"", ""Maintain and support existing data products""]}",,"Required skills WHAT WE NEED‚Äî Minimum of 3 years of relevant experience working in Business Intelligence / Report Development or ETL ‚Äî Experience with SQL 2012 or later, and SQL Server Reporting Services. Clear understanding of Microsoft‚Äôs SQL Stack with Integration Services (SSIS), Analysis Services (SSAS), Reporting Services (SSRS) and SQL Agent Jobs; and the role of data movement in data analyticsExperience with Power BI, C# in database Assembly integration, or SharePoint integration a plus‚Äî Knowledge and understanding of relational database design and programming‚Äî Knowledge and understanding of data warehouse star schema design and structure‚Äî In depth SQL querying skills with understanding and experience in optimizing speed of queries‚Äî Master‚Äôs degree or equivalent experience ‚Äî Background knowledge in finance, economics, statistics or mathematics a plus‚Äî Full professional proficiency in written/spoken English We offer WHAT WE OFFER‚Äî Fun, fast-paced work environment‚Äî Constantly evolving, cutting edge technology‚Äî The ability to make a significant impact immediately upon jumping in‚Äî An opportunity to work with some of the best firms and the best people in the financial industry‚Äî The ability to create change in the product we sell, by using the very same solution (in a very different way) to perform your daily job‚Äî Commitment to diversity and inclusion in our workplace and workforceAllvue Academy continuous training and education‚Äî Numerous team building activities to promote collaboration Responsibilities YOUR RESPONSIBILITIES‚Äî Serve in a leadership role in the Power BI and ETL team ‚Äî welcome and mentor new developers‚Äî Lead Power BI and ETL team to develop, test, implement and maintain Allvue data products ‚Äî Lead, design and develop ETL processes for the data warehouse lifecycle ‚Äî Collaborate and work alongside with Product Owner and other technical professionals ‚Äî Design and develop SQL Server stored procedures, functions and views be used and to feed SSRS reports‚Äî Troubleshoot data issues and performance tuning of SQL queries‚Äî Maintain and support existing data products",BI¬†Lead@Allvue,https://jobs.dou.ua/companies/allvue/vacancies/134447/, Kyiv,BI¬†Lead,08 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/pdffiller/,pdfFiller,"{""Required skills"": [""\u041e\u043f\u044b\u0442 \u0440\u0430\u0431\u043e\u0442\u044b \u0441 \u0431\u0430\u0437\u0430\u043c\u0438 \u0434\u0430\u043d\u043d\u044b\u0445 \u043e\u0442 1 \u0433\u043e\u0434\u0430;"", ""\u041d\u0430\u043f\u0438\u0441\u0430\u043d\u0438\u0435 \u0432 \u043c\u0435\u0440\u0443 \u043a\u043e\u043c\u043f\u043b\u0435\u043a\u0441\u043d\u044b\u0445 SELECT \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u0432;"", ""\u0410\u0433\u0440\u0435\u0433\u0430\u0446\u0438\u043e\u043d\u043d\u044b\u0435 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 + \u0433\u0440\u0443\u043f\u0438\u0440\u043e\u0432\u043a\u0430;"", ""\u041f\u043e\u043d\u0438\u043c\u0430\u043d\u0438\u0435 \u043e\u0442\u043b\u0438\u0447\u0438\u044f \u0440\u0430\u0437\u043d\u044b\u0445 \u0442\u0438\u043f\u043e\u0432 JOIN ;"", ""\u041e\u043f\u044b\u0442 \u0440\u0430\u0431\u043e\u0442\u044b \u0441 Python/R \u0434\u043b\u044f \u0440\u0430\u0431\u043e\u0442\u044b \u0441 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430\u043c\u0438 \u0438 \u0441\u043e\u043f\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u043c\u0438 \u043f\u0430\u043a\u0435\u0442\u0430\u043c\u0438 \u0434\u043b\u044f \u0430\u043d\u0430\u043b\u0438\u0437\u0430 \u0434\u0430\u043d\u043d\u044b\u0445;"", ""\u0423\u0432\u0435\u0440\u0435\u043d\u043d\u044b\u0435 \u0437\u043d\u0430\u043d\u0438\u044f \u043e\u043f\u0438\u0441\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0439 \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u043a\u0438, \u043f\u043e\u043d\u0438\u043c\u0430\u043d\u0438\u0435 \u043c\u0435\u0442\u043e\u0434\u043e\u0432 \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0438 \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u0433\u0438\u043f\u043e\u0442\u0435\u0437;"", ""\u0423\u0432\u0435\u0440\u0435\u043d\u043d\u044b\u0435 \u0437\u043d\u0430\u043d\u0438\u044f \u0442\u0435\u043e\u0440\u0438\u0438 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0438 \u0438 \u043f\u043e\u043d\u0438\u043c\u0430\u043d\u0438\u0435 \u0431\u0430\u0437\u043e\u0432\u044b\u0445 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0439 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0435\u0439 \u0438 \u0438\u0445 \u043f\u0440\u0438\u043c\u0435\u043d\u0438\u043c\u043e\u0441\u0442\u044c \u043a \u0440\u0435\u0430\u043b\u044c\u043d\u044b\u043c \u0437\u0430\u0434\u0430\u0447\u0430\u043c;"", ""\u0417\u043d\u0430\u043d\u0438\u0435 \u043a\u043b\u0430\u0441\u0441\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u043b\u0438\u043d\u0435\u0439\u043d\u044b\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u0438 \u043c\u0435\u0442\u043e\u0434\u043e\u0432 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u0438 \u0438 \u0438\u0445 \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u0438;"", ""\u0411\u0430\u0437\u043e\u0432\u044b\u0435 \u0437\u043d\u0430\u043d\u0438\u044f \u043c\u0435\u0442\u043e\u0434\u043e\u0432 NLP (\u0447\u0430\u0441\u0442\u043e\u0442\u043d\u044b\u0439 \u0430\u043d\u0430\u043b\u0438\u0437, bag of words, tf-idf, Ngrams);"", ""\u041f\u043e\u043d\u0438\u043c\u0430\u043d\u0438\u0435 \u043f\u043e\u0434\u0445\u043e\u0434\u043e\u0432 \u043a \u0440\u0430\u0437\u0432\u0435\u0434\u044b\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u043c\u0443 \u0430\u043d\u0430\u043b\u0438\u0437\u0443 \u0434\u0430\u043d\u043d\u044b\u0445, \u0432\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438, \u043f\u0440\u0438\u043d\u0446\u0438\u043f\u043e\u0432 \u0440\u0430\u0431\u043e\u0442\u044b \u0441 \u043d\u0435\u043f\u043e\u043b\u043d\u044b\u043c\u0438 \u0434\u0430\u043d\u043d\u044b\u043c\u0438, \u043d\u0435\u0441\u0431\u0430\u043b\u0430\u043d\u0441\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u043c\u0438 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430\u043c\u0438;"", ""\u041f\u043e\u043d\u0438\u043c\u0430\u043d\u0438\u0435 \u0431\u0430\u0437\u043e\u0432\u044b\u0445 \u043c\u0430\u0440\u043a\u0435\u0442\u0438\u043d\u0433\u043e\u0432\u044b\u0445 \u0438 \u043f\u0440\u043e\u0434\u0443\u043a\u0442\u043e\u0432\u044b\u0445 \u043c\u0435\u0442\u0440\u0438\u043a \u0438 \u043c\u0435\u0442\u043e\u0434\u044b \u0438\u0445 \u043a\u0430\u043b\u044c\u043a\u0443\u043b\u044f\u0446\u0438\u0439;""], ""As a plus"": [""\u041f\u043e\u043d\u0438\u043c\u0430\u043d\u0438\u0435 \u043f\u0440\u0438\u043d\u0446\u0438\u043f\u043e\u0432 \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u0438 \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u0432;"", ""\u041f\u043e\u043d\u0438\u043c\u0430\u043d\u0438\u0435 \u0411\u0430\u0439\u0435\u0441\u043e\u0432\u0441\u043a\u043e\u0439 \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u043a\u0438 \u0438 \u043c\u0435\u0442\u043e\u0434\u043e\u0432 MCMC;"", ""\u0417\u043d\u0430\u043d\u0438\u0435 bash, Git/Github, Docker;"", ""\u041e\u043f\u044b\u0442 \u0440\u0430\u0431\u043e\u0442\u044b \u0441 GoogleAnalytics (Reporting API, RealTime API), GoogleAds;""], ""We offer"": [""\u0414\u043b\u044f \u0431\u043e\u043b\u0435\u0435 \u043a\u043e\u043c\u0444\u043e\u0440\u0442\u043d\u043e\u0439 \u0440\u0430\u0431\u043e\u0442\u044b \u043d\u0430\u0448\u0438\u0445 \u0441\u043e\u0442\u0440\u0443\u0434\u043d\u0438\u043a\u043e\u0432, \u043c\u044b \u0432\u043d\u0435\u0434\u0440\u0438\u043b\u0438 \u0434\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0435 \u0431\u0435\u043d\u0435\u0444\u0438\u0442\u044b:""], ""Responsibilities"": [""\u041c\u0435\u0434\u0438\u0446\u0438\u043d\u0441\u043a\u043e\u0435 \u0441\u0442\u0440\u0430\u0445\u043e\u0432\u0430\u043d\u0438\u0435."", ""\u0420\u0435\u0433\u0443\u043b\u044f\u0440\u043d\u044b\u0435 \u0437\u0430\u043d\u044f\u0442\u0438\u044f \u0439\u043e\u0433\u043e\u0439, \u0441\u0442\u0440\u0435\u0442\u0447\u0438\u043d\u0433\u043e\u043c (3 \u0440\u0430\u0437\u0430 \u0432 \u043d\u0435\u0434\u0435\u043b\u044e) \u0438 \u0431\u043e\u043a\u0441\u043e\u043c (2 \u0440\u0430\u0437\u0430 \u0432 \u043d\u0435\u0434\u0435\u043b\u044e) \u0441\u043f\u0440\u043e\u0444\u0435\u0441\u0441\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u044b\u043c\u0438 \u0442\u0440\u0435\u043d\u0435\u0440\u0430\u043c\u0438 \u0432 \u043e\u0444\u0438\u0441\u0435. \u0412\u043e\u043b\u0435\u0439\u0431\u043e\u043b \u0438 \u0444\u0443\u0442\u0431\u043e\u043b."", ""\u0411\u0435\u0441\u043f\u043b\u0430\u0442\u043d\u044b\u0435 \u043e\u0431\u0435\u0434\u044b \u0441 \u0434\u043e\u0441\u0442\u0430\u0432\u043a\u043e\u0439 \u0432 \u043e\u0444\u0438\u0441."", ""\u041e\u043f\u043b\u0430\u0442\u0430 \u043a\u0443\u0440\u0441\u043e\u0432 \u043f\u043e\u0432\u044b\u0448\u0435\u043d\u0438\u044f \u043a\u0432\u0430\u043b\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438 \u0434\u043b\u044f \u043d\u0430\u0448\u0438\u0445 \u0441\u043e\u0442\u0440\u0443\u0434\u043d\u0438\u043a\u043e\u0432, \u043e\u0431\u0435\u0441\u043f\u0435\u0447\u0435\u043d\u0438\u0435 \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e\u0441\u0442\u0438\u043f\u043e\u0441\u0435\u0449\u0435\u043d\u0438\u044f \u0438\u043c\u0438 \u043b\u0443\u0447\u0448\u0438\u0445 \u043f\u0440\u043e\u0444\u0435\u0441\u0441\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u044b\u0445 \u043a\u043e\u043d\u0444\u0435\u0440\u0435\u043d\u0446\u0438\u0439 \u0432 \u043e\u0442\u0440\u0430\u0441\u043b\u0438."", ""\u0428\u0442\u0430\u0442\u043d\u044b\u0439 \u043c\u0430\u0441\u0441\u0430\u0436\u0438\u0441\u0442."", ""\u0412\u043e\u0437\u0432\u0440\u0430\u0442\u043d\u0430\u044f \u0444\u0438\u043d\u0430\u043d\u0441\u043e\u0432\u0430\u044f \u043f\u043e\u043c\u043e\u0449\u044c."", ""\u041e\u043f\u043b\u0430\u0447\u0438\u0432\u0430\u0435\u043c\u044b\u0435 \u0437\u0430\u043d\u044f\u0442\u0438\u044f \u043f\u043e \u0430\u043d\u0433\u043b\u0438\u0439\u0441\u043a\u043e\u043c\u0443 \u044f\u0437\u044b\u043a\u0443 (2 \u0440\u0430\u0437\u0430 \u0432 \u043d\u0435\u0434\u0435\u043b\u044e \u0432 \u043e\u0444\u0438\u0441\u0435)."", ""\u041a\u0430\u0436\u0434\u044b\u0439 \u0441\u043e\u0442\u0440\u0443\u0434\u043d\u0438\u043a, \u0440\u0430\u0431\u043e\u0442\u0430\u044f \u0432 \u043a\u043e\u043c\u043f\u0430\u043d\u0438\u0438 \u0433\u043e\u0434, \u043f\u043e\u043b\u0443\u0447\u0430\u0435\u0442 \u0441\u0432\u043e\u0439 \u043e\u043f\u0446\u0438\u043e\u043d\u043d\u044b\u0439 \u043a\u043e\u043d\u0442\u0440\u0430\u043a\u0442."", ""\u0412\u0441\u044f \u0444\u0438\u043d\u0430\u043d\u0441\u043e\u0432\u0430\u044f \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044f \u043e \u0440\u043e\u0441\u0442\u0435 \u043a\u043e\u043c\u043f\u0430\u043d\u0438\u0438 \u043e\u0442\u043a\u0440\u044b\u0442\u0430 \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0440\u0430\u0431\u043e\u0442\u043d\u0438\u043a\u0430 PDFfiller, \u0432\u043d\u0435\u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u0438 \u043e\u0442 \u0435\u0433\u043e \u0440\u043e\u043b\u0438 \u0432 \u043a\u043e\u043c\u0430\u043d\u0434\u0435. \u041a\u0430\u0436\u0434\u044b\u0435 \u043f\u043e\u043b\u0433\u043e\u0434\u0430 \u043c\u044b, \u0447\u043b\u0435\u043d\u044b \u043a\u043e\u043c\u0430\u043d\u0434\u044b, \u043f\u043e\u043b\u0443\u0447\u0430\u0435\u043c \u043f\u043e\u043b\u043d\u044b\u0439\u0444\u0438\u043d\u0430\u043d\u0441\u043e\u0432\u044b\u0439 \u043e\u0442\u0447\u0435\u0442 \u043e\u0442 \u043f\u0440\u0435\u0437\u0438\u0434\u0435\u043d\u0442\u0430 \u043a\u043e\u043c\u043f\u0430\u043d\u0438\u0438.""], ""Project description"": [""\u0424\u043e\u0440\u043c\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u043e\u0442\u0447\u0435\u0442\u043e\u0432 \u0438\u0437 \u0431\u0430\u0437 \u0434\u0430\u043d\u043d\u044b\u0445;"", ""\u041f\u0440\u043e\u0432\u0435\u0434\u0435\u043d\u0438\u0435 \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0433\u043e \u0430\u043d\u0430\u043b\u0438\u0437\u0430 \u043d\u0430 \u0434\u0430\u043d\u043d\u044b\u0445;"", ""\u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0434\u0430\u043d\u043d\u044b\u0445, \u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u0435 \u0434\u0430\u0448\u0431\u043e\u0440\u0434\u043e\u0432 \u0432 DataStudio, Grahana;"", ""\u0424\u043e\u0440\u043c\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u0430\u0432\u0442\u043e\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u043e\u0431\u043d\u043e\u0432\u043b\u0435\u043d\u0438\u0435 \u043e\u0442\u0447\u0435\u0442\u043e\u0432 (\u043a\u0440\u043e\u043d\u044b);"", ""\u0412\u043e\u0432\u043b\u0435\u0447\u0435\u043d\u0438\u0435 \u0432 \u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u043c\u0430\u0448\u0438\u043d\u043d\u043e\u0433\u043e \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f (\u0432\u044b\u044f\u0432\u043b\u0435\u043d\u0438\u0435 \u0430\u043d\u043e\u043c\u0430\u043b\u0438\u0439, \u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435 \u0440\u044f\u0434\u044b, \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u044f, \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u044f)""]}",,"Required skills ‚Äî –û–ø—ã—Ç —Ä–∞–±–æ—Ç—ã —Å –±–∞–∑–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö –æ—Ç 1 –≥–æ–¥–∞;‚Äî –ù–∞–ø–∏—Å–∞–Ω–∏–µ –≤ –º–µ—Ä—É –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã—Ö SELECT –∑–∞–ø—Ä–æ—Å–æ–≤;‚Äî –ê–≥—Ä–µ–≥–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ + –≥—Ä—É–ø–∏—Ä–æ–≤–∫–∞;‚Äî –ü–æ–Ω–∏–º–∞–Ω–∏–µ –æ—Ç–ª–∏—á–∏—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ JOIN ;‚Äî –û–ø—ã—Ç —Ä–∞–±–æ—Ç—ã —Å Python/R –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –¥–∞—Ç–∞—Å–µ—Ç–∞–º–∏ –∏ —Å–æ–ø—É—Ç—Å—Ç–≤—É—é—â–∏–º–∏ –ø–∞–∫–µ—Ç–∞–º–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö;‚Äî –£–≤–µ—Ä–µ–Ω–Ω—ã–µ –∑–Ω–∞–Ω–∏—è –æ–ø–∏—Å–∞—Ç–µ–ª—å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏, –ø–æ–Ω–∏–º–∞–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –≥–∏–ø–æ—Ç–µ–∑;‚Äî –£–≤–µ—Ä–µ–Ω–Ω—ã–µ –∑–Ω–∞–Ω–∏—è —Ç–µ–æ—Ä–∏–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –∏ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –∏ –∏—Ö –ø—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å –∫ —Ä–µ–∞–ª—å–Ω—ã–º –∑–∞–¥–∞—á–∞–º;‚Äî –ó–Ω–∞–Ω–∏–µ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏—Ö –ª–∏–Ω–µ–π–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∏ –º–µ—Ç–æ–¥–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –∏ –∏—Ö –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏;‚Äî –ë–∞–∑–æ–≤—ã–µ –∑–Ω–∞–Ω–∏—è –º–µ—Ç–æ–¥–æ–≤ NLP (—á–∞—Å—Ç–æ—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑, bag of words, tf-idf, Ngrams);‚Äî –ü–æ–Ω–∏–º–∞–Ω–∏–µ –ø–æ–¥—Ö–æ–¥–æ–≤ –∫ —Ä–∞–∑–≤–µ–¥—ã–≤–∞—Ç–µ–ª—å–Ω–æ–º—É –∞–Ω–∞–ª–∏–∑—É –¥–∞–Ω–Ω—ã—Ö, –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏, –ø—Ä–∏–Ω—Ü–∏–ø–æ–≤ —Ä–∞–±–æ—Ç—ã —Å –Ω–µ–ø–æ–ª–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏, –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞–º–∏;‚Äî –ü–æ–Ω–∏–º–∞–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö –º–∞—Ä–∫–µ—Ç–∏–Ω–≥–æ–≤—ã—Ö –∏ –ø—Ä–æ–¥—É–∫—Ç–æ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫ –∏ –º–µ—Ç–æ–¥—ã –∏—Ö –∫–∞–ª—å–∫—É–ª—è—Ü–∏–π; As a plus ‚Äî –ü–æ–Ω–∏–º–∞–Ω–∏–µ –ø—Ä–∏–Ω—Ü–∏–ø–æ–≤ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–æ–≤;‚Äî –ü–æ–Ω–∏–º–∞–Ω–∏–µ –ë–∞–π–µ—Å–æ–≤—Å–∫–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏ –º–µ—Ç–æ–¥–æ–≤ MCMC;‚Äî –ó–Ω–∞–Ω–∏–µ bash, Git/Github, Docker;‚Äî –û–ø—ã—Ç —Ä–∞–±–æ—Ç—ã —Å GoogleAnalytics (Reporting API, RealTime API), GoogleAds; We offer –î–ª—è –±–æ–ª–µ–µ –∫–æ–º—Ñ–æ—Ä—Ç–Ω–æ–π —Ä–∞–±–æ—Ç—ã –Ω–∞—à–∏—Ö —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–æ–≤, –º—ã –≤–Ω–µ–¥—Ä–∏–ª–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –±–µ–Ω–µ—Ñ–∏—Ç—ã: ‚Äî –ú–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–µ —Å—Ç—Ä–∞—Ö–æ–≤–∞–Ω–∏–µ.‚Äî –†–µ–≥—É–ª—è—Ä–Ω—ã–µ –∑–∞–Ω—è—Ç–∏—è –π–æ–≥–æ–π, —Å—Ç—Ä–µ—Ç—á–∏–Ω–≥–æ–º (3 —Ä–∞–∑–∞ –≤ –Ω–µ–¥–µ–ª—é) –∏ –±–æ–∫—Å–æ–º (2 —Ä–∞–∑–∞ –≤ –Ω–µ–¥–µ–ª—é) —Å–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–º–∏ —Ç—Ä–µ–Ω–µ—Ä–∞–º–∏ –≤ –æ—Ñ–∏—Å–µ. –í–æ–ª–µ–π–±–æ–ª –∏ —Ñ—É—Ç–±–æ–ª.‚Äî –ë–µ—Å–ø–ª–∞—Ç–Ω—ã–µ –æ–±–µ–¥—ã —Å –¥–æ—Å—Ç–∞–≤–∫–æ–π –≤ –æ—Ñ–∏—Å.‚Äî –û–ø–ª–∞—Ç–∞ –∫—É—Ä—Å–æ–≤ –ø–æ–≤—ã—à–µ–Ω–∏—è –∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏–∏ –¥–ª—è –Ω–∞—à–∏—Ö —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–æ–≤, –æ–±–µ—Å–ø–µ—á–µ–Ω–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏–ø–æ—Å–µ—â–µ–Ω–∏—è –∏–º–∏ –ª—É—á—à–∏—Ö –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã—Ö –∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏–π –≤ –æ—Ç—Ä–∞—Å–ª–∏.‚Äî –®—Ç–∞—Ç–Ω—ã–π –º–∞—Å—Å–∞–∂–∏—Å—Ç.‚Äî –í–æ–∑–≤—Ä–∞—Ç–Ω–∞—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤–∞—è –ø–æ–º–æ—â—å.‚Äî –û–ø–ª–∞—á–∏–≤–∞–µ–º—ã–µ –∑–∞–Ω—è—Ç–∏—è –ø–æ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º—É —è–∑—ã–∫—É (2 —Ä–∞–∑–∞ –≤ –Ω–µ–¥–µ–ª—é –≤ –æ—Ñ–∏—Å–µ).‚Äî –ö–∞–∂–¥—ã–π —Å–æ—Ç—Ä—É–¥–Ω–∏–∫, —Ä–∞–±–æ—Ç–∞—è –≤ –∫–æ–º–ø–∞–Ω–∏–∏ –≥–æ–¥, –ø–æ–ª—É—á–∞–µ—Ç —Å–≤–æ–π –æ–ø—Ü–∏–æ–Ω–Ω—ã–π –∫–æ–Ω—Ç—Ä–∞–∫—Ç.‚Äî –í—Å—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ä–æ—Å—Ç–µ –∫–æ–º–ø–∞–Ω–∏–∏ –æ—Ç–∫—Ä—ã—Ç–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ä–∞–±–æ—Ç–Ω–∏–∫–∞ PDFfiller, –≤–Ω–µ–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –µ–≥–æ —Ä–æ–ª–∏ –≤ –∫–æ–º–∞–Ω–¥–µ. –ö–∞–∂–¥—ã–µ –ø–æ–ª–≥–æ–¥–∞ –º—ã, —á–ª–µ–Ω—ã –∫–æ–º–∞–Ω–¥—ã, –ø–æ–ª—É—á–∞–µ–º –ø–æ–ª–Ω—ã–π—Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π –æ—Ç—á–µ—Ç –æ—Ç –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç–∞ –∫–æ–º–ø–∞–Ω–∏–∏. Responsibilities ‚Äî –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–æ–≤ –∏–∑ –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö;‚Äî –ü—Ä–æ–≤–µ–¥–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞ –¥–∞–Ω–Ω—ã—Ö;‚Äî –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö, –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –¥–∞—à–±–æ—Ä–¥–æ–≤ –≤ DataStudio, Grahana;‚Äî –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–æ–≤ (–∫—Ä–æ–Ω—ã);‚Äî –í–æ–≤–ª–µ—á–µ–Ω–∏–µ –≤ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è (–≤—ã—è–≤–ª–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π, –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä—è–¥—ã, –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è, –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è) Project description airSlate ‚Äî data-driven –∫–æ–º–ø–∞–Ω–∏—è. –≠—Ç–æ –∑–Ω–∞—á–∏—Ç, —á—Ç–æ –ª—é–±–æ–µ —Ä–µ—à–µ–Ω–∏–µ –ø—Ä–∏–Ω–∏–º–∞–µ—Ç—Å—è, –∫–æ–≥–¥–∞ –æ–Ω–æ –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–æ –¥–∞–Ω–Ω—ã–º–∏. –ó–∞ –≥–æ–¥—ã —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è –∫–æ–º–ø–∞–Ω–∏–∏ –º—ã –Ω–∞–∫–æ–ø–∏–ª–∏ –æ–≥—Ä–æ–º–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö. –ö–æ–º–∞–Ω–¥–∞ –î–∞—Ç–∞ –ê–Ω–∞–ª–∏—Ç–∏–∫–æ–≤ –ø–æ–º–æ–≥–∞–µ—Ç –≤—Å–µ–º –ø—Ä–æ–¥—É–∫—Ç–æ–≤—ã–º –∏ –º–∞—Ä–∫–µ—Ç–∏–Ω–≥–æ–≤–æ–π –∫–æ–º–∞–Ω–¥–∞–º –≤ –∫–æ–º–ø–∞–Ω–∏–∏. –†–µ–±—è—Ç–∞ –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç –ø–æ–≤–µ–¥–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π, –≤–µ–± —Ç—Ä–∞—Ñ–∏–∫ –∏ –¥–∞–Ω–Ω—ã–µ –æ —Ä–µ–∫–ª–∞–º–µ; —Ñ–æ—Ä–º–∏—Ä—É—é—Ç –≥–∏–ø–æ—Ç–µ–∑—ã –∏ —Ç–µ—Å—Ç–∏—Ä—É—é—Ç –∏—Ö, —Å—Ç—Ä–æ—è—Ç –¥–∞—à–±–æ—Ä–¥—ã. –ù–∞ –æ—Å–Ω–æ–≤–∞–Ω–∏–∏ –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ –ø—Ä–æ–¥—É–∫—Ç–æ–≤—ã–µ –∫–æ–º–∞–Ω–¥—ã –ø—Ä–∏–Ω–∏–º–∞—é—Ç —Ä–µ—à–µ–Ω–∏—è –æ —Ä–∞–∑–≤–∏—Ç–∏–∏ –ø—Ä–æ–¥—É–∫—Ç–æ–≤.",Junior Data Analyst@pdfFiller,https://jobs.dou.ua/companies/pdffiller/vacancies/134444/, Kyiv,Junior Data Analyst,07 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/genesis-technology-partners/,Genesis,{},,"Genesis ‚Äî –º—ñ–∂–Ω–∞—Ä–æ–¥–Ω–∞ –ø—Ä–æ–¥—É–∫—Ç–æ–≤–∞ –Ü–¢-–∫–æ–º–ø–∞–Ω—ñ—è –ø–æ–≤–Ω–æ–≥–æ —Ü–∏–∫–ª—É. –ü–æ–Ω–∞–¥ 1500 –æ—Å—ñ–± —É –ø‚Äô—è—Ç–∏ –∫—Ä–∞—ó–Ω–∞—Ö —Å—Ç–≤–æ—Ä—é—é—Ç—å –ø—Ä–æ–¥—É–∫—Ç–∏ –¥–ª—è –±—ñ–ª—å—à –Ω—ñ–∂ 200 –º—ñ–ª—å–π–æ–Ω—ñ–≤ —É–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤ —â–æ–º—ñ—Å—è—Ü—è. –û–¥–∏–Ω —ñ–∑ –Ω–∞–π–±—ñ–ª—å—à–∏—Ö –ø–∞—Ä—Ç–Ω–µ—Ä—ñ–≤ Facebook, Google, Snapchat —Ç–∞ Apple –≤ –°—Ö—ñ–¥–Ω—ñ–π –Ñ–≤—Ä–æ–ø—ñ. –ï–∫–æ—Å–∏—Å—Ç–µ–º–∞ Genesis —Å–∫–ª–∞–¥–∞—î—Ç—å—Å—è –∑ –±—ñ–ª—å—à –Ω—ñ–∂ 15 –∫–æ–º–ø–∞–Ω—ñ–π —Ç–∞ —ñ–Ω–≤–µ—Å—Ç–∏—Ü—ñ–π–Ω–æ–≥–æ —Ñ–æ–Ω–¥—É. Keiki ‚Äî —Ü–µ –±—ñ–∑–Ω–µ—Å —é–Ω—ñ—Ç Genesis, —â–æ —Å—Ç–≤–æ—Ä—é—î —Ä–æ–∑–≤–∏–≤–∞—é—á—ñ –¥–æ–¥–∞—Ç–∫–∏ –¥–ª—è –¥—ñ—Ç–µ–π –Ω–∞ —Ä–∏–Ω–∫–∏ –≤—Å—å–æ–≥–æ —Å–≤—ñ—Ç—É. –ó–∞—Ä–∞–∑ —É –∫–æ–º–∞–Ω–¥—ñ —î —Ç—Ä–∏ –ø—Ä–æ–¥—É–∫—Ç–∏ –≤ –ø–æ—Ä—Ç—Ñ–æ–ª—ñ–æ ‚Äî Keiki, Keiki World, Keiki Puzzles. –ù–∞—à –¥–æ–¥–∞—Ç–æ–∫ Keiki –≤–∂–µ –±—É–ª–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–æ –∫–æ–º–ø–∞–Ω—ñ—î—é Apple –≤ –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó Our Favorite Kids App! –ú–∏ –º–∞—î–º–æ –∞–º–±—ñ—Ç–Ω—ñ –ø–ª–∞–Ω–∏ –∑ —Ä–æ–∑–≤–∏—Ç–∫—É –Ω–∞—à–∏—Ö –¥–∏—Ç—è—á–∏—Ö –¥–æ–¥–∞—Ç–∫—ñ–≤ —ñ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –Ω–æ–≤–∏—Ö —ñ–≥—Ä–æ–≤–∏—Ö –ø—Ä–æ–¥—É–∫—Ç—ñ–≤ –¥–ª—è –±—ñ–ª—å—à –¥–æ—Ä–æ—Å–ª–æ—ó –∞—É–¥–∏—Ç–æ—Ä—ñ—ó. –ó–∞—Ä–∞–∑ –º–∏ –≤ –ø–æ—à—É–∫—É Data Analyst , —è–∫–∏–π –∑–º–æ–∂–µ –∑–∞–∫—Ä–∏—Ç–∏ –≤–µ–ª–∏–∫–∏–π –æ–±—Å—è–≥ –∞–Ω–∞–ª—ñ—Ç–∏—á–Ω–∏—Ö –∑–∞–¥–∞—á –∑ –º–∞—Ä–∫–µ—Ç–∏–Ω–≥—É —Ç–∞ –ø—Ä–æ–¥—É–∫—Ç—É, –æ—Ü—ñ–Ω–∫–∏ a\b —Ç–µ—Å—Ç—ñ–≤ —Ç–∞ –±—É–¥–µ –≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ –ø—Ä–æ–¥—É–∫—Ç–æ–≤—ñ —Ä—ñ—à–µ–Ω–Ω—è. –¢–≤–æ—ó–º–∏ –∑–∞–≤–¥–∞–Ω–Ω—è–º–∏ –±—É–¥—É—Ç—å: ‚Äî –ø—Ä–æ–¥—É–∫—Ç–æ–≤–∞ –∞–Ω–∞–ª—ñ—Ç–∏–∫–∞ —Ç–∞ –º–∞—Ä–∫–µ—Ç–∏–Ω–≥ –∞–Ω–∞–ª—ñ—Ç–∏–∫–∞‚Äî —Å—Ç—Ä—É–∫—Ç—É—Ä–∏–∑–∞—Ü—ñ—è —ñ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü—ñ—è –∑–≤—ñ—Ç–Ω–æ—Å—Ç—ñ‚Äî –∞–Ω–∞–ª—ñ–∑ —ñ –æ—Ü—ñ–Ω–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ A / B —Ç–µ—Å—Ç—ñ–≤‚Äî –ø—ñ–¥—Ç—Ä–∏–º–∫–∞ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—ñ —Ç–∞ —è–∫–æ—Å—Ç—ñ –¥–∞–Ω–∏—Ö –©–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ, —â–æ–± –ø—Ä–∏—î–¥–Ω–∞—Ç–∏—Å—è –¥–æ –Ω–∞—Å: ‚Äî –ø—Ä–∞–∫—Ç–∏—á–Ω–∏–π –¥–æ—Å–≤—ñ–¥ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è SQL / MySQL‚Äî –∑–Ω–∞–Ω–Ω—è R / Python.‚Äî –≤–ø–µ–≤–Ω–µ–Ω—ñ –∑–Ω–∞–Ω–Ω—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —ñ —Ç–µ–æ—Ä—ñ—ó –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç–µ–π‚Äî –¥–æ—Å–≤—ñ–¥ –∑–∞–ø—É—Å–∫—É –ê / B —Ç–µ—Å—Ç—ñ–≤ ‚Äî –≤–æ–ª–æ–¥—ñ–Ω–Ω—è –∞–Ω–≥–ª—ñ–π—Å—å–∫–æ—é –Ω–∞ —Ä—ñ–≤–Ω—ñ Intermediate+ –ë—É–¥–µ –ø–µ—Ä–µ–≤–∞–≥–æ—é, —è–∫—â–æ —Ç–∏: ‚Äî –º–∞—î—à –¥–æ—Å–≤—ñ–¥ –≤ –∞–Ω–∞–ª—ñ—Ç–∏—Ü—ñ –º–æ–±—ñ–ª—å–Ω–∏—Ö –¥–æ–¥–∞—Ç–∫—ñ–≤ Genesis —Å—Ç–≤–æ—Ä–∏–≤ –≤—Å—ñ —É–º–æ–≤–∏ –¥–ª—è —Ç–≤–æ–≥–æ —É—Å–ø—ñ—Ö—É –ö–æ–º—Ñ–æ—Ä—Ç–Ω—ñ —É–º–æ–≤–∏ —Ç–∞ –≥–Ω—É—á–∫–∏–π –≥—Ä–∞—Ñ—ñ–∫ –ö–ª–∞—Å–Ω–∏–π –æ—Ñ—ñ—Å —É 5 —Ö–≤–∏–ª–∏–Ω–∞—Ö –≤—ñ–¥ –º–µ—Ç—Ä–æ –¢–∞—Ä–∞—Å–∞ –®–µ–≤—á–µ–Ω–∫–æ –∑ —Ç–µ—Ä–∞—Å–æ—é, –ª–∞—É–Ω–∂ –∑–æ–Ω–∞–º–∏, –∫—É—Ö–Ω–µ—é, PlayStation. –£ –Ω–∞—à–∏—Ö –æ—Ñ—ñ—Å–∞—Ö –±–∞–≥–∞—Ç–æ —Ä–æ—Å–ª–∏–Ω —Ç–∞ —Å–ø–µ—Ü—ñ–∞–ª—å–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ –≤–µ–Ω—Ç–∏–ª—è—Ü—ñ—ó —ñ –∫–æ–Ω–¥–∏—Ü—ñ–æ–Ω—É–≤–∞–Ω–Ω—è. –ú–∏ –∑–∞–±–µ–∑–ø–µ—á—É—î–º–æ 20 –¥–Ω—ñ–≤ –æ–ø–ª–∞—á—É–≤–∞–Ω–æ—ó –≤—ñ–¥–ø—É—Å—Ç–∫–∏ –Ω–∞ —Ä—ñ–∫ —ñ –∑—Ä—É—á–Ω–∏–π —Ä–æ–±–æ—á–∏–π –≥—Ä–∞—Ñ—ñ–∫. –¢—É—Ä–±–æ—Ç–∞ –ø—Ä–æ –∑–¥–æ—Ä–æ–≤‚Äô—è —ñ —Å–ø–æ—Ä—Ç –°–Ω—ñ–¥–∞–Ω–∫–∏, –æ–±—ñ–¥–∏, –±–µ–∑–º–µ–∂–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ñ—Ä—É–∫—Ç—ñ–≤, —Å–Ω–µ–∫—ñ–≤, —Å–º—É–∑—ñ —Ç–∞ –π–æ–≥—É—Ä—Ç—ñ–≤ –≤ –æ—Ñ—ñ—Å—ñ. –ö–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–∏–π –ª—ñ–∫–∞—Ä —ñ –º–µ–¥–∏—á–Ω–µ —Å—Ç—Ä–∞—Ö—É–≤–∞–Ω–Ω—è. –ë–µ–∑–∫–æ—à—Ç–æ–≤–Ω—ñ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –∑ –±—ñ–≥—É, —Ñ—É—Ç–±–æ–ª—É, –±–∞—Å–∫–µ—Ç–±–æ–ª—É, –≤–æ–ª–µ–π–±–æ–ª—É —Ç–∞ –π–æ–≥–∏. –ó–Ω–∏–∂–∫–∏ –≤ –Ω–∞–π–±–ª–∏–∂—á—ñ —Å–ø–æ—Ä—Ç–∑–∞–ª–∏ —Ç–∞ –æ–ø–ª–∞—Ç–∞ —É—á–∞—Å—Ç—ñ —É —Å–ø–æ—Ä—Ç–∏–≤–Ω–∏—Ö –∑–º–∞–≥–∞–Ω–Ω—è—Ö. –ù–∞–≤—á–∞–Ω–Ω—è —Ç–∞ —Ä–æ–∑–≤–∏—Ç–æ–∫ Business —ñ Management School –¥–ª—è —Å–ø—ñ–≤—Ä–æ–±—ñ—Ç–Ω–∏–∫—ñ–≤ –∫–æ–º–ø–∞–Ω—ñ—ó. –í–µ–ª–∏–∫–∞ –µ–ª–µ–∫—Ç—Ä–æ–Ω–Ω–∞ –±—ñ–±–ª—ñ–æ—Ç–µ–∫–∞ —Ç–∞ –¥–æ—Å—Ç—É–ø –¥–æ –ø–ª–∞—Ç–Ω–∏—Ö –æ–Ω–ª–∞–π–Ω-–∫—É—Ä—Å—ñ–≤ —ñ –∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü—ñ–π, –≤–Ω—É—Ç—Ä—ñ—à–Ω—ñ –±–µ—Å—ñ–¥–∏ —ñ –≤–æ—Ä–∫—à–æ–ø–∏, –∫—É—Ä—Å–∏ –∞–Ω–≥–ª—ñ–π—Å—å–∫–æ—ó —ñ —Ä–æ–∑–º–æ–≤–Ω–∏–π –∫–ª—É–±. –ö–æ–º–ø–µ–Ω—Å–∞—Ü—ñ—è –¥–æ–¥–∞—Ç–∫–æ–≤–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è –Ω–∞ –∑–æ–≤–Ω—ñ—à–Ω—ñ—Ö —Ç—Ä–µ–Ω—ñ–Ω–≥–∞—Ö —ñ —Å–µ–º—ñ–Ω–∞—Ä–∞—Ö. –ì—É—á–Ω—ñ –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–∏ –î–≤–∞ —Ä–∞–∑–∏ –Ω–∞ —Ä—ñ–∫ ‚Äî –≤–ª—ñ—Ç–∫—É —ñ –≤–∑–∏–º–∫—É ‚Äî –º–∏ –ø—Ä–æ–≤–æ–¥–∏–º–æ –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–∏ –∑ —à–∞–ª–µ–Ω–∏–º –ª–∞–π–Ω–∞–ø–æ–º —É –∫—Ä—É—Ç–∏—Ö –ª–æ–∫–∞—Ü—ñ—è—Ö. –©–æ–º—ñ—Å—è—Ü—è –æ—Ñ—ñ—Å–∏ –∑—É—Å—Ç—Ä—ñ—á–∞—é—Ç—å—Å—è –Ω–∞ –ª–æ–∫–∞–ª—å–Ω—ñ —Ç—É—Å–æ–≤–∫–∏ —Ç–∞ –≤–∏—ó–∑–Ω—ñ –∑–∞—Ö–æ–¥–∏, –∞ –ø—ñ—Å–ª—è –∫–≤–∞—Ä—Ç–∞–ª—å–Ω–æ–≥–æ –∑–≤—ñ—Ç—É –¥–ª—è –≤—Å—ñ—î—ó –∫–æ–º–∞–Ω–¥–∏ –≤–ª–∞—à—Ç–æ–≤—É—é—Ç—å –≤–µ—á—ñ—Ä–∫—É. Genesis ‚Äî —Ü–µ –Ω–µ –ø—Ä–æ—Å—Ç–æ –∫–æ–º–ø–∞–Ω—ñ—è, –∞ –ø—ñ–¥–ø—Ä–∏—î–º–Ω–∏—Ü—å–∫–∞ –µ–∫–æ—Å–∏—Å—Ç–µ–º–∞, –¥–µ –∫–æ–∂–µ–Ω —Å–ø—ñ–≤—Ä–æ–±—ñ—Ç–Ω–∏–∫ –º–æ–∂–µ –≤—Ç—ñ–ª–∏—Ç–∏ —Å–≤–æ—ó —ñ–¥–µ—ó –≤ —Ä–µ–∞–ª—å–Ω—ñ—Å—Ç—å. –ú–∞–π–∂–µ –≤—Å—ñ –∫–µ—Ä—ñ–≤–Ω–∏–∫–∏ Genesis –≤–∏—Ä–æ—Å–ª–∏ –≤—Å–µ—Ä–µ–¥–∏–Ω—ñ –∫–æ–º–ø–∞–Ω—ñ—ó —ñ –ø–æ—á–∞–ª–∏ –∫–µ—Ä—É–≤–∞—Ç–∏ —Ü—ñ–ª–∏–º–∏ –ø—Ä–æ–µ–∫—Ç–∞–º–∏ —ñ –Ω–∞–ø—Ä—è–º–∫–∞–º–∏ –±—ñ–∑–Ω–µ—Å—É —É 22-27 —Ä–æ–∫—ñ–≤. –ú–∏ —ñ–Ω–≤–µ—Å—Ç—É—î–º–æ —Ç–∞ –ø—ñ–¥—Ç—Ä–∏–º—É—î–º–æ —ñ–Ω—ñ—Ü—ñ–∞—Ç–∏–≤–∏ —Å–≤–æ—ó—Ö —Å–ø—ñ–≤—Ä–æ–±—ñ—Ç–Ω–∏–∫—ñ–≤. –ó–∞—Ä–∞–∑ 70% –≤–∏—Ä—É—á–∫–∏ Genesis –ø—Ä–∏–Ω–æ—Å—è—Ç—å —Ç—ñ –ø—Ä–æ—î–∫—Ç–∏, —è–∫–∏—Ö –ø‚Äô—è—Ç—å —Ä–æ–∫—ñ–≤ —Ç–æ–º—É —â–µ –ø—Ä–æ—Å—Ç–æ –Ω–µ –±—É–ª–æ. –ó–∞–ª–∏—à —Å–≤–æ—î —Ä–µ–∑—é–º–µ —ñ –∑—Ä–æ–±–∏ –∫—Ä–æ–∫ –Ω–∞ –∑—É—Å—Ç—Ä—ñ—á –Ω–æ–≤–∏–º –º–æ–∂–ª–∏–≤–æ—Å—Ç—è–º!",Data Analyst (Keiki)@Genesis,https://jobs.dou.ua/companies/genesis-technology-partners/vacancies/134428/, Kyiv,Data Analyst (Keiki),07 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/gismart_com/,Gismart,"{""Required skills"": [""Responsibilities"", ""Develop data pipelines for internal DWH"", ""Create integrations for 3rd party systems"", ""Work closely with the BI/DWH team""], ""Project description"": [""Requirements"", ""At least 2 years of commercial development experience with Python"", ""Excellent knowledge of database concepts and practical experience in SQL"", ""Experience with Docker"", ""Good knowledge of Linux / shell scripting""]}",,"Required skills Responsibilities‚Äî Develop data pipelines for internal DWH‚Äî Create integrations for 3rd party systems‚Äî Work closely with the BI/DWH team Requirements‚Äî At least 2 years of commercial development experience with Python‚Äî Excellent knowledge of database concepts and practical experience in SQL‚Äî Experience with Docker‚Äî Good knowledge of Linux / shell scripting Will be a plus‚Äî Knowledge of Apache Airflow, Apache Kafka, Amazon Redshift‚Äî Experience in other programming/scripting languages, such as JS, Go, etc. Benefits&Perks‚Äî Medical Insurance (100% sick leave coverage).‚Äî Paid vacation (18 working days per year, all national holidays, and 3 personal days).‚Äî Educational possibilities and free corporate English classes.‚Äî Breakfast in the office.‚Äî Gifting for major life events.‚Äî PE accounting and support.‚Äî Trip abroad with all the Gismart employees once a year.‚Äî –°ompensation for sports activities. Project description Gismart is a leading developer and publisher of mobile games and entertainment-focused apps with over 500 million downloads. The company operates within three primary focus areas ‚Äî hyper-casual games, third-party game publishing, and entertainment apps. Gismart‚Äôs top titles include Cool Goal!, Domino Smash, Cleon, Oil Tycoon, Happy Hockey!, Physics Puzzle Idle, Beat Maker Go, Piano Crush, DJ it!, WeDrum, and others. Today, Gismart unites over 300 professionals. The company is based in London, Minsk, Beijing, and Kyiv. In 2020, the Financial Times named Gismart the fastest-growing company in the games industry placing the company sixth overall out of 1000 European companies. For more information on Gismart, visit gismart.com.",Data Engineer@Gismart,https://jobs.dou.ua/companies/gismart_com/vacancies/134402/, Kyiv,Data Engineer,07 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/everymatrix-ltd/,EveryMatrix LTD,"{""Required skills"": [""Requirements:""], ""Responsibilities"": [""\u00b7 Minimum 3 years experience in a similar role within the online sports betting industry;""], ""Project description"": [""\u00b7 Extensive experience with odds models from a leading sportsbook;""]}",,"Required skills Requirements: ¬∑ Minimum 3 years experience in a similar role within the online sports betting industry; ¬∑ Extensive experience with odds models from a leading sportsbook; ¬∑ A master‚Äôs degree in computer science, mathematics, statistics or a related field is required (PhD in Mathematics or Statistics would be considered a huge plus); ¬∑ Applied math / statistics in other industries (e.g. Finance) is an asset; ¬∑ Excellent understanding of probability and statistics; ¬∑ Excellent knowledge of BI tools, ETL, Data structure, Data analysis; ¬∑ Experience with analytical platform such as: Matlab, R, Python etc; ¬∑ Excellent SQL skills; ¬∑ Experienced in sports data mining and sports analytics; ¬∑ Development experience in one of the languages: C, C++, #C, JAVA would be a plus; ¬∑ Proficiency in English. ¬∑ Strong leadership skills with ability to achieve results by working through others; ¬∑ Ability to understand business challenges and formalise them into appropriate analytics and modelling requirements . ¬∑ Delivery and results focused with a problem solving attitude; ¬∑ Very good at organizing and prioritizing tasks Responsibilities Responsibilities: ¬∑ Design, build and maintain derivative pricing data models, live and pre-live, across a broad range of sports; ¬∑ Improve the Risk Management model by implementing new numeric/quantitative techniques; ¬∑ Analyze business trends to identify scope for future product development; ¬∑ Provide reporting and performance monitoring of our products; ¬∑ Define and implement strategy for collecting, reporting and analyzing data in order to improve decisions relating to our sportsbook; ¬∑ Supporting other teams who are integrating or testing the sports models. ¬∑ Ensure data quality and availability; ¬∑ Ensure distribution of knowledge and be available to team members to assist with any issues they may have. ¬∑ Learn the touchpoints where changes within a model may have an impact on other business areas. ¬∑ Understand, identify and mitigate any risks in delivery of work. Project description We‚Äôre looking to add a skilled, positive, and energetic Head of Quantitative Analysis to join the ever-growing sports betting operations. The candidate will have excellent analytic skills and the business vision to provide the data models and insights that will drive our strategy. He will work closely with the Product Owner and Head of Trading. The Quants team is new and will be growing.",Head of¬†Quantitative Analysis@EveryMatrix LTD,https://jobs.dou.ua/companies/everymatrix-ltd/vacancies/134395/," Lviv, remote",Head of¬†Quantitative Analysis,07 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/qublix/,Qublix,"{""Required skills"": [""\u25cf BA/BS degree in Statistics, Math, Computer Science, or other quantitative \u03d0ield;\u25cf 2-5 years of experience as a data scientist or in a similar analytical role\u25cf Experience with relational databases (PostgreSQL/MySQL)\u25cf Python knowledge is a plus\u25cf Strong knowledge of experimental design, data mining, predictive modeling, and statisticalanalysis\u25cf Experience working with large datasets\u25cf Excellent communication, presentation, and visual reporting skills\u25cf Ability to apply business strategy to data analysis and recommendations\u25cf Loves problem-solving, has an innate ability to \u03d0ind the optimal solution\u25cf Desire to work independently on a project from inception to completion\u25cf Experience with Amazon cloud (Redshift, S3, RDS, EC2) is a plus\u25cf Ability to work under pressure on a variety of projects""], ""We offer"": [""Qublix is a Toronto based mobile gaming company building high quality games for iOS, Google Play, Amazon, and Facebook. Qublix published over 50 tites across mobile and web since 2009. At Qublix, you\u2019ll be part of a startup culture where every employee takes pride in their work and where the impact of their efforts is felt.""], ""Responsibilities"": [""Qublix is looking for an experienced Data Scientist to manage and evolve its mobile gaming data infrastructure. The Data Scientist will leverage billions of data points to answer key business questions, tell compelling stories, and develop meaningful insights into user behavior and engagement. This person will design and conduct analysis to provide clear insights to complex questions and provide recommendations that shape future game development and company initiatives. The ideal candidate is intelligent, sharp, detail-oriented, and highly quantitative individual with a passion for mobile gaming, data modeling, and analysis.""]}",,"Required skills ‚óè BA/BS degree in Statistics, Math, Computer Science, or other quantitative œêield;‚óè 2-5 years of experience as a data scientist or in a similar analytical role‚óè Experience with relational databases (PostgreSQL/MySQL)‚óè Python knowledge is a plus‚óè Strong knowledge of experimental design, data mining, predictive modeling, and statisticalanalysis‚óè Experience working with large datasets‚óè Excellent communication, presentation, and visual reporting skills‚óè Ability to apply business strategy to data analysis and recommendations‚óè Loves problem-solving, has an innate ability to œêind the optimal solution‚óè Desire to work independently on a project from inception to completion‚óè Experience with Amazon cloud (Redshift, S3, RDS, EC2) is a plus‚óè Ability to work under pressure on a variety of projects We offer Qublix is a Toronto based mobile gaming company building high quality games for iOS, Google Play, Amazon, and Facebook. Qublix published over 50 tites across mobile and web since 2009. At Qublix, you‚Äôll be part of a startup culture where every employee takes pride in their work and where the impact of their efforts is felt. Qublix is looking for an experienced Data Scientist to manage and evolve its mobile gaming data infrastructure. The Data Scientist will leverage billions of data points to answer key business questions, tell compelling stories, and develop meaningful insights into user behavior and engagement. This person will design and conduct analysis to provide clear insights to complex questions and provide recommendations that shape future game development and company initiatives. The ideal candidate is intelligent, sharp, detail-oriented, and highly quantitative individual with a passion for mobile gaming, data modeling, and analysis. Responsibilities ‚óè Create thoughtful and elegant analysis to monitor game performance, understand userbehavior, and provide speciœêic answers to key questions‚óè Develop algorithms, predictive models, and new ways to visualize data ‚óè Collaborate with backend developers on data collection and delivery pipelines speciœêications‚óè Develop and maintain reporting and performance monitoring‚óè Lead the research and development of new tools and processes for data analysis‚óè Collaborate with product, design, and marketing teams to understand key businessquestions and analysis needs‚óè Design and document tracking speciœêications for core KPIs, feature additions, andexperimentation‚óè Work closely with production and engineering teams on tracking and implementation‚Äî Collaborate with project managers to establish requirements for A/B testing- Use SQL/Excel to analyze data",Data Analyst@Qublix,https://jobs.dou.ua/companies/qublix/vacancies/128962/, remote,Data Analyst,07 October 2020,–æ—Ç¬†$1500,2020-10-13,,dou
https://jobs.dou.ua/companies/rakuten/,Rakuten,{},,"Rakuten Americas is looking for a talented Senior Data Scientist to join our amazing Machine Learning team in Ukraine. Rakuten Americas is a global leader in internet services, empowering individuals, communities, businesses and society. Our 20+ businesses span e-commerce, digital content, communications and data analytics, bringing the joy of discovery to millions of members around the world. We‚Äôve brought together a unique set of internet services that are changing the way retailers and marketers do business. With businesses such as Rakuten Marketing, Rakuten Rewards and Rakuten Intelligence, we can offer marketers the platform and data intelligence to reach consumers at the moment they want to make a purchase. Our Data Science group consists of ~40 Data Scientists and Engineers, part of them based in Ukraine. We apply SOTA and classical machine learning techniques to Rakuten data to improve the product, internal processes and personalization for users, which in turn improves profitability of Rakuten business units. Our products are composed of many microservices. We own the products we deliver and maintain them. We follow DevOps approach when developers are responsible for code delivery with help of existing tools created by a dedicated Infra team. As a team is distributed, you can work remotely or from our Odessa/Kiev offices. We cover english classes, health insurance, sport activities, certifications/trainings and participation in conferences. ‚Ä¢BS or MS in a technical field ‚Ä¢5+ years of experience in machine learning with a minimum of 3 years experience in at least one of next domains: NLP, Recommendation Systems, Predictive Analytics, Knowledge graphs ‚Ä¢Expertise in Python and SQL. ‚Ä¢Strong CS fundamentals, such as algorithms and data structures ‚Ä¢Experience with cloud computing stacks such as Amazon Web Services preferred ‚Ä¢Experience in e-commerce field preferred ‚Ä¢Excellent written and verbal communication skills ‚Ä¢Enthusiasm for working hard and having fun in a dynamic environment ‚Ä¢Research and experiment with different machine learning algorithms and techniques to solve business problems ‚Ä¢Conduct design and code reviews ‚Ä¢Work with engineers to make sure the engines scale well on high volumes of data ‚Ä¢Work with our junior data scientists in Odessa to support their development and growth ‚Ä¢Collaborate with our remote data scientists and engineers based in the United States or Taiwan.",Senior Data Scientist@Rakuten,https://jobs.dou.ua/companies/rakuten/vacancies/129699/," Odesa, remote",Senior Data Scientist,07 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/rakuten/,Rakuten,{},,"Rakuten Americas is looking for a talented Senior Software Engineer to join our amazing Machine Learning team in Ukraine. Rakuten Americas is a global leader in internet services, empowering individuals, communities, businesses and society. Our 20+ businesses span e-commerce, digital content, communications and data analytics, bringing the joy of discovery to millions of members around the world. We‚Äôve brought together a unique set of internet services that are changing the way retailers and marketers do business. With businesses such as Rakuten Marketing, Rakuten Rewards and Rakuten Intelligence, we can offer marketers the platform and data intelligence to reach consumers at the moment they want to make a purchase. Our Data Science group consists of ~40 Data Scientists and Engineers, part of them based in Ukraine. We apply SOTA and classical machine learning techniques to Rakuten data to improve the product, internal processes and personalization for users, which in turn improves profitability of Rakuten business units. Our products are composed of many microservices. We own the products we deliver and maintain them. We follow DevOps approach when developers are responsible for code delivery with help of existing tools created by a dedicated Infra team. As a team is distributed, you can work remotely or from our Odessa/Kiev offices. We cover english classes, health insurance, sport activities, certifications/trainings and participation in conferences. ‚Ä¢BSc in Computer Science or a related field ‚Ä¢5+ years of experience in the design and development of software with a minimum of 3 years experience in Python development building large-scale data/ml applications. ‚Ä¢Expertise in developing microservice architectures and RESTful services ‚Ä¢Experience with distributed systems and big data architectures, including SQL and NoSQL databases (e.g., MySQL, MongoDB, Redis), and ETL processes and tools ‚Ä¢Strong CS fundamentals, such as algorithms and data structures ‚Ä¢Experience with DevOps: Docker, Kubernetes, CI/CD (Jenkins) ‚Ä¢Interest in and familiarity with Machine Learning and Python‚Äôs data analysis libraries and frameworks (e.g., numpy, scikit-learn, pandas, pytorch) preferred ‚Ä¢Experience with cloud computing stacks such as Amazon Web Services preferred ‚Ä¢Knowledge of streaming platforms (Kafka, Kinesis, Pulsar) will be a plus ‚Ä¢Experience with Apache Airflow will be a plus ‚Ä¢Excellent written and verbal communication skills ‚Ä¢Enthusiasm for working hard and having fun in a dynamic environment ‚Ä¢Productionize Machine Learning solutions ‚Ä¢Conduct/participate in design and code reviews, identify potential problems and performance issues, evaluate code compliance with standards and best practices, provide feedback ‚Ä¢Analyze complex functions, procedures, and problems to deliver creative, logical, and effective solutions that meet the specified requirements ‚Ä¢Make sure the machine learning engines scale well on high volumes of data ‚Ä¢Coach teammates on delivering high-quality software using modern software development practices (TDD, Pair programming, CICD, etc) ‚Ä¢Collaborate with our remote data scientists and engineers based in the United States or Taiwan.",Senior Software Engineer in¬†Machine Learning@Rakuten,https://jobs.dou.ua/companies/rakuten/vacancies/129698/," Odesa, remote",Senior Software Engineer in¬†Machine Learning,07 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/kyivstar/,Kyivstar,"{""Required skills"": [""- 7+ years of success in consultative/complex technical sales and deployment projects, architecture, design, implementation, and/or support of highly distributed applications required- 5+ years experience in Data Platforms (RDBMS, noSQL), Analytics (SQL including postgre, Azure SQL, Power BI), Artificial Intelligence/Machine Learning (Spark, Azure ML, Azure Databricks)- Enterprise with cloud and hybrid infrastructures, preparing complex architecture designs, leading database migrations, and technology management required- Expertise in data estate workloads like HDInsight, Hadoop, Cloudera, Spark- Strong executive presence including communication and presentation skills with a high degree of comfort to large and small technical audiences- Strong partner relationship management skills- Data Security knowledge, data masking and data encryption""], ""As a plus"": [""- Knowledge of cloud development platforms preferred""], ""We offer"": [""- A unique experience of working the most customers beloved and largest mobile operator in Ukraine.- Real opportunity to ship digital products to millions of customers- To contribute into building the biggest analytical cloud environment in Ukraine- To create Big Data/AI products, changing the whole industry and influencing Ukraine- To be involved in real Big Data projects with Petabytes of data and Billions of events daily processed in Real-time. - A competitive salary.- Great possibilities for professional development and career growth.- Medical insurance.- Life insurance- Friendly & Collaborative Environment""], ""Responsibilities"": [""- Understand customers\u2019 overall data estate, IT and business priorities and success measures to design implementation architectures and solutions.- Apply technical knowledge to architect solutions that meet business and IT needs, create Data Platform, and ensure long term technical viability of new deployments, infusing key analytics and AI technologies - Ensure that solution exhibits high levels of performance, security, scalability, maintainability, appropriate reusability and reliability upon deployment- Collaborate with other Cloud Solution Architects in developing complex end-to-end Enterprise solutions on Microsoft Azure platform, Maintain technical skills and knowledge, keeping up to date with market trends and competitive insights; collaborate and share with the technical community while educate customers on Azure platform""], ""Project description"": [""We are innovative Cloud Big Data & Artificial Intelligence Center of Excellence in Ukraine, and the main target for us is to design, deliver and implement the cloud-based data management and BI platform across the Business using Big Data, IoT, Machine Learning, and AI Cloud technologies. We help our Clients to get deep understanding of Cloud benefits by providing the services like: client infrastructure discovery, preparing Future proof Architecture and Cloud transformation program, building cloud-based data Platform and High-grade data visualizations. We use the best worldwide practice to be a fully compliance with Data Security and Customer\u2019s legal requirements.""]}",,"Required skills - 7+ years of success in consultative/complex technical sales and deployment projects, architecture, design, implementation, and/or support of highly distributed applications required- 5+ years experience in Data Platforms (RDBMS, noSQL), Analytics (SQL including postgre, Azure SQL, Power BI), Artificial Intelligence/Machine Learning (Spark, Azure ML, Azure Databricks)- Enterprise with cloud and hybrid infrastructures, preparing complex architecture designs, leading database migrations, and technology management required- Expertise in data estate workloads like HDInsight, Hadoop, Cloudera, Spark- Strong executive presence including communication and presentation skills with a high degree of comfort to large and small technical audiences- Strong partner relationship management skills- Data Security knowledge, data masking and data encryption As a plus - Knowledge of cloud development platforms preferred We offer - A unique experience of working the most customers beloved and largest mobile operator in Ukraine.- Real opportunity to ship digital products to millions of customers- To contribute into building the biggest analytical cloud environment in Ukraine- To create Big Data/AI products, changing the whole industry and influencing Ukraine- To be involved in real Big Data projects with Petabytes of data and Billions of events daily processed in Real-time. - A competitive salary.- Great possibilities for professional development and career growth.- Medical insurance.- Life insurance- Friendly & Collaborative Environment Responsibilities - Understand customers‚Äô overall data estate, IT and business priorities and success measures to design implementation architectures and solutions.- Apply technical knowledge to architect solutions that meet business and IT needs, create Data Platform, and ensure long term technical viability of new deployments, infusing key analytics and AI technologies - Ensure that solution exhibits high levels of performance, security, scalability, maintainability, appropriate reusability and reliability upon deployment- Collaborate with other Cloud Solution Architects in developing complex end-to-end Enterprise solutions on Microsoft Azure platform, Maintain technical skills and knowledge, keeping up to date with market trends and competitive insights; collaborate and share with the technical community while educate customers on Azure platform Project description We are innovative Cloud Big Data & Artificial Intelligence Center of Excellence in Ukraine, and the main target for us is to design, deliver and implement the cloud-based data management and BI platform across the Business using Big Data, IoT, Machine Learning, and AI Cloud technologies. We help our Clients to get deep understanding of Cloud benefits by providing the services like: client infrastructure discovery, preparing Future proof Architecture and Cloud transformation program, building cloud-based data Platform and High-grade data visualizations. We use the best worldwide practice to be a fully compliance with Data Security and Customer‚Äôs legal requirements. Our clients are more than 100 biggest Ukrainian Companies, and our team successfully delivered over 50 analytical products every month. Joining our team you will get a unique experience to deep dive and drive development team within high priority customer‚Äôs BigData and digital transformation Projects using Microsoft Azure Services in collaboration with the best Microsoft team players",Data Architect@Kyivstar,https://jobs.dou.ua/companies/kyivstar/vacancies/125708/, Kyiv,Data Architect,07 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/proxet/,Proxet,"{""Required skills"": [""Develop and maintain Roku\u2019s cutting edge advertising planning, delivery, and insights products/solutions.You\u2019ll be working on the next generation of Roku\u2019s DMM (Digital Marketing Management) platform based on a real-time bidding solution with high-load up to 3M QPS used by leading advertisers to manage their online ad campaigns across all media channels, device platforms, and advertising exchanges.You\u2019ll become part of a distributed team developing a product that is used by thousands of businesses worldwide.""], ""As a plus"": [""Skills & Experience"", ""Background in computer science or similar quantitative field"", ""5+ years of professional software development experience"", ""Expert Knowledge of Java"", ""Proficiency in writing efficient SQL"", ""Experience with data frameworks like Spark SQL, Spark Streaming etc"", ""Experience developing high scale and high performance distributed systems for real-time data processing"", ""Product-focused mindset"", ""Team-player with strong interpersonal skills"", ""English"", ""Upper-intermediate or above""], ""We offer"": [""Experience with Big Data and AWS services is a plus"", ""Experience with Python, S\u0441ala, etc"", ""Experience in the advertising domain is a big plus""], ""Responsibilities"": [""Challenging work in an international professional environment"", ""40-hour working week with flexible working hours"", ""Flexible work-from-home policy"", ""Competitive salary"", ""PE accounting and support"", ""20 paid vacation days per year"", ""14 paid sick leave days per year"", ""Medical insurance"", ""Annual 250$ deposit for attending external events (conferences, workshops, etc.)"", ""Long-term employment and real opportunities to change roles and projects within the company"", ""Yoga classes, workout corner"", ""Collaborative and friendly team environment""], ""Project description"": [""Work with a highly skilled engineering team in all phases of the Agile development process from design to deployment"", ""Design, develop, and maintain a high scale, highly performant real-time data processing solutions."", ""Work with quality assurance, release engineering, and product management to deliver quality software"", ""Be part of a continuous improvement atmosphere, proactively suggesting improvements to the platform and development processes; anticipate problems or issues that may arise"", ""Deliver constant value back to the business in a highly agile team approaching near-continuous deployment""]}",,"Required skills Develop and maintain Roku‚Äôs cutting edge advertising planning, delivery, and insights products/solutions.You‚Äôll be working on the next generation of Roku‚Äôs DMM (Digital Marketing Management) platform based on a real-time bidding solution with high-load up to 3M QPS used by leading advertisers to manage their online ad campaigns across all media channels, device platforms, and advertising exchanges.You‚Äôll become part of a distributed team developing a product that is used by thousands of businesses worldwide. Skills & Experience‚Äî Background in computer science or similar quantitative field‚Äî 5+ years of professional software development experience‚Äî Expert Knowledge of Java‚Äî Proficiency in writing efficient SQL‚Äî Experience with data frameworks like Spark SQL, Spark Streaming etc‚Äî Experience developing high scale and high performance distributed systems for real-time data processing‚Äî Product-focused mindset‚Äî Team-player with strong interpersonal skills‚Äî English ‚Äî Upper-intermediate or above As a plus ‚Äî Experience with Big Data and AWS services is a plus‚Äî Experience with Python, S—Åala, etc‚Äî Experience in the advertising domain is a big plus We offer ‚Äî Challenging work in an international professional environment‚Äî 40-hour working week with flexible working hours‚Äî Flexible work-from-home policy‚Äî Competitive salary‚Äî PE accounting and support‚Äî 20 paid vacation days per year‚Äî 14 paid sick leave days per year‚Äî Medical insurance‚Äî Annual 250$ deposit for attending external events (conferences, workshops, etc.)‚Äî Long-term employment and real opportunities to change roles and projects within the company‚Äî Yoga classes, workout corner‚Äî Collaborative and friendly team environment Responsibilities ‚Äî Work with a highly skilled engineering team in all phases of the Agile development process from design to deployment‚Äî Design, develop, and maintain a high scale, highly performant real-time data processing solutions.‚Äî Work with quality assurance, release engineering, and product management to deliver quality software‚Äî Be part of a continuous improvement atmosphere, proactively suggesting improvements to the platform and development processes; anticipate problems or issues that may arise‚Äî Deliver constant value back to the business in a highly agile team approaching near-continuous deployment Project description The project: www.roku.com",Java Engineer with Data Engineering skills@Proxet,https://jobs.dou.ua/companies/proxet/vacancies/134338/," Kyiv, remote",Java Engineer with Data Engineering skills,07 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/lohika-systems/,Lohika,{},,"PURPOSE OF THE JOB:Lohika company provides premium software engineering services to leading technology companies. Our customers usually range from startup to high growth and VC backed companies, which drives a culture of acceleration and innovation. We are sure that team extension is the only engagement model, which works best. Our customer is one of the biggest fashion accessories retailers. Focused on design, confection, manufacturing, as well as distribution and retail. Multi-branded and fast-expanding with a remarkable presence globally. The main idea is to bring customers closer to the products they need.Currently we are looking for a team leader for a small team to migrate operational data from regional DWHs into global worldwide representation. MAIN TASKS AND RESPONSIBILITIES:‚Ä¢ Creating and maintaining existing ETL processes‚Ä¢ Transform raw data (RDBMS, NoSQL) and raw events (Kafka, MQ) into the data that business users can use‚Ä¢ Performing end-to-end data flows checks‚Ä¢ Optimizing existing data processes, improving the performance and monitoring of existing solutions‚Ä¢ Designing DB/DWH schema according to data processing needs‚Ä¢ Create new views and improve reporting dashboard‚Ä¢ Participate in daily meetings, technical discussions and regular planning sessions‚Ä¢ Work in a close contact with team customer, provide technical solutions add value to the product‚Ä¢ Troubleshooting problems, maintain integration points (changes in kafka topics, etc)‚Ä¢ Manage team, provide day-to-day team and tech leadership EDUCATION, SKILLS AND EXPERIENCE:‚Ä¢ Experience in datawarehousing and multi-dimensional database design and development using formal methodologies; ‚Ä¢ Strong knowledge of SQL (and be passionate about it)‚Ä¢ Strong knowledge of 1 of RDBMS (MS SQL, MySQL/MariaDB, Oracle);‚Ä¢ Experience with SQL optimization and performance tuning ‚Ä¢ Experience with Snowflake (or analogs: Presto, DataBricks, BigQuery, RedShift)‚Ä¢ Experience in building ETL processes, using ETL tools (Oracle Data Integrator, Talend, SSIS, Pentaho)‚Ä¢ Experience with event processing (Kafka, IBM MQ)‚Ä¢ Good team player‚Ä¢ Upper-intermediate English level‚Ä¢ Ability to focus on producing results WOULD BE A PLUS:‚Ä¢ Experience with BI/Reporting tools (Domo, Tableau, Looker, etc) ‚Ä¢ Hands-on experience with 1 of major cloud service (Azure, GCP)‚Ä¢ Java development experience‚Ä¢ Basic Linux/Unix skills (user level)‚Ä¢ Understanding CI/CD pipelines (Git/Jenkins) LOHIKA BENEFITS:‚Ä¢ Friendly and highly professional teams‚Ä¢ Flexible working hours with no overtime‚Ä¢ Regular performance reviews‚Ä¢ Internal training‚Ä¢ Comfortable office facilities (kitchens, gym, sports activities, yoga, lounge rooms, coffee machines, etc.)‚Ä¢ Christmas holidays (31st December ‚Äî7th January) and state holidays‚Ä¢ Fully paid English classes (twice per week) with own English teachers and native speakers‚Ä¢ Premium Medical insurance (medication, massage, and doctor in the office, etc.)‚Ä¢ Paid sick-leaves‚Ä¢ Life insurance‚Ä¢ 20 working days of annual paid vacation‚Ä¢ Incentives (marriage, childbirth)‚Ä¢ Corporate events (corporate parties and sports competitions) And much more! Please send your CV or contact us with more questions!",Lead Data Engineer #7731@Lohika,https://jobs.dou.ua/companies/lohika-systems/vacancies/134321/, Lviv,Lead Data Engineer #7731,07 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/brainstack/,Brainstack_,"{""Required skills"": [""\u0422\u0435\u0445\u043d\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u0438\u043b\u0438 \u044d\u043a\u043e\u043d\u043e\u043c\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u0435."", ""2+ \u043b\u0435\u0442 \u043f\u0440\u043e\u0444\u0435\u0441\u0441\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e\u0433\u043e \u043e\u043f\u044b\u0442\u0430 \u0432 \u0440\u043e\u043b\u0438 \u0430\u043d\u0430\u043b\u0438\u0442\u0438\u043a\u0430."", ""\u0425\u043e\u0440\u043e\u0448\u0438\u0435 \u0437\u043d\u0430\u043d\u0438\u044f Python/R, SQL (MySQL, PostgreSQL)."", ""\u0425\u043e\u0440\u043e\u0448\u0438\u0439 \u0442\u0435\u043e\u0440\u0435\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u0431\u0435\u043a\u0433\u0440\u0430\u0443\u043d\u0434 \u0432 \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u043a\u0435, \u0442\u0435\u0441\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0438 \u0433\u0438\u043f\u043e\u0442\u0435\u0437 \u0438 \u0437\u043d\u0430\u043d\u0438\u044f \u0432 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u043e\u043d\u043d\u043e\u043c \u0430\u043d\u0430\u043b\u0438\u0437\u0435 (Logistic Regression, \u0438 \u0442.\u043f.)"", ""\u041e\u043f\u044b\u0442 \u0440\u0430\u0431\u043e\u0442\u044b \u0441 \u0445\u0440\u0430\u043d\u0438\u043b\u0438\u0449\u0430\u043c\u0438 \u0434\u0430\u043d\u043d\u044b\u0445 \u0438 \u0438\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442\u0430\u043c\u0438 \u0432\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438."", ""\u0417\u043d\u0430\u043d\u0438\u0435 web \u0430\u043d\u0430\u043b\u0438\u0442\u0438\u043a\u0438"", ""\u0412\u044b\u0441\u043e\u043a\u0438\u0439 \u0443\u0440\u043e\u0432\u0435\u043d\u044c \u0432\u043b\u0430\u0434\u0435\u043d\u0438\u044f Excel (\u0441\u0432\u043e\u0434\u043d\u044b\u0435 \u0442\u0430\u0431\u043b\u0438\u0446\u044b, \u043f\u043e\u0434\u043a\u043b\u044e\u0447\u0435\u043d\u0438\u0435 \u043a \u0432\u043d\u0435\u0448\u043d\u0438\u043c \u0438\u0441\u0442\u043e\u0447\u043d\u0438\u043a\u0430\u043c)."", ""\u0410\u043d\u0430\u043b\u0438\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u043c\u044b\u0448\u043b\u0435\u043d\u0438\u0435, \u0441\u043f\u043e\u0441\u043e\u0431\u043d\u043e\u0441\u0442\u044c \u043d\u0430\u0445\u043e\u0434\u0438\u0442\u044c \u0437\u0430\u043a\u043e\u043d\u043e\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u044c \u0432 \u0438\u0437\u0443\u0447\u0435\u043d\u043d\u044b\u0445 \u043d\u0430\u0431\u043e\u0440\u0430\u0445 \u0434\u0430\u043d\u043d\u044b\u0445, \u043f\u0440\u0438\u0447\u0438\u043d\u044b \u043e\u0442\u043a\u043b\u043e\u043d\u0435\u043d\u0438\u0439 \u043f\u043e\u043a\u0430\u0437\u0430\u0442\u0435\u043b\u0435\u0439, \u043e\u043f\u0438\u0441\u044b\u0432\u0430\u044e\u0449\u0438\u0445 \u0431\u0438\u0437\u043d\u0435\u0441-\u043f\u0440\u043e\u0446\u0435\u0441\u0441\u044b, \u043a\u0440\u0430\u0442\u043a\u043e \u0438 \u0442\u043e\u0447\u043d\u043e \u0430\u043a\u0446\u0435\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0432\u043d\u0438\u043c\u0430\u043d\u0438\u0435 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u043d\u044b\u0445 \u043f\u0440\u0438\u0447\u0438\u043d\u0430\u0445 \u043e\u0442\u043a\u043b\u043e\u043d\u0435\u043d\u0438\u0439."", ""\u0417\u043d\u0430\u043d\u0438\u0435 \u0430\u043d\u0433\u043b\u0438\u0439\u0441\u043a\u043e\u0433\u043e \u043d\u0430 \u0443\u0440\u043e\u0432\u043d\u0435 Pre-Intermediate \u0438\u043b\u0438 \u0432\u044b\u0448\u0435""], ""As a plus"": [""\u0425\u043e\u0440\u043e\u0448\u0435\u0435 \u043f\u043e\u043d\u0438\u043c\u0430\u043d\u0438\u0435 digital \u043c\u0430\u0440\u043a\u0435\u0442\u0438\u043d\u0433\u0430, \u043f\u043e\u043d\u0438\u043c\u0430\u043d\u0438\u0435 \u043e\u043d\u043b\u0430\u0439\u043d \u0431\u0438\u0437\u043d\u0435\u0441\u0430, \u0437\u043d\u0430\u043d\u0438\u0435 \u0435\u0433\u043e \u0441\u043f\u0435\u0446\u0438\u0444\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u043f\u043e\u043a\u0430\u0437\u0430\u0442\u0435\u043b\u0435\u0439"", ""\u041e\u043f\u044b\u0442 \u0432 \u043f\u0440\u043e\u0433\u043d\u043e\u0437\u043d\u043e\u0439 \u0430\u043d\u0430\u043b\u0438\u0442\u0438\u043a\u0435 (\u0430\u043d\u0430\u043b\u0438\u0437 \u0438 \u043f\u0440\u043e\u0433\u043d\u043e\u0437\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0445 \u0440\u044f\u0434\u043e\u0432, \u0430\u043d\u0430\u043b\u0438\u0437 \u0432\u044b\u0436\u0438\u0432\u0430\u0435\u043c\u043e\u0441\u0442\u0438 \u0438 \u0442. \u0434.)"", ""\u041e\u043f\u044b\u0442 \u0441 \u0441\u0435\u0440\u0432\u0438\u0441\u0430\u043c\u0438 \u043e\u0431\u043b\u0430\u0447\u043d\u044b\u0445 \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u0439 (GCP, etc.)"", ""\u041e\u043f\u044b\u0442 \u0440\u0430\u0431\u043e\u0442\u044b \u0441 \u043f\u043b\u0430\u0442\u0435\u0436\u043d\u044b\u043c\u0438 \u0441\u0435\u0440\u0432\u0438\u0441\u0430\u043c\u0438 / \u044d\u043b\u0435\u043a\u0442\u0440\u043e\u043d\u043d\u043e\u0439 \u043a\u043e\u043c\u043c\u0435\u0440\u0446\u0438\u0435\u0439 / \u0431\u0430\u043d\u043a\u0430\u043c\u0438 / \u043f\u0440\u043e\u0432\u0430\u0439\u0434\u0435\u0440\u0430\u043c\u0438 \u043f\u043b\u0430\u0442\u0435\u0436\u0435\u0439""], ""We offer"": [""\u041c\u043d\u043e\u0433\u043e \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e\u0441\u0442\u0435\u0439 \u0434\u043b\u044f \u043f\u0440\u043e\u0444\u0435\u0441\u0441\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e\u0439 \u0440\u0435\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 \u0438 \u0440\u0430\u0437\u0432\u0438\u0442\u0438\u044f."", ""\u0414\u0440\u0443\u0436\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u0430\u044f \u0438 \u0442\u0435\u043f\u043b\u0430\u044f \u0430\u0442\u043c\u043e\u0441\u0444\u0435\u0440\u0430 \u0432 \u043a\u043e\u043b\u043b\u0435\u043a\u0442\u0438\u0432\u0435."", ""\u041e\u0444\u043e\u0440\u043c\u043b\u0435\u043d\u0438\u0435 \u0447\u0435\u0440\u0435\u0437 \u0424\u041e\u041f."", ""24 \u043a\u0430\u043b\u0435\u043d\u0434\u0430\u0440\u043d\u044b\u0445 \u0434\u043d\u044f \u043e\u0442\u043f\u0443\u0441\u043a\u0430."", ""\u041c\u0435\u0434\u0438\u0446\u0438\u043d\u0441\u043a\u0430\u044f \u0441\u0442\u0440\u0430\u0445\u043e\u0432\u043a\u0430 \u043f\u043e\u0441\u043b\u0435 \u0432\u044b\u0445\u043e\u0434\u0430 \u043d\u0430 \u0440\u0430\u0431\u043e\u0442\u0443."", ""\u041a\u043e\u043c\u043f\u0435\u043d\u0441\u0430\u0446\u0438\u044f \u0438\u0437\u0443\u0447\u0435\u043d\u0438\u044f \u0430\u043d\u0433\u043b\u0438\u0439\u0441\u043a\u043e\u0433\u043e \u044f\u0437\u044b\u043a\u0430."", ""\u041e\u043f\u043b\u0430\u0442\u0430 \u0431\u043e\u043b\u044c\u043d\u0438\u0447\u043d\u044b\u0445."", ""\u0414\u0440\u0430\u0439\u0432\u043e\u0432\u044b\u0435 \u043a\u043e\u0440\u043f\u043e\u0440\u0430\u0442\u0438\u0432\u044b \u0438 \u043c\u043d\u043e\u0433\u043e-\u043c\u043d\u043e\u0433\u043e \u0434\u0440\u0443\u0433\u0438\u0445 \u043f\u043b\u044e\u0448\u0435\u043a.""], ""Responsibilities"": [""\u0420\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u043a\u0430 Business Intelligent (\u0441\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u0430\u0432\u0442\u043e\u043c\u0430\u0442\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u043e\u0439 \u043e\u0442\u0447\u0435\u0442\u043d\u043e\u0441\u0442\u0438 \u0432 Google Data Studio \u0438 Tableau, \u0440\u0430\u0431\u043e\u0442\u0430 \u0441 \u0434\u0430\u0442\u0430-\u0438\u043d\u0436\u0435\u043d\u0435\u0440\u0430\u043c\u0438 \u0434\u043b\u044f \u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u0441\u0445\u0435\u043c\u044b DWH \u0438 \u043f\u0440\u043e\u0446\u0435\u0441\u0441\u043e\u0432 ETL)."", ""\u0410\u043d\u0430\u043b\u0438\u0437 \u0434\u0430\u043d\u043d\u044b\u0445 \u0434\u043b\u044f \u0432\u044b\u044f\u0432\u043b\u0435\u043d\u0438\u044f \u043f\u0440\u0438\u0447\u0438\u043d \u043e\u0442\u043a\u043b\u043e\u043d\u0435\u043d\u0438\u0439 \u0431\u0438\u0437\u043d\u0435\u0441-\u043c\u0435\u0442\u0440\u0438\u043a, \u0430 \u0442\u0430\u043a\u0436\u0435 \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u0438 \u0434\u0435\u044f\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438."", ""\u041e\u0446\u0435\u043d\u043a\u0430 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432 \u044d\u043a\u0441\u043f\u0435\u0440\u0438\u043c\u0435\u043d\u0442\u043e\u0432 (A/B-\u0442\u0435\u0441\u0442\u044b)."", ""\u041f\u0440\u043e\u0433\u043d\u043e\u0437\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u0444\u0438\u043d\u0430\u043d\u0441\u043e\u0432\u044b\u0445 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432."", ""\u0423\u0447\u0430\u0441\u0442\u0438\u0435 \u0432 \u043f\u0440\u043e\u0435\u043a\u0442\u0430\u0445 \u043f\u043e \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u0438 \u0431\u0438\u0437\u043d\u0435\u0441\u0430.""], ""Project description"": [""Brainstack_ is a team of intelligent, fun-loving people working on their own products that truly have value. Some of our products have already made their name In US, Europe, Central and South America, and we keep catching up with new products conquering new territories. We think that creating great products requires absolute teamwork, no matter which position you occupy: every task, every pixel, every code line makes a difference.""]}",,"Required skills ‚Äî –¢–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ –∏–ª–∏ —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–æ–µ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ.‚Äî 2+ –ª–µ—Ç –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ –æ–ø—ã—Ç–∞ –≤ —Ä–æ–ª–∏ –∞–Ω–∞–ª–∏—Ç–∏–∫–∞. ‚Äî –•–æ—Ä–æ—à–∏–µ –∑–Ω–∞–Ω–∏—è Python/R, SQL (MySQL, PostgreSQL).‚Äî –•–æ—Ä–æ—à–∏–π —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–π –±–µ–∫–≥—Ä–∞—É–Ω–¥ –≤ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–µ, —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –≥–∏–ø–æ—Ç–µ–∑ –∏ –∑–Ω–∞–Ω–∏—è –≤ —Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–æ–º –∞–Ω–∞–ª–∏–∑–µ (Logistic Regression, –∏ —Ç.–ø.)‚Äî –û–ø—ã—Ç —Ä–∞–±–æ—Ç—ã —Å —Ö—Ä–∞–Ω–∏–ª–∏—â–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö –∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏.‚Äî –ó–Ω–∞–Ω–∏–µ web –∞–Ω–∞–ª–∏—Ç–∏–∫–∏‚Äî –í—ã—Å–æ–∫–∏–π —É—Ä–æ–≤–µ–Ω—å –≤–ª–∞–¥–µ–Ω–∏—è Excel (—Å–≤–æ–¥–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã, –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –≤–Ω–µ—à–Ω–∏–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º).‚Äî –ê–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–æ–µ –º—ã—à–ª–µ–Ω–∏–µ, —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –Ω–∞—Ö–æ–¥–∏—Ç—å –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç—å –≤ –∏–∑—É—á–µ–Ω–Ω—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö, –ø—Ä–∏—á–∏–Ω—ã –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–π –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π, –æ–ø–∏—Å—ã–≤–∞—é—â–∏—Ö –±–∏–∑–Ω–µ—Å-–ø—Ä–æ—Ü–µ—Å—Å—ã, –∫—Ä–∞—Ç–∫–æ –∏ —Ç–æ—á–Ω–æ –∞–∫—Ü–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–Ω—ã—Ö –ø—Ä–∏—á–∏–Ω–∞—Ö –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–π.‚Äî –ó–Ω–∞–Ω–∏–µ –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ –Ω–∞ —É—Ä–æ–≤–Ω–µ Pre-Intermediate –∏–ª–∏ –≤—ã—à–µ As a plus ‚Äî –•–æ—Ä–æ—à–µ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ digital –º–∞—Ä–∫–µ—Ç–∏–Ω–≥–∞, –ø–æ–Ω–∏–º–∞–Ω–∏–µ –æ–Ω–ª–∞–π–Ω –±–∏–∑–Ω–µ—Å–∞, –∑–Ω–∞–Ω–∏–µ –µ–≥–æ —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏—Ö –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π‚Äî –û–ø—ã—Ç –≤ –ø—Ä–æ–≥–Ω–æ–∑–Ω–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–µ (–∞–Ω–∞–ª–∏–∑ –∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤, –∞–Ω–∞–ª–∏–∑ –≤—ã–∂–∏–≤–∞–µ–º–æ—Å—Ç–∏ –∏ —Ç. –¥.)‚Äî –û–ø—ã—Ç —Å —Å–µ—Ä–≤–∏—Å–∞–º–∏ –æ–±–ª–∞—á–Ω—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π (GCP, etc.)‚Äî –û–ø—ã—Ç —Ä–∞–±–æ—Ç—ã —Å –ø–ª–∞—Ç–µ–∂–Ω—ã–º–∏ —Å–µ—Ä–≤–∏—Å–∞–º–∏ / —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω–æ–π –∫–æ–º–º–µ—Ä—Ü–∏–µ–π / –±–∞–Ω–∫–∞–º–∏ / –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞–º–∏ –ø–ª–∞—Ç–µ–∂–µ–π We offer ‚Äî –ú–Ω–æ–≥–æ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –¥–ª—è –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ —Ä–∞–∑–≤–∏—Ç–∏—è.‚Äî –î—Ä—É–∂–µ—Å—Ç–≤–µ–Ω–Ω–∞—è –∏ —Ç–µ–ø–ª–∞—è –∞—Ç–º–æ—Å—Ñ–µ—Ä–∞ –≤ –∫–æ–ª–ª–µ–∫—Ç–∏–≤–µ.‚Äî –û—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –§–û–ü.‚Äî 24 –∫–∞–ª–µ–Ω–¥–∞—Ä–Ω—ã—Ö –¥–Ω—è –æ—Ç–ø—É—Å–∫–∞.‚Äî –ú–µ–¥–∏—Ü–∏–Ω—Å–∫–∞—è —Å—Ç—Ä–∞—Ö–æ–≤–∫–∞ –ø–æ—Å–ª–µ –≤—ã—Ö–æ–¥–∞ –Ω–∞ —Ä–∞–±–æ—Ç—É.‚Äî –ö–æ–º–ø–µ–Ω—Å–∞—Ü–∏—è –∏–∑—É—á–µ–Ω–∏—è –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —è–∑—ã–∫–∞.‚Äî –û–ø–ª–∞—Ç–∞ –±–æ–ª—å–Ω–∏—á–Ω—ã—Ö.‚Äî –î—Ä–∞–π–≤–æ–≤—ã–µ –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤—ã –∏ –º–Ω–æ–≥–æ-–º–Ω–æ–≥–æ –¥—Ä—É–≥–∏—Ö –ø–ª—é—à–µ–∫. Responsibilities ‚Äî –†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ Business Intelligent (—Å–æ–∑–¥–∞–Ω–∏–µ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç–∏ –≤ Google Data Studio –∏ Tableau, —Ä–∞–±–æ—Ç–∞ —Å –¥–∞—Ç–∞-–∏–Ω–∂–µ–Ω–µ—Ä–∞–º–∏ –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ —Å—Ö–µ–º—ã DWH –∏ –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ ETL). ‚Äî –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è –ø—Ä–∏—á–∏–Ω –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–π –±–∏–∑–Ω–µ—Å-–º–µ—Ç—Ä–∏–∫, –∞ —Ç–∞–∫–∂–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.‚Äî –û—Ü–µ–Ω–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ (A/B-—Ç–µ—Å—Ç—ã).‚Äî –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.‚Äî –£—á–∞—Å—Ç–∏–µ –≤ –ø—Ä–æ–µ–∫—Ç–∞—Ö –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –±–∏–∑–Ω–µ—Å–∞. Project description Brainstack_ is a team of intelligent, fun-loving people working on their own products that truly have value. Some of our products have already made their name In US, Europe, Central and South America, and we keep catching up with new products conquering new territories. We think that creating great products requires absolute teamwork, no matter which position you occupy: every task, every pixel, every code line makes a difference.",Data Analyst (Python or/and R)@Brainstack_,https://jobs.dou.ua/companies/brainstack/vacancies/134289/, Kyiv,Data Analyst (Python or/and R),06 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/recruit-alliance/,Recruit Alliance,"{""Required skills"": [""2+ years of experience as a Data Engineer working on big data analytics;"", ""Excellent knowledge of Python and SQL;"", ""Good knowledge of Scala and Spark;"", ""Good knowledge of Apache Airflow;"", ""Experience with integration of data from multiple data sources and APIs;"", ""Knowledge of various ETL techniques and frameworks;"", ""English"", ""fluent (written and oral), additional languages a plus.""], ""As a plus"": [""Have experience/exposure to mobile User Acquisition and Marketing;"", ""Worked in the gaming industry and knowledge game specific data;"", ""Comfortable with AWS cloud (S3, EC2, Redshift, EKS(Kubernetes));"", ""Knowledge of existing third party analytics visualization software (Amplitude, Looker, Tableau).""], ""We offer"": [""Startup environment"", ""where each individual makes a large impact;"", ""Ability to own technical direction of various products and systems;"", ""Work with great people on great games that reach millions of people each month.""], ""Responsibilities"": [""Work with data analysts, game producers, and customer service to help define technology needs for the logging, processing, storage and presentation of data in a manner that delivers business value;"", ""Design, implement, maintain tools and reports for User Acquisition and LiveOps teams from start to finish;"", ""Work with various internal teams to satisfy analytical needs by building both large systematic reports and small custom pieces;"", ""Build data expertise and own data quality for a variety of products.""], ""Project description"": [""As part of the team, you will work on collecting, storing, processing, and analyzing huge sets of game data. The primary focus will be in owning a complete project and developing tools and systems for internal stakeholders for improving User Acquisition and LiveOps. Opportunity to combine data engineering, full stack development and data science. You will interface with various internal teams to understand their data needs and provide high quality data and insights which drive business value.""]}",,"Required skills ‚Äî 2+ years of experience as a Data Engineer working on big data analytics;‚Äî Excellent knowledge of Python and SQL;‚Äî Good knowledge of Scala and Spark;‚Äî Good knowledge of Apache Airflow;‚Äî Experience with integration of data from multiple data sources and APIs;‚Äî Knowledge of various ETL techniques and frameworks;‚Äî English ‚Äî fluent (written and oral), additional languages a plus. As a plus ‚Äî Have experience/exposure to mobile User Acquisition and Marketing;‚Äî Worked in the gaming industry and knowledge game specific data;‚Äî Comfortable with AWS cloud (S3, EC2, Redshift, EKS(Kubernetes));‚Äî Knowledge of existing third party analytics visualization software (Amplitude, Looker, Tableau). We offer ‚Äî Startup environment ‚Äî where each individual makes a large impact;‚Äî Ability to own technical direction of various products and systems;‚Äî Work with great people on great games that reach millions of people each month. Responsibilities ‚Äî Work with data analysts, game producers, and customer service to help define technology needs for the logging, processing, storage and presentation of data in a manner that delivers business value;‚Äî Design, implement, maintain tools and reports for User Acquisition and LiveOps teams from start to finish;‚Äî Work with various internal teams to satisfy analytical needs by building both large systematic reports and small custom pieces;‚Äî Build data expertise and own data quality for a variety of products. Project description As part of the team, you will work on collecting, storing, processing, and analyzing huge sets of game data. The primary focus will be in owning a complete project and developing tools and systems for internal stakeholders for improving User Acquisition and LiveOps. Opportunity to combine data engineering, full stack development and data science. You will interface with various internal teams to understand their data needs and provide high quality data and insights which drive business value.",Data Engineer@Recruit Alliance,https://jobs.dou.ua/companies/recruit-alliance/vacancies/134287/, Kyiv,Data Engineer,06 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/socialtech/,SocialTech,{},,"SocialTech is a multimillion IT company and a game-changer in the social discovery niche. Our HQ is located in Kiev. We manage a leading product in the industry, and all our key metrics keep growing exponentially from year to year. What you will find in SocialTech:Exciting tasks in each of the following fields: Marketing, Product, Retention, Monetization.The feeling of making an impact on a booming IT industry.A powerful team of analysts and product managers to inspire you to constant professional growth and new accomplishments. Key requirements:‚óè Data stack: SQL + BI (Tableau / Power BI / QLikView) + R / Python.‚óè Love and understanding of numbers, and a will for finding hidden data patterns.‚óè Strong systemic and algorithmic thinking.‚óè Good comprehension and interest in various parts of a business (marketing, product, etc.).‚óè Independence and self-sufficiency in your areas of competence.What we will do together:‚óè Use data to find answers to complex business challenges.‚óè Create, develop, and maintain analytical solutions to optimize business processes.‚óè Automate traffic buying and traffic quality assessment.‚óè Analyze split test results.‚óè Develop a system for automated reporting. Some internal workings of SocialTech:‚óè We use common sense as our guiding principle in everything from taking days off to workflow.‚óè We dislike bureaucracy, give and take open feedback, and ask each other questions, even the stupid ones.‚óè We enjoy spending some of our free time together, both at the office and outside it. If you think we‚Äôre a good fit for each other, don‚Äôt hesitate to send us your CV!",Product Data Analyst@SocialTech,https://jobs.dou.ua/companies/socialtech/vacancies/134277/, Kyiv,Product Data Analyst,06 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/socialtech/,SocialTech,{},,"SocialTech ‚Äî –≥–ª–æ–±–∞–ª—å–Ω–∞—è IT-–∫–æ–º–ø–∞–Ω–∏—è –≤ –∏–Ω–¥—É—Å—Ç—Ä–∏–∏ Social Discovery. –ú—ã —Ä–∞–∑–≤–∏–≤–∞–µ–º —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–¥—É–∫—Ç—ã, –∫–æ—Ç–æ—Ä—ã–º–∏ –ø–æ–ª—å–∑—É—é—Ç—Å—è –º–∏–ª–ª–∏–æ–Ω—ã –ª—é–¥–µ–π –ø–æ –≤—Å–µ–º—É –º–∏—Ä—É. –ê–Ω–∞–ª–∏—Ç–∏–∫–∞ –≤ SocialTech —ç—Ç–æ 10 —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –≤ –¥–µ—Å—è—Ç–∫–∞—Ö —Ç–µ—Ä–∞–±–∞–π—Ç –¥–∞–Ω–Ω—ã—Ö –µ–∂–µ–¥–Ω–µ–≤–Ω–æ –∏—â—É—Ç –æ—Ç–≤–µ—Ç—ã –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –±–∏–∑–Ω–µ—Å–∞. –û—Ç–¥–µ–ª —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –°–ê–û, 4 Data Analysts, 2 Data Scientists, 2 Product Analysts, Bi Officer. –ù–µ–±–æ–ª—å—à–∞—è —á–∞—Å—Ç—å —Ç–æ–≥–æ, —á—Ç–æ –º—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –∑–∞ –¥–≤–∞ –≥–æ–¥–∞, —Å –º–æ–º–µ–Ω—Ç–∞ –Ω–∞–π–º–∞ –ø–µ—Ä–≤–æ–≥–æ –∞–Ω–∞–ª–∏—Ç–∏–∫–∞: ‚óè –ü–ª–∞—Ç—Ñ–æ—Ä–º—É –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã—Ö —Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π.‚óè –†—è–¥ ¬´—Ç–æ—á–Ω—ã—Ö¬ª –∏ ¬´–Ω–µ—Ç–æ—á–Ω—ã—Ö¬ª —Å–∏—Å—Ç–µ–º –ø—Ä–æ–≥–Ω–æ–∑–∞ LTV, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–∑–≤–æ–ª—è—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —É–ø—Ä–∞–≤–ª—è—Ç—å –º–∞—Ä–∫–µ—Ç–∏–Ω–≥–æ–≤—ã–º–∏ –±—é–¥–∂–µ—Ç–∞–º–∏.‚óè –ú–æ–¥–µ–ª—å, –ø–æ–∑–≤–æ–ª—è—é—â—É—é —Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ –∞—Ç—Ä–∏–±—É—Ü–∏—Ä–æ–≤–∞—Ç—å –≤—ã—Ä—É—á–∫—É –º–µ–∂–¥—É –∞–∫–∫–∞—É–Ω—Ç–∞–º–∏ —é–∑–µ—Ä–æ–≤ –≤ —É—Å–ª–æ–≤–∏—è—Ö Repurchase.‚óè –ü—Ä–æ—Ç–æ—Ç–∏–ø –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã –æ—Ü–µ–Ω–∫–∏ –ê/–ë-—Ç–µ—Å—Ç–æ–≤. –°–µ–≥–æ–¥–Ω—è –º—ã –∏—â–µ–º Data Analyst‚Äôa, –∫–æ—Ç–æ—Ä—ã–π —É—Å–∏–ª–∏—Ç –∞–Ω–∞–ª–∏—Ç–∏–∫—É –º–∞—Ä–∫–µ—Ç–∏–Ω–≥–∞ (—Å–∞–º–∞—è –∫—Ä—É–ø–Ω–∞—è –∑–æ–Ω–∞) –∏ –ø—Ä–æ–¥–æ–ª–∂–∏—Ç —Ä–∞–∑–≤–∏–≤–∞—Ç—å –ø–∞—Ä—Ç–Ω–µ—Ä—Å–∫–∏–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è –º–µ–∂–¥—É –æ—Ç–¥–µ–ª–∞–º–∏. –ú—ã –æ–∂–∏–¥–∞–µ–º, —á—Ç–æ —Ç—ã –ø–æ–º–æ–∂–µ—à—å –Ω–∞–º —Å–ø—Ä–∞–≤–∏—Ç—å—Å—è —Å: ‚óè –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º —Å–µ–º–∏–∑–Ω–∞—á–Ω—ã—Ö –º–∞—Ä–∫–µ—Ç–∏–Ω–≥–æ–≤—ã—Ö –±—é–¥–∂–µ—Ç–æ–≤.‚óè –†–∞–∑—Ä–∞–±–æ—Ç–∫–æ–π –Ω–æ–≤—ã—Ö –∏ —Ä–∞–∑–≤–∏—Ç–∏–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–æ–¥–µ–ª–µ–π –ø—Ä–æ–≥–Ω–æ–∑–∞/–∞—Ç—Ä–∏–±—É—Ü–∏–∏ –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ Repurchase, ReMarketing‚Äôa –∏ —É–ª—å—Ç—Ä–∞ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏.‚óè –î–∏–∑–∞–π–Ω–æ–º –º–µ—Ç—Ä–∏–∫ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π, –∫—Ä–∏–≤–∞—è –¥–æ—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –∫–æ—Ç–æ—Ä—ã—Ö —Ä–∞—Å—Ç—è–Ω—É—Ç–∞ –Ω–∞ –≥–æ–¥—ã.‚óè –†–∞–∑—Ä–∞–±–æ—Ç–∫–æ–π —Å–∏—Å—Ç–µ–º—ã –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –±–∞–ª–∞–Ω—Å–∞ —Å–ø—Ä–æ—Å–∞ –∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –≤ –∞—Å—Å–∏–º–µ—Ç—Ä–∏—á–Ω–æ–º –º–∞—Ä–∫–µ—Ç–ø–ª–µ–π—Å–µ. ‚óè –§–∞–∫—Ç–æ—Ä–Ω—ã–º, –∫–æ–≥–æ—Ä—Ç–Ω—ã–º, —Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã–º –∏ –¥—Ä—É–≥–∏–º–∏ –≤–∏–¥–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞.‚óè –ü—Ä–∞–≤–∏–ª—å–Ω–æ–π –ø–æ—Å—Ç–∞–Ω–æ–≤–∫–æ–π –≤–æ–ø—Ä–æ—Å–æ–≤ —Å—Ç–µ–π–∫—Ö–æ–ª–¥–µ—Ä–∞–º –∏ –ø–∏—Å—å–º–µ–Ω–Ω–æ–π/–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–π –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–µ–π –ø–æ –∏—Ç–æ–≥–∞–º –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á. –î–ª—è —ç—Ç–æ–≥–æ —Ç–µ–±–µ –ø–æ–Ω–∞–¥–æ–±–∏—Ç—Å—è: ‚óè 1+ –Ω–∞ –ø–æ–∑–∏—Ü–∏–∏ Data Analyst/Scientist.‚óè Data stack: R/Python + SQL + Tableau/PowerBi/Qlik.‚óè –õ—é–±–æ–≤—å –∫ —Ü–∏—Ñ—Ä–∞–º –∏ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –±–∞–∑–æ–≤–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏/—Ç–µ–æ—Ä. –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏.‚óè –†–∞–∑–≤–∏—Ç–æ–µ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–µ –∏ —Å–∏—Å—Ç–µ–º–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ.‚óè –°–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å ¬´–Ω–∞ –ø–∞–ª—å—Ü–∞—Ö¬ª –¥–æ–Ω–æ—Å–∏—Ç—å —Å–ª–æ–∂–Ω—ã–µ –º—ã—Å–ª–∏.‚óè –ë—É–¥–µ—Ç –ø–ª—é—Å–æ–º –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏–π –æ–ø—ã—Ç –≤ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ/–º–∞—Ä–∫–µ—Ç–∏–Ω–≥–µ/—Ñ–∏–Ω–∞–Ω—Å–∞—Ö.‚óè –ö—Ä–æ–º–µ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á –∏ —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–≤—à–µ–π—Å—è –∫–æ–º–∞–Ω–¥—ã —É –Ω–∞—Å –µ—â–µ –µ—Å—Ç—å —á—Ç–æ –ø—Ä–µ–¥–ª–æ–∂–∏—Ç—å: –ù–µ–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–µ –∫–∞—Ä—å–µ—Ä–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏.‚óè –£—é—Ç–Ω—ã–π –æ—Ñ–∏—Å, —Å—Ç—Ä–∞—Ö–æ–≤–∫–∞, –∑–∞–≤—Ç—Ä–∞–∫–∏ / –æ–±–µ–¥—ã / —Å–Ω–µ–∫–∏.‚óè –ö–æ–º–ø–µ–Ω—Å–∞—Ü–∏—é –ª—é–±—ã—Ö —É—á–µ–±–Ω—ã—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤, –∑–∞–Ω—è—Ç–∏–π —Å–ø–æ—Ä—Ç–æ–º –∏ —É—á–∞—Å—Ç–∏—è –≤ –∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏—è—Ö.‚óè –ï–∂–µ–≥–æ–¥–Ω—ã–µ –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã–µ –ø–æ–µ–∑–¥–∫–∏ –∑–∞ –≥—Ä–∞–Ω–∏—Ü—É.‚óè –°–ø–ª–æ—á–µ–Ω–Ω—ã–π –∏ –¥—Ä—É–∂–Ω—ã–π –∫–æ–ª–ª–µ–∫—Ç–∏–≤.",Data Analyst (User acquisition)@SocialTech,https://jobs.dou.ua/companies/socialtech/vacancies/134272/, Kyiv,Data Analyst (User acquisition),06 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/n-ix/,N-iX,{},,"About the Client:Our client is a US company that specializes in delivering reliable Internet connectivity and entertainment multimedia to aircrafts worldwide, enhancing the experience for both passengers and crew. At this moment we started our cooperation with the Business Intelligence, Data Analysis and BigData solutions support direction and looking for talents who can contribute to the complex data management and analysis projects. About Role:We are looking for a Quality Assurance Engineer to develop and execute exploratory and automated tests. Your work will ensure the highest levels of product quality ‚Äî directly impacting customers, airborne products, and internal stakeholders. Additionally, you will work with the latest cloud technologies in AWS and big data pipelines in a highly Agile and collaborative setting that promotes continuous learning and growth. Main Responsibilities:‚Ä¢ Develop and execute exploratory and automated tests to ensure product quality‚Ä¢ Design and implementing test cases, debugging and defining corrective actionsReview requirements, specifications and technical design documents to provide timely and meaningful feedback‚Ä¢ Estimate, prioritize, plan and coordinate testing activities‚Ä¢ Design, develop and execute automation scripts as needed‚Ä¢ Identify, document and track bugs‚Ä¢ Perform thorough regression testing when bugs are resolved‚Ä¢ Develop and apply testing processes for new and existing products to meet client needs‚Ä¢ Liaise with internal teams (e.g. developers, product managers, analysts) to identify system requirements‚Ä¢ Assist with monitoring, investigating and resolving issues as needed ‚Äî includes authoring post mortems‚Ä¢ Stay up-to-date with latest testing technologies, strategies and best practices‚Ä¢ Continuously provide feedback to improve testing Requirements:‚Ä¢ 2 + years of experience with software testing‚Ä¢ Solid knowledge of SQL‚Ä¢ Ideally, experience working with various AWS services (EMR, EC2, Lambda, Athena, Redshift, DynamoDB, and S3 etc.)‚Ä¢ Strong Kanban/Agile work ethic ‚Äî Ability to contribute productively to a fast-paced development team‚Ä¢ BS/MS degree in Computer Science, Engineering or a related subject‚Ä¢ Adaptive and collaborative and perfect English Nice to have:‚Ä¢ Experience and/or interest in telecommunications and/or aviation industries‚Ä¢ Experience with performance testing‚Ä¢ Experience working with Airflow‚Ä¢ Experience working with Python‚Ä¢ Experience working with Spark We offer:‚Ä¢ Flexible working hours‚Ä¢ A competitive salary and good compensation package‚Ä¢ Possibility of partial remote work‚Ä¢ Best hardware‚Ä¢ A masseur and a corporate doctor‚Ä¢ Healthcare & sport benefits‚Ä¢ An inspiring and comfy office Professional growth:‚Ä¢ Challenging tasks and innovative projects‚Ä¢ Meetups and events for professional development‚Ä¢ An individual development plan‚Ä¢ Mentorship program Fun:‚Ä¢ Corporate events and outstanding parties‚Ä¢ Exciting team buildings‚Ä¢ Memorable anniversary presents‚Ä¢ A fun zone where you can play video games, foosball, ping pong, and more.",QA¬†Engineer for BigData project@N-iX,https://jobs.dou.ua/companies/n-ix/vacancies/134270/, Lviv,QA¬†Engineer for BigData project,06 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/enestech-software/,ENESTECH Software,"{""Required skills"": [""4-5+ years of business experience in the same roleFamiliarity with core statistical concepts Fluency with scripting in PythonExperience with building ETL processExperience with any related SQL databases (mysql, postgresql, google big query as a +)Excellent organizational, communication, writing and interpersonal skills""], ""We offer"": [""Working hours: from 9(10) am till 6(7) pm, 5 working days of a business weekProduct training, adaptationFree English courses Private Entrepreneur employment and support (\u0424\u041e\u041f)Medical insurance after 6+ months of employmentFlexible motivation conditions Real career opportunities and professional developmentThe office is located 5 minutes from m. PoznyakiA friendly team of people who like not only working together but also spend time together after work""], ""Responsibilities"": [""Conducting and managing data projects with a product manager\u2019s vision of success in mind Collecting and managing data to tailor projectsBuilding math and statistical models for internal predictions Visualizing data with any familiar tool (e.g. Power BI, Google data studio)Building a long-term trusted relationship across organizationBuilding reports for tracking performance.""], ""Project description"": [""About us: ENESTECH software is an IT developer of software and hardware for the esports business headquartered in Kyiv.The core product is a SENET cloud solution"", ""a tool for managing a huge set of PCs and full control over the activities of the center of the e-sports. SENET is represented in more than 35 countries. In fact, it is a platform for managing computer and console clubs, which will help simplify accounting for finances, optimize staff time, establish communication with customers and increase profits. Our own development team regularly updates the software and improves it in accordance with the latest technologies.""]}",,"Required skills 4-5+ years of business experience in the same roleFamiliarity with core statistical concepts Fluency with scripting in PythonExperience with building ETL processExperience with any related SQL databases (mysql, postgresql, google big query as a +)Excellent organizational, communication, writing and interpersonal skills We offer Working hours: from 9(10) am till 6(7) pm, 5 working days of a business weekProduct training, adaptationFree English courses Private Entrepreneur employment and support (–§–û–ü)Medical insurance after 6+ months of employmentFlexible motivation conditions Real career opportunities and professional developmentThe office is located 5 minutes from m. PoznyakiA friendly team of people who like not only working together but also spend time together after work Responsibilities Conducting and managing data projects with a product manager‚Äôs vision of success in mind Collecting and managing data to tailor projectsBuilding math and statistical models for internal predictions Visualizing data with any familiar tool (e.g. Power BI, Google data studio)Building a long-term trusted relationship across organizationBuilding reports for tracking performance. Project description About us: ENESTECH software is an IT developer of software and hardware for the esports business headquartered in Kyiv.The core product is a SENET cloud solution ‚Äî a tool for managing a huge set of PCs and full control over the activities of the center of the e-sports. SENET is represented in more than 35 countries. In fact, it is a platform for managing computer and console clubs, which will help simplify accounting for finances, optimize staff time, establish communication with customers and increase profits. Our own development team regularly updates the software and improves it in accordance with the latest technologies.",Data Analyst@ENESTECH Software,https://jobs.dou.ua/companies/enestech-software/vacancies/134227/, Kyiv,Data Analyst,06 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/dataart/,DataArt,"{""Required skills"": [""2+ years of experience in Hadoop administration2+ years of experience in Linux administrationExperience in scripting in BashGood spoken English""], ""As a plus"": [""Experience in installing and maintaining ClouderaExperience with KafkaExperience with AWSExperience in integrating Hadoop with ActiveDirectoryUnderstanding of the process and experience in production support and Hadoop performance optimization""], ""We offer"": [""DataArt offers:"", ""Professional Development:"", ""Experienced colleagues who are ready to share knowledge;"", ""The ability to switch projects, technology stacks, try yourself in different roles;"", ""Over 150 courses for workplace-based training"", ""Study and practice of English: courses and communication with colleagues and clients from different countries;"", ""Support of speakers who make presentations at conferences and meetings of technology communities."", ""The ability to focus on your work: a lack of bureaucracy and micromanagement, and convenient corporate services;"", ""Lack of dress code, friendly atmosphere, concern for the comfort of specialists;"", ""Flexible schedule and the ability to work remotely;"", ""The ability to work in any of our development centers.""], ""Project description"": [""The client is one of the UK\u2019s leading investment firms. DataArt\u2019s specialists are supporting and developing their Big Data platform based on the Hadoop ecosystem.""]}",,"Required skills 2+ years of experience in Hadoop administration2+ years of experience in Linux administrationExperience in scripting in BashGood spoken English As a plus Experience in installing and maintaining ClouderaExperience with KafkaExperience with AWSExperience in integrating Hadoop with ActiveDirectoryUnderstanding of the process and experience in production support and Hadoop performance optimization We offer DataArt offers:‚Ä¢ Professional Development:‚Äî Experienced colleagues who are ready to share knowledge;‚Äî The ability to switch projects, technology stacks, try yourself in different roles;‚Äî Over 150 courses for workplace-based training‚Äî Study and practice of English: courses and communication with colleagues and clients from different countries;‚Äî Support of speakers who make presentations at conferences and meetings of technology communities.‚Ä¢ The ability to focus on your work: a lack of bureaucracy and micromanagement, and convenient corporate services;‚Ä¢ Lack of dress code, friendly atmosphere, concern for the comfort of specialists;‚Ä¢ Flexible schedule and the ability to work remotely;‚Ä¢ The ability to work in any of our development centers. Project description The client is one of the UK‚Äôs leading investment firms. DataArt‚Äôs specialists are supporting and developing their Big Data platform based on the Hadoop ecosystem. The Big Data Platform administrator/Hadoop DevOps engineer will join the technical team on the client-side. The team of this project is dedicated to supporting the Hadoop platform. The project uses the Hadoop ecosystem, Kafka, and AWS. Note that we hire specialists not to the project, but to the DataArt. If the project is over, or you become uncomfortable in it, you can discuss a transition to another project.","Big Data Platform Administrator, Multinational Financial Services@DataArt",https://jobs.dou.ua/companies/dataart/vacancies/134219/," Kyiv, Kharkiv, Lviv, Dnipro, Odesa, Kherson, remote","Big Data Platform Administrator, Multinational Financial Services",06 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/fuib/,BANK PUMB (FUIB),"{""Required skills"": [""\u041c\u0430\u0442\u0438 \u0434\u043e\u0441\u0432\u0456\u0434 \u0440\u043e\u0431\u043e\u0442\u0438 \u0432 \u043d\u0430\u043f\u0440\u044f\u043c\u043a\u0443 \u0431\u0456\u0437\u043d\u0435\u0441 \u0442\u0430 \u0441\u0438\u0441\u0442\u0435\u043c\u043d\u043e\u0433\u043e \u0430\u043d\u0430\u043b\u0456\u0437\u0443."", ""\u0414\u043e\u0441\u0432\u0456\u0434 \u0437 \u0432\u043f\u0440\u043e\u0432\u0430\u0434\u0436\u0435\u043d\u043d\u044f \u043d\u043e\u0432\u0438\u0445 \u0406\u0422 \u0441\u0438\u0441\u0442\u0435\u043c \u0442\u0430 \u0443\u043f\u0440\u0430\u0432\u043b\u0456\u043d\u043d\u044f \u044f\u043a\u0456\u0441\u0442\u044e \u0434\u0430\u043d\u0438\u0445."", ""\u0420\u043e\u0437\u0443\u043c\u0456\u043d\u043d\u044f \u0432\u0430\u0436\u043b\u0438\u0432\u043e\u0441\u0442\u0456 \u044f\u043a\u043e\u0441\u0442\u0456 \u0434\u0430\u043d\u0438\u0445 \u0434\u043b\u044f Data Science \u0431\u0443\u0434\u0435 \u043f\u0435\u0440\u0435\u0432\u0430\u0433\u043e\u044e."", ""\u0417\u043d\u0430\u043d\u043d\u044f ISO 90001"", ""\u041c\u0438 \u043f\u0440\u0430\u0446\u044e\u0454\u043c\u043e Power BI (\u0432\u0430\u0436\u043b\u0438\u0432\u043e \u0449\u043e\u0431 \u0442\u0438 \u0439\u043e\u0433\u043e \u0437\u043d\u0430\u0432)""], ""We offer"": [""\u041d\u043e\u0432\u0438\u0439 \u0442\u0430 \u0441\u0443\u0447\u0430\u0441\u043d\u0438\u0439 IT \u043e\u0444\u0456\u0441"", ""\u041a\u0443\u0440\u0441\u0438 \u0430\u043d\u0433\u043b\u0456\u0439\u0441\u044c\u043a\u043e\u0457 \u043c\u043e\u0432\u0438"", ""\u0421\u0442\u0440\u0430\u0445\u0443\u0432\u0430\u043d\u043d\u044f"", ""\u0411\u0435\u0437\u043b\u0456\u043c\u0456\u0442\u043d\u0430 \u043a\u0430\u0432\u0430"", ""\u041d\u0430\u0439\u043a\u0440\u0430\u0449\u0430 \u0442\u0430 \u043d\u0430\u0439\u0430\u043c\u0431\u0456\u0442\u043d\u0456\u0448\u0430 \u043a\u043e\u043c\u0430\u043d\u0434\u0430 \u0430\u043d\u0430\u043b\u0456\u0442\u0438\u043a\u0438 \u0432 \u0444\u0456\u043d\u0430\u043d\u0441\u043e\u0432\u043e\u043c\u0443 \u0441\u0435\u043a\u0442\u043e\u0440\u0456""], ""Responsibilities"": [""\u041a\u043e\u043e\u0440\u0434\u0438\u043d\u0430\u0446\u0456\u044f \u043f\u0440\u043e\u0435\u043a\u0442\u0456\u0432 \u0437 \u0440\u043e\u0437\u0440\u043e\u0431\u043a\u0438 \u041f\u041e \u0442\u0430 \u0432\u043f\u0440\u043e\u0432\u0430\u0434\u0436\u0435\u043d\u043d\u044f \u0441\u0442\u0440\u0430\u0442\u0435\u0433\u0456\u0457 \u0443\u043f\u0440\u0430\u0432\u043b\u0456\u043d\u043d\u044f \u044f\u043a\u0456\u0441\u0442\u044e \u0434\u0430\u043d\u0438\u0445"", ""\u041c\u043e\u043d\u0456\u0442\u043e\u0440\u0438\u043d\u0433 \u043f\u043e\u0432\u043d\u043e\u0442\u0438 \u0456 \u0441\u0432\u043e\u0454\u0447\u0430\u0441\u043d\u043e\u0441\u0442\u0456 \u0434\u0430\u043d\u0438\u0445, \u0457\u0445 \u0432\u0456\u0434\u043f\u043e\u0432\u0456\u0434\u043d\u043e\u0441\u0442\u0456 \u043c\u0456\u0436 \u0441\u0438\u0441\u0442\u0435\u043c\u0430\u043c\u0438""], ""Project description"": [""\u041f\u0440\u043e\u0435\u043a\u0442 \u0437 \u044f\u043a\u043e\u0441\u0442\u0456 \u0434\u0430\u043d\u0438\u0445 \u0431\u0443\u0434\u0435 \u0441\u043a\u043b\u0430\u0434\u0430\u0442\u0438\u0441\u044f \u0437 3 \u0447\u0430\u0441\u0442\u0438\u043d:1) \u041f\u0440\u043e\u0432\u0435\u0441\u0442\u0438 \u0430\u0443\u0434\u0438\u0442 \u044f\u043a\u043e\u0441\u0442\u0456 \u0434\u0430\u043d\u0438\u0445;2) \u0421\u0442\u0432\u043e\u0440\u0435\u043d\u043d\u044f \u0441\u0442\u0440\u0430\u0442\u0435\u0433\u0456\u0457 \u0440\u043e\u0437\u0432\u0438\u0442\u043a\u0443 \u044f\u043a\u043e\u0441\u0442\u0456 \u0434\u0430\u043d\u0438\u0445 (\u0434\u043b\u044f \u0456\u0441\u043d\u0443\u044e\u0447\u0438\u0445 \u0442\u0430 \u043d\u043e\u0432\u0438\u0445 \u0441\u0438\u0441\u0442\u0435\u043c);3) \u0412\u043f\u0440\u043e\u0432\u0430\u0434\u0436\u0435\u043d\u043d\u044f \u0441\u0442\u0440\u0430\u0442\u0435\u0433\u0456\u0457.""]}",,"Required skills ‚Ä¢ –ú–∞—Ç–∏ –¥–æ—Å–≤—ñ–¥ —Ä–æ–±–æ—Ç–∏ –≤ –Ω–∞–ø—Ä—è–º–∫—É –±—ñ–∑–Ω–µ—Å —Ç–∞ —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –∞–Ω–∞–ª—ñ–∑—É.‚Ä¢ –î–æ—Å–≤—ñ–¥ –∑ –≤–ø—Ä–æ–≤–∞–¥–∂–µ–Ω–Ω—è –Ω–æ–≤–∏—Ö –Ü–¢ —Å–∏—Å—Ç–µ–º —Ç–∞ —É–ø—Ä–∞–≤–ª—ñ–Ω–Ω—è —è–∫—ñ—Å—Ç—é –¥–∞–Ω–∏—Ö.‚Ä¢ –†–æ–∑—É–º—ñ–Ω–Ω—è –≤–∞–∂–ª–∏–≤–æ—Å—Ç—ñ —è–∫–æ—Å—Ç—ñ –¥–∞–Ω–∏—Ö –¥–ª—è Data Science –±—É–¥–µ –ø–µ—Ä–µ–≤–∞–≥–æ—é.‚Ä¢ –ó–Ω–∞–Ω–Ω—è ISO 90001‚Ä¢ –ú–∏ –ø—Ä–∞—Ü—é—î–º–æ Power BI (–≤–∞–∂–ª–∏–≤–æ —â–æ–± —Ç–∏ –π–æ–≥–æ –∑–Ω–∞–≤) We offer ‚Ä¢ –ù–æ–≤–∏–π —Ç–∞ —Å—É—á–∞—Å–Ω–∏–π IT –æ—Ñ—ñ—Å‚Ä¢ –ö—É—Ä—Å–∏ –∞–Ω–≥–ª—ñ–π—Å—å–∫–æ—ó –º–æ–≤–∏‚Ä¢ –°—Ç—Ä–∞—Ö—É–≤–∞–Ω–Ω—è‚Ä¢ –ë–µ–∑–ª—ñ–º—ñ—Ç–Ω–∞ –∫–∞–≤–∞‚Ä¢ –ù–∞–π–∫—Ä–∞—â–∞ —Ç–∞ –Ω–∞–π–∞–º–±—ñ—Ç–Ω—ñ—à–∞ –∫–æ–º–∞–Ω–¥–∞ –∞–Ω–∞–ª—ñ—Ç–∏–∫–∏ –≤ —Ñ—ñ–Ω–∞–Ω—Å–æ–≤–æ–º—É —Å–µ–∫—Ç–æ—Ä—ñ Responsibilities ‚Ä¢ –ö–æ–æ—Ä–¥–∏–Ω–∞—Ü—ñ—è –ø—Ä–æ–µ–∫—Ç—ñ–≤ –∑ —Ä–æ–∑—Ä–æ–±–∫–∏ –ü–û —Ç–∞ –≤–ø—Ä–æ–≤–∞–¥–∂–µ–Ω–Ω—è —Å—Ç—Ä–∞—Ç–µ–≥—ñ—ó —É–ø—Ä–∞–≤–ª—ñ–Ω–Ω—è —è–∫—ñ—Å—Ç—é –¥–∞–Ω–∏—Ö‚Ä¢ –ú–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥ –ø–æ–≤–Ω–æ—Ç–∏ —ñ —Å–≤–æ—î—á–∞—Å–Ω–æ—Å—Ç—ñ –¥–∞–Ω–∏—Ö, —ó—Ö –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–æ—Å—Ç—ñ –º—ñ–∂ —Å–∏—Å—Ç–µ–º–∞–º–∏ Project description –ü—Ä–æ–µ–∫—Ç –∑ —è–∫–æ—Å—Ç—ñ –¥–∞–Ω–∏—Ö –±—É–¥–µ —Å–∫–ª–∞–¥–∞—Ç–∏—Å—è –∑ 3 —á–∞—Å—Ç–∏–Ω:1) –ü—Ä–æ–≤–µ—Å—Ç–∏ –∞—É–¥–∏—Ç —è–∫–æ—Å—Ç—ñ –¥–∞–Ω–∏—Ö;2) –°—Ç–≤–æ—Ä–µ–Ω–Ω—è —Å—Ç—Ä–∞—Ç–µ–≥—ñ—ó —Ä–æ–∑–≤–∏—Ç–∫—É —è–∫–æ—Å—Ç—ñ –¥–∞–Ω–∏—Ö (–¥–ª—è —ñ—Å–Ω—É—é—á–∏—Ö —Ç–∞ –Ω–æ–≤–∏—Ö —Å–∏—Å—Ç–µ–º);3) –í–ø—Ä–æ–≤–∞–¥–∂–µ–Ω–Ω—è —Å—Ç—Ä–∞—Ç–µ–≥—ñ—ó.",Data Quality manager@BANK PUMB (FUIB),https://jobs.dou.ua/companies/fuib/vacancies/134211/," Kyiv, remote",Data Quality manager,06 October 2020,–¥–æ¬†$5000,2020-10-13,,dou
https://jobs.dou.ua/companies/unhcr/vacancies/,All company jobs,"{""Required skills"": [""ESSENTIAL MINIMUM QUALIFICATIONS AND PROFESSIONAL EXPERIENCE REQUIRED:- Completion of the Secondary Education with post-secondary training/certificate in information and communication technology or related field.- Minimum 4 years of previous job experience relevant to the function.- Solid work experience with Microsoft Office and Access.- Experience in statistics data analyses and applications. - Fluency in English and local language.""], ""Responsibilities"": [""DESIRABLE QUALIFICATIONS & COMPETENCIES:- Completion of UNHCR learning programmes or specified training relevant to the functions of the position.- Previous experience working with UN/UNHCR.- Knowledge of UNHCR corporate applications MSRP, ProGres, etc.- Microsoft SQL server certification.- Training experience.""], ""Project description"": [""- Develop forms, applications and tools for data collection, data processing and analysis.- Ensure that the staff are trained on correct entering of data related to persons of concern to UNHCR into the computer databases.- Monitor the regular data entry activities and verify accuracy and relevance of data on a regular basis.- Installation, maintenance and troubleshooting of Biometrical Equipment, management of operators accounts.- Develop and run required reports from the databases used in the operation including Financial Platforms and Registration database.- Compile data and interpret the statistics for various reports.- Identify knowledge and hardware gaps of the operation in relation to all aspects of data management.- Provide technical support to UNHCR staff and Partners on data management and IT related matters.- Undertake technical needs assessment on site and installations of UNHCR Telecommunications equipment taking into account the overall ICT strategy of the region, the operational needs and the security constraints. - Ensure that the equipment under area of responsibility is in good working condition, by arranging regular inspections of hardware and installations and, if required, arrange promptly upgrades, repairs or replacement as necessary according to established procedures.- Supervise monitor and maintain of software, networks and hubs, etc. to prevent faults occurring.- Maintain up to date inventory of all infrastructure equipment under AOR for asset management purpose.- Draft procedures and instructions to promote a better understanding of the use of the ICT equipment and data bases.- Experience with SQL and FetchXML reporting - Practical deep knowledge in Power BI- Advanced Excel user - Perform any other related duties as required""]}",,"Required skills ESSENTIAL MINIMUM QUALIFICATIONS AND PROFESSIONAL EXPERIENCE REQUIRED:- Completion of the Secondary Education with post-secondary training/certificate in information and communication technology or related field.- Minimum 4 years of previous job experience relevant to the function.- Solid work experience with Microsoft Office and Access.- Experience in statistics data analyses and applications. - Fluency in English and local language. DESIRABLE QUALIFICATIONS & COMPETENCIES:- Completion of UNHCR learning programmes or specified training relevant to the functions of the position.- Previous experience working with UN/UNHCR.- Knowledge of UNHCR corporate applications MSRP, ProGres, etc.- Microsoft SQL server certification.- Training experience. Responsibilities - Develop forms, applications and tools for data collection, data processing and analysis.- Ensure that the staff are trained on correct entering of data related to persons of concern to UNHCR into the computer databases.- Monitor the regular data entry activities and verify accuracy and relevance of data on a regular basis.- Installation, maintenance and troubleshooting of Biometrical Equipment, management of operators accounts.- Develop and run required reports from the databases used in the operation including Financial Platforms and Registration database.- Compile data and interpret the statistics for various reports.- Identify knowledge and hardware gaps of the operation in relation to all aspects of data management.- Provide technical support to UNHCR staff and Partners on data management and IT related matters.- Undertake technical needs assessment on site and installations of UNHCR Telecommunications equipment taking into account the overall ICT strategy of the region, the operational needs and the security constraints. - Ensure that the equipment under area of responsibility is in good working condition, by arranging regular inspections of hardware and installations and, if required, arrange promptly upgrades, repairs or replacement as necessary according to established procedures.- Supervise monitor and maintain of software, networks and hubs, etc. to prevent faults occurring.- Maintain up to date inventory of all infrastructure equipment under AOR for asset management purpose.- Draft procedures and instructions to promote a better understanding of the use of the ICT equipment and data bases.- Experience with SQL and FetchXML reporting - Practical deep knowledge in Power BI- Advanced Excel user - Perform any other related duties as required Project description The position of Snr Data Management/ICT Associate is the most senior technical position within the UNHCR operation in Ukraine. The main task of the position is to ensure routine functioning of the ICT and information management systems of the country operation and provide technical advice and support to management. The position is supervised by an Associate Administrative Officer who provides the incumbent with general guidance and work plans. The incumbent liaises directly with the Snr Admin/Programme Officer and the Information Management Officer regarding data management needs. Since the operation is without a professional ICT supervisor, the incumbent must be able to work independently due to the technical nature of the work and liaise directly with regional and HQs technical units for problem solving and consultations. The position directly supervises ICT support staff. The incumbent has regular contacts with all staff members maintaining smooth and friendly relations with a high responsive attitude towards problem solving and finding solutions. S/he has also contacts with external parties such as contractors and suppliers of ICT equipment and services, as well as partners for gathering and exchange of data. Main tasks of the position are to ensure routine functioning of the ICT system within the country operation, assist the management to improve and refine it with the technical progress, support of data-driven decisions.","Senior Data Management/ICT Associate/G7 (Temporary Appointment), UNHCR CO¬†Kyiv, Ukraine. Deadline for applications: 19¬†October 2020@All company jobs",https://jobs.dou.ua/companies/unhcr/vacancies/134168/, Kyiv,"Senior Data Management/ICT Associate/G7 (Temporary Appointment), UNHCR CO¬†Kyiv, Ukraine. Deadline for applications: 19¬†October 2020",06 October 2020,$1860‚Äì2275,2020-10-13,,dou
https://jobs.dou.ua/companies/sandsiv-ag/,sandsiv +,"{""Required skills"": [""Degree in Computer Science or a related technical fieldGood written and spoken English Good knowledge of ML/DL theoretical foundations At least one year of industry experience researching and developing machine learning and deep learning technologies for real-world productsExperience in ETL Development Research mentality and math-oriented mindset""], ""As a plus"": [""Proficiency in programming language"", ""Python Practical experience in solving NLP problemsText mining (data preprocessing, classification, clustering, topic modeling, word embedding, parts of speech etc.)Skills in working with real texts & features (imbalanced, noisy)Hands-on experience with KNIME""], ""We offer"": [""Competitive salary\u041effice 5 minutes metro PecherskAll modern"", ""quarantine sanitary requirements are taken into account in the organization of office workFlexible work model: office, part time office/remote, full time remote (for non Kiev employees)Free mealsPaid lunchesAnnual big paid vacation with flexible schedule and covered sick leavesThe opportunity of professional growthPossible business trips to SwitzerlandWelcoming atmosphere (awesome team of professionals always ready to help)""], ""Responsibilities"": [""Research various machine learning and data analysis techniques that could improve our productDesign and develop ETL solutionsPrototype coding and testingDelivering of the the idea based on the results and explanation of its mathematical basisWorking on additional customers projects and depending on the request provide:data mining, finding patterns, selection of appropriate mathematical models, testing, coding to the status of fully automated project, present to client and \u201cteach\u201d how to use it.""], ""Project description"": [""SandSIV Switzerland enables leading companies to gather Superior Customer Intelligence. Through its end-to-end Customer Experience Management (CXM) platform, SandSIV directly contributes to increased operational efficiency, helps accelerate business performance and provides measurable impact on revenues and the bottom-line.""]}",,"Required skills Degree in Computer Science or a related technical fieldGood written and spoken English Good knowledge of ML/DL theoretical foundations At least one year of industry experience researching and developing machine learning and deep learning technologies for real-world productsExperience in ETL Development Research mentality and math-oriented mindset As a plus Proficiency in programming language ‚Äî Python Practical experience in solving NLP problemsText mining (data preprocessing, classification, clustering, topic modeling, word embedding, parts of speech etc.)Skills in working with real texts & features (imbalanced, noisy)Hands-on experience with KNIME We offer Competitive salary–ûffice 5 minutes metro PecherskAll modern ‚Äî quarantine sanitary requirements are taken into account in the organization of office workFlexible work model: office, part time office/remote, full time remote (for non Kiev employees)Free mealsPaid lunchesAnnual big paid vacation with flexible schedule and covered sick leavesThe opportunity of professional growthPossible business trips to SwitzerlandWelcoming atmosphere (awesome team of professionals always ready to help) Responsibilities Research various machine learning and data analysis techniques that could improve our productDesign and develop ETL solutionsPrototype coding and testingDelivering of the the idea based on the results and explanation of its mathematical basisWorking on additional customers projects and depending on the request provide:data mining, finding patterns, selection of appropriate mathematical models, testing, coding to the status of fully automated project, present to client and ‚Äúteach‚Äù how to use it. Project description SandSIV Switzerland enables leading companies to gather Superior Customer Intelligence. Through its end-to-end Customer Experience Management (CXM) platform, SandSIV directly contributes to increased operational efficiency, helps accelerate business performance and provides measurable impact on revenues and the bottom-line.",Data Scientist Engineer@sandsiv +,https://jobs.dou.ua/companies/sandsiv-ag/vacancies/134157/, Kyiv,Data Scientist Engineer,06 October 2020,$1000‚Äì1500,2020-10-13,,dou
https://jobs.dou.ua/companies/validbook/,Valid-X,"{""Required skills"": [""2+ years Experience with Python ML/DL libraries and frameworks (scikit-learn/TensorFlow/Keras/Pytorch)"", ""2+ years Python programming proficiency"", ""Deep learning architectures experience"", ""Strong knowledge of designing end-to-end ML flows (Experience with MLFlow or Kubeflow)""], ""As a plus"": [""Proven experience as a Machine Learning Engineer or Data Scientist"", ""Understanding of data structures, data modeling and software architecture"", ""Deep knowledge of math, probability, statistics and algorithms"", ""Ability to write robust code in Python, Java and R"", ""Familiarity with machine learning frameworks (like Keras or PyTorch) and libraries (like scikit-learn)"", ""Excellent communication skills"", ""Ability to work in a team"", ""Outstanding analytical and problem-solving skills"", ""BSc in Computer Science, Mathematics or similar field; Master\u2019s degree is a plus""], ""We offer"": [""Competitive salary"", ""Dynamic environment of a fast growing company"", ""Bonuses"", ""Health insurance""], ""Responsibilities"": [""Study and transform data science prototypes"", ""Design machine learning systems"", ""Research and implement appropriate ML algorithms and tools"", ""Develop machine learning applications according to requirements"", ""Select appropriate datasets and data representation methods"", ""Run machine learning tests and experiments"", ""Perform statistical analysis and fine-tuning using test results"", ""Train and retrain systems when necessary"", ""Extend existing ML libraries and frameworks"", ""Keep abreast of developments in the field""], ""Project description"": [""Develop, modernize and optimize a back-end for a portfolio of online games that are currently being used by 100K+ users. You will work with our data & AI team to make these games safe, secure and fair.""]}",,"Required skills ‚Äî 2+ years Experience with Python ML/DL libraries and frameworks (scikit-learn/TensorFlow/Keras/Pytorch)‚Äî 2+ years Python programming proficiency‚Äî Deep learning architectures experience‚Äî Strong knowledge of designing end-to-end ML flows (Experience with MLFlow or Kubeflow) As a plus ‚Äî Proven experience as a Machine Learning Engineer or Data Scientist‚Äî Understanding of data structures, data modeling and software architecture‚Äî Deep knowledge of math, probability, statistics and algorithms‚Äî Ability to write robust code in Python, Java and R‚Äî Familiarity with machine learning frameworks (like Keras or PyTorch) and libraries (like scikit-learn)‚Äî Excellent communication skills‚Äî Ability to work in a team‚Äî Outstanding analytical and problem-solving skills‚Äî BSc in Computer Science, Mathematics or similar field; Master‚Äôs degree is a plus We offer ‚Äî Competitive salary‚Äî Dynamic environment of a fast growing company‚Äî Bonuses‚Äî Health insurance Responsibilities ‚Äî Study and transform data science prototypes‚Äî Design machine learning systems‚Äî Research and implement appropriate ML algorithms and tools‚Äî Develop machine learning applications according to requirements‚Äî Select appropriate datasets and data representation methods‚Äî Run machine learning tests and experiments‚Äî Perform statistical analysis and fine-tuning using test results‚Äî Train and retrain systems when necessary‚Äî Extend existing ML libraries and frameworks‚Äî Keep abreast of developments in the field Project description Develop, modernize and optimize a back-end for a portfolio of online games that are currently being used by 100K+ users. You will work with our data & AI team to make these games safe, secure and fair.","Machine Learning Engineer, Data Scientist@Valid-X",https://jobs.dou.ua/companies/validbook/vacancies/134132/, remote,"Machine Learning Engineer, Data Scientist",06 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/validbook/,Valid-X,"{""Required skills"": [""Our ideal candidate would have experience with all of the below technologies or most of them and and with desire to learn and work with the others.""], ""As a plus"": [""Experience with building data pipelines with Kafka"", ""Airflow"", ""Spark, Databricks"", ""AWS""], ""We offer"": [""Experience building and optimizing streaming data pipelines coming from event-driven applications"", ""Working knowledge of message queuing, stream processing, and highly scalable \u2018big data\u2019 data stores."", ""Advanced working SQL knowledge and experience working with relational databases"", ""A successful history of manipulating, processing and extracting value from large disconnected datasets."", ""Experience supporting and working with cross-functional teams in a dynamic environment."", ""Experience with big data tools: Databricks, Spark, Kafka, etc."", ""Experience with relational SQL and NoSQL databases, including Postgres/MySQL and MongoDB."", ""Experience with data pipeline and workflow management tools: Airflow/etc"", ""Experience with AWS cloud services: EC2, ECS, MSK, RDS, Redshift"", ""Experience with stream-processing systems: Spark-Streaming, ksqlDB, Kafka streams etc.""], ""Responsibilities"": [""Competitive salary"", ""Dynamic environment of a fast growing company"", ""Bonuses"", ""Health insurance""], ""Project description"": [""Create and maintain optimal data pipeline architecture,"", ""Assemble large, complex data sets that meet functional / non-functional business requirements."", ""Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc."", ""Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using AWS \u2018big data\u2019 and SQL technologies."", ""Build analytics tools that utilize the data pipeline to provide actionable insights into customer behaviour, operational efficiency and other key business performance metrics."", ""Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs."", ""Keep our data separated and secure across national boundaries through multiple data centers and AWS regions."", ""Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader."", ""Work with data and analytics experts to strive for greater functionality in our data systems.""]}",,"Required skills Our ideal candidate would have experience with all of the below technologies or most of them and and with desire to learn and work with the others. ‚Äî Experience with building data pipelines with Kafka‚Äî Airflow‚Äî Spark, Databricks‚Äî AWS As a plus ‚Äî Experience building and optimizing streaming data pipelines coming from event-driven applications‚Äî Working knowledge of message queuing, stream processing, and highly scalable ‚Äòbig data‚Äô data stores.‚Äî Advanced working SQL knowledge and experience working with relational databases‚Äî A successful history of manipulating, processing and extracting value from large disconnected datasets.‚Äî Experience supporting and working with cross-functional teams in a dynamic environment.‚Äî Experience with big data tools: Databricks, Spark, Kafka, etc.‚Äî Experience with relational SQL and NoSQL databases, including Postgres/MySQL and MongoDB.‚Äî Experience with data pipeline and workflow management tools: Airflow/etc‚Äî Experience with AWS cloud services: EC2, ECS, MSK, RDS, Redshift‚Äî Experience with stream-processing systems: Spark-Streaming, ksqlDB, Kafka streams etc. We offer ‚Äî Competitive salary‚Äî Dynamic environment of a fast growing company‚Äî Bonuses‚Äî Health insurance Responsibilities ‚Äî Create and maintain optimal data pipeline architecture,‚Äî Assemble large, complex data sets that meet functional / non-functional business requirements.‚Äî Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.‚Äî Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using AWS ‚Äòbig data‚Äô and SQL technologies.‚Äî Build analytics tools that utilize the data pipeline to provide actionable insights into customer behaviour, operational efficiency and other key business performance metrics.‚Äî Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.‚Äî Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.‚Äî Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.‚Äî Work with data and analytics experts to strive for greater functionality in our data systems. Project description Develop, modernize and optimize a back-end for a portfolio of online games that are currently being used by 100K+ users. You will work with our data & AI team to make these games safe, secure and fair.",Data Engineer@Valid-X,https://jobs.dou.ua/companies/validbook/vacancies/134129/, remote,Data Engineer,06 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/n-ix/,N-iX,{},,"N-iX is looking for a Data Architect/Lead Data Engineer to join our team. Our customer, as a proven leader in global commerce, provides flexible E-commerce solutions for monetizing digital goods, online services and SaaS. Their cloud-based E-commerce platform simplifies recurring billing, optimizes the customer experience and offers comprehensive global payment capabilities. Leveraging customer‚Äôs expertise, technology and services, clients effectively increase customer acquisition and retention while rapidly expanding into international markets for revenue growth.The ideal candidate will drive the implementation on best practices for design, architecture, and the execution of high-performance, scalable and optimized data solutions for internal and external stakeholders. As a member of our Agile development team, you will be collaborating with data engineers, software developers, product owners and project managers to select the appropriate design solution and ensure the compatibility of various system components. Responsibilities:‚Ä¢ You will work with our Director of Finance Applications and serve as a pivotal role in the oversight, architecture and implementation of our ERP Project.‚Ä¢ You will also collaborate with our Data Development Team to understand current-state data schemas and data models then contribute to define future-state data schema, model and flow.‚Ä¢ You will provide input in each cycle of the development phase (develop, test, and release) to ensure we produce a leading-edge solution and we continually learn and evolve, adapting to changing business landscapes.‚Ä¢ You will verify the stability, inter-operability, portability, security, or scalability of existing and new data architectures.‚Ä¢ You will establish data quality check approaches, tools, and data governance where ever necessary, to ensure the delivery of trusted and accurate data.‚Ä¢ You will drive to establish a process for documenting data models, data schemas, lineages and data dictionaries across the data stores of our project.‚Ä¢ Provide mentorship, technical expertise and recommendations on the current and emerging data strategies and platform trends to our team of data engineers and scientists. Skills/Requirements:‚Ä¢ Computer science or related engineering degree and 6+ years of professional experience as a Data Engineer/Data Architect‚Ä¢ Worked with large volumes of structured and semi-structured data on the cloud, implemented real-time and/or batch data processing and analytics‚Ä¢ Proficiency with Azure Ecosystem (Data Factories, Cosmos DB, DataBricks)‚Ä¢ Experience in creating and maintaining ETL/ELT pipeline architecture‚Ä¢ Used relational and/or NoSQL databases, with strong skills with Microsoft SQL Server technology.‚Ä¢ Experience with DevOps and engineering best practices, especially Docker, Git, testing in Python, and SQL (T-SQL)‚Ä¢ Working knowledge of using a Business Intelligence platform (Tableau, PowerBI, Qlik) Would be a plus:‚Ä¢ AWS: Redshift, Athena, S3, EMR‚Ä¢ Industry-specific knowledge and experience (e-commerce)‚Ä¢ Knowledge in statistics and understanding of machine learning algorithms.‚Ä¢ Worked in an agile environment‚Ä¢ Data visualization experience, exposure to the UI/UX field‚Ä¢ Worked on predictive modeling for e-commerce-relevant use cases, such as: Customer Churn Prediction, Sales Forecasting, Predictive Marketing, Fraud Detection. We offer:‚Ä¢ Flexible working hours‚Ä¢ A competitive salary and good compensation package‚Ä¢ Best hardware‚Ä¢ A masseur and a corporate doctor‚Ä¢ Healthcare & sport benefits‚Ä¢ An inspiring and comfy office Professional growth:‚Ä¢ Challenging tasks and innovative projects‚Ä¢ Meetups and events for professional development‚Ä¢ An individual development plan‚Ä¢ Mentorship program Fun:‚Ä¢ Corporate events and outstanding parties‚Ä¢ Exciting team buildings‚Ä¢ Memorable anniversary presents‚Ä¢ A fun zone where you can play video games, foosball, ping pong, and more",Data Architect/Lead Data Engineer@N-iX,https://jobs.dou.ua/companies/n-ix/vacancies/134086/," Kyiv, Lviv, remote",Data Architect/Lead Data Engineer,05 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/solid/,Solid - Fintech Company,{},,"Solid ‚Äî —Ü–µ —Ñ—ñ–Ω—Ç–µ—Ö-–∫–æ–º–ø–∞–Ω—ñ—è, —è–∫–∞ –¥–æ–ø–æ–º–∞–≥–∞—î –±—ñ–∑–Ω–µ—Å–∞–º –ø—Ä–∏–π–º–∞—Ç–∏ –ø–ª–∞—Ç–µ–∂—ñ –æ–Ω–ª–∞–π–Ω –ø–æ –≤—Å—å–æ–º—É —Å–≤—ñ—Ç—É –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é –ø–ª–µ—è–¥–∏ —Ä—ñ–∑–Ω–∏—Ö –º–µ—Ç–æ–¥—ñ–≤ —Ç–∞ —Ç–µ—Ö–Ω–æ–ª–æ–≥—ñ–π. –ó —Ä–æ–∫—É –≤ —Ä—ñ–∫ –º–∏ –∑—Ä–æ—Å—Ç–∞—î–º–æ –Ω–∞ —Å–æ—Ç–Ω—ñ –≤—ñ–¥—Å–æ—Ç–∫—ñ–≤ —Ç–∞ —Å–ø—ñ–ª—å–Ω–æ –∑ –º—ñ–∂–Ω–∞—Ä–æ–¥–Ω–∏–º–∏ –ø–ª–∞—Ç—ñ–∂–Ω–∏–º–∏ —Å–∏—Å—Ç–µ–º–∞–º–∏ Visa, Mastercard i –±–∞–Ω–∫–∞–º–∏-–ø–∞—Ä—Ç–Ω–µ—Ä–∞–º–∏ –∑ —É—Å—å–æ–≥–æ —Å–≤—ñ—Ç—É, —Å—Ç–≤–æ—Ä—é—î–º–æ –≤–ª–∞—Å–Ω—ñ —Å–µ—Ä–≤—ñ—Å–∏. –ù–∞—à –ø—Ä–æ–¥—É–∫—Ç –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—î –º—ñ–∂–Ω–∞—Ä–æ–¥–Ω–∏–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º –±–µ–∑–ø–µ–∫–∏ –¥–∞–Ω–∏—Ö —ñ–Ω–¥—É—Å—Ç—Ä—ñ—ó –ø–ª–∞—Ç—ñ–∂–Ω–∏—Ö –∫–∞—Ä—Ç, —î –±–∞–≥–∞—Ç–æ—à–∞—Ä–æ–≤–∏–º —Ç–∞ –º–æ–∂–µ –±—É—Ç–∏ –∞–¥–∞–ø—Ç–æ–≤–∞–Ω–∏–π –ø—ñ–¥ –Ω–∞–π–≤–∏–±–∞–≥–ª–∏–≤—ñ—à–æ–≥–æ –∫–ª—ñ—î–Ω—Ç–∞. Solid ‚Äî —Ü–µ –∫–æ–º–∞–Ω–¥–∞, —è–∫–∞ –º–∞—î –∫–æ–ª–æ—Å–∞–ª—å–Ω–∏–π –¥–æ—Å–≤—ñ–¥ –≤ —Ñ—ñ–Ω—Ç–µ—Ö, –µ–∫–≤–∞–π—Ä–∏–Ω–≥—É, –∞–Ω–∞–ª—ñ—Ç–∏—Ü—ñ —Ç–∞ —Ä–∏–∑–∏–∫-–º–µ–Ω–µ–¥–∂–º–µ–Ω—Ç—ñ —ñ, —è–∫–∞ –ø—ñ–¥—Ç—Ä–∏–º—É—î —Å—Ç—Ä—ñ–º–∫–µ –∑—Ä–æ—Å—Ç–∞–Ω–Ω—è –ø–∞—Ä—Ç–Ω–µ—Ä—ñ–≤ –Ω–∞ –≤—Å—ñ—Ö –∫–ª—é—á–æ–≤–∏—Ö —Ä–∏–Ω–∫–∞—Ö. –ú–∏ —à—É–∫–∞—î–º–æ –∞–∫—Ç–∏–≤–Ω–æ–≥–æ —ñ —Ü—ñ–ª–µ—Å–ø—Ä—è–º–æ–≤–∞–Ω–æ–≥–æ Middle Data Analyst , —è–∫–∏–π –º–æ–∂–µ –∑–º—ñ—Ü–Ω–∏—Ç–∏ –Ω–∞—à Data Squad, —ñ –≤—ñ–∑—å–º–µ —É—á–∞—Å—Ç—å –≤ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—ñ –Ω–∞—Å—Ç—É–ø–Ω–∏—Ö —Å–µ—Ä–≤—ñ—Å—ñ–≤:‚Ä¢ Automatic Routing System ‚Äî —Å–∏—Å—Ç–µ–º–∞ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü—ñ—ó —Ç—Ä–∞–Ω–∑–∞–∫—Ü—ñ–π –º—ñ–∂ –±–∞–Ω–∫–∞–º–∏;‚Ä¢ Risk Scoring Engine ‚Äî —Å–∫–æ—Ä–∏–Ω–≥–æ–≤–∞ –º–æ–¥–µ–ª—å –æ—Ü—ñ–Ω–∫–∏ –ø—ñ–¥–æ–∑—Ä—ñ–ª–∏—Ö —Ç—Ä–∞–Ω–∑–∞–∫—Ü—ñ–π;‚Ä¢ Subscription Booster ‚Äî —Å–µ—Ä–≤—ñ—Å –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó —Ä–µ–≥—É–ª—è—Ä–Ω–∏—Ö —Å–ø–∏—Å–∞–Ω—å –¥–ª—è –∫–ª—ñ—î–Ω—Ç—ñ–≤, —â–æ –ø—Ä–∞—Ü—é—é—Ç—å –ø–æ –º–æ–¥–µ–ª—ñ –ø—ñ–¥–ø–∏—Å–æ–∫. –í–∏ –º–æ–∂–µ—Ç–µ –±—É—Ç–∏ —Å–∞–º–µ —Ç–∏–º, –∫–æ–≥–æ –º–∏ —à—É–∫–∞—î–º–æ! –£ —Ç–µ–±–µ –±—É–¥—É—Ç—å —Ç–∞–∫—ñ –∑–∞–¥–∞—á—ñ :‚Ä¢ —É—á–∞—Å—Ç—å –≤ –ø–æ–±—É–¥–æ–≤—ñ –∞–Ω–∞–ª—ñ—Ç–∏–∫–∏ –º–µ—Ç—Ä–∏–∫ —ñ –ø—Ä–æ—Ü–µ—Å—ñ–≤ —Ä–æ–±–æ—Ç–∏ –ø–ª–∞—Ç—ñ–∂–Ω–æ–≥–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞;—Å—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ç–∞ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è –∞–Ω–∞–ª—ñ—Ç–∏—á–Ω–∏—Ö —Å–µ—Ä–≤—ñ—Å—ñ–≤ –∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º ML –∞–ª–≥–æ—Ä–∏—Ç–º—ñ–≤ ‚Äî –≤—ñ–¥ —ñ–¥–µ—ó –¥–æ –∑–∞–ø—É—Å–∫—É –≤ –ø—Ä–æ–¥–∞–∫—à–∏–Ω—ñ;‚Ä¢ –ø—Ä–æ–≤–µ–¥–µ–Ω–Ω—è –¥–æ—Å–ª—ñ–¥–∂–µ–Ω—å –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ —Ç–∞ –ø–æ—à—É–∫—É –Ω–æ–≤–∏—Ö –ø—Ä–æ–¥—É–∫—Ç–æ–≤–∏—Ö –≥—ñ–ø–æ—Ç–µ–∑;‚Ä¢ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è —ñ—Å–Ω—É—é—á–æ—ó —Å–∏—Å—Ç–µ–º–∏ –º–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥—É –æ—Å–Ω–æ–≤–Ω–∏—Ö –º–µ—Ç—Ä–∏–∫;‚Ä¢ —Ä–æ–∑—Ä–æ–±–∫–∞ —Å–∏—Å—Ç–µ–º–∏ –∫–æ–Ω—Ç—Ä–æ–ª—é —è–∫–æ—Å—Ç—ñ –¥–∞–Ω–∏—Ö;‚Ä¢ —Ç—ñ—Å–Ω–∞ —Ä–æ–±–æ—Ç–∞ –∑ –ø—Ä–æ–¥—É–∫—Ç–æ–≤–æ—é, —Ç–µ—Ö–Ω—ñ—á–Ω–æ—é –∫–æ–º–∞–Ω–¥–∞–º–∏ –ø—Ä–æ–µ–∫—Ç—É –∑ —Ü—ñ–ª–ª—é –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è —Ä–æ–±–æ—á–∏—Ö —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ñ–≤, —Ñ—É–Ω–∫—Ü—ñ–æ–Ω–∞–ª—É —Å–∏—Å—Ç–µ–º–∏, –¥–∂–µ—Ä–µ–ª –¥–∞–Ω–∏—Ö —Ç–∞ –º–µ—Ç–æ–¥—ñ–≤ –≤–∏—è–≤–ª–µ–Ω–Ω—è –∞–Ω–æ–º–∞–ª—ñ–π. –î–ª—è –Ω–∞—Å –≤–∞–∂–ª–∏–≤–æ, —â–æ–± —Ç–∏ –º–∞–≤ :‚Ä¢ 1,5 —Ä–æ–∫–∏ –∫–æ–º–µ—Ä—Ü—ñ–π–Ω–æ–≥–æ –¥–æ—Å–≤—ñ–¥—É –Ω–∞ –∞–Ω–∞–ª–æ–≥—ñ—á–Ω—ñ–π –ø–æ–∑–∏—Ü—ñ—ó;‚Ä¢ –¥–æ—Å–≤—ñ–¥ —Ä–æ–±–æ—Ç–∏ –∑ BI —Å–∏—Å—Ç–µ–º–∞–º–∏ (Tableau/PowerBI) –∞–±–æ —ñ–Ω—à–∏–º–∏ –∑–∞—Å–æ–±–∞–º–∏ –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó Mode, plot.ly;‚Ä¢ —á—É–¥–æ–≤—ñ –∑–Ω–∞–Ω–Ω—è SQL —ñ –¥–æ—Å–≤—ñ–¥ —Ä–æ–±–æ—Ç–∏ –∑ —Ä–µ–ª—è—Ü—ñ–π–Ω–∏–º–∏ –±–∞–∑–∞–º–∏ –¥–∞–Ω–∏—Ö;‚Ä¢ –¥–æ—Å–≤—ñ–¥ —Ä–µ–∞–ª—ñ–∑–∞—Ü—ñ—ó –∑–∞–¥–∞—á –Ω–∞ Python: –≤–ø–µ–≤–Ω–µ–Ω—ñ –∑–Ω–∞–Ω–Ω—è –æ—Å–Ω–æ–≤–Ω–∏—Ö –±—ñ–±–ª—ñ–æ—Ç–µ–∫ ‚Äî pandas, numpy, scikit, xgboost, catboost, etc.;‚Ä¢ –Ω–∞–≤–∏—á–∫–∏ —Ä–æ–±–æ—Ç–∏ –∑ Shell, Git;‚Ä¢ English ‚Äî Intermediate. –ë—É–¥–µ –ø–ª—é—Å–æ–º :‚Ä¢ –∑–Ω–∞–Ω–Ω—è Elastic —Å—Ç–µ–∫—É ‚Äî Search, Kibana;‚Ä¢ –¥–æ—Å–≤—ñ–¥ –≤–∏—Ä—ñ—à–µ–Ω–Ω—è –ø—Ä–∞–∫—Ç–∏—á–Ω–∏—Ö –∑–∞–¥–∞—á Data Science;‚Ä¢ –¥–æ—Å–≤—ñ–¥ –ø–æ–±—É–¥–æ–≤–∏ Data Pipeline. –ú–∏ —Å—Ç–≤–æ—Ä–∏–ª–∏ –≤—Å—ñ —É–º–æ–≤–∏ –¥–ª—è —Ç–≤–æ–≥–æ —É—Å–ø—ñ—Ö—É –ö–æ–º—Ñ–æ—Ä—Ç–Ω—ñ —É–º–æ–≤–∏ —Ç–∞ –≥–Ω—É—á–∫–∏–π –≥—Ä–∞—Ñ—ñ–∫ –ö–ª–∞—Å–Ω–∏–π –æ—Ñ—ñ—Å –≤ 5 —Ö–≤–∏–ª–∏–Ω–∞—Ö –≤—ñ–¥ –º–µ—Ç—Ä–æ –¢–∞—Ä–∞—Å–∞ –®–µ–≤—á–µ–Ω–∫–æ –∑ —Ç–µ—Ä–∞—Å–æ—é, –ª–∞—É–Ω–∂ –∑–æ–Ω–∞–º–∏, –∫—É—Ö–Ω–µ—é, PlayStation. –í –Ω–∞—à–∏—Ö –æ—Ñ—ñ—Å–∞—Ö –±–∞–≥–∞—Ç–æ —Ä–æ—Å–ª–∏–Ω —Ç–∞ —Å–ø–µ—Ü—ñ–∞–ª—å–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ –≤–µ–Ω—Ç–∏–ª—è—Ü—ñ—ó —ñ –∫–æ–Ω–¥–∏—Ü—ñ–æ–Ω—É–≤–∞–Ω–Ω—è. –ú–∏ –∑–∞–±–µ–∑–ø–µ—á—É—î–º–æ 20 –¥–Ω—ñ–≤ –æ–ø–ª–∞—á—É–≤–∞–Ω–æ—ó –≤—ñ–¥–ø—É—Å—Ç–∫–∏ –≤ —Ä—ñ–∫ —ñ –∑—Ä—É—á–Ω–∏–π —Ä–æ–±–æ—á–∏–π –≥—Ä–∞—Ñ—ñ–∫. –¢—É—Ä–±–æ—Ç–∞ –ø—Ä–æ –∑–¥–æ—Ä–æ–≤‚Äô—è —ñ —Å–ø–æ—Ä—Ç –°–Ω—ñ–¥–∞–Ω–∫–∏, –æ–±—ñ–¥–∏, –±–µ–∑–º–µ–∂–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ñ—Ä—É–∫—Ç—ñ–≤, —Å–Ω–µ–∫—ñ–≤, —Å–º—É–∑—ñ —Ç–∞ –π–æ–≥—É—Ä—Ç—ñ–≤ –≤ –æ—Ñ—ñ—Å—ñ. –ö–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–∏–π –ª—ñ–∫–∞—Ä —ñ –º–µ–¥–∏—á–Ω–µ —Å—Ç—Ä–∞—Ö—É–≤–∞–Ω–Ω—è. –ë–µ–∑–∫–æ—à—Ç–æ–≤–Ω—ñ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –∑ –±—ñ–≥—É, —Ñ—É—Ç–±–æ–ª—É, –±–∞—Å–∫–µ—Ç–±–æ–ª—É, –≤–æ–ª–µ–π–±–æ–ª—É —Ç–∞ –π–æ–≥–∏. –ó–Ω–∏–∂–∫–∏ –≤ –Ω–∞–π–±–ª–∏–∂—á—ñ —Å–ø–æ—Ä—Ç–∑–∞–ª–∏ —Ç–∞ –æ–ø–ª–∞—Ç–∞ —É—á–∞—Å—Ç—ñ —É —Å–ø–æ—Ä—Ç–∏–≤–Ω–∏—Ö –∑–º–∞–≥–∞–Ω–Ω—è—Ö. –ù–∞–≤—á–∞–Ω–Ω—è —Ç–∞ —Ä–æ–∑–≤–∏—Ç–æ–∫ Business —ñ Management School –¥–ª—è —Å–ø—ñ–≤—Ä–æ–±—ñ—Ç–Ω–∏–∫—ñ–≤ –∫–æ–º–ø–∞–Ω—ñ—ó. –í–µ–ª–∏–∫–∞ –µ–ª–µ–∫—Ç—Ä–æ–Ω–Ω–∞ –±—ñ–±–ª—ñ–æ—Ç–µ–∫–∞ —Ç–∞ –¥–æ—Å—Ç—É–ø –¥–æ –ø–ª–∞—Ç–Ω–∏—Ö –æ–Ω–ª–∞–π–Ω-–∫—É—Ä—Å—ñ–≤ —ñ –∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü—ñ–π, –≤–Ω—É—Ç—Ä—ñ—à–Ω—ñ –±–µ—Å—ñ–¥–∏ —ñ –≤–æ—Ä–∫—à–æ–ø–∏, –∫—É—Ä—Å–∏ –∞–Ω–≥–ª—ñ–π—Å—å–∫–æ—ó —ñ speaking club. –ö–æ–º–ø–µ–Ω—Å–∞—Ü—ñ—è –¥–æ–¥–∞—Ç–∫–æ–≤–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è –Ω–∞ –∑–æ–≤–Ω—ñ—à–Ω—ñ—Ö —Ç—Ä–µ–Ω—ñ–Ω–≥–∞—Ö —ñ —Å–µ–º—ñ–Ω–∞—Ä–∞—Ö. –ì—É—á–Ω—ñ –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–∏ –î–≤–∞ —Ä–∞–∑–∏ –Ω–∞ —Ä—ñ–∫ ‚Äî –≤–ª—ñ—Ç–∫—É —ñ –≤–∑–∏–º–∫—É ‚Äî –º–∏ –ø—Ä–æ–≤–æ–¥–∏–º–æ –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–∏ –∑ —à–∞–ª–µ–Ω–∏–º –ª–∞–π–Ω–∞–ø–æ–º –≤ –∫—Ä—É—Ç–∏—Ö –ª–æ–∫–∞—Ü—ñ—è—Ö. –©–æ–º—ñ—Å—è—Ü—è –æ—Ñ—ñ—Å–∏ –∑—É—Å—Ç—Ä—ñ—á–∞—é—Ç—å—Å—è –Ω–∞ –ª–æ–∫–∞–ª—å–Ω—ñ —Ç—É—Å–æ–≤–∫–∏ —Ç–∞ –≤–∏—ó–∑–Ω—ñ –∑–∞—Ö–æ–¥–∏, –∞ –ø—ñ—Å–ª—è –∫–≤–∞—Ä—Ç–∞–ª—å–Ω–æ–≥–æ –∑–≤—ñ—Ç—É –¥–ª—è –≤—Å—ñ—î—ó –∫–æ–º–∞–Ω–¥–∏ –≤–ª–∞—à—Ç–æ–≤—É—é—Ç—å –≤–µ—á—ñ—Ä–∫—É. –ü—Ä–∏—î–¥–Ω—É–π—Å—è –¥–æ –Ω–∞—Å –≤ –∫–æ–º–∞–Ω–¥—É!",Middle Data Analyst@Solid - Fintech Company,https://jobs.dou.ua/companies/solid/vacancies/130407/, Kyiv,Middle Data Analyst,05 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/solid/,Solid - Fintech Company,{},,"Solid ‚Äî —Ü–µ —Ñ—ñ–Ω—Ç–µ—Ö-–∫–æ–º–ø–∞–Ω—ñ—è, —è–∫–∞ –¥–æ–ø–æ–º–∞–≥–∞—î –±—ñ–∑–Ω–µ—Å–∞–º –ø—Ä–∏–π–º–∞—Ç–∏ –ø–ª–∞—Ç–µ–∂—ñ –æ–Ω–ª–∞–π–Ω –ø–æ –≤—Å—å–æ–º—É —Å–≤—ñ—Ç—É –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é –ø–ª–µ—è–¥–∏ —Ä—ñ–∑–Ω–∏—Ö –º–µ—Ç–æ–¥—ñ–≤ —Ç–∞ —Ç–µ—Ö–Ω–æ–ª–æ–≥—ñ–π. –ó —Ä–æ–∫—É –≤ —Ä—ñ–∫ –º–∏ –∑—Ä–æ—Å—Ç–∞—î–º–æ –Ω–∞ —Å–æ—Ç–Ω—ñ –≤—ñ–¥—Å–æ—Ç–∫—ñ–≤ —Ç–∞ —Å–ø—ñ–ª—å–Ω–æ –∑ –º—ñ–∂–Ω–∞—Ä–æ–¥–Ω–∏–º–∏ –ø–ª–∞—Ç—ñ–∂–Ω–∏–º–∏ —Å–∏—Å—Ç–µ–º–∞–º–∏ Visa, Mastercard i –±–∞–Ω–∫–∞–º–∏-–ø–∞—Ä—Ç–Ω–µ—Ä–∞–º–∏ –∑ —É—Å—å–æ–≥–æ —Å–≤—ñ—Ç—É, —Å—Ç–≤–æ—Ä—é—î–º–æ –≤–ª–∞—Å–Ω—ñ —Å–µ—Ä–≤—ñ—Å–∏. –ù–∞—à –ø—Ä–æ–¥—É–∫—Ç –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—î –º—ñ–∂–Ω–∞—Ä–æ–¥–Ω–∏–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º –±–µ–∑–ø–µ–∫–∏ –¥–∞–Ω–∏—Ö —ñ–Ω–¥—É—Å—Ç—Ä—ñ—ó –ø–ª–∞—Ç—ñ–∂–Ω–∏—Ö –∫–∞—Ä—Ç, —î –±–∞–≥–∞—Ç–æ—à–∞—Ä–æ–≤–∏–º —Ç–∞ –º–æ–∂–µ –±—É—Ç–∏ –∞–¥–∞–ø—Ç–æ–≤–∞–Ω–∏–π –ø—ñ–¥ –Ω–∞–π–≤–∏–±–∞–≥–ª–∏–≤—ñ—à–æ–≥–æ –∫–ª—ñ—î–Ω—Ç–∞. Solid ‚Äî —Ü–µ –∫–æ–º–∞–Ω–¥–∞, —è–∫–∞ –º–∞—î –∫–æ–ª–æ—Å–∞–ª—å–Ω–∏–π –¥–æ—Å–≤—ñ–¥ –≤ —Ñ—ñ–Ω—Ç–µ—Ö, –µ–∫–≤–∞–π—Ä–∏–Ω–≥—É, –∞–Ω–∞–ª—ñ—Ç–∏—Ü—ñ —Ç–∞ —Ä–∏–∑–∏–∫-–º–µ–Ω–µ–¥–∂–º–µ–Ω—Ç—ñ —ñ, —è–∫–∞ –ø—ñ–¥—Ç—Ä–∏–º—É—î —Å—Ç—Ä—ñ–º–∫–µ –∑—Ä–æ—Å—Ç–∞–Ω–Ω—è –ø–∞—Ä—Ç–Ω–µ—Ä—ñ–≤ –Ω–∞ –≤—Å—ñ—Ö –∫–ª—é—á–æ–≤–∏—Ö —Ä–∏–Ω–∫–∞—Ö. –ú–∏ —à—É–∫–∞—î–º–æ –∞–∫—Ç–∏–≤–Ω–æ–≥–æ —ñ —Ü—ñ–ª–µ—Å–ø—Ä—è–º–æ–≤–∞–Ω–æ–≥–æ Senior Data Analyst, —è–∫–∏–π –º–æ–∂–µ –∑–º—ñ—Ü–Ω–∏—Ç–∏ –Ω–∞—à Data Squad, —ñ –≤—ñ–∑—å–º–µ —É—á–∞—Å—Ç—å –≤ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—ñ –Ω–∞—Å—Ç—É–ø–Ω–∏—Ö —Å–µ—Ä–≤—ñ—Å—ñ–≤:‚Ä¢ Automatic Routing System ‚Äî —Å–∏—Å—Ç–µ–º–∞ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü—ñ—ó —Ç—Ä–∞–Ω–∑–∞–∫—Ü—ñ–π –º—ñ–∂ –±–∞–Ω–∫–∞–º–∏;‚Ä¢ Risk Scoring Engine ‚Äî —Å–∫–æ—Ä–∏–Ω–≥–æ–≤–∞ –º–æ–¥–µ–ª—å –æ—Ü—ñ–Ω–∫–∏ –ø—ñ–¥–æ–∑—Ä—ñ–ª–∏—Ö —Ç—Ä–∞–Ω–∑–∞–∫—Ü—ñ–π;‚Ä¢ Subscription Booster ‚Äî —Å–µ—Ä–≤—ñ—Å –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó —Ä–µ–≥—É–ª—è—Ä–Ω–∏—Ö —Å–ø–∏—Å–∞–Ω—å –¥–ª—è –∫–ª—ñ—î–Ω—Ç—ñ–≤, —â–æ –ø—Ä–∞—Ü—é—é—Ç—å –ø–æ –º–æ–¥–µ–ª—ñ –ø—ñ–¥–ø–∏—Å–æ–∫. –í–∏ –º–æ–∂–µ—Ç–µ –±—É—Ç–∏ —Å–∞–º–µ —Ç–∏–º, –∫–æ–≥–æ –º–∏ —à—É–∫–∞—î–º–æ! –£ —Ç–µ–±–µ –±—É–¥—É—Ç—å —Ç–∞–∫—ñ –∑–∞–¥–∞—á—ñ:‚Ä¢ —É—á–∞—Å—Ç—å –≤ –ø–æ–±—É–¥–æ–≤—ñ –∞–Ω–∞–ª—ñ—Ç–∏–∫–∏ –º–µ—Ç—Ä–∏–∫ —ñ –ø—Ä–æ—Ü–µ—Å—ñ–≤ —Ä–æ–±–æ—Ç–∏ –ø–ª–∞—Ç—ñ–∂–Ω–æ–≥–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞;‚Ä¢ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ç–∞ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è –∞–Ω–∞–ª—ñ—Ç–∏—á–Ω–∏—Ö —Å–µ—Ä–≤—ñ—Å—ñ–≤ –∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º ML –∞–ª–≥–æ—Ä–∏—Ç–º—ñ–≤ ‚Äî –≤—ñ–¥ —ñ–¥–µ—ó –¥–æ –∑–∞–ø—É—Å–∫—É –≤ –ø—Ä–æ–¥–∞–∫—à–∏–Ω—ñ;‚Ä¢ –ø—Ä–æ–≤–µ–¥–µ–Ω–Ω—è –¥–æ—Å–ª—ñ–¥–∂–µ–Ω—å –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ —Ç–∞ –ø–æ—à—É–∫—É –Ω–æ–≤–∏—Ö –ø—Ä–æ–¥—É–∫—Ç–æ–≤–∏—Ö –≥—ñ–ø–æ—Ç–µ–∑;‚Ä¢ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è —ñ—Å–Ω—É—é—á–æ—ó —Å–∏—Å—Ç–µ–º–∏ –º–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥—É –æ—Å–Ω–æ–≤–Ω–∏—Ö –º–µ—Ç—Ä–∏–∫;‚Ä¢ —Ä–æ–∑—Ä–æ–±–∫–∞ —Å–∏—Å—Ç–µ–º–∏ –∫–æ–Ω—Ç—Ä–æ–ª—é —è–∫–æ—Å—Ç—ñ –¥–∞–Ω–∏—Ö;‚Ä¢ —Ç—ñ—Å–Ω–∞ —Ä–æ–±–æ—Ç–∞ –∑ –ø—Ä–æ–¥—É–∫—Ç–æ–≤–æ—é, —Ç–µ—Ö–Ω—ñ—á–Ω–æ—é –∫–æ–º–∞–Ω–¥–∞–º–∏ –ø—Ä–æ–µ–∫—Ç—É –∑ —Ü—ñ–ª–ª—é –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è —Ä–æ–±–æ—á–∏—Ö —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ñ–≤, —Ñ—É–Ω–∫—Ü—ñ–æ–Ω–∞–ª—É —Å–∏—Å—Ç–µ–º–∏, –¥–∂–µ—Ä–µ–ª –¥–∞–Ω–∏—Ö —Ç–∞ –º–µ—Ç–æ–¥—ñ–≤ –≤–∏—è–≤–ª–µ–Ω–Ω—è –∞–Ω–æ–º–∞–ª—ñ–π. –î–ª—è –Ω–∞—Å –≤–∞–∂–ª–∏–≤–æ, —â–æ–± —Ç–∏ –º–∞–≤:‚Ä¢ –∫–æ–º–µ—Ä—Ü—ñ–π–Ω–∏–π –¥–æ—Å–≤—ñ–¥ –Ω–∞ –∞–Ω–∞–ª–æ–≥—ñ—á–Ω—ñ–π –ø–æ–∑–∏—Ü—ñ—ó –≤—ñ–¥ 3—Ö —Ä–æ–∫—ñ–≤;‚Ä¢ –¥–æ—Å–≤—ñ–¥ —Ä–æ–±–æ—Ç–∏ –∑ BI —Å–∏—Å—Ç–µ–º–∞–º–∏ (Tableau/PowerBI) –∞–±–æ —ñ–Ω—à–∏–º–∏ –∑–∞—Å–æ–±–∞–º–∏ –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó Mode, plot.ly;‚Ä¢ —á—É–¥–æ–≤—ñ –∑–Ω–∞–Ω–Ω—è SQL —ñ –¥–æ—Å–≤—ñ–¥ —Ä–æ–±–æ—Ç–∏ –∑ —Ä–µ–ª—è—Ü—ñ–π–Ω–∏–º–∏ –±–∞–∑–∞–º–∏ –¥–∞–Ω–∏—Ö;‚Ä¢ –¥–æ—Å–≤—ñ–¥ —Ä–µ–∞–ª—ñ–∑–∞—Ü—ñ—ó –∑–∞–¥–∞—á –Ω–∞ Python: –≤–ø–µ–≤–Ω–µ–Ω—ñ –∑–Ω–∞–Ω–Ω—è –æ—Å–Ω–æ–≤–Ω–∏—Ö –±—ñ–±–ª—ñ–æ—Ç–µ–∫ ‚Äî pandas, numpy, scikit, xgboost, catboost, etc.;‚Ä¢ –Ω–∞–≤–∏—á–∫–∏ —Ä–æ–±–æ—Ç–∏ –∑ Shell, Git;‚Ä¢ English ‚Äî Intermediate. –ë—É–¥–µ –ø–ª—é—Å–æ–º:‚Ä¢ –∑–Ω–∞–Ω–Ω—è Elastic —Å—Ç–µ–∫—É ‚Äî Search, Kibana;‚Ä¢ –¥–æ—Å–≤—ñ–¥ –≤–∏—Ä—ñ—à–µ–Ω–Ω—è –ø—Ä–∞–∫—Ç–∏—á–Ω–∏—Ö –∑–∞–¥–∞—á Data Science;‚Ä¢ –¥–æ—Å–≤—ñ–¥ –ø–æ–±—É–¥–æ–≤–∏ Data Pipeline. –ú–∏ —Å—Ç–≤–æ—Ä–∏–ª–∏ –≤—Å—ñ —É–º–æ–≤–∏ –¥–ª—è —Ç–≤–æ–≥–æ —É—Å–ø—ñ—Ö—É –ö–æ–º—Ñ–æ—Ä—Ç–Ω—ñ —É–º–æ–≤–∏ —Ç–∞ –≥–Ω—É—á–∫–∏–π –≥—Ä–∞—Ñ—ñ–∫ –ö–ª–∞—Å–Ω–∏–π –æ—Ñ—ñ—Å –≤ 5 —Ö–≤–∏–ª–∏–Ω–∞—Ö –≤—ñ–¥ –º–µ—Ç—Ä–æ –¢–∞—Ä–∞—Å–∞ –®–µ–≤—á–µ–Ω–∫–æ –∑ —Ç–µ—Ä–∞—Å–æ—é, –ª–∞—É–Ω–∂ –∑–æ–Ω–∞–º–∏, –∫—É—Ö–Ω–µ—é, PlayStation. –í –Ω–∞—à–∏—Ö –æ—Ñ—ñ—Å–∞—Ö –±–∞–≥–∞—Ç–æ —Ä–æ—Å–ª–∏–Ω —Ç–∞ —Å–ø–µ—Ü—ñ–∞–ª—å–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ –≤–µ–Ω—Ç–∏–ª—è—Ü—ñ—ó —ñ –∫–æ–Ω–¥–∏—Ü—ñ–æ–Ω—É–≤–∞–Ω–Ω—è. –ú–∏ –∑–∞–±–µ–∑–ø–µ—á—É—î–º–æ 20 –¥–Ω—ñ–≤ –æ–ø–ª–∞—á—É–≤–∞–Ω–æ—ó –≤—ñ–¥–ø—É—Å—Ç–∫–∏ –≤ —Ä—ñ–∫ —ñ –∑—Ä—É—á–Ω–∏–π —Ä–æ–±–æ—á–∏–π –≥—Ä–∞—Ñ—ñ–∫. –¢—É—Ä–±–æ—Ç–∞ –ø—Ä–æ –∑–¥–æ—Ä–æ–≤‚Äô—è —ñ —Å–ø–æ—Ä—Ç –°–Ω—ñ–¥–∞–Ω–∫–∏, –æ–±—ñ–¥–∏, –±–µ–∑–º–µ–∂–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ñ—Ä—É–∫—Ç—ñ–≤, —Å–Ω–µ–∫—ñ–≤, —Å–º—É–∑—ñ —Ç–∞ –π–æ–≥—É—Ä—Ç—ñ–≤ –≤ –æ—Ñ—ñ—Å—ñ. –ö–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–∏–π –ª—ñ–∫–∞—Ä —ñ –º–µ–¥–∏—á–Ω–µ —Å—Ç—Ä–∞—Ö—É–≤–∞–Ω–Ω—è. –ë–µ–∑–∫–æ—à—Ç–æ–≤–Ω—ñ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –∑ –±—ñ–≥—É, —Ñ—É—Ç–±–æ–ª—É, –±–∞—Å–∫–µ—Ç–±–æ–ª—É, –≤–æ–ª–µ–π–±–æ–ª—É —Ç–∞ –π–æ–≥–∏. –ó–Ω–∏–∂–∫–∏ –≤ –Ω–∞–π–±–ª–∏–∂—á—ñ —Å–ø–æ—Ä—Ç–∑–∞–ª–∏ —Ç–∞ –æ–ø–ª–∞—Ç–∞ —É—á–∞—Å—Ç—ñ —É —Å–ø–æ—Ä—Ç–∏–≤–Ω–∏—Ö –∑–º–∞–≥–∞–Ω–Ω—è—Ö. –ù–∞–≤—á–∞–Ω–Ω—è —Ç–∞ —Ä–æ–∑–≤–∏—Ç–æ–∫ Business —ñ Management School –¥–ª—è —Å–ø—ñ–≤—Ä–æ–±—ñ—Ç–Ω–∏–∫—ñ–≤ –∫–æ–º–ø–∞–Ω—ñ—ó. –í–µ–ª–∏–∫–∞ –µ–ª–µ–∫—Ç—Ä–æ–Ω–Ω–∞ –±—ñ–±–ª—ñ–æ—Ç–µ–∫–∞ —Ç–∞ –¥–æ—Å—Ç—É–ø –¥–æ –ø–ª–∞—Ç–Ω–∏—Ö –æ–Ω–ª–∞–π–Ω-–∫—É—Ä—Å—ñ–≤ —ñ –∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü—ñ–π, –≤–Ω—É—Ç—Ä—ñ—à–Ω—ñ –±–µ—Å—ñ–¥–∏ —ñ –≤–æ—Ä–∫—à–æ–ø–∏, –∫—É—Ä—Å–∏ –∞–Ω–≥–ª—ñ–π—Å—å–∫–æ—ó —ñ speaking club. –ö–æ–º–ø–µ–Ω—Å–∞—Ü—ñ—è –¥–æ–¥–∞—Ç–∫–æ–≤–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è –Ω–∞ –∑–æ–≤–Ω—ñ—à–Ω—ñ—Ö —Ç—Ä–µ–Ω—ñ–Ω–≥–∞—Ö —ñ —Å–µ–º—ñ–Ω–∞—Ä–∞—Ö. –ì—É—á–Ω—ñ –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–∏ –î–≤–∞ —Ä–∞–∑–∏ –Ω–∞ —Ä—ñ–∫ ‚Äî –≤–ª—ñ—Ç–∫—É —ñ –≤–∑–∏–º–∫—É ‚Äî –º–∏ –ø—Ä–æ–≤–æ–¥–∏–º–æ –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–∏ –∑ —à–∞–ª–µ–Ω–∏–º –ª–∞–π–Ω–∞–ø–æ–º –≤ –∫—Ä—É—Ç–∏—Ö –ª–æ–∫–∞—Ü—ñ—è—Ö. –©–æ–º—ñ—Å—è—Ü—è –æ—Ñ—ñ—Å–∏ –∑—É—Å—Ç—Ä—ñ—á–∞—é—Ç—å—Å—è –Ω–∞ –ª–æ–∫–∞–ª—å–Ω—ñ —Ç—É—Å–æ–≤–∫–∏ —Ç–∞ –≤–∏—ó–∑–Ω—ñ –∑–∞—Ö–æ–¥–∏, –∞ –ø—ñ—Å–ª—è –∫–≤–∞—Ä—Ç–∞–ª—å–Ω–æ–≥–æ –∑–≤—ñ—Ç—É –¥–ª—è –≤—Å—ñ—î—ó –∫–æ–º–∞–Ω–¥–∏ –≤–ª–∞—à—Ç–æ–≤—É—é—Ç—å –≤–µ—á—ñ—Ä–∫—É. –ü—Ä–∏—î–¥–Ω—É–π—Å—è –¥–æ –Ω–∞—Å –≤ –∫–æ–º–∞–Ω–¥—É!",Senior Data Analyst@Solid - Fintech Company,https://jobs.dou.ua/companies/solid/vacancies/130409/, Kyiv,Senior Data Analyst,05 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/admixer-advertising-technologies/,Admixer,"{""Required skills"": [""\u0414\u0430\u0432\u0430\u0439 \u0437\u043d\u0430\u043a\u043e\u043c\u0438\u0442\u044c\u0441\u044f. \u041c\u044b"", ""\u043a\u043e\u043c\u0430\u043d\u0434\u0430 Admixer. \u0412\u043e\u0442 \u0443\u0436\u0435 10 \u043b\u0435\u0442 \u043c\u044b \u0440\u0430\u0437\u0440\u0430\u0431\u0430\u0442\u044b\u0432\u0430\u0435\u043c \u0440\u0435\u0448\u0435\u043d\u0438\u044f \u0434\u043b\u044f \u0432\u0441\u0435\u0445 \u0443\u0447\u0430\u0441\u0442\u043d\u0438\u043a\u043e\u0432 AdTech \u0440\u044b\u043d\u043a\u0430:-\u0441\u043e\u0437\u0434\u0430\u0435\u043c \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u044b\u0435 \u0438\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442\u044b \u0434\u043b\u044f \u043f\u043e\u043a\u0443\u043f\u043a\u0438, \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u0438, \u0438\u0437\u043c\u0435\u0440\u0435\u043d\u0438\u044f \u0438 \u043f\u0440\u043e\u0432\u0435\u0434\u0435\u043d\u0438\u044f \u043c\u0435\u0436\u043f\u043b\u0430\u0442\u0444\u043e\u0440\u043c\u0435\u043d\u043d\u044b\u0445 \u0438 \u043c\u043d\u043e\u0433\u043e\u043a\u0430\u043d\u0430\u043b\u044c\u043d\u044b\u0445 \u0440\u0435\u043a\u043b\u0430\u043c\u043d\u044b\u0445 \u043a\u0430\u043c\u043f\u0430\u043d\u0438\u0439;-\u0440\u0435\u0430\u043b\u0438\u0437\u043e\u0432\u044b\u0432\u0430\u0435\u043c \u0443\u0441\u043f\u0435\u0448\u043d\u044b\u0435 \u0441\u0442\u0440\u0430\u0442\u0435\u0433\u0438\u0438 \u043f\u0440\u043e\u0434\u0430\u0436 \u0447\u0435\u0440\u0435\u0437 RTB \u0438 Header Bidding \u0430\u0443\u043a\u0446\u0438\u043e\u043d\u044b \u0431\u043b\u0430\u0433\u043e\u0434\u0430\u0440\u044f \u043d\u0430\u0448\u0438\u043c \u043f\u0430\u0440\u0442\u043d\u0435\u0440\u0441\u0442\u0432\u0430\u043c \u043c\u0438\u0440\u043e\u0432\u043e\u0433\u043e \u0443\u0440\u043e\u0432\u043d\u044f;-\u043d\u0430\u0448\u0438 \u043e\u0444\u0438\u0441\u044b \u0438 \u0434\u0430\u0442\u0430-\u0446\u0435\u043d\u0442\u0440\u044b \u0440\u0430\u0441\u043f\u043e\u043b\u043e\u0436\u0435\u043d\u044b \u043f\u043e \u0432\u0441\u0435\u043c\u0443 \u043c\u0438\u0440\u0443.""], ""We offer"": [""\u041c\u044b \u043e\u0447\u0435\u043d\u044c \u0446\u0435\u043d\u0438\u043c \u0442\u0430\u043a\u0438\u0435 \u043d\u0430\u0432\u044b\u043a\u0438:"", ""\u0415\u0441\u043b\u0438 \u0442\u044b \u0438\u043c\u0435\u0435\u0448\u044c 1-3 \u0433\u043e\u0434\u0430 \u043f\u0440\u0438\u043c\u0435\u043d\u0438\u043c\u043e\u0433\u043e \u043e\u043f\u044b\u0442\u0430 (\u043e\u043f\u044b\u0442 \u0440\u0430\u0431\u043e\u0442\u044b \u0441 \u043b\u044e\u0431\u043e\u0439 DMP \u043f\u043b\u0430\u0442\u0444\u043e\u0440\u043c\u043e\u0439)"", ""\u0423\u043c\u0435\u0435\u0448\u044c \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u043e \u043f\u0440\u043e\u0432\u043e\u0434\u0438\u0442\u044c \u0430\u043d\u0430\u043b\u0438\u0437 \u0434\u0430\u043d\u043d\u044b\u0445, \u0435\u0441\u0442\u044c \u043e\u043f\u044b\u0442 \u0440\u0430\u0431\u043e\u0442\u044b \u0441 \u0446\u0438\u0444\u0440\u043e\u0432\u044b\u043c\u0438 / \u043e\u043d\u043b\u0430\u0439\u043d-\u043f\u0440\u043e\u0434\u0443\u043a\u0442\u0430\u043c\u0438 \u0438 \u0441\u0440\u0435\u0434\u043e\u0439 \u044d\u043b\u0435\u043a\u0442\u0440\u043e\u043d\u043d\u043e\u0439 \u043a\u043e\u043c\u043c\u0435\u0440\u0446\u0438\u0438"", ""English upper intermediate"", ""\u0417\u043d\u0430\u043d\u0438\u0435 \u0438\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442\u043e\u0432 \u0446\u0438\u0444\u0440\u043e\u0432\u043e\u0433\u043e \u043c\u0430\u0440\u043a\u0435\u0442\u0438\u043d\u0433\u0430"", ""Google, Adobe \u0438 \u0442.\u0434 \u041e\u043f\u044b\u0442 \u0440\u0430\u0431\u043e\u0442\u044b \u0441 Google Tag Manager, \u0442\u0435\u0433\u0430\u043c\u0438 \u0432\u0435\u0431-\u0441\u0430\u0439\u0442\u043e\u0432, SEO \u0438 SEM, DV360 \u0438\u043b\u0438 \u0434\u0440\u0443\u0433\u0438\u043c\u0438 DSP"", ""\u041f\u043e\u043b\u043d\u043e\u0435 \u043f\u043e\u043d\u0438\u043c\u0430\u043d\u0438\u0435 \u043f\u043b\u0430\u0442\u0444\u043e\u0440\u043c \u0434\u0430\u043d\u043d\u044b\u0445 \u0432 \u043c\u0435\u0434\u0438\u0430\u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0441\u0442\u0432\u0435, \u043a\u0430\u043a \u043d\u0430 \u0438\u043d\u0444\u0440\u0430\u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u043d\u043e\u043c (\u0431\u0430\u0437\u043e\u0432\u043e\u043c), \u0442\u0430\u043a \u0438 \u043d\u0430 \u043e\u043f\u0435\u0440\u0430\u0446\u0438\u043e\u043d\u043d\u043e\u043c \u0443\u0440\u043e\u0432\u043d\u0435 (\u044d\u043a\u0441\u043f\u0435\u0440\u0442)"", ""\u041f\u043e\u043d\u0438\u043c\u0430\u043d\u0438\u0435 \u0438\u043d\u0442\u0435\u0433\u0440\u0430\u0446\u0438\u0438 \u043f\u043e\u0441\u0442\u0430\u0432\u043a\u0438 \u0434\u0430\u043d\u043d\u044b\u0445, \u0438\u0441\u0442\u043e\u0447\u043d\u0438\u043a\u043e\u0432 \u0434\u0430\u043d\u043d\u044b\u0445, \u0442\u0430\u043a\u0441\u043e\u043d\u043e\u043c\u0438\u0438, \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u0441\u043a\u043e\u0433\u043e \u0438\u043d\u0442\u0435\u0440\u0444\u0435\u0439\u0441\u0430, \u0444\u0443\u043d\u043a\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e\u0441\u0442\u0438 \u043f\u043b\u0430\u0442\u0444\u043e\u0440\u043c\u044b"", ""\u0416\u0435\u043b\u0430\u0442\u0435\u043b\u044c\u043d\u043e \u0438\u043c\u0435\u0442\u044c \u043e\u043f\u044b\u0442 \u043d\u0430\u043f\u0438\u0441\u0430\u043d\u0438\u044f \u0442\u0435\u0445\u043d\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438 \u0438 \u043f\u0440\u0430\u043a\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u0439 \u0434\u043b\u044f \u043e\u043f\u0435\u0440\u0430\u0442\u0438\u0432\u043d\u044b\u0445 \u0437\u0430\u0434\u0430\u0447""], ""Responsibilities"": [""\u0443\u0447\u0430\u0441\u0442\u0438\u0435 \u0432 \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0441\u0442\u0432\u0435 \u043f\u0435\u0440\u0435\u0434\u043e\u0432\u044b\u0445 \u0442\u0435\u0445\u043d\u043e\u043b\u043e\u0433\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u043f\u0440\u043e\u0434\u0443\u043a\u0442\u043e\u0432 \u0434\u043b\u044f \u043a\u043b\u0438\u0435\u043d\u0442\u043e\u0432 \u043f\u043e \u0432\u0441\u0435\u043c\u0443 \u043c\u0438\u0440\u0443;"", ""\u043c\u043d\u043e\u0433\u043e \u0438\u043d\u0442\u0435\u0440\u0435\u0441\u043d\u044b\u0445 \u0437\u0430\u0434\u0430\u0447, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043f\u043e\u0442\u0440\u0435\u0431\u0443\u044e\u0442 \u0442\u0435\u0445\u043d\u0438\u0447\u0435\u0441\u043a\u0438\u0445, \u0430\u043d\u0430\u043b\u0438\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u0438 \u043a\u043e\u043c\u043c\u0443\u043d\u0438\u043a\u0430\u0442\u0438\u0432\u043d\u044b\u0445 \u043d\u0430\u0432\u044b\u043a\u043e\u0432;"", ""\u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u0443\u044e \u043f\u0440\u043e\u0444\u0435\u0441\u0441\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u0443\u044e \u0441\u0440\u0435\u0434\u0443 (\u0442\u0443\u0442 \u0434\u0440\u0443\u0436\u0431\u0430 \u0438 \u043f\u043e\u0434\u0434\u0435\u0440\u0436\u043a\u0430 \u0434\u0440\u0443\u0433 \u0434\u0440\u0443\u0433\u0430 \u0437\u0430\u043a\u0430\u043b\u044f\u043b\u0430\u0441\u044c \u0433\u043e\u0434\u0430\u043c\u0438!);"", ""\u043d\u0430\u0448 \u043f\u0440\u0438\u043d\u0446\u0438\u043f: \u0440\u0430\u0437\u0432\u0438\u0432\u0430\u044f \u0441\u0435\u0431\u044f, \u0442\u044b \u0440\u0430\u0437\u0432\u0438\u0432\u0430\u0435\u0448\u044c \u0441\u0432\u043e\u044e \u043a\u043e\u043c\u0430\u043d\u0434\u0443, \u0440\u0430\u0437\u0432\u0438\u0432\u0430\u044f \u0441\u0432\u043e\u044e \u043a\u043e\u043c\u0430\u043d\u0434\u0443, \u0442\u044b \u0440\u0430\u0437\u0432\u0438\u0432\u0430\u0435\u0448\u044c \u043a\u043e\u043c\u043f\u0430\u043d\u0438\u044e. \u041c\u044b \u0432\u0441\u0435 \u0442\u0440\u0443\u0434\u0438\u043c\u0441\u044f \u043d\u0430\u0434 \u043e\u0434\u043d\u043e\u0439 \u0446\u0435\u043b\u044c\u044e, \u0438 \u043e\u0434\u043d\u0430 \u0438\u0437 \u043a\u043b\u044e\u0447\u0435\u0432\u044b\u0445 \u0437\u0430\u0434\u0430\u0447"", ""\u043f\u0435\u0440\u0441\u043e\u043d\u0430\u043b\u044c\u043d\u043e \u0440\u0430\u0437\u0432\u0438\u0442\u0438\u0435 \u043a\u0430\u0436\u0434\u043e\u0433\u043e!"", ""\u043a\u043e\u043d\u043a\u0443\u0440\u0435\u043d\u0442\u043d\u0443\u044e \u0437\u0430\u0440\u043f\u043b\u0430\u0442\u0443, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u0437\u0430\u0432\u0438\u0441\u0438\u0442 \u043e\u0442 \u043e\u043f\u044b\u0442\u0430 \u0438 \u043d\u0430\u0432\u044b\u043a\u043e\u0432;"", ""\u0445\u043e\u0440\u043e\u0448\u0438\u0435 \u0443\u0441\u043b\u043e\u0432\u0438\u044f \u0440\u0430\u0431\u043e\u0442\u044b \u0432 \u0443\u0434\u043e\u0431\u043d\u0435\u0439\u0448\u0435\u043c \u043e\u0444\u0438\u0441\u0435 (\u0435\u0441\u043b\u0438 \u0442\u0435\u0431\u0435 \u0443\u0434\u043e\u0431\u043d\u043e \u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c \u0432\u0432\u0435\u0440\u0445 \u0442\u043e\u0440\u043c\u0430\u0448\u043a\u0430\u043c\u0438"", ""\u043f\u043e\u0436\u0430\u043b\u0443\u0439\u0441\u0442\u0430, \u0434\u043b\u044f \u043d\u0430\u0441 \u0433\u043b\u0430\u0432\u043d\u043e\u0435"", ""\u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442!"", ""\u043e\u0442\u043f\u0443\u0441\u043a (28 \u0434\u043d\u0435\u0439), \u0431\u043e\u043b\u044c\u043d\u0438\u0447\u043d\u044b\u0435, \u0441\u0442\u0440\u0430\u0445\u043e\u0432\u043a\u0430, \u0442\u0440\u0443\u0434\u043e\u0443\u0441\u0442\u0440\u043e\u0439\u0441\u0442\u0432\u043e"", ""\u0432\u0441\u0435 \u043a\u0430\u043a \u0443 \u043b\u044e\u0434\u0435\u0439;"", ""\u043a\u0430\u0436\u0434\u044b\u0435 4 \u043a\u0432\u0430\u0440\u0442\u0430\u043b\u0430"", ""\u043f\u0435\u0440\u0435\u0441\u043c\u043e\u0442\u0440 \u0442\u0432\u043e\u0438\u0445 \u043d\u0430\u0432\u044b\u043a\u043e\u0432 \u0438 \u0437\u0430\u0440\u043f\u043b\u0430\u0442\u044b;"", ""\u043d\u0443 \u0438 \u043a\u043e\u0440\u043f\u043e\u0440\u0430\u0442\u0438\u0432\u044b, \u043a\u0443\u0434\u0430 \u0436 \u0431\u0435\u0437 \u043d\u0438\u0445!:)""], ""Project description"": [""\u041f\u043e\u043c\u043e\u0449\u044c BizDev \u0432 Research (\u0430\u043d\u0430\u043b\u0438\u0437 \u043a\u043e\u043d\u043a\u0443\u0440\u0435\u043d\u0442\u043e\u0432, \u0441\u043e\u043f\u043e\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0441\u043f\u0435\u0446\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0439)"", ""\u0423\u043c\u0435\u043d\u0438\u0435 \u043f\u043e\u043d\u044f\u0442\u043d\u043e \u0441\u0444\u043e\u0440\u043c\u0443\u043b\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0438 \u043e\u0431\u044a\u044f\u0441\u043d\u0438\u0442\u044c \u0442\u0440\u0435\u043d\u0434\u044b, \u043f\u0440\u043e\u0431\u043b\u0435\u043c\u0430\u0442\u0438\u043a\u0443 \u0438 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u0438 \u0434\u043b\u044f \u0432\u043d\u0443\u0442\u0440\u0435\u043d\u043d\u0438\u0445 \u0438 \u043a\u043b\u0438\u0435\u043d\u0442\u0441\u043a\u0438\u0445 \u043a\u043e\u043c\u0430\u043d\u0434 \u0432 \u0447\u0435\u0442\u043a\u043e\u0439, \u0441\u0436\u0430\u0442\u043e\u0439 \u0438 \u0446\u0435\u043b\u0435\u043d\u0430\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u043d\u043e\u0439 \u0444\u043e\u0440\u043c\u0435"", ""\u0423\u0447\u0430\u0441\u0442\u0438\u0435/\u043f\u0440\u043e\u0432\u0435\u0434\u0435\u043d\u0438\u0435 \u0432\u0441\u0442\u0440\u0435\u0447 \u0441 \u043a\u043b\u044e\u0447\u0435\u0432\u044b\u043c\u0438 \u0438 \u0441\u0442\u0440\u0430\u0442\u0435\u0433\u0438\u0447\u0435\u0441\u043a\u0438\u043c\u0438 \u043f\u0430\u0440\u0442\u043d\u0435\u0440\u0430\u043c\u0438 \u0438 \u043a\u043b\u0438\u0435\u043d\u0442\u0430\u043c\u0438 DSP"", ""\u041f\u043e\u0434\u043a\u043b\u044e\u0447\u0435\u043d\u0438\u0435 \u0434\u0430\u0442\u0430 \u043f\u0440\u043e\u0432\u0430\u0439\u0434\u0435\u0440\u043e\u0432 (\u043f\u0435\u0440\u0435\u0433\u043e\u0432\u043e\u0440\u044b, \u043f\u043e\u0434\u043a\u043b\u044e\u0447\u0435\u043d\u0438\u044f, \u0438\u043d\u0442\u0435\u0433\u0440\u0430\u0446\u0438\u0438, \u0441\u043e\u0431\u043b\u044e\u0434\u0435\u043d\u0438\u0435 \u0434\u043e\u0433\u043e\u0432\u0440\u0435\u043d\u043d\u043e\u0441\u0442\u0435\u0439 \u0441 \u043d\u0438\u043c\u0438)"", ""\u0421\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u043e\u0432\u0430\u0442\u044c \u0432 \u0444\u043e\u0440\u043c\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0438, \u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u043a\u0435 \u0438 \u0440\u0435\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 \u043a\u0430\u043c\u043f\u0430\u043d\u0438\u0439, \u043e\u0441\u043d\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u043d\u0430 \u0434\u0430\u043d\u043d\u044b\u0445. \u041d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0430 \u0438 \u0443\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0445 \u0438\u0437 \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u0438\u0445 \u0438\u0441\u0442\u043e\u0447\u043d\u0438\u043a\u043e\u0432"", ""\u041f\u043e\u043c\u043e\u0449\u044c \u0432\u043e \u0432\u043d\u0435\u0434\u0440\u0435\u043d\u0438\u0438 \u0434\u0430\u043d\u043d\u044b\u0445 \u043a\u043b\u0438\u0435\u043d\u0442\u043e\u0432 \u0432 \u043f\u043b\u0430\u0442\u0444\u043e\u0440\u043c\u0443 Admixer. \u0412 \u0441\u043e\u0442\u0440\u0443\u0434\u043d\u0438\u0447\u0435\u0441\u0442\u0432\u0435 \u0441 \u043a\u043e\u043c\u0430\u043d\u0434\u0430\u043c\u0438 Data Engineering \u0438 Customer Success \u043e\u0431\u0435\u0441\u043f\u0435\u0447\u0438\u0442\u044c \u0431\u0435\u0441\u043f\u0435\u0440\u0435\u0431\u043e\u0439\u043d\u044b\u0439 \u0438 \u00ab\u0433\u043b\u0430\u0434\u043a\u0438\u0439\u00bb \u043f\u0440\u043e\u0446\u0435\u0441\u0441 \u043e\u043d\u0431\u043e\u0440\u0434\u0438\u043d\u0433\u0430."", ""\u0412\u044b\u044f\u0432\u043b\u044f\u0442\u044c \u0438 \u0443\u0441\u0442\u0440\u0430\u043d\u044f\u0442\u044c \u043b\u044e\u0431\u044b\u0435 \u043f\u0440\u043e\u0431\u043b\u0435\u043c\u044b \u0441 \u0438\u043d\u0442\u0435\u0433\u0440\u0430\u0446\u0438\u044f\u043c\u0438 \u0438 / \u0438\u043b\u0438 \u043f\u043e\u0434\u043a\u043b\u044e\u0447\u0435\u043d\u0438\u0435\u043c, \u0440\u0435\u0448\u0430\u044f \u0438\u0445 \u0441 \u043a\u043e\u043c\u0430\u043d\u0434\u0430\u043c\u0438 \u0432\u043d\u0443\u0442\u0440\u0438 \u043a\u043e\u043c\u043f\u0430\u043d\u0438\u0438 \u0438 \u043a\u043b\u0438\u0435\u043d\u0442\u0430\u043c\u0438"", ""\u0423\u0447\u0430\u0441\u0442\u0438\u0435 \u0432 \u043f\u0440\u043e\u0446\u0435\u0441\u0441\u0435 \u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u043a\u0438 (Product ownership)"", ""\u0441\u043e\u0433\u043b\u0430\u0441\u043e\u0432\u0430\u043d\u0438\u0435 \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0439 \u0444\u0443\u043d\u043a\u0446\u0438\u043e\u043d\u0430\u043b\u0430 \u0432 \u043f\u0440\u043e\u0446\u0435\u0441\u0441\u0435 \u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u043a\u0438. \u0424\u043e\u0440\u043c\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u0442\u0440\u0435\u0431\u043e\u0432\u0430\u043d\u0438\u0439."", ""\u0414\u0435\u043a\u043e\u043c\u043f\u043e\u0437\u0438\u0446\u0438\u044f \u0437\u0430\u0434\u0430\u0447 (\u0440\u0430\u0437\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0442\u0440\u0435\u0431\u043e\u0432\u0430\u043d\u0438\u0439 \u043d\u0430 \u0438\u0442\u0435\u0440\u0430\u0446\u0438\u0438). \u0420\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0442\u0435\u0445 \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438"", ""\u0418\u043c\u043f\u043b\u0435\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u044f. \u041f\u0440\u043e\u0432\u0435\u0440\u043a\u0430 \u0437\u0430\u043f\u0443\u0449\u0435\u043d\u043d\u043e\u0439 \u0444\u0443\u043d\u043a\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e\u0441\u0442\u0438 \u00ab\u0432 \u0431\u043e\u044e\u00bb"", ""\u0441\u043e\u043f\u043e\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0444\u0430\u043a\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432 \u0441 \u043e\u0436\u0438\u0434\u0430\u0435\u043c\u044b\u043c\u0438. \u0421\u043e\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u043e\u0442\u0447\u0435\u0442\u043e\u0432 \u0438 \u0430\u043d\u0430\u043b\u0438\u0442\u0438\u043a\u0438.""]}",,"Required skills –î–∞–≤–∞–π –∑–Ω–∞–∫–æ–º–∏—Ç—å—Å—è. –ú—ã ‚Äî –∫–æ–º–∞–Ω–¥–∞ Admixer. –í–æ—Ç —É–∂–µ 10 –ª–µ—Ç –º—ã —Ä–∞–∑—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ—à–µ–Ω–∏—è –¥–ª—è –≤—Å–µ—Ö —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ AdTech —Ä—ã–Ω–∫–∞:-—Å–æ–∑–¥–∞–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è –ø–æ–∫—É–ø–∫–∏, –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏, –∏–∑–º–µ—Ä–µ–Ω–∏—è –∏ –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è –º–µ–∂–ø–ª–∞—Ç—Ñ–æ—Ä–º–µ–Ω–Ω—ã—Ö –∏ –º–Ω–æ–≥–æ–∫–∞–Ω–∞–ª—å–Ω—ã—Ö —Ä–µ–∫–ª–∞–º–Ω—ã—Ö –∫–∞–º–ø–∞–Ω–∏–π;-—Ä–µ–∞–ª–∏–∑–æ–≤—ã–≤–∞–µ–º —É—Å–ø–µ—à–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –ø—Ä–æ–¥–∞–∂ —á–µ—Ä–µ–∑ RTB –∏ Header Bidding –∞—É–∫—Ü–∏–æ–Ω—ã –±–ª–∞–≥–æ–¥–∞—Ä—è –Ω–∞—à–∏–º –ø–∞—Ä—Ç–Ω–µ—Ä—Å—Ç–≤–∞–º –º–∏—Ä–æ–≤–æ–≥–æ —É—Ä–æ–≤–Ω—è;-–Ω–∞—à–∏ –æ—Ñ–∏—Å—ã –∏ –¥–∞—Ç–∞-—Ü–µ–Ω—Ç—Ä—ã —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω—ã –ø–æ –≤—Å–µ–º—É –º–∏—Ä—É. –ú—ã –æ—á–µ–Ω—å —Ü–µ–Ω–∏–º —Ç–∞–∫–∏–µ –Ω–∞–≤—ã–∫–∏:‚Äî –ï—Å–ª–∏ —Ç—ã –∏–º–µ–µ—à—å 1-3 –≥–æ–¥–∞ –ø—Ä–∏–º–µ–Ω–∏–º–æ–≥–æ –æ–ø—ã—Ç–∞ (–æ–ø—ã—Ç —Ä–∞–±–æ—Ç—ã —Å –ª—é–±–æ–π DMP –ø–ª–∞—Ç—Ñ–æ—Ä–º–æ–π)‚Äî –£–º–µ–µ—à—å –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ –ø—Ä–æ–≤–æ–¥–∏—Ç—å –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö, –µ—Å—Ç—å –æ–ø—ã—Ç —Ä–∞–±–æ—Ç—ã —Å —Ü–∏—Ñ—Ä–æ–≤—ã–º–∏ / –æ–Ω–ª–∞–π–Ω-–ø—Ä–æ–¥—É–∫—Ç–∞–º–∏ –∏ —Å—Ä–µ–¥–æ–π —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω–æ–π –∫–æ–º–º–µ—Ä—Ü–∏–∏‚Äî English upper intermediate‚Äî –ó–Ω–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ —Ü–∏—Ñ—Ä–æ–≤–æ–≥–æ –º–∞—Ä–∫–µ—Ç–∏–Ω–≥–∞ ‚Äî Google, Adobe –∏ —Ç.–¥ –û–ø—ã—Ç —Ä–∞–±–æ—Ç—ã —Å Google Tag Manager, —Ç–µ–≥–∞–º–∏ –≤–µ–±-—Å–∞–π—Ç–æ–≤, SEO –∏ SEM, DV360 –∏–ª–∏ –¥—Ä—É–≥–∏–º–∏ DSP‚Äî –ü–æ–ª–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –ø–ª–∞—Ç—Ñ–æ—Ä–º –¥–∞–Ω–Ω—ã—Ö –≤ –º–µ–¥–∏–∞–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ, –∫–∞–∫ –Ω–∞ –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–º (–±–∞–∑–æ–≤–æ–º), —Ç–∞–∫ –∏ –Ω–∞ –æ–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω–æ–º —É—Ä–æ–≤–Ω–µ (—ç–∫—Å–ø–µ—Ä—Ç)‚Äî –ü–æ–Ω–∏–º–∞–Ω–∏–µ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –ø–æ—Å—Ç–∞–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö, –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö, —Ç–∞–∫—Å–æ–Ω–æ–º–∏–∏, –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞, —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã‚Äî –ñ–µ–ª–∞—Ç–µ–ª—å–Ω–æ –∏–º–µ—Ç—å –æ–ø—ã—Ç –Ω–∞–ø–∏—Å–∞–Ω–∏—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –∏ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –¥–ª—è –æ–ø–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –∑–∞–¥–∞—á We offer ‚Äî —É—á–∞—Å—Ç–∏–µ –≤ –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–µ –ø–µ—Ä–µ–¥–æ–≤—ã—Ö —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–¥—É–∫—Ç–æ–≤ –¥–ª—è –∫–ª–∏–µ–Ω—Ç–æ–≤ –ø–æ –≤—Å–µ–º—É –º–∏—Ä—É;‚Äî –º–Ω–æ–≥–æ –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã—Ö –∑–∞–¥–∞—á, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ—Ç—Ä–µ–±—É—é—Ç —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö, –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏—Ö –∏ –∫–æ–º–º—É–Ω–∏–∫–∞—Ç–∏–≤–Ω—ã—Ö –Ω–∞–≤—ã–∫–æ–≤;‚Äî —É–Ω–∏–∫–∞–ª—å–Ω—É—é –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—É—é —Å—Ä–µ–¥—É (—Ç—É—Ç –¥—Ä—É–∂–±–∞ –∏ –ø–æ–¥–¥–µ—Ä–∂–∫–∞ –¥—Ä—É–≥ –¥—Ä—É–≥–∞ –∑–∞–∫–∞–ª—è–ª–∞—Å—å –≥–æ–¥–∞–º–∏!);‚Äî –Ω–∞—à –ø—Ä–∏–Ω—Ü–∏–ø: —Ä–∞–∑–≤–∏–≤–∞—è —Å–µ–±—è, —Ç—ã —Ä–∞–∑–≤–∏–≤–∞–µ—à—å —Å–≤–æ—é –∫–æ–º–∞–Ω–¥—É, —Ä–∞–∑–≤–∏–≤–∞—è —Å–≤–æ—é –∫–æ–º–∞–Ω–¥—É, —Ç—ã —Ä–∞–∑–≤–∏–≤–∞–µ—à—å –∫–æ–º–ø–∞–Ω–∏—é. –ú—ã –≤—Å–µ —Ç—Ä—É–¥–∏–º—Å—è –Ω–∞–¥ –æ–¥–Ω–æ–π —Ü–µ–ª—å—é, –∏ –æ–¥–Ω–∞ –∏–∑ –∫–ª—é—á–µ–≤—ã—Ö –∑–∞–¥–∞—á ‚Äî –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–æ —Ä–∞–∑–≤–∏—Ç–∏–µ –∫–∞–∂–¥–æ–≥–æ! ‚Äî –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω—É—é –∑–∞—Ä–ø–ª–∞—Ç—É, –∫–æ—Ç–æ—Ä–∞—è –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –æ–ø—ã—Ç–∞ –∏ –Ω–∞–≤—ã–∫–æ–≤;‚Äî —Ö–æ—Ä–æ—à–∏–µ —É—Å–ª–æ–≤–∏—è —Ä–∞–±–æ—Ç—ã –≤ —É–¥–æ–±–Ω–µ–π—à–µ–º –æ—Ñ–∏—Å–µ (–µ—Å–ª–∏ —Ç–µ–±–µ —É–¥–æ–±–Ω–æ —Ä–∞–±–æ—Ç–∞—Ç—å –≤–≤–µ—Ä—Ö —Ç–æ—Ä–º–∞—à–∫–∞–º–∏ ‚Äî –ø–æ–∂–∞–ª—É–π—Å—Ç–∞, –¥–ª—è –Ω–∞—Å –≥–ª–∞–≤–Ω–æ–µ ‚Äî —Ä–µ–∑—É–ª—å—Ç–∞—Ç!‚Äî –æ—Ç–ø—É—Å–∫ (28 –¥–Ω–µ–π), –±–æ–ª—å–Ω–∏—á–Ω—ã–µ, —Å—Ç—Ä–∞—Ö–æ–≤–∫–∞, —Ç—Ä—É–¥–æ—É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ ‚Äî –≤—Å–µ –∫–∞–∫ —É –ª—é–¥–µ–π;‚Äî –∫–∞–∂–¥—ã–µ 4 –∫–≤–∞—Ä—Ç–∞–ª–∞ ‚Äî –ø–µ—Ä–µ—Å–º–æ—Ç—Ä —Ç–≤–æ–∏—Ö –Ω–∞–≤—ã–∫–æ–≤ –∏ –∑–∞—Ä–ø–ª–∞—Ç—ã;‚Äî –Ω—É –∏ –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤—ã, –∫—É–¥–∞ –∂ –±–µ–∑ –Ω–∏—Ö!:) Responsibilities ‚Äî –ü–æ–º–æ—â—å BizDev –≤ Research (–∞–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤, —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–π)‚Äî –£–º–µ–Ω–∏–µ –ø–æ–Ω—è—Ç–Ω–æ —Å—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –∏ –æ–±—ä—è—Å–Ω–∏—Ç—å —Ç—Ä–µ–Ω–¥—ã, –ø—Ä–æ–±–ª–µ–º–∞—Ç–∏–∫—É –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –∏ –∫–ª–∏–µ–Ω—Ç—Å–∫–∏—Ö –∫–æ–º–∞–Ω–¥ –≤ —á–µ—Ç–∫–æ–π, —Å–∂–∞—Ç–æ–π –∏ —Ü–µ–ª–µ–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–π —Ñ–æ—Ä–º–µ‚Äî –£—á–∞—Å—Ç–∏–µ/–ø—Ä–æ–≤–µ–¥–µ–Ω–∏–µ –≤—Å—Ç—Ä–µ—á —Å –∫–ª—é—á–µ–≤—ã–º–∏ –∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–∏–º–∏ –ø–∞—Ä—Ç–Ω–µ—Ä–∞–º–∏ –∏ –∫–ª–∏–µ–Ω—Ç–∞–º–∏ DSP‚Äî –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –¥–∞—Ç–∞ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤ (–ø–µ—Ä–µ–≥–æ–≤–æ—Ä—ã, –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è, –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏, —Å–æ–±–ª—é–¥–µ–Ω–∏–µ –¥–æ–≥–æ–≤—Ä–µ–Ω–Ω–æ—Å—Ç–µ–π —Å –Ω–∏–º–∏)‚Äî –°–æ–¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å –≤ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–∏, —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ –∏ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∫–∞–º–ø–∞–Ω–∏–π, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞ –¥–∞–Ω–Ω—ã—Ö. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤‚Äî –ü–æ–º–æ—â—å –≤–æ –≤–Ω–µ–¥—Ä–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤ –≤ –ø–ª–∞—Ç—Ñ–æ—Ä–º—É Admixer. –í —Å–æ—Ç—Ä—É–¥–Ω–∏—á–µ—Å—Ç–≤–µ —Å –∫–æ–º–∞–Ω–¥–∞–º–∏ Data Engineering –∏ Customer Success –æ–±–µ—Å–ø–µ—á–∏—Ç—å –±–µ—Å–ø–µ—Ä–µ–±–æ–π–Ω—ã–π –∏ ¬´–≥–ª–∞–¥–∫–∏–π¬ª –ø—Ä–æ—Ü–µ—Å—Å –æ–Ω–±–æ—Ä–¥–∏–Ω–≥–∞. ‚Äî –í—ã—è–≤–ª—è—Ç—å –∏ —É—Å—Ç—Ä–∞–Ω—è—Ç—å –ª—é–±—ã–µ –ø—Ä–æ–±–ª–µ–º—ã —Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è–º–∏ –∏ / –∏–ª–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ–º, —Ä–µ—à–∞—è –∏—Ö —Å –∫–æ–º–∞–Ω–¥–∞–º–∏ –≤–Ω—É—Ç—Ä–∏ –∫–æ–º–ø–∞–Ω–∏–∏ –∏ –∫–ª–∏–µ–Ω—Ç–∞–º–∏‚Äî –£—á–∞—Å—Ç–∏–µ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ (Product ownership) ‚Äî —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–∞ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏. –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π. ‚Äî –î–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏—è –∑–∞–¥–∞—á (—Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –Ω–∞ –∏—Ç–µ—Ä–∞—Ü–∏–∏). –†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏‚Äî –ò–º–ø–ª–µ–º–µ–Ω—Ç–∞—Ü–∏—è. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–ø—É—â–µ–Ω–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ ¬´–≤ –±–æ—é¬ª ‚Äî —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å –æ–∂–∏–¥–∞–µ–º—ã–º–∏. –°–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–æ–≤ –∏ –∞–Ω–∞–ª–∏—Ç–∏–∫–∏. Project description –°–µ–≥–æ–¥–Ω—è –≤ –∞–∫—Ç–∏–≤–µ Admixer –ª–∏–Ω–µ–π–∫–∞ —Ä–µ—à–µ–Ω–∏–π –¥–ª—è –±–∏–∑–Ω–µ—Å–æ–≤ —Ä–∞–∑–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∞. –ë–æ–ª—å—à–µ –æ —Ç–æ–º, –Ω–∞–¥ —á–µ–º –º—ã —Ä–∞–±–æ—Ç–∞–µ–º —Ç—ã –º–æ–∂–µ—à—å —É–∑–Ω–∞—Ç—å —Ç—É—Ç: admixer.com –ë–æ–ª—å—à–µ –æ –Ω–∞—Å:Facebook www.facebook.com/AdmixerInstagram www.instagram.com/admixertechnologies–°–∞–π—Ç –∞–∫–∞–¥–µ–º–∏–∏: www.admixer.academy –î–∞–≤–∞–π –ø–æ—Ä–∞–±–æ—Ç–∞–µ–º –≤–º–µ—Å—Ç–µ?",Product Manager DMP¬†/ Data Operations@Admixer,https://jobs.dou.ua/companies/admixer-advertising-technologies/vacancies/120220/, Kyiv,Product Manager DMP¬†/ Data Operations,05 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/intellias/,Intellias,"{""Required skills"": [""S3 (AWS Lambda);Athena database tables;Experience with data modeling.""], ""We offer"": [""Initially it would be part time for a few months, with the possibility to become full time.""], ""Responsibilities"": [""Besides such basics as a competitive salary, comfortable and motivating work environment, here at Intellias we offer:""], ""Project description"": [""For your professional growth"", ""Innovative projects with advanced technologies;Individual approach to professional and career growth (Personal Development Plan);Regular educational events with leading industry experts;English and German courses.""]}",,"Required skills S3 (AWS Lambda);Athena database tables;Experience with data modeling. Initially it would be part time for a few months, with the possibility to become full time. We offer Besides such basics as a competitive salary, comfortable and motivating work environment, here at Intellias we offer: For your professional growth ‚ÄîInnovative projects with advanced technologies;Individual approach to professional and career growth (Personal Development Plan);Regular educational events with leading industry experts;English and German courses. For your comfort ‚ÄîFlexible working hours;Spacious office with lots of meeting rooms;Relocation program;Kids‚Äô room with professional baby-sitter (offices in Lviv & Kyiv). For your health ‚Äî3 health packages to choose from ‚Äî medical insurance, sports attendance or mix of both;Annual vitaminization program;Annual vaccination and ophthalmologist check-up. For your leisure ‚ÄîCorporate celebrations and fun activities;On-site massages;Beauty parlor (offices in Lviv & Kyiv). Responsibilities Prepares raw data for analysis;Loads prepared data to a data repository;Performs statistical modeling and analysis datasets to develop metrics, reports and visualizations;Works with technology and business stakeholders to understand data and analysis needs;Creates and maintains database and statistical models for ongoing and ad hoc review and analysis of data;Automate log file load from contract manufacturers to S3 (AWS Lambda);Parse log files for Audio and Focus tests and validate results;Automate loading of a seven day history of Audio and Focus tests to Athena database tables;Archive Audio and Focus test results > 7 days in separate Athena database tables;Connect QuickSight dashboard to seven day history tables in Athena database. Project description Our client is a leading US-based IT services provider for large tech companies seeking to build an Eastern European dev team for a new product platform. Client is successfully operates for 17 years and represented in 33 countries. Client develops solutions for workplace services, IT logistics, network & data center, cloud and provide subject matter expertise, framework, templates and methodologies.",Data Engineer with AWS Lambda and Athena@Intellias,https://jobs.dou.ua/companies/intellias/vacancies/130816/," Kyiv, Kharkiv, Lviv, Odesa, Ivano-Frankivsk",Data Engineer with AWS Lambda and Athena,05 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/startus/,StartUs Insights,"{""Required skills"": [""We are StartUs Insights, an international data science company with HQ in Vienna analyzing startups and emerging technologies to anticipate industry trends, new business models & innovation areas.""], ""We offer"": [""We are looking for a great personality to join our team and work on Data Administration together with our international teams.""], ""Responsibilities"": [""ABOUT YOU:"", ""you have an intermediate level of English (B2)"", ""you are detail-oriented, self managerial and have strong analytical skills"", ""you have good knowledge of Excel/Google sheets"", ""you are result-oriented"", ""you have a growth mindset""]}",,"Required skills We are StartUs Insights, an international data science company with HQ in Vienna analyzing startups and emerging technologies to anticipate industry trends, new business models & innovation areas. We are looking for a great personality to join our team and work on Data Administration together with our international teams. ABOUT YOU:‚Äî you have an intermediate level of English (B2)‚Äî you are detail-oriented, self managerial and have strong analytical skills‚Äî you have good knowledge of Excel/Google sheets‚Äî you are result-oriented‚Äî you have a growth mindset We offer ‚Äî have constant communication and mentorship from senior data admins‚Äî become part of a collaborative international team you‚Äôll enjoy working with‚Äî receive a clear view of your KPIs and progression possibilities from day one‚Äî enjoy remote work and don‚Äôt waste your valuable time in traffic‚Äî fulltime (40 hrs/week) or part-time (30 hrs/week) remote position‚Äî enjoy paid work-free time and public holidays according to the Ukrainian calendar Responsibilities ‚Äî maintain existing and create new databases (based on company requirements)‚Äî clean and update data accumulated in the databases‚Äî optimize existing databases using appropriate Excel formulas and other actionable approaches‚Äî manage and maintain data sets on the startup ecosystem‚Äî work collaboratively with Sales and Marketing to develop and grow the pipeline to meet quarter goals‚Äî continuous engagement with the other company departments",Data Administrator (Remote Position)@StartUs Insights,https://jobs.dou.ua/companies/startus/vacancies/134024/, remote,Data Administrator (Remote Position),05 October 2020,$450‚Äì850,2020-10-13,,dou
https://jobs.dou.ua/companies/n-ix/,N-iX,{},,"Are you a Big Data enthusiast who dreams of a fulfilling career in a fast-growing company? Do you want to work with the latest technologies in a friendly and professional team? Don‚Äôt miss out on the perfect opportunity to join N-iX as a Big Data Engineer. About the client:Our client is a US company that specializes in delivering reliable Internet connectivity and entertainment multimedia to aircrafts worldwide, enhancing the experience for both passengers and crew. At this moment we cooperate on Business Intelligence, Data Analysis, and Big Data directions and look for talents who can contribute to the complex data management and analysis projects. Responsibilities:‚Ä¢ Develop and support existing end-to-end data solutions for structured and unstructured data including, but not limited to, ingestion, parsing, integration, auditing, logging, aggregation, normalization, and error handling‚Ä¢ Create jobs and queries to perform auditing and error handling Qualifications:‚Ä¢ Experience with SQL‚Ä¢ Knowledge of Python or Scala‚Ä¢ Knowledge of Hadoop architecture‚Ä¢ Experience with Linux‚Ä¢ Result-oriented attitude, ability to get things done in a highly dynamic and stressful environment‚Ä¢ Good communication skills and good English‚Ä¢ A good team player Nice to have:‚Ä¢ Experience working with AWS (S3, EMR cluster, Lambda, Kinesis)‚Ä¢ Experience with Apache Spark, including an understanding of optimization techniques We offer:‚Äã‚Ä¢ Flexible working hours‚Äã‚Ä¢ A competitive salary and good compensation package‚Äã‚Ä¢ Best hardware‚Äã‚Ä¢ A masseur and a corporate doctor‚Äã‚Ä¢ Healthcare & sport benefits‚Äã‚Ä¢ An inspiring and comfy office Professional growth:‚Äã‚Ä¢ Challenging tasks and innovative projects‚Äã‚Ä¢ Meetups and events for professional development‚Äã‚Ä¢ An individual development plan‚Äã‚Ä¢ Mentorship program Fun:‚Äã‚Ä¢ Corporate events and outstanding parties‚Äã‚Ä¢ Exciting team buildings‚Ä¢ ‚ÄãMemorable anniversary presents‚Äã‚Ä¢ A fun zone where you can play video games, foosball, ping pong, and more",Trainee Big Data Engineer@N-iX,https://jobs.dou.ua/companies/n-ix/vacancies/134023/?from=first-job, Lviv,Trainee Big Data Engineer,05 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/newxel/,Newxel,"{""Required skills"": [""Have at least 2 years of intensive experience using SQLHave at least one year of Tableau development experienceBe able to tell stories and visualize with dataBe result-oriented and focused on the value created by your developmentsHave excellent communication skills and great verbal and written EnglishHold a BSc in a quantitative field""], ""As a plus"": [""Experience with Amazon Web ServicesExperience working in a Gaming or Advertising companyExperience or familiarity with ETL processesHold an MSC in a quantitative""], ""We offer"": [""Competitive salaryMedical insuranceTax compensationModern and comfortable office near the Vystavkovyi centerLong-term employment with 20 working-days paid vacationPaid sick leaves (10 per year)Flexible working scheduleTeam buildings""], ""Responsibilities"": [""Work on data science projects such as Customer value predictionSupport and maintain current modelsWork in a very agile environment with a fast decision processCollaborate directly with people while working on mobile & back-end development, data engineering, mobile games, product, and marketing""], ""Project description"": [""CrazyLabs looking for a Reporting Data Analyst to help support and maintain the current BI reporting system and help to develop new ones to help improve decision making.As a top 10 mobile games publisher (according to AppAnnie and Sensor Tower) and with over 3 billion downloads to date, Crazy Labs is now a worldwide leader in casual games development, distribution, and innovation.""]}",,"Required skills Have at least 2 years of intensive experience using SQLHave at least one year of Tableau development experienceBe able to tell stories and visualize with dataBe result-oriented and focused on the value created by your developmentsHave excellent communication skills and great verbal and written EnglishHold a BSc in a quantitative field As a plus Experience with Amazon Web ServicesExperience working in a Gaming or Advertising companyExperience or familiarity with ETL processesHold an MSC in a quantitative We offer Competitive salaryMedical insuranceTax compensationModern and comfortable office near the Vystavkovyi centerLong-term employment with 20 working-days paid vacationPaid sick leaves (10 per year)Flexible working scheduleTeam buildings Responsibilities Work on data science projects such as Customer value predictionSupport and maintain current modelsWork in a very agile environment with a fast decision processCollaborate directly with people while working on mobile & back-end development, data engineering, mobile games, product, and marketing Project description CrazyLabs looking for a Reporting Data Analyst to help support and maintain the current BI reporting system and help to develop new ones to help improve decision making.As a top 10 mobile games publisher (according to AppAnnie and Sensor Tower) and with over 3 billion downloads to date, Crazy Labs is now a worldwide leader in casual games development, distribution, and innovation.",Reporting Data Analyst@Newxel,https://jobs.dou.ua/companies/newxel/vacancies/130259/, Kyiv,Reporting Data Analyst,05 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/ciklum/,Ciklum,{},,"On behalf of Ciklum Digital, Ciklum is looking for a Senior Data Engineer to join the team on a full-time basis. You will join a highly motivated team and will be working on a modern solution for our existing client. We are looking for technology experts who want to make an impact on new business by applying best practices and taking ownership. Project description:",Senior Data Engineer for Ciklum Digital (20000497)@Ciklum,https://jobs.dou.ua/companies/ciklum/vacancies/133976/, Kyiv,Senior Data Engineer for Ciklum Digital (20000497),05 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/solringair/,Solring Holdings Ltd,"{""Required skills"": [""Qualifications:"", ""BS |MS preferred in Computer Science (preferred), Computer Engineering , Physics, Applied Mathematics"", ""2+ years of experience with Artificial Intelligence/Machine Learning in complex software systems"", ""Strong knowledge of probability theory, linear algebra, numerical analysis"", ""Proficiency in English ( speaking and reading)""], ""We offer"": [""Required Technical and Professional Expertise:"", ""Strong knowledge in Computer Science Fundamentals and Programming: data structures (stacks, queues, multi-dimensional arrays, trees, graphs, etc.), algorithms (searching, sorting, optimization, dynamic programming, etc.), computability and complexity (P vs. NP, NP-complete problems, big-O notation, approximate algorithms, etc.), and computer architecture (memory, cache, bandwidth, deadlocks, distributed processing, etc.)."", ""Strong understanding of various database storage principles, experience with various SQL and NoSQL systems"", ""Proficiency in machine learning and deep learning languages and platforms (Python, R, Julia, TensorFlow, Keras, PyTorch, MXNet etc.)"", ""Experience in creating planning algorithms and using PDDL"", ""Proven record of experience with ML algorithms: formal characterization of probability (conditional probability, Bayes rule, likelihood, independence, etc.) and techniques derived from it (Bayes Nets, Markov Decision Processes, Hidden Markov Models, etc.)"", ""Experience in Dimensionality Reduction (Principal Components Analysis), Anomaly Detection"", ""Knowledge and understanding of different methods to evaluate the performance of learned models"", ""Experience with Agile methodologies preferred""], ""Responsibilities"": [""The perks:"", ""Awesome projects in the travel industry and challenging tasks"", ""Positive environment and friendly team"", ""Training conferences and workshops"", ""Paid time-off every year"", ""Competitive salary"", ""Corporate events.""], ""Project description"": [""Responsibilities:This person will work on creating a platform for training ML models, using symbolic reasoners, modeling, creating, and populating data storage. You will work closely with the backend team to make AI-related algorithms scalable. Further, you will work with the frontend team to make AI systems usable and understandable."", ""Gather, analyze, and model data and key performance indicators to develop quantitative and qualitative business insights via statistical and other analytical models"", ""Identify new data sources and evaluate emerging technologies and analytic trends for data discovery and visualization"", ""Processing, cleansing, and verifying the integrity of data used for analysis"", ""Doing ad-hoc analysis and presenting results in a clear manner"", ""Provide guidance on the modeling approaches and technology for customer analytics and insights team"", ""Applying effectively Machine Learning Algorithms and Libraries"", ""Software Engineering and System Design""]}",,"Required skills Qualifications:‚Äî BS |MS preferred in Computer Science (preferred), Computer Engineering , Physics, Applied Mathematics ‚Äî 2+ years of experience with Artificial Intelligence/Machine Learning in complex software systems‚Äî Strong knowledge of probability theory, linear algebra, numerical analysis‚Äî Proficiency in English ( speaking and reading) Required Technical and Professional Expertise:‚Äî Strong knowledge in Computer Science Fundamentals and Programming: data structures (stacks, queues, multi-dimensional arrays, trees, graphs, etc.), algorithms (searching, sorting, optimization, dynamic programming, etc.), computability and complexity (P vs. NP, NP-complete problems, big-O notation, approximate algorithms, etc.), and computer architecture (memory, cache, bandwidth, deadlocks, distributed processing, etc.). ‚Äî Strong understanding of various database storage principles, experience with various SQL and NoSQL systems‚Äî Proficiency in machine learning and deep learning languages and platforms (Python, R, Julia, TensorFlow, Keras, PyTorch, MXNet etc.) ‚Äî Experience in creating planning algorithms and using PDDL‚Äî Proven record of experience with ML algorithms: formal characterization of probability (conditional probability, Bayes rule, likelihood, independence, etc.) and techniques derived from it (Bayes Nets, Markov Decision Processes, Hidden Markov Models, etc.) ‚Äî Experience in Dimensionality Reduction (Principal Components Analysis), Anomaly Detection‚Äî Knowledge and understanding of different methods to evaluate the performance of learned models‚Äî Experience with Agile methodologies preferred We offer The perks:‚Äî Awesome projects in the travel industry and challenging tasks‚Äî Positive environment and friendly team‚Äî Training conferences and workshops‚Äî Paid time-off every year‚Äî Competitive salary‚Äî Corporate events. Responsibilities Responsibilities:This person will work on creating a platform for training ML models, using symbolic reasoners, modeling, creating, and populating data storage. You will work closely with the backend team to make AI-related algorithms scalable. Further, you will work with the frontend team to make AI systems usable and understandable. ‚Äî Gather, analyze, and model data and key performance indicators to develop quantitative and qualitative business insights via statistical and other analytical models‚Äî Identify new data sources and evaluate emerging technologies and analytic trends for data discovery and visualization‚Äî Processing, cleansing, and verifying the integrity of data used for analysis‚Äî Doing ad-hoc analysis and presenting results in a clear manner‚Äî Provide guidance on the modeling approaches and technology for customer analytics and insights team‚Äî Applying effectively Machine Learning Algorithms and Libraries‚Äî Software Engineering and System Design Project description As a Software Engineer within our AI Team you will work on new and exciting projects where you will help to design and create state of the art AI systems that will impact the travel world. You will help bring AI research ideas into scalable, robust systems. We want to bring out the best in you and expect you to do the same to us. Your proactive approach to creative problem solving will be essential to the success of our team and the company.","Data Science, Artificial Intelligence@Solring Holdings Ltd",https://jobs.dou.ua/companies/solringair/vacancies/128736/," Kyiv, remote","Data Science, Artificial Intelligence",05 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/data-science-ua/,Data Science UA,"{""Required skills"": [""Experience of programming in Python;"", ""Experience of working with Deep Learning frameworks (preferably PyTorch);"", ""Solid knowledge of differential geometry, analysis, linear algebra"", ""Keeping track of recent and relevant research papers, the ability to implement them.""], ""As a plus"": [""Experience in 3D deep learning;"", ""Deep understanding of game engine physics"", ""Experience with signed distance fields (occupancy functions and other analogs)"", ""Experience of coding in CUDA, C++""], ""We offer"": [""cozy office"", ""friendly team"", ""professional growth"", ""really challenges every day""], ""Responsibilities"": [""Your main focus will be developing new efficient algorithms and approaches to approximating representations and interactions of objects in 3D spaces with neural networks.""], ""Project description"": [""Our friends is looking for an enthusiast who is skilled in Machine Learning applied to 3D, with strong problem-solving abilities and research skills. They are a team working on 3D model representation, rendering and estimating physical interactions of 3D bodies via deep learning.""]}",,"Required skills ‚Äî Experience of programming in Python;‚Äî Experience of working with Deep Learning frameworks (preferably PyTorch);‚Äî Solid knowledge of differential geometry, analysis, linear algebra‚Äî Keeping track of recent and relevant research papers, the ability to implement them. As a plus ‚Äî Experience in 3D deep learning;‚Äî Deep understanding of game engine physics‚Äî Experience with signed distance fields (occupancy functions and other analogs)‚Äî Experience of coding in CUDA, C++ We offer ‚Äî cozy office‚Äî friendly team‚Äî professional growth‚Äî really challenges every day Responsibilities Your main focus will be developing new efficient algorithms and approaches to approximating representations and interactions of objects in 3D spaces with neural networks. Project description Our friends is looking for an enthusiast who is skilled in Machine Learning applied to 3D, with strong problem-solving abilities and research skills. They are a team working on 3D model representation, rendering and estimating physical interactions of 3D bodies via deep learning.",3D¬†Machine learning engineer@Data Science UA,https://jobs.dou.ua/companies/data-science-ua/vacancies/115351/, Kyiv,3D¬†Machine learning engineer,05 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/open-data-science-conference/,Open Data Science Conference,"{""Required skills"": [""Ability to work and handle a large amount of data and information in ExcelRecruiting quality candidatesInterviewing SkillsPeople SkillsEmployment Law ProfessionalismProject ManagementGood at JudgmentExcellent English verbal and written communication skillsAbility to work under general supervision and must be detail-oriented""], ""We offer"": [""Competitive salary and bonus system;Working in a team, introductory and team building activitiesContinuous practice of communicating with native English speakers and English courses;Overseas travel opportunities;Apple MacBook workstation;Modern office 5 min walk from Lviv city center;Healthcare Insurance;20 days paid vacation.""], ""Responsibilities"": [""Establishes recruiting requirements by studying organization plans and objectives; meeting with managers to discuss needs.Attracts applicants by placing job advertisements; contacting recruiters, using newsgroups and job sites.Utilize ODSC Jobs Platform to generate new revenues for the companyProvide support to research and compile background information on potential event partners & potential candidates""], ""Project description"": [""The Open Data Science Conference (ODSC) is an international conference series that brings together the best minds in data science and artificial intelligence. Every year the ODSC team runs multiple conferences including ODSC East in Boston, ODSC Europe in London, ODSC Ukraine, ODSC India, and ODSC West in San Francisco having a global presence with over 17,000 attendees worldwide and an online community of over 300,000.""]}",,"Required skills Ability to work and handle a large amount of data and information in ExcelRecruiting quality candidatesInterviewing SkillsPeople SkillsEmployment Law ProfessionalismProject ManagementGood at JudgmentExcellent English verbal and written communication skillsAbility to work under general supervision and must be detail-oriented We offer Competitive salary and bonus system;Working in a team, introductory and team building activitiesContinuous practice of communicating with native English speakers and English courses;Overseas travel opportunities;Apple MacBook workstation;Modern office 5 min walk from Lviv city center;Healthcare Insurance;20 days paid vacation. Responsibilities Establishes recruiting requirements by studying organization plans and objectives; meeting with managers to discuss needs.Attracts applicants by placing job advertisements; contacting recruiters, using newsgroups and job sites.Utilize ODSC Jobs Platform to generate new revenues for the companyProvide support to research and compile background information on potential event partners & potential candidates Project description The Open Data Science Conference (ODSC) is an international conference series that brings together the best minds in data science and artificial intelligence. Every year the ODSC team runs multiple conferences including ODSC East in Boston, ODSC Europe in London, ODSC Ukraine, ODSC India, and ODSC West in San Francisco having a global presence with over 17,000 attendees worldwide and an online community of over 300,000.","Recruiter (Contractor, part-time)@Open Data Science Conference",https://jobs.dou.ua/companies/open-data-science-conference/vacancies/133969/," Lviv, remote","Recruiter (Contractor, part-time)",05 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/open-data-science-conference/,Open Data Science Conference,"{""Required skills"": [""Project management experience, preferably in a marketing, IT, or related roleA well-organized individual that is known to be task/goal-oriented and good at giving clear instructions. Ability to communicate effectively with business stakeholders, team, and partnersExcellent attention to detail, ability to prioritize and multitask while driving results.Knowledge of marketing tools such as CRM, content management, social and analytics.Team leading experience is a plusExperience with marketing for staffing firms is desiredSoftware Product Marketing Experience a plus""], ""We offer"": [""Full-time job (working hours mostly from 11:00 to 20:00);Competitive salary and bonus system;Working in a team, introductory and team building activitiesContinuous practice of communicating with native English speakers and English courses;Overseas travel opportunities;Apple MacBook workstation;Modern office 5 min walk from Lviv city center;Healthcare Insurance;20 days paid vacation.""], ""Responsibilities"": [""Work with our design team, marketing teams to deliver digitals assets and campaign for social media. Liaise with Product Management, Marketing, and web teams to build an integrated and customer-centric web experienceIdentify opportunities to improve scientific and technical content, drive updates through content refresh and development processesMaintain and update product digital marketing assets (data, images, videos, citations, etc.) Leverage web analytics to track improvements and uncover relevant insights, content gaps and opportunities for improvementMonitor and optimize campaigns with a focus on measurable performanceReport on all marketing activity to assess reach, impact, and ROI""], ""Project description"": [""The Open Data Science Conference (ODSC) is one of the leaders in the exciting field of Data Science and Artificial Intelligence. Our conference series that spans Boston, New York, San Francisco, London, Dublin, Bangalore and other cities in the world\u2019s leading applied data science event. Along with our digital media assets including our blog site, video site, and webinars, we host some of the most groundbreaking sessions in topics such as machine learning, deep learning, conversational AI, and much more.""]}",,"Required skills Project management experience, preferably in a marketing, IT, or related roleA well-organized individual that is known to be task/goal-oriented and good at giving clear instructions. Ability to communicate effectively with business stakeholders, team, and partnersExcellent attention to detail, ability to prioritize and multitask while driving results.Knowledge of marketing tools such as CRM, content management, social and analytics.Team leading experience is a plusExperience with marketing for staffing firms is desiredSoftware Product Marketing Experience a plus We offer Full-time job (working hours mostly from 11:00 to 20:00);Competitive salary and bonus system;Working in a team, introductory and team building activitiesContinuous practice of communicating with native English speakers and English courses;Overseas travel opportunities;Apple MacBook workstation;Modern office 5 min walk from Lviv city center;Healthcare Insurance;20 days paid vacation. Responsibilities Work with our design team, marketing teams to deliver digitals assets and campaign for social media. Liaise with Product Management, Marketing, and web teams to build an integrated and customer-centric web experienceIdentify opportunities to improve scientific and technical content, drive updates through content refresh and development processesMaintain and update product digital marketing assets (data, images, videos, citations, etc.) Leverage web analytics to track improvements and uncover relevant insights, content gaps and opportunities for improvementMonitor and optimize campaigns with a focus on measurable performanceReport on all marketing activity to assess reach, impact, and ROI Project description The Open Data Science Conference (ODSC) is one of the leaders in the exciting field of Data Science and Artificial Intelligence. Our conference series that spans Boston, New York, San Francisco, London, Dublin, Bangalore and other cities in the world‚Äôs leading applied data science event. Along with our digital media assets including our blog site, video site, and webinars, we host some of the most groundbreaking sessions in topics such as machine learning, deep learning, conversational AI, and much more. ODSC is seeking an experienced Digital Content Manager with digital content experience for ODSC inception, our Bootcamp, career lab events, and new data scientist hiring platform to speak to our growing data science and AI community. We offer a dynamic role in a fast-growing company, full benefits, competitive salary. Remote applicants may apply to this position. Alternatively, for Lviv residents our modern office environment is located 5 min from Lviv city center.",Project Coordinator for Marketing team@Open Data Science Conference,https://jobs.dou.ua/companies/open-data-science-conference/vacancies/114057/, Lviv,Project Coordinator for Marketing team,05 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/quantum/,Quantum,"{""Required skills"": [""Knowledge and experience with algorithms and data structures;Strong knowledge of linear algebra, calculus, statistics and probability theory.Experience with Machine Learning libraries (NumPy, SciPy, Pandas, ScikitLearn ,etc);Experience with at least one of Deep Learning frameworks (Tensorflow, Keras, PyTorch, etc);Strong knowledge of OOP;Intermediate English.""], ""As a plus"": [""Docker practical experience;Experience in Computer Vision (segmentation, object detection, recognition, tracking, etc);Knowledge of modern Neural Networks architectures (DNN, CNN, LSTM, etc).Participation in Kaggle competitions.Research experience in university.""], ""We offer"": [""Exchange of experience, professional development;A strong team, a healthy atmosphere;Guidance by experienced Mentor;Flexible working time;20 hours of work per week.""], ""Responsibilities"": [""During your internship, you will work on a research project for ESA using remote sensing data""]}",,"Required skills Knowledge and experience with algorithms and data structures;Strong knowledge of linear algebra, calculus, statistics and probability theory.Experience with Machine Learning libraries (NumPy, SciPy, Pandas, ScikitLearn ,etc);Experience with at least one of Deep Learning frameworks (Tensorflow, Keras, PyTorch, etc);Strong knowledge of OOP;Intermediate English. As a plus Docker practical experience;Experience in Computer Vision (segmentation, object detection, recognition, tracking, etc);Knowledge of modern Neural Networks architectures (DNN, CNN, LSTM, etc).Participation in Kaggle competitions.Research experience in university. We offer Exchange of experience, professional development;A strong team, a healthy atmosphere;Guidance by experienced Mentor;Flexible working time;20 hours of work per week. Responsibilities During your internship, you will work on a research project for ESA using remote sensing data",Data Science Intern@Quantum,https://jobs.dou.ua/companies/quantum/vacancies/130800/?from=first-job, Kharkiv,Data Science Intern,05 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/eleks/,ELEKS,"{""Required skills"": [""Understanding theoretical concepts of statistics/probability, data mining, machine learningUnderstanding how these theoretical concepts could be applied to real world problemsAbility to understand the nature of business problems and see the place of analytical models in the solutionKnowledge and hands-on experience with one or more of the following: Matlab/Octave, R, PythonFamiliarity with the concept of HPC, parallel computingKnowledge and hands-on experience with one or more of the following for data analytics: Spark/Theano/TensorFlow/Caffee""], ""We offer"": [""Above average compensation and competitive Social packageClose cooperation with a customerBusiness tripsChallenging tasksCompetence developmentAbility to influence project technologiesProject from scratchTeam of professionalsDynamic environment with low level of bureaucracy""], ""Responsibilities"": [""Getting insight into business problem, understanding the opportunity and value of analytical models for the customerCollecting, transforming and preprocessing raw data to prepare it for analysisDeriving descriptive statistics out of the preprocessed dataBuilding statistical and probabilistic modelsDesigning, developing, training and testing data mining, machine learning and artificial intelligence models and algorithmsProviding comparative research on different algorithms and modelsImplementing the model in a form that can be easily used by engineers, documenting its interfacesDelivering the model in a form that can be easily deployable and maintained""], ""Project description"": [""Data scientist approaches business problems of diverse scopes, within different industries, using applied math. She talks to customers, understands their needs, suggests data science approach, develops statistical, probabilistic, data mining, machine learning models, improves accuracy of existing systems by enriching them with advanced analytical models, provides solutions in a form that can be easily deployed, used and maintained.""]}",,"Required skills Understanding theoretical concepts of statistics/probability, data mining, machine learningUnderstanding how these theoretical concepts could be applied to real world problemsAbility to understand the nature of business problems and see the place of analytical models in the solutionKnowledge and hands-on experience with one or more of the following: Matlab/Octave, R, PythonFamiliarity with the concept of HPC, parallel computingKnowledge and hands-on experience with one or more of the following for data analytics: Spark/Theano/TensorFlow/Caffee We offer Above average compensation and competitive Social packageClose cooperation with a customerBusiness tripsChallenging tasksCompetence developmentAbility to influence project technologiesProject from scratchTeam of professionalsDynamic environment with low level of bureaucracy Responsibilities Getting insight into business problem, understanding the opportunity and value of analytical models for the customerCollecting, transforming and preprocessing raw data to prepare it for analysisDeriving descriptive statistics out of the preprocessed dataBuilding statistical and probabilistic modelsDesigning, developing, training and testing data mining, machine learning and artificial intelligence models and algorithmsProviding comparative research on different algorithms and modelsImplementing the model in a form that can be easily used by engineers, documenting its interfacesDelivering the model in a form that can be easily deployable and maintained Project description Data scientist approaches business problems of diverse scopes, within different industries, using applied math. She talks to customers, understands their needs, suggests data science approach, develops statistical, probabilistic, data mining, machine learning models, improves accuracy of existing systems by enriching them with advanced analytical models, provides solutions in a form that can be easily deployed, used and maintained.",Middle Data Scientist@ELEKS,https://jobs.dou.ua/companies/eleks/vacancies/101457/, Lviv,Middle Data Scientist,03 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/eleks/,ELEKS,"{""Required skills"": [""3+ years of working experience in Data ScienceExperience with Deep Learning 3+ yearsFamiliar with weighs quantization approachesStrong understanding of TensorFlow and Keras frameworksExperience with other deep learning frameworks such as Pytorch, MXnet is a plusUpper-intermediate level of English""], ""We offer"": [""Above average compensation and competitive Social packageClose cooperation with a customerBusiness tripsChallenging tasksCompetence developmentAbility to influence project technologiesProjects from scratchTeam of professionalsDynamic environment with low level of bureaucracyMedical insurance""], ""Responsibilities"": [""Getting insight into the business problem, understanding the opportunity and value of analytical models for the customerCollecting, transforming and preprocessing raw data to prepare it for analysisDeriving descriptive statistics out of the preprocessed dataBuilding statistical and probabilistic modelsDesigning, developing, training and testing data mining, machine learning and artificial intelligence models and algorithmsProviding comparative research on different algorithms and modelsImplementing the model in a form that can be easily used by engineers, documenting its interfacesDelivering the model in a form that can be easily deployable and maintainedCommunication with customer""], ""Project description"": [""The system we develop is designed to enable developers working on edge AI projects to optimize within compute, energy, and memory budget Deep Learning models so they can run on MCUs and not losing the accuracy.""]}",,"Required skills 3+ years of working experience in Data ScienceExperience with Deep Learning 3+ yearsFamiliar with weighs quantization approachesStrong understanding of TensorFlow and Keras frameworksExperience with other deep learning frameworks such as Pytorch, MXnet is a plusUpper-intermediate level of English We offer Above average compensation and competitive Social packageClose cooperation with a customerBusiness tripsChallenging tasksCompetence developmentAbility to influence project technologiesProjects from scratchTeam of professionalsDynamic environment with low level of bureaucracyMedical insurance Responsibilities Getting insight into the business problem, understanding the opportunity and value of analytical models for the customerCollecting, transforming and preprocessing raw data to prepare it for analysisDeriving descriptive statistics out of the preprocessed dataBuilding statistical and probabilistic modelsDesigning, developing, training and testing data mining, machine learning and artificial intelligence models and algorithmsProviding comparative research on different algorithms and modelsImplementing the model in a form that can be easily used by engineers, documenting its interfacesDelivering the model in a form that can be easily deployable and maintainedCommunication with customer Project description The system we develop is designed to enable developers working on edge AI projects to optimize within compute, energy, and memory budget Deep Learning models so they can run on MCUs and not losing the accuracy.",Data Scientist@ELEKS,https://jobs.dou.ua/companies/eleks/vacancies/127071/, Lviv,Data Scientist,03 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/kuatro-technologies/,Jabil Software Services,"{""Required skills"": [""Job requirements:\u2219 Minimum of 6 or more years working as Data Engineer on Advanced Analytics and Data Provisioning projects.\u2219 Business Requirements Analysis and technical design\u2219 Communication with key business users, Data Scientists, IT Tech leads.\u2219 Hands-on experience with BIG data processing, clean, transform and prepare data.\u2219 Hands-on experience working with Data Lakes (Hadoop, AWS S3, Azure ADLS)\u2219 Expertise on joining datasets and creation of de-normalized views\u2219 Experience with Data Preparation and Features Engineering as part of Machine Learning or Advance Analytics project\u2219 Experience using Code/Version management tools.""], ""As a plus"": [""Will be a plus:\u2219 Some basic knowledge of Supply chain and Manufacturing process domain\u2219 Understanding of Cloud Solutions and Architecture""], ""We offer"": [""We offer:\u2219 competitive compensation\u2219 excellent benefit package\u2219 flexible schedule\u2219 sports activities support\u2219 health insurance support\u2219 corporate English\u2219 free lunches\u2219 opportunities to grow professionally\u2219 well-equipped workplace\u2219 convenient office location (Botanichniy Sad metro station)""], ""Responsibilities"": [""Experience and knowledge:\u2219 Deep knowledge of AWS Data Services\u2219 Experienced writing AWS Glue, Spark code for data processing and preparation\u2219 Experienced working with Glue Endpoints. Step functions, Lambda\u2219 Experienced in SageMaker notebooks using Python and PySpark to create features and write to S3 & AuroraDB\u2219 Experienced in IoT Analytics using AWS Kinesis data streams or Kafka streaming\u2219 Automate with CI/CD pipelines by Modularizing / generalizing with variables\u2219 Experienced working in any of the popular ETL tools\u2219 Experienced or Working knowledge of Talend\u2219 Working knowledge of Snowflake\u2219 Working knowledge of Hadoop toolset like HDFS, Hive, Impala\u2219 Working knowledge of Linux Shell Scripting""], ""Project description"": [""Data Engineer will be responsible for the design, development and implementation of BIG data & IoT Analytics solutions within Jabil\u2019s Enterprise Information Platform. A successful candidate will be able to create, evaluate and implement plans and design proposals for high-impact analytics solutions and their use involving leading-edge cloud technologies and methods considering key factors such as their long-term effectiveness (service delivery and cost), practicality, technical limitations and criticality. Must be a team player who is both compassionate for learning and firm on standards to effectively deliver reporting solutions.""]}",,"Required skills Job requirements:‚àô Minimum of 6 or more years working as Data Engineer on Advanced Analytics and Data Provisioning projects.‚àô Business Requirements Analysis and technical design‚àô Communication with key business users, Data Scientists, IT Tech leads.‚àô Hands-on experience with BIG data processing, clean, transform and prepare data.‚àô Hands-on experience working with Data Lakes (Hadoop, AWS S3, Azure ADLS)‚àô Expertise on joining datasets and creation of de-normalized views‚àô Experience with Data Preparation and Features Engineering as part of Machine Learning or Advance Analytics project‚àô Experience using Code/Version management tools. As a plus Will be a plus:‚àô Some basic knowledge of Supply chain and Manufacturing process domain‚àô Understanding of Cloud Solutions and Architecture We offer We offer:‚àô competitive compensation‚àô excellent benefit package‚àô flexible schedule‚àô sports activities support‚àô health insurance support‚àô corporate English‚àô free lunches‚àô opportunities to grow professionally‚àô well-equipped workplace‚àô convenient office location (Botanichniy Sad metro station) Responsibilities Experience and knowledge:‚àô Deep knowledge of AWS Data Services‚àô Experienced writing AWS Glue, Spark code for data processing and preparation‚àô Experienced working with Glue Endpoints. Step functions, Lambda‚àô Experienced in SageMaker notebooks using Python and PySpark to create features and write to S3 & AuroraDB‚àô Experienced in IoT Analytics using AWS Kinesis data streams or Kafka streaming‚àô Automate with CI/CD pipelines by Modularizing / generalizing with variables‚àô Experienced working in any of the popular ETL tools‚àô Experienced or Working knowledge of Talend‚àô Working knowledge of Snowflake‚àô Working knowledge of Hadoop toolset like HDFS, Hive, Impala‚àô Working knowledge of Linux Shell Scripting Project description Data Engineer will be responsible for the design, development and implementation of BIG data & IoT Analytics solutions within Jabil‚Äôs Enterprise Information Platform. A successful candidate will be able to create, evaluate and implement plans and design proposals for high-impact analytics solutions and their use involving leading-edge cloud technologies and methods considering key factors such as their long-term effectiveness (service delivery and cost), practicality, technical limitations and criticality. Must be a team player who is both compassionate for learning and firm on standards to effectively deliver reporting solutions.",Middle/Senior Data Engineer@Jabil Software Services,https://jobs.dou.ua/companies/kuatro-technologies/vacancies/133921/," Kharkiv, remote",Middle/Senior Data Engineer,02 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/itera-consulting-group-ukraine/,Itera,"{""Required skills"": [""Professional requirements:"", ""At least 3+ years\u2019 experience within software development / Data Engineering"", ""Experience with Python, preferably also Spark. Databricks is a plus"", ""Experience with Microsoft Azure, especially Azure Functions"", ""Familiarity with Monitoring and logging"", ""Experience with CI / CD pipelines where automated testing is also part of the pipeline"", ""Experience in taking larger projects from prototypes to production (ie know the requirements for \u201cproduction quality\u201d)"", ""Understanding of the concept \u201cdata platform\u201d, ie both technology (cloud and distributed solutions) and business model"", ""Experience of direct communication with customer and requirements clarification"", ""English: Intermediate or higher. You must be able to talk to Norwegian manager directly."", ""High-level of responsibility and loyalty""], ""As a plus"": [""Will be a plus:"", ""Previous experience at Scandinavian market"", ""Previous experience in Oil & Gas projects"", ""Ability to build trust with the major international data providers by understanding data and use cases""], ""We offer"": [""Company offers:"", ""Competitive compensation"", ""Paid vacation, 100% paid sick leave (+3 paid sick days without sick list)"", ""Medical insurance with sport and stomatology programs"", ""Free English and Norwegian Courses"", ""Comfortable office in the center of Kyiv with excellent office facilities"", ""Friendly team of high experienced specialists"", ""Trainings and travel abroad, certifications covered by the company""], ""Responsibilities"": [""Tasks and responsibilities:"", ""Able to work independently"", ""Communicate with customer, gather technical requirements"", ""Implement software development solution and modules""], ""Project description"": [""Itera is looking for a Senior Data Engineer who will work on the big Norwegian industrial IOT data platform provider. Clients data platform aggregates and processes data from millions of industrial sensors. Combine that data with 3D-models and drawings, P&IDs, and historical data. Current goal for our client is to build a team which going to work on customization of existed solution for oil & gas industry. We are looking for an open-minded person, self-going and proactive who confess continuous self-development and able to solve challenging tasks. We need someone that can understand the customer\u2019s goals and find relevant and proper solutions without continuous supervision.""]}",,"Required skills Professional requirements:‚Ä¢ At least 3+ years‚Äô experience within software development / Data Engineering‚Ä¢ Experience with Python, preferably also Spark. Databricks is a plus‚Ä¢ Experience with Microsoft Azure, especially Azure Functions‚Ä¢ Familiarity with Monitoring and logging‚Ä¢ Experience with CI / CD pipelines where automated testing is also part of the pipeline‚Ä¢ Experience in taking larger projects from prototypes to production (ie know the requirements for ‚Äúproduction quality‚Äù)‚Ä¢ Understanding of the concept ‚Äúdata platform‚Äù, ie both technology (cloud and distributed solutions) and business model‚Ä¢ Experience of direct communication with customer and requirements clarification‚Ä¢ English: Intermediate or higher. You must be able to talk to Norwegian manager directly.‚Ä¢ High-level of responsibility and loyalty As a plus Will be a plus:‚Ä¢ Previous experience at Scandinavian market ‚Ä¢ Previous experience in Oil & Gas projects ‚Ä¢ Ability to build trust with the major international data providers by understanding data and use cases We offer Company offers:‚Ä¢ Competitive compensation‚Ä¢ Paid vacation, 100% paid sick leave (+3 paid sick days without sick list)‚Ä¢ Medical insurance with sport and stomatology programs‚Ä¢ Free English and Norwegian Courses‚Ä¢ Comfortable office in the center of Kyiv with excellent office facilities‚Ä¢ Friendly team of high experienced specialists‚Ä¢ Trainings and travel abroad, certifications covered by the company Responsibilities Tasks and responsibilities:‚Ä¢ Able to work independently ‚Ä¢ Communicate with customer, gather technical requirements‚Ä¢ Implement software development solution and modules Project description Itera is looking for a Senior Data Engineer who will work on the big Norwegian industrial IOT data platform provider. Clients data platform aggregates and processes data from millions of industrial sensors. Combine that data with 3D-models and drawings, P&IDs, and historical data. Current goal for our client is to build a team which going to work on customization of existed solution for oil & gas industry. We are looking for an open-minded person, self-going and proactive who confess continuous self-development and able to solve challenging tasks. We need someone that can understand the customer‚Äôs goals and find relevant and proper solutions without continuous supervision.",Senior Data Engineer with Spark and Python@Itera,https://jobs.dou.ua/companies/itera-consulting-group-ukraine/vacancies/130059/," Kyiv, Lviv",Senior Data Engineer with Spark and Python,02 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/itera-consulting-group-ukraine/,Itera,"{""Required skills"": [""Professional requirements:"", ""Experience in taking larger projects from prototypes to production (ie know the requirements for \u201cproduction quality\u201d)"", ""Understanding of the concept \u201cdata platform\u201d, ie both technology (cloud and distributed solutions) and business model"", ""Experience and hands-on in data engineering (or data science) programming, preferably Python as it is used most with us on the backend. We need professional who can both program and think / communicate on a conceptual level"", ""Experience with test-driven development and test automation (the road from test strategy via test plans to combination of structured manual testing and automation)"", ""Experience with relevant Azure services (storage, analysis etc) and infrastructure-as-a-code (Terraform etc)"", ""Experience with CI / CD pipelines where automated testing is also part of the pipeline"", ""Experience with writing robust and maintainable code, ie living out sensible principles such as KISS, DRY, YAGNI, SOLID etc"", ""Experience with IoT (we work with ocean IoT and underwater robotics)"", ""Knowledge of DevOps and DataOps principles / processes"", ""Knowledge of geospatial data and data adapted for GIS solutions""], ""We offer"": [""Personal requirements:"", ""Self-running, flexible, able to work in Agile environment"", ""Proactive when it comes to solution proposals and improvement proposals"", ""Ability to understand and emphatize with end users"", ""Ability to build trust with the major international data providers in marine research by understanding data and use cases"", ""Good at customer / user dialogue, understand \u201cpain points\u201d"", ""Experience and understanding of the importance of delivering quality, as well as understanding maintainability, testability, scalability, safety etc"", ""Thrives on working in an international interdisciplinary scrum team consisting of UXs, UI designers, front-end developers, solution architects, data engineers, data scientist and infrastructure / backend people""], ""Responsibilities"": [""Company offers:"", ""Competitive compensation"", ""Paid vacation, 100% paid sick leave (+3 paid sick days without sick list)"", ""Medical insurance with sport and stomatology programs"", ""Free English and Norwegian Courses"", ""Comfortable office in the center of Kyiv with excellent office facilities"", ""Friendly team of high experienced specialists"", ""Trainings and travel abroad, certifications covered by the company""], ""Project description"": [""Tasks and responsibilities:"", ""Able to work independently"", ""Communicate with customer, gather technical requirements"", ""Implement software development solution and modules""]}",,"Required skills Professional requirements:‚Ä¢ Experience in taking larger projects from prototypes to production (ie know the requirements for ‚Äúproduction quality‚Äù)‚Ä¢ Understanding of the concept ‚Äúdata platform‚Äù, ie both technology (cloud and distributed solutions) and business model‚Ä¢ Experience and hands-on in data engineering (or data science) programming, preferably Python as it is used most with us on the backend. We need professional who can both program and think / communicate on a conceptual level‚Ä¢ Experience with test-driven development and test automation (the road from test strategy via test plans to combination of structured manual testing and automation)‚Ä¢ Experience with relevant Azure services (storage, analysis etc) and infrastructure-as-a-code (Terraform etc)‚Ä¢ Experience with CI / CD pipelines where automated testing is also part of the pipeline‚Ä¢ Experience with writing robust and maintainable code, ie living out sensible principles such as KISS, DRY, YAGNI, SOLID etc‚Ä¢ Experience with IoT (we work with ocean IoT and underwater robotics)‚Ä¢ Knowledge of DevOps and DataOps principles / processes‚Ä¢ Knowledge of geospatial data and data adapted for GIS solutions Personal requirements:‚Ä¢ Self-running, flexible, able to work in Agile environment ‚Ä¢ Proactive when it comes to solution proposals and improvement proposals‚Ä¢ Ability to understand and emphatize with end users‚Ä¢ Ability to build trust with the major international data providers in marine research by understanding data and use cases‚Ä¢ Good at customer / user dialogue, understand ‚Äúpain points‚Äù ‚Ä¢ Experience and understanding of the importance of delivering quality, as well as understanding maintainability, testability, scalability, safety etc‚Ä¢ Thrives on working in an international interdisciplinary scrum team consisting of UXs, UI designers, front-end developers, solution architects, data engineers, data scientist and infrastructure / backend people We offer Company offers:‚Ä¢ Competitive compensation‚Ä¢ Paid vacation, 100% paid sick leave (+3 paid sick days without sick list)‚Ä¢ Medical insurance with sport and stomatology programs‚Ä¢ Free English and Norwegian Courses‚Ä¢ Comfortable office in the center of Kyiv with excellent office facilities‚Ä¢ Friendly team of high experienced specialists‚Ä¢ Trainings and travel abroad, certifications covered by the company Responsibilities Tasks and responsibilities:‚Ä¢ Able to work independently ‚Ä¢ Communicate with customer, gather technical requirements‚Ä¢ Implement software development solution and modules Project description Itera is looking for a Senior Data Engineer who will work on the big Norwegian industrial IOT data platform provider. Clients data platform aggregates and processes data from millions of industrial sensors. Combine that data with 3D-models and drawings, P&IDs, and historical data. Current goal for our client is to build a team which going to work on customization of existed solution for oil & gas industry. We are looking for an open-minded person, self-going and proactive who confess continuous self-development and able to solve challenging tasks. We need someone that can understand the customer‚Äôs goals and find relevant and proper solutions without continuous supervision.",Senior Data Engineer with Python or¬†Java@Itera,https://jobs.dou.ua/companies/itera-consulting-group-ukraine/vacancies/130063/," Kyiv, Lviv",Senior Data Engineer with Python or¬†Java,02 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/sciforce/,SciForce,"{""Required skills"": [""No less than 2 years in ML development;"", ""Strong technical skills regarding data analysis, statistics and programming;"", ""Working experience in Python and/or C++;"", ""Experience with DNN frameworks as PyTorch, or TensorFlow;"", ""Knowledge of machine learning and deep learning technologies;"", ""Research mindset and math-oriented expertise;"", ""Good technical written and spoken English.""], ""As a plus"": [""Ability to work on AWS stack;"", ""Familiarity with Linux/Unix/Shell environments""], ""We offer"": [""Transparency in communication between the company and an employee;"", ""Paid vacation (20 business days) and paid sick leave;"", ""Opportunity to join advanced, innovative projects;"", ""Accounting as a service;"", ""Competitive salary;"", ""Friendly working environment;"", ""Language classes;""], ""Responsibilities"": [""Designing and implementation of ML models and solutions;"", ""Developing best Machine Learning pipelines and proof-of-concept prototypes for a variety of use cases;"", ""Creating API endpoints and wrapping models into Docker containers.""], ""Project description"": [""SciForce is looking for an experienced Machine Learning Engineer to strengthen our ML experts team. You will be a part of a cross-functional team focused on product innovations and new technologies in such areas as Speech Processing, NLP, Computer Vision, Anomaly Detection/Prediction, Data Science.""]}",,"Required skills ‚Äî No less than 2 years in ML development;‚Äî Strong technical skills regarding data analysis, statistics and programming;‚Äî Working experience in Python and/or C++;‚Äî Experience with DNN frameworks as PyTorch, or TensorFlow;‚Äî Knowledge of machine learning and deep learning technologies;‚Äî Research mindset and math-oriented expertise;‚Äî Good technical written and spoken English. As a plus ‚Äî Ability to work on AWS stack;‚Äî Familiarity with Linux/Unix/Shell environments We offer ‚Äî Transparency in communication between the company and an employee;‚Äî Paid vacation (20 business days) and paid sick leave;‚Äî Opportunity to join advanced, innovative projects;‚Äî Accounting as a service;‚Äî Competitive salary;‚Äî Friendly working environment;‚Äî Language classes; Responsibilities ‚Äî Designing and implementation of ML models and solutions;‚Äî Developing best Machine Learning pipelines and proof-of-concept prototypes for a variety of use cases;‚Äî Creating API endpoints and wrapping models into Docker containers. Project description SciForce is looking for an experienced Machine Learning Engineer to strengthen our ML experts team. You will be a part of a cross-functional team focused on product innovations and new technologies in such areas as Speech Processing, NLP, Computer Vision, Anomaly Detection/Prediction, Data Science.",Middle Machine Learning Engineer@SciForce,https://jobs.dou.ua/companies/sciforce/vacancies/103155/," Kharkiv, Lviv",Middle Machine Learning Engineer,02 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/playtech/,Playtech,"{""Required skills"": [""3+ years of development experience with Business Intelligence tools"", ""A Bachelor\u2019s Degree in Computer Science, Engineering, Information Technology, Information Systems or equivalent"", ""Understanding of ETL (Extract, Transform, Load) techniques and processes"", ""Knowledge of databases and data warehouse design"", ""Experience with developing dashboards and reports"", ""Experience working with MicroStrategy"", ""advantage"", ""Experience with fundamental SQL and data modeling"", ""Understanding and knowledge of Big Data technologies such as Presto/Hive/HBase/Redis/MemSQL/MySQL/Kafka/Spark processes"", ""advantage"", ""Experience and knowledge working with SQL Server Integration Services (SSIS)"", ""Knowledge of working with large datasets"", ""Knowledge of data modeling and data visualization"", ""Prior experience with SCRUM/Agile methodologies"", ""Strong technical, analytical, and mathematical skills desired"", ""Strong self-initiative"", ""English as a native language"", ""Great communication skills and strong customer facing abilities""], ""We offer"": [""\u00b7 Possibility to cooperate with a product company\u00b7 Professional growth\u00b7 Educational possibilities\u00b7 Competitive compensation\u00b7 Fully-equipped perfect office space located in the city center (\u201cPalats Sportu\u201d metro station)\u00b7 Warm and friendly attitude to every specialist""], ""Responsibilities"": [""Creating and managing BI solutions. Working with business unit stakeholders to define Playtech\u2019s BI delivery roadmap and scope"", ""Analyzing business requirements and developing Business Intelligence solutions that provide business value to users"", ""Developing views, custom reports, and dashboards pursuant to business requirements"", ""Recommending and developing extendable and state of the art information delivery solutions to optimize ease of use and user adoption"", ""Providing training and assistance to business users""]}",,"Required skills ‚Ä¢ 3+ years of development experience with Business Intelligence tools‚Ä¢ A Bachelor‚Äôs Degree in Computer Science, Engineering, Information Technology, Information Systems or equivalent‚Ä¢ Understanding of ETL (Extract, Transform, Load) techniques and processes‚Ä¢ Knowledge of databases and data warehouse design‚Ä¢ Experience with developing dashboards and reports‚Ä¢ Experience working with MicroStrategy ‚Äî advantage‚Ä¢ Experience with fundamental SQL and data modeling ‚Ä¢ Understanding and knowledge of Big Data technologies such as Presto/Hive/HBase/Redis/MemSQL/MySQL/Kafka/Spark processes ‚Äî advantage‚Ä¢ Experience and knowledge working with SQL Server Integration Services (SSIS)‚Ä¢ Knowledge of working with large datasets ‚Ä¢ Knowledge of data modeling and data visualization‚Ä¢ Prior experience with SCRUM/Agile methodologies ‚Ä¢ Strong technical, analytical, and mathematical skills desired ‚Ä¢ Strong self-initiative ‚Ä¢ English as a native language ‚Ä¢ Great communication skills and strong customer facing abilities We offer ¬∑ Possibility to cooperate with a product company¬∑ Professional growth¬∑ Educational possibilities¬∑ Competitive compensation¬∑ Fully-equipped perfect office space located in the city center (‚ÄúPalats Sportu‚Äù metro station)¬∑ Warm and friendly attitude to every specialist Responsibilities ‚Ä¢ Creating and managing BI solutions. Working with business unit stakeholders to define Playtech‚Äôs BI delivery roadmap and scope‚Ä¢ Analyzing business requirements and developing Business Intelligence solutions that provide business value to users‚Ä¢ Developing views, custom reports, and dashboards pursuant to business requirements‚Ä¢ Recommending and developing extendable and state of the art information delivery solutions to optimize ease of use and user adoption‚Ä¢ Providing training and assistance to business users",BI¬†Developer@Playtech,https://jobs.dou.ua/companies/playtech/vacancies/133830/, Kyiv,BI¬†Developer,02 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/kitrum/,KitRUM,"{""Required skills"": [""5+ years of professional experience"", ""Solid experience with Scala and Spark."", ""Experience with AWS"", ""Great coding skills and software development experience"", ""Level of English: Upper-Intermediate"", ""Watched all seasons of \u201cRick and Morty\u201d""], ""We offer"": [""High compensation according to your technical skills"", ""Long-term projects (12m+) with great Customers"", ""5-day working week, 8-hour working day, flexible schedule"", ""Democratic management style & friendly environment"", ""WFH option (Possibility to work from home)"", ""Annual Paid vacation"", ""15 b/days + unpaid vacation"", ""Paid sick leaves"", ""6 b/days per year"", ""Ukrainian official holidays"", ""Corporate Perks (external training, English courses, corporate events/team buildings)"", ""Cozy office in the center of the city"", ""Coffee, cookies and other goodies"", ""Professional and personal growth""], ""Responsibilities"": [""Perform tasks related to data migration"", ""Work under a close supervision of Lead Big Data Engineer and help other engineers to execute the compute migration.""], ""Project description"": [""Client is an American e-book and audiobook subscription service that includes one million titles. Platform hosts 60 million documents on its open publishing platform.The platform allows:"", ""anyone to share his/her ideas with the world;"", ""access to audio books;"", ""access to world\u2019s composers who publish their music;"", ""incorporates articles from private publishers and world magazines;"", ""allows access to exclusive content.""]}",,"Required skills ‚Äî 5+ years of professional experience‚Äî Solid experience with Scala and Spark. ‚Äî Experience with AWS‚Äî Great coding skills and software development experience ‚Äî Level of English: Upper-Intermediate‚Äî Watched all seasons of ‚ÄúRick and Morty‚Äù We offer ‚Äî High compensation according to your technical skills‚Äî Long-term projects (12m+) with great Customers‚Äî 5-day working week, 8-hour working day, flexible schedule‚Äî Democratic management style & friendly environment‚Äî WFH option (Possibility to work from home)‚Äî Annual Paid vacation ‚Äî 15 b/days + unpaid vacation‚Äî Paid sick leaves ‚Äî 6 b/days per year‚Äî Ukrainian official holidays‚Äî Corporate Perks (external training, English courses, corporate events/team buildings)‚Äî Cozy office in the center of the city‚Äî Coffee, cookies and other goodies‚Äî Professional and personal growth Responsibilities ‚Äî Perform tasks related to data migration‚Äî Work under a close supervision of Lead Big Data Engineer and help other engineers to execute the compute migration. Project description Client is an American e-book and audiobook subscription service that includes one million titles. Platform hosts 60 million documents on its open publishing platform.The platform allows: ‚Äî anyone to share his/her ideas with the world;‚Äî access to audio books;‚Äî access to world‚Äôs composers who publish their music;‚Äî incorporates articles from private publishers and world magazines;‚Äî allows access to exclusive content. –°ompany description KitRUM is a one-stop custom software development company headquartered in sunny Florida with development centers in Ukraine and Poland.With a pool of 300+ top-notch engineering resources, we help CxOs of VC-backed startups and fast-growing tech companies in the US, EU and Australia to custom build software engineering teams packed with top-tier talent. Why us? We realize that one of the most crucial things for developers ‚Äî adequate client and fascinating projects. So we qualify our clients to make sure that they:‚Äî have an idea that they believe will make the world a better place;‚Äî think long-term and looking for a trusted-tech partner;‚Äî want to rely on and avoid micromanaging;‚Äî are not f$%$ing jackasses :-)Follow our team on Instagram to know more about our daily life :)We adore making new friends on the board!",Senior Big Data Engineer (Scala/Spark)@KitRUM,https://jobs.dou.ua/companies/kitrum/vacancies/124906/," Kyiv, Kharkiv, Lviv, Dnipro, Uzhhorod, remote",Senior Big Data Engineer (Scala/Spark),02 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/teamsoft/,LLC Proxima Research International,"{""Required skills"": [""\u0412\u044b\u0441\u0448\u0435\u0435 \u0442\u0435\u0445\u043d\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u0435 (\u043c\u043e\u0436\u043d\u043e \u043d\u0435 \u0437\u0430\u043a\u043e\u043d\u0447\u0435\u043d\u043d\u043e\u0435)\u041e\u043f\u044b\u0442 \u0440\u0430\u0431\u043e\u0442\u044b \u0441 \u0431\u043e\u043b\u044c\u0448\u0438\u043c\u0438 \u043c\u0430\u0441\u0441\u0438\u0432\u0430\u043c\u0438 \u0434\u0430\u043d\u043d\u044b\u0445 \u043f\u0440\u0438\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u0435\u0442\u0441\u044f\u0417\u043d\u0430\u043d\u0438\u0435 Excel/VBA/SQL \u043e\u0431\u044f\u0437\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u041f\u043e\u043d\u0438\u043c\u0430\u043d\u0438\u0435 \u043f\u0440\u0438\u043d\u0446\u0438\u043f\u043e\u0432 \u0440\u0430\u0431\u043e\u0442\u044b ETL \u0423\u043c\u0435\u043d\u0438\u0435 \u0433\u0440\u0430\u043c\u043e\u0442\u043d\u043e \u0438\u0437\u043b\u0430\u0433\u0430\u0442\u044c \u0441\u0432\u043e\u0438 \u043c\u044b\u0441\u043b\u0438 \u0438 \u043e\u0431\u043e\u0441\u043d\u043e\u0432\u044b\u0432\u0430\u0442\u044c \u0442\u0435\u0445\u043d\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u0440\u0435\u0448\u0435\u043d\u0438\u044f, \u0443\u0441\u0442\u043d\u043e \u0438 \u043f\u0438\u0441\u044c\u043c\u0435\u043d\u043d\u043e\u041e\u0440\u0438\u0435\u043d\u0442\u0430\u0446\u0438\u044f \u043d\u0430 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442, \u0443\u043c\u0435\u043d\u0438\u0435 \u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c \u0432 \u043a\u043e\u043c\u0430\u043d\u0434\u0435, \u0431\u044b\u0441\u0442\u0440\u0430\u044f \u043e\u0431\u0443\u0447\u0430\u0435\u043c\u043e\u0441\u0442\u044c, \u0430\u043d\u0430\u043b\u0438\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u043c\u044b\u0448\u043b\u0435\u043d\u0438\u0435""], ""As a plus"": [""\u0417\u043d\u0430\u043d\u0438\u0435 QlikView/SSIS"", ""\u043a\u0430\u043a \u043f\u043b\u044e\u0441""], ""We offer"": [""\u0418\u043d\u0442\u0435\u0440\u0435\u0441\u043d\u044b\u0435 \u0437\u0430\u0434\u0430\u0447\u0438\u0412\u043e\u0437\u043c\u043e\u0436\u043d\u043e\u0441\u0442\u044c \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u0438 \u0441\u043e\u0432\u0435\u0440\u0448\u0435\u043d\u0441\u0442\u0432\u043e\u0432\u0430\u043d\u0438\u044f\u0413\u0440\u0430\u0444\u0438\u043a \u0440\u0430\u0431\u043e\u0442\u044b \u0441 9.00"", ""18.00, \u043f\u043e\u043d\u0435\u0434\u0435\u043b\u044c\u043d\u0438\u043a"", ""\u043f\u044f\u0442\u043d\u0438\u0446\u0430\u041e\u043f\u043b\u0430\u0447\u0438\u0432\u0430\u0435\u043c\u044b\u0439 \u043e\u0442\u043f\u0443\u0441\u043a 24 \u043a.\u0434.\u041e\u043f\u043b\u0430\u0447\u0438\u0432\u0430\u0435\u043c\u044b\u0439 \u0431\u043e\u043b\u044c\u043d\u0438\u0447\u043d\u044b\u0439""], ""Responsibilities"": [""\u041e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430, \u043a\u043e\u043d\u0442\u0440\u043e\u043b\u044c \u043f\u043e\u0441\u0442\u0443\u043f\u0430\u044e\u0449\u0438\u0445 \u0434\u0430\u043d\u043d\u044b\u0445\u0420\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0441\u043a\u0440\u0438\u043f\u0442\u043e\u0432 \u043f\u043e \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u044e \u0434\u0430\u043d\u043d\u044b\u0445\u0412\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0435 \u0441 \u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u0447\u0438\u043a\u0430\u043c\u0438""]}",,"Required skills –í—ã—Å—à–µ–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ (–º–æ–∂–Ω–æ –Ω–µ –∑–∞–∫–æ–Ω—á–µ–Ω–Ω–æ–µ)–û–ø—ã—Ç —Ä–∞–±–æ—Ç—ã —Å –±–æ–ª—å—à–∏–º–∏ –º–∞—Å—Å–∏–≤–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤—É–µ—Ç—Å—è–ó–Ω–∞–Ω–∏–µ Excel/VBA/SQL –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ–ü–æ–Ω–∏–º–∞–Ω–∏–µ –ø—Ä–∏–Ω—Ü–∏–ø–æ–≤ —Ä–∞–±–æ—Ç—ã ETL –£–º–µ–Ω–∏–µ –≥—Ä–∞–º–æ—Ç–Ω–æ –∏–∑–ª–∞–≥–∞—Ç—å —Å–≤–æ–∏ –º—ã—Å–ª–∏ –∏ –æ–±–æ—Å–Ω–æ–≤—ã–≤–∞—Ç—å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ä–µ—à–µ–Ω–∏—è, —É—Å—Ç–Ω–æ –∏ –ø–∏—Å—å–º–µ–Ω–Ω–æ–û—Ä–∏–µ–Ω—Ç–∞—Ü–∏—è –Ω–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç, —É–º–µ–Ω–∏–µ —Ä–∞–±–æ—Ç–∞—Ç—å –≤ –∫–æ–º–∞–Ω–¥–µ, –±—ã—Å—Ç—Ä–∞—è –æ–±—É—á–∞–µ–º–æ—Å—Ç—å, –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–æ–µ –º—ã—à–ª–µ–Ω–∏–µ As a plus –ó–Ω–∞–Ω–∏–µ QlikView/SSIS ‚Äî –∫–∞–∫ –ø–ª—é—Å We offer –ò–Ω—Ç–µ—Ä–µ—Å–Ω—ã–µ –∑–∞–¥–∞—á–∏–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è –∏ —Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–∏—è–ì—Ä–∞—Ñ–∏–∫ —Ä–∞–±–æ—Ç—ã —Å 9.00 ‚Äî 18.00, –ø–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫ ‚Äî –ø—è—Ç–Ω–∏—Ü–∞–û–ø–ª–∞—á–∏–≤–∞–µ–º—ã–π –æ—Ç–ø—É—Å–∫ 24 –∫.–¥.–û–ø–ª–∞—á–∏–≤–∞–µ–º—ã–π –±–æ–ª—å–Ω–∏—á–Ω—ã–π Responsibilities –û–±—Ä–∞–±–æ—Ç–∫–∞, –∫–æ–Ω—Ç—Ä–æ–ª—å –ø–æ—Å—Ç—É–ø–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö–†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ —Å–∫—Ä–∏–ø—Ç–æ–≤ –ø–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—é –¥–∞–Ω–Ω—ã—Ö–í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∞–º–∏",Data Engineer@LLC Proxima Research International,https://jobs.dou.ua/companies/teamsoft/vacancies/133801/, Kyiv,Data Engineer,02 October 2020,$700‚Äì900,2020-10-13,,dou
https://jobs.dou.ua/companies/ooo-kadkon-ukraina/,SII UKRAINE,"{""Required skills"": [""About usSII Ukraine is a subsidiary of the German company SII TECHNOLOGIES (www.de.sii.group), part of the French company SII Group, which has offices in 19 countries with 8000 employees worldwide. SII UKRAINE provides software development, engineering and technical consulting services in various branches of mechanical engineering (general, heavy, special, automotive and medical engineering), production automation as well as electrical engineering. Customers of services are such well-known companies as BMW, AUDI, VW, PORSCHE, AIRBUS, BSH, KRONES, SIEMENS, ZEISS, KUKA, ABB and many others, with which our company works directly, without intermediaries.""], ""We offer"": [""Qualifications"", ""Completed studies in the field of information technology or comparable"", ""Experience in ASAM ODS and/or CORBA technology is desirable"", ""Sound knowledge of Java development required"", ""ISTQB certification from the field of testing will be an advantage"", ""Good skills English (written and spoken)"", ""German will be an advantage""], ""Responsibilities"": [""Competitive salary after Interview"", ""Help with relocation""], ""Project description"": [""Development of sophisticated and innovative IT solutions for the automotive industry"", ""Design and implementation of the projects up to the rollout"", ""Implement Code and GUI tests and technical analysis of incidents"", ""Configuration of software components"", ""Use of tools (CI Server) and code reviews to achieve the best possible results""]}",,"Required skills About usSII Ukraine is a subsidiary of the German company SII TECHNOLOGIES (www.de.sii.group), part of the French company SII Group, which has offices in 19 countries with 8000 employees worldwide. SII UKRAINE provides software development, engineering and technical consulting services in various branches of mechanical engineering (general, heavy, special, automotive and medical engineering), production automation as well as electrical engineering. Customers of services are such well-known companies as BMW, AUDI, VW, PORSCHE, AIRBUS, BSH, KRONES, SIEMENS, ZEISS, KUKA, ABB and many others, with which our company works directly, without intermediaries. Qualifications ‚Ä¢ Completed studies in the field of information technology or comparable‚Ä¢ Experience in ASAM ODS and/or CORBA technology is desirable‚Ä¢ Sound knowledge of Java development required‚Ä¢ ISTQB certification from the field of testing will be an advantage‚Ä¢ Good skills English (written and spoken) ‚Ä¢ German will be an advantage We offer ‚Ä¢ Competitive salary after Interview‚Ä¢ Help with relocation Responsibilities ‚Ä¢ Development of sophisticated and innovative IT solutions for the automotive industry‚Ä¢ Design and implementation of the projects up to the rollout‚Ä¢ Implement Code and GUI tests and technical analysis of incidents‚Ä¢ Configuration of software components‚Ä¢ Use of tools (CI Server) and code reviews to achieve the best possible results Project description A complex cloud-based measurement system management solution by one of the German carmaker brands. The solution was designed to:‚Ä¢ Ingest and analyze all possible kinds of measurement data from production sites of the brand‚Ä¢ Store raw data for the duration of lifetime of a vehicle model (10-20 years)‚Ä¢ Support all kinds of legal restrictions and regulations‚Ä¢ Support different geographies‚Ä¢ Support external partners, willing to get access to the data",Java Developer of¬†Measurement data management /relocation to¬†Germany@SII UKRAINE,https://jobs.dou.ua/companies/ooo-kadkon-ukraina/vacancies/130161/, Odesa,Java Developer of¬†Measurement data management /relocation to¬†Germany,02 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/eteam/,eTeam,"{""Required skills"": [""Experience in at least two of the following: Go, Python, Node.js (Typescript, Javascript)"", ""Experience in SQL and NoSQL databases (Postgres, MongoDB)"", ""Experience in AWS EKS / Kubernetes, AWS Elastic Beanstalk, AWS EC2, and Docker as infrastructure platform technology"", ""Experience in implementing unit, integration, end-to-end, and API tests"", ""Experience in refactoring"", ""Experience in CI/CD technology"", ""Experience in analyzing large datasets (SQL, Spark, Python, R, etc.)"", ""Familiarity with Scrum as a development methodology"", ""Have a great sense of end-to-end ownership of the products and tools you build"", ""Upper-intermediate written and spoken English.""], ""We offer"": [""Grow professionally with subsidized certifications, courses, and conferences"", ""Improve your English with conversation clubs and direct client communication"", ""Enjoy our extra perks: unlimited vacation, flexible schedule, bonuses for important life events, fresh food & snacks in the office"", ""Let loose with fun parties, BBQs, online activities and off-sites.""], ""Responsibilities"": [""Maintain and improve our microservices that support our fraud detection systems to ensure high reliability and security"", ""Building new data features that can be used in real-time to support our machine learning algorithms as well as fraud rules"", ""Improving our fraud detection system to be able to process complex fraud rules involving different parameters which will provide merchants more flexibility in tuning their fraud detection performance"", ""Collaborating with tech lead, project manager, and engineers to deliver feature based on considerations around system performance, availability, reliability, and scalability""], ""Project description"": [""On behalf of Xendit, eTeam is looking for a Senior Data Engineer for our team in Kyiv on a full-time basis.""]}",,"Required skills ‚Äî Experience in at least two of the following: Go, Python, Node.js (Typescript, Javascript)‚Äî Experience in SQL and NoSQL databases (Postgres, MongoDB)‚Äî Experience in AWS EKS / Kubernetes, AWS Elastic Beanstalk, AWS EC2, and Docker as infrastructure platform technology‚Äî Experience in implementing unit, integration, end-to-end, and API tests‚Äî Experience in refactoring‚Äî Experience in CI/CD technology‚Äî Experience in analyzing large datasets (SQL, Spark, Python, R, etc.)‚Äî Familiarity with Scrum as a development methodology‚Äî Have a great sense of end-to-end ownership of the products and tools you build‚Äî Upper-intermediate written and spoken English. We offer ‚Äî Grow professionally with subsidized certifications, courses, and conferences‚Äî Improve your English with conversation clubs and direct client communication‚Äî Enjoy our extra perks: unlimited vacation, flexible schedule, bonuses for important life events, fresh food & snacks in the office‚Äî Let loose with fun parties, BBQs, online activities and off-sites. Responsibilities ‚Äî Maintain and improve our microservices that support our fraud detection systems to ensure high reliability and security‚Äî Building new data features that can be used in real-time to support our machine learning algorithms as well as fraud rules‚Äî Improving our fraud detection system to be able to process complex fraud rules involving different parameters which will provide merchants more flexibility in tuning their fraud detection performance‚Äî Collaborating with tech lead, project manager, and engineers to deliver feature based on considerations around system performance, availability, reliability, and scalability Project description On behalf of Xendit, eTeam is looking for a Senior Data Engineer for our team in Kyiv on a full-time basis. Xendit is an Indonesian fintech company that processes payments, runs marketplaces, disburses payroll and loans, detects fraud and helps other businesses grow exponentially. It serves these companies by providing a suite of world-class APIs, eCommerce platform integrations, and easy to use applications for individual entrepreneurs, SMBs, and enterprises alike. Xendit‚Äôs mission is to make payments simple, secure and easy for everyone. It‚Äôs one of the fastest-growing companies in South East Asia that processes millions of transactions monthly, growing 8% month on month for the last 4 years. They are trusted and backed by some of the largest VCs in the world, who invested in Facebook, Slack, Twitch and Grab, and are alumni of the prestigious YCombinator (S15).",Senior Data Engineer for Xendit@eTeam,https://jobs.dou.ua/companies/eteam/vacancies/133785/," Kyiv, remote",Senior Data Engineer for Xendit,02 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/thirstysprout-llc/,ThirstySprout,{},,"Hello) We are looking for a Senior Data Engineer. Our client is a Seattle based company, founded by former Amazonians, offering a suite of services to empower brands to successfully drive their eCommerce business. They provide strategic advisory and business planning, pricing strategy, marketing, content creation and optimization, catalog management, inventory management, and reporting data and software for our diverse set of clients. Our high-quality, global customer base includes businesses such as Clorox, Kellogg‚Äôs, Unilever, CVS and many other top brands and retailers. We‚Äôre an entrepreneurial organization that is growing fast and looking for a strong Data Engineer to help us take our technology to the next level!Our data platform is currently based on Microsoft Sql Server hosted in Azure. The main consumers of our data are our platform written in C# which provides a .NET API to our applications, and in-house business analysts who connect directly using MS Power BI. What you‚Äôll do:Update and further evolve a robust data modelUpdate and further evolve a code ingesting huge amounts of data daily to support business needsDrive initiatives to expose the health and quality of our data to all stakeholders in the businessKeep up to date on the latest technologies and understand their benefits and tradeoffs and apply that knowledge as you build Stack: Sql Server, Azure, C#, .NET API What you‚Äôve done:Designed and implemented relational databases, preferably using Sql ServerBuilt cloud native data platforms, preferably Azure, preferably using C#Data acquisition focusing on traceability, repeatability, and exposing data quality metrics up th—É stack Stayed up to date on latest industry trendsDeveloped strong collaboration skills across many different disciplinesDemonstrated ability to plan, schedule, and deliver high quality software What you‚Äôve got:Ability and interest in thriving in a fast-paced, startup like environmentA strong desire for building robust, reliable, and traceable data systemsUnderstanding of various data storage technologies available and an understanding of their relative strengths and weaknesses Passion for customersExcellent written and verbal communication skillsOpen mind to explore different solutions and ideas presented by others What‚Äôs in it for you?Working with a talented, dynamic, and collaborative group peopleCompetitive salary and benefit packageMedical, Dental and Vision insuranceFlexible working hours",Senior Data Engineer@ThirstySprout,https://jobs.dou.ua/companies/thirstysprout-llc/vacancies/133770/, remote,Senior Data Engineer,01 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/netguru/,Netguru,"{""Required skills"": [""Not sure if this ad fits your experience? Check out our other open positions for Data Engineers:www.netguru.com/...\u200bply/regular-data-engineerwww.netguru.com/...\u200be/apply/data-engineer-dba""], ""We offer"": [""\u2713 6+ years of experience in a Data Engineering role;\u2713 A proven track record of substantially impacting the development of complex distributed data pipelines and databases;\u2713 Experience with Java, Python or Scala with focus on distributed systems;\u2713 Experience with big data ecosystem & tools (e.g. Hadoop, Spark, Kafka, Kafka-Connect, Presto);\u2713 Relational databases like Postgres, Mysql (advanced SQL writing and optimization skills are required);\u2713 Strong experience with Cloud Platforms like AWS or GCP;\u2713 Experience with stream-processing systems (e.g. Storm, Spark-Streaming);\u2713 Knowledge of containerization technologies like Kubernetes/Dockers;\u2713 Cloud Data Warehouses like AWS Redshift or Google Bigquery;\u2713 Experience with Linux and shell scripting;\u2713 English level: B2+.""], ""Responsibilities"": [""work with an experienced team of developers and continuously develop your hard and soft skills;100% remote work"", ""we\u2019ve developed a perfect remote work culture;A mentor who will assist you during your first days;processes based on the Scrum and Agile methodologies;dev-friendly processes such as Continuous Integration, Continuous Delivery, Code Review, and bug bashes;Collaboration on challenging products (FinTech, B2B software, E-commerce, and more).""], ""Project description"": [""developing products for our clients from all over the world;assisting clients in making good decisions and choosing best solutions;good news: you won\u2019t have to manage the project as each project has a dedicated Project Manager.""]}",,"Required skills Not sure if this ad fits your experience? Check out our other open positions for Data Engineers:www.netguru.com/...‚Äãply/regular-data-engineerwww.netguru.com/...‚Äãe/apply/data-engineer-dba ‚úì 6+ years of experience in a Data Engineering role;‚úì A proven track record of substantially impacting the development of complex distributed data pipelines and databases;‚úì Experience with Java, Python or Scala with focus on distributed systems;‚úì Experience with big data ecosystem & tools (e.g. Hadoop, Spark, Kafka, Kafka-Connect, Presto);‚úì Relational databases like Postgres, Mysql (advanced SQL writing and optimization skills are required);‚úì Strong experience with Cloud Platforms like AWS or GCP;‚úì Experience with stream-processing systems (e.g. Storm, Spark-Streaming);‚úì Knowledge of containerization technologies like Kubernetes/Dockers;‚úì Cloud Data Warehouses like AWS Redshift or Google Bigquery;‚úì Experience with Linux and shell scripting;‚úì English level: B2+. We offer work with an experienced team of developers and continuously develop your hard and soft skills;100% remote work ‚Äî we‚Äôve developed a perfect remote work culture;A mentor who will assist you during your first days;processes based on the Scrum and Agile methodologies;dev-friendly processes such as Continuous Integration, Continuous Delivery, Code Review, and bug bashes;Collaboration on challenging products (FinTech, B2B software, E-commerce, and more). Responsibilities developing products for our clients from all over the world;assisting clients in making good decisions and choosing best solutions;good news: you won‚Äôt have to manage the project as each project has a dedicated Project Manager. Project description Over the past ten years, Netguru has changed the way people bank, listen to music, learn languages, and rent bicycles. Some of our clients include Fortune 500 companies and startups like Shine, Countr, Petro Niche, and more. Netguru works with the largest brands in the world, such as Volkswagen, IKEA, and Keller Williams. Our team of 650+ allows us to deliver well-designed and optimized custom mobile app development solutions for both iOS and Android mobile platforms, in turn, increasing the productivity of a business enterprise. The main reason behind custom mobile apps is that they not only help business owners transform their unique ideas into reality, but also help them deliver personalized UX.",Senior Data Engineer@Netguru,https://jobs.dou.ua/companies/netguru/vacancies/133752/, remote,Senior Data Engineer,01 October 2020,$3200‚Äì4500,2020-10-13,,dou
https://jobs.dou.ua/companies/intellias/,Intellias,"{""Required skills"": [""2+ years of overall programming experience (Java);1+ years of commercial application development experience;Profound level of Java expertise (Concurrency, Parallel, Distributed);Good knowledge and expertise using core Java;Expertise building and supporting Hadoop stack applications (HDFS, HIVE, YARN, TEZ, etc.);Experience with analytical data processing on top of Hadoop stack;Knowledge of AWS technologies, such EBS, EC2, ELB and other;Strong expertise with Linux, experience in close-to-system-level performance troubleshooting;Experience with Spark and Spark Streaming is a plus;Practical experience with CI/CD processes (Maven, Gradle, JUnit, CI/CD tools);Experience working with IDEA, GIT, Atlassian Suite (JIRA, Confluence, Bitbucket);Methodology: Agile / Scrum or Kanban.""], ""As a plus"": [""Experience with Docker tech stack would be a plus;Knowledge of Grafana, ELK stack, Artifactory is nice to have;Understanding of Telecom billing or provisioning systems architecture.""], ""We offer"": [""Higher Education: Bachelor\u2019s Degree or Master\u2019s Degree.""], ""Responsibilities"": [""Besides such basics as a competitive salary, comfortable and motivating work environment, here at Intellias we offer:""], ""Project description"": [""For your professional growth"", ""Innovative projects with advanced technologies;Individual approach to professional and career growth (Personal Development Plan);Regular educational events with leading industry experts;English and German courses.""]}",,"Required skills 2+ years of overall programming experience (Java);1+ years of commercial application development experience;Profound level of Java expertise (Concurrency, Parallel, Distributed);Good knowledge and expertise using core Java;Expertise building and supporting Hadoop stack applications (HDFS, HIVE, YARN, TEZ, etc.);Experience with analytical data processing on top of Hadoop stack;Knowledge of AWS technologies, such EBS, EC2, ELB and other;Strong expertise with Linux, experience in close-to-system-level performance troubleshooting;Experience with Spark and Spark Streaming is a plus;Practical experience with CI/CD processes (Maven, Gradle, JUnit, CI/CD tools);Experience working with IDEA, GIT, Atlassian Suite (JIRA, Confluence, Bitbucket);Methodology: Agile / Scrum or Kanban. As a plus Experience with Docker tech stack would be a plus;Knowledge of Grafana, ELK stack, Artifactory is nice to have;Understanding of Telecom billing or provisioning systems architecture. Higher Education: Bachelor‚Äôs Degree or Master‚Äôs Degree. We offer Besides such basics as a competitive salary, comfortable and motivating work environment, here at Intellias we offer: For your professional growth ‚ÄîInnovative projects with advanced technologies;Individual approach to professional and career growth (Personal Development Plan);Regular educational events with leading industry experts;English and German courses. For your comfort ‚ÄîFlexible working hours;Spacious office with lots of meeting rooms;Relocation program;Kids‚Äô room with professional baby-sitter (offices in Lviv & Kyiv). For your health ‚Äî3 health packages to choose from ‚Äî medical insurance, sports attendance or mix of both;Annual vitaminization program;Annual vaccination and ophthalmologist check-up. For your leisure ‚ÄîCorporate celebrations and fun activities;On-site massages;Beauty parlor (offices in Lviv & Kyiv). Responsibilities Operational support and incidents fixing of CRM applications;Working as a part of one or more agile teams: analysis and implementation of features, configuration and fixes of existing functionality, root cause analysis for incidents. Project description The largest Ukrainian telecommunication operator providing communications and data services based on a broad range of mobile and fixed-line technologies, including 4G. Company‚Äôs customer base amounts to over 26 million in mobile and more than 1 mln in broadband internet. Kyivstar is a part of VEON, one of the world‚Äôs largest integrated telecommunications companies, headquartered in Netherlands. The holding company owns telecom assets in the CIS countries, Europe, Asia, and Africa, and its shares are freely traded on the stock exchange (NASDAQ). Kyivstar has achieved big success thanks to investment into development of its own fiber-optic network which is an important link in the traffic exchange between Europe and Asia, and it is a reliable partner of Ukrainians in the connection with the other countries of the world. The company provides roaming services in 195 countries on 5 continents.",Middle Big Data Support engineer@Intellias,https://jobs.dou.ua/companies/intellias/vacancies/133720/," Kyiv, Lviv",Middle Big Data Support engineer,01 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/data-science-ua/,Data Science UA,"{""Required skills"": [""What you need to be successful:\u25cf Required Technical Experience: JavaScript, Cypress, Docker and Kubernetes\u25cf Load testing experience\u25cf Experience working within a multi-discipline, agile environment\u25cf Good analytical skills\u25cf Positive attitude\u25cf Strong communication skills""], ""As a plus"": [""As a plus:\u25cf Experience with Jenkins\u25cf Would be a huge plus to provide a Github account or any links on projects on anyprogramming language.""], ""We offer"": [""\u25cf Flexible working \u25cf Sharing culture\u25cf Diversity\u25cf Days off""], ""Responsibilities"": [""What you will be doing:\u25cf Develop and integrate end to end tests\u25cf Support our existing test suite\u25cf Support the deployment process\u25cf Responding to and working with testing requirements\u25cf Maintain the test documentation""], ""Project description"": [""We are looking for QA Engineer for our partner that will work as a part of the team responsible for writing andmaintaining existing tests for our products. Company works in small cross-functional pods of developers who focus on a particular product areas and encourages people to rotate across teams. More detail around each of the team\u2019s responsibilities can be found here.""]}",,"Required skills What you need to be successful:‚óè Required Technical Experience: JavaScript, Cypress, Docker and Kubernetes‚óè Load testing experience‚óè Experience working within a multi-discipline, agile environment‚óè Good analytical skills‚óè Positive attitude‚óè Strong communication skills As a plus As a plus:‚óè Experience with Jenkins‚óè Would be a huge plus to provide a Github account or any links on projects on anyprogramming language. We offer ‚óè Flexible working ‚óè Sharing culture‚óè Diversity‚óè Days off Responsibilities What you will be doing:‚óè Develop and integrate end to end tests‚óè Support our existing test suite‚óè Support the deployment process‚óè Responding to and working with testing requirements‚óè Maintain the test documentation Project description We are looking for QA Engineer for our partner that will work as a part of the team responsible for writing andmaintaining existing tests for our products. Company works in small cross-functional pods of developers who focus on a particular product areas and encourages people to rotate across teams. More detail around each of the team‚Äôs responsibilities can be found here.",Automation QA¬†Engineer@Data Science UA,https://jobs.dou.ua/companies/data-science-ua/vacancies/133710/, Kyiv,Automation QA¬†Engineer,01 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/n-ix/,N-iX,{},,"N-iX is looking for a passionate and motivated Data Engineer to join our team. Our customer is the multicloud solutions expert. They combine their expertise with the world‚Äôs leading technologies- across applications, data and security- to deliver end-to-end multicloud solutions. As a global, multicloud technology services pioneer, they deliver the innovative capabilities of the cloud to help customers build new revenue streams, increase efficiency and create incredible experiences. Responsibilities:‚Ä¢ Building Data Lake using Google Cloud Platform‚Ä¢ Develop end-to-end data solutions for structured and unstructured data including, but not limited to, ingestion, parsing, integration, auditing, logging, aggregation, and error handling Requirements:‚Ä¢ Strong experience in building cloud native data engineering solutions using GCP or AWS platforms‚Ä¢ 4+ years of development experience with Python at an advanced level including concurrent and metaprogramming‚Ä¢ Prior experience of building data ingestion pipelines of telemetry data in GCP/AWS including app monitoring, performance monitoring and network monitoring logs‚Ä¢ Deep background in building data integration applications using Spark or mapReduce frameworks‚Ä¢ Track record of producing software artefacts of exceptional quality by adhering to coding standards, design patterns and best practices‚Ä¢ Well versed in SQL / NoSQL technologies and deep familiarity with dimensional modeling and Data warehousing concepts‚Ä¢ Expertise in workflow scheduling and orchestration frameworks such as Airflow or Oozie‚Ä¢ Prefer someone who has worked with GCP products such as BigQuery, Cloud Composer, Data Fusion, GCS and GKE or corresponding technologies on AWS platform‚Ä¢ High proficiency in working with Git, automated build and CI/CD pipelines‚Ä¢ Upper-Intermediate level of English We offer:‚Ä¢ Flexible working hours‚Ä¢ A competitive salary and good compensation package‚Ä¢ Possibility of partial remote work‚Ä¢ Best hardware‚Ä¢ A masseur and a corporate doctor‚Ä¢ Healthcare & sport benefits‚Ä¢ An inspiring and comfy office Professional growth:‚Ä¢ Challenging tasks and innovative projects‚Ä¢ Meetups and events for professional development‚Ä¢ An individual development plan‚Ä¢ Mentorship program Fun:‚Ä¢ Corporate events and outstanding parties‚Ä¢ Exciting team buildings‚Ä¢ Memorable anniversary presents‚Ä¢ A fun zone where you can play video games, foosball, ping pong, and more",GCP Data Engineer@N-iX,https://jobs.dou.ua/companies/n-ix/vacancies/133684/," Kyiv, Lviv, remote",GCP Data Engineer,01 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/lifecell/,lifecell,"{""Required skills"": [""3+ years of experience as a Data Engineer or Database Developer."", ""Experience with RDBMS\u2019s like Oracle or MySQL, experience in SQL and PL/SQL development and performance tuning."", ""Knowledge of Data warehouse concepts and experience in ETL development."", ""Proficiency in one of Python, Java or Scala."", ""Experience with Big Data technologies such as Hadoop, Spark, Hive, Kafka, HBase, Sqoop and other tools."", ""Experience in implementation of Machine Learning pipelines using Spark ML or Scikit-learn would be advantage."", ""Experience in Telecom domain would be advantage."", ""Good analytical and problem solving skills.""], ""Responsibilities"": [""1. Work with analysts and business stakeholders to clarify their requirements.2. Develop ETL processes, implementing new or extending existing data marts.3. Design and develop data processing pipelines in Hadoop data platform, for both batch and streaming modes.4. Participate in implementation and integration of new data sources.5. Develop API for integration of external systems with data platforms.6. Perform root-cause analysis of data pipelines issues, bug fixing and performance tuning.7. Drive data quality and manage SLA\u2019s for the data platforms.""], ""Project description"": [""At this position you will work with data about customers activity in one of the largest telecom operators on the market. You will develop ETL processes and data pipelines in data platforms: Data Warehouse, BI and reporting systems and Hadoop Data Platform. You will participate in development and integration of data driven products and services for internal and B2B clients.""]}",,"Required skills ‚Äî 3+ years of experience as a Data Engineer or Database Developer.‚Äî Experience with RDBMS‚Äôs like Oracle or MySQL, experience in SQL and PL/SQL development and performance tuning.‚Äî Knowledge of Data warehouse concepts and experience in ETL development. ‚Äî Proficiency in one of Python, Java or Scala.‚Äî Experience with Big Data technologies such as Hadoop, Spark, Hive, Kafka, HBase, Sqoop and other tools.‚Äî Experience in implementation of Machine Learning pipelines using Spark ML or Scikit-learn would be advantage.‚Äî Experience in Telecom domain would be advantage.‚Äî Good analytical and problem solving skills. Responsibilities 1. Work with analysts and business stakeholders to clarify their requirements.2. Develop ETL processes, implementing new or extending existing data marts.3. Design and develop data processing pipelines in Hadoop data platform, for both batch and streaming modes.4. Participate in implementation and integration of new data sources.5. Develop API for integration of external systems with data platforms.6. Perform root-cause analysis of data pipelines issues, bug fixing and performance tuning.7. Drive data quality and manage SLA‚Äôs for the data platforms. Project description At this position you will work with data about customers activity in one of the largest telecom operators on the market. You will develop ETL processes and data pipelines in data platforms: Data Warehouse, BI and reporting systems and Hadoop Data Platform. You will participate in development and integration of data driven products and services for internal and B2B clients.",Big Data Engineer@lifecell,https://jobs.dou.ua/companies/lifecell/vacancies/67462/," Kyiv, remote",Big Data Engineer,01 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/sd-solutions/,SD Solutions,"{""Required skills"": [""On behalf of Undertone, SD Solutions is looking for a Data Engineer in a support role to provide rapid resolutions for data service disruptions in our state-of-the-art Ad Platform. In order to be successful, this individual must be proactive and remain informed on data technologies and trends in various operating systems and languages. Engage in and improve the whole lifecycle of services"", ""from inception and design, through deployment, operation, and refinement. The Engineer here will be working closely with other business units to gather requirements, communicate status, and proposal of the undertone Software products.""], ""As a plus"": [""Bachelor\u2019s degree and 3+ years of experience OR MS degree and 2+ years of experience in software engineering (Degree in Computer Science or related field preferred)A minimum of 2 years of development experience with PythonA minimum of 2 years of experience in developing/supporting DW/BI Applications.A minimum of 2 years\u2019 experience in supporting/troubleshooting failed Pentaho Data Integration/Kettle ETL processes.Strong in writing complex SQL queries and performance tuning techniques.Experience in working on AWS S3, AWS EMR, AWS Redshift, AWS Athena.Basic scripting skills in Bash/Perl.""], ""We offer"": [""Database administration and development skills, especially MySQL and Postgres databases.Experience in Hadoop, Flume, Kafka, Hive, Spark.Experience in developing Tableau Dashboards.Familiarity with monitoring systems like Datadog, Pingdom to catch/prevent issues.Ability to effectively prioritize workload and work well under pressure in a fast-pacedenvironment.Knowledge and passion for media, data and technology.Quick learner, creative and ability to retain / source relevant information for references.Great work ethic, proactive, self-motivated with excellent communication skills.Ability to work independently and as part of a team, including remote teams.""], ""Responsibilities"": [""Be part of a young growing company;Build great products with emerging technologies;Work with sharp and success-oriented team;Gain experience in one of the hottest technological markets;Competitive compensation;Comfortable office with modern infrastructure;Flexible working schedule;21 calendar days of paid vacation, paid sick leave.""], ""Project description"": [""Support reporting services and database management systems before they go live throughout the development lifecycle activities such as system design, developing the data platforms and frameworks, capacity planning, and launch reviews.Spend 50% in applying software and data engineering background in resolving reporting and data platform disruptions in Undertone\u2019s Ad Platform. This will involve investigation on data and integration of various services used in the data management platform"", ""Amazon Web Services such as Redshift, Elastic Map Reduce, Data pipelines, etc.Provide a quick temporary solution so that there is minimum downtime for the business and then work on escalation for providing a permanent fix. Need to prioritize the reporting and data refresh issues to ensure the business users are equipped with near real-time data.Use 10% of your time to participate in System design review meetings to provide your inputs based on data engineering experience.Another 10% of time will be spent on validating the data quality and integrity of the newly developed projects.At the end of every Sprint cycle, participate in post-production activities such as code reviews and preparation of documentation of runbooks for ETL /data pipeline.Spend 30% of the time learning new data technologies and implementing data solutions for providing real time insights that will enable the business users to take right decisions.Work on 3 rd party integration projects by implementing Third Party API\u2019s.Apply your creative mind in addressing urgent business requirements by providing quick data solutions with minimal turnaround time to avoid any disruption to the business.Collaborate with System Administrators and Database Administrators to fix software infrastructure problems.""]}",,"Required skills On behalf of Undertone, SD Solutions is looking for a Data Engineer in a support role to provide rapid resolutions for data service disruptions in our state-of-the-art Ad Platform. In order to be successful, this individual must be proactive and remain informed on data technologies and trends in various operating systems and languages. Engage in and improve the whole lifecycle of services‚Äîfrom inception and design, through deployment, operation, and refinement. The Engineer here will be working closely with other business units to gather requirements, communicate status, and proposal of the undertone Software products. Bachelor‚Äôs degree and 3+ years of experience OR MS degree and 2+ years of experience in software engineering (Degree in Computer Science or related field preferred)A minimum of 2 years of development experience with PythonA minimum of 2 years of experience in developing/supporting DW/BI Applications.A minimum of 2 years‚Äô experience in supporting/troubleshooting failed Pentaho Data Integration/Kettle ETL processes.Strong in writing complex SQL queries and performance tuning techniques.Experience in working on AWS S3, AWS EMR, AWS Redshift, AWS Athena.Basic scripting skills in Bash/Perl. As a plus Database administration and development skills, especially MySQL and Postgres databases.Experience in Hadoop, Flume, Kafka, Hive, Spark.Experience in developing Tableau Dashboards.Familiarity with monitoring systems like Datadog, Pingdom to catch/prevent issues.Ability to effectively prioritize workload and work well under pressure in a fast-pacedenvironment.Knowledge and passion for media, data and technology.Quick learner, creative and ability to retain / source relevant information for references.Great work ethic, proactive, self-motivated with excellent communication skills.Ability to work independently and as part of a team, including remote teams. We offer Be part of a young growing company;Build great products with emerging technologies;Work with sharp and success-oriented team;Gain experience in one of the hottest technological markets;Competitive compensation;Comfortable office with modern infrastructure;Flexible working schedule;21 calendar days of paid vacation, paid sick leave. Responsibilities Support reporting services and database management systems before they go live throughout the development lifecycle activities such as system design, developing the data platforms and frameworks, capacity planning, and launch reviews.Spend 50% in applying software and data engineering background in resolving reporting and data platform disruptions in Undertone‚Äôs Ad Platform. This will involve investigation on data and integration of various services used in the data management platform ‚Äî Amazon Web Services such as Redshift, Elastic Map Reduce, Data pipelines, etc.Provide a quick temporary solution so that there is minimum downtime for the business and then work on escalation for providing a permanent fix. Need to prioritize the reporting and data refresh issues to ensure the business users are equipped with near real-time data.Use 10% of your time to participate in System design review meetings to provide your inputs based on data engineering experience.Another 10% of time will be spent on validating the data quality and integrity of the newly developed projects.At the end of every Sprint cycle, participate in post-production activities such as code reviews and preparation of documentation of runbooks for ETL /data pipeline.Spend 30% of the time learning new data technologies and implementing data solutions for providing real time insights that will enable the business users to take right decisions.Work on 3 rd party integration projects by implementing Third Party API‚Äôs.Apply your creative mind in addressing urgent business requirements by providing quick data solutions with minimal turnaround time to avoid any disruption to the business.Collaborate with System Administrators and Database Administrators to fix software infrastructure problems. Project description Undertone is at the intersection of media, creative, and technology. The company has formed deep partnerships with the world‚Äôs best digital media properties, developed a suite of groundbreaking multi-screen creative formats for brands to leverage, and built technology platforms that underpin every aspect of campaign planning, delivery, optimization, and measurement. www.undertone.com",Data Engineer (Undertone)@SD Solutions,https://jobs.dou.ua/companies/sd-solutions/vacancies/130038/," Kyiv, remote",Data Engineer (Undertone),01 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/snap/,Snap Inc.,{},,"Snap Inc. is a camera company. We believe that reinventing the camera represents our greatest opportunity to improve the way people live and communicate. Our products empower people to express themselves, live in the moment, learn about the world, and have fun together.Snapchat is the camera used by millions of people every day to Snap with family, watch Stories from friends, see events from around the world, and explore expertly curated content from top publishers. In short, we are a passionate team working hard to build the best platform in the world for communication and storytelling. We‚Äôre looking for a Software Engineer, Machine Learning to join Snap Inc! Working from Kyiv office, you will collaborate with researchers, engineers, and designers and develop machine learning frameworks to create exciting products and breakthrough interactive experiences for millions of Snapchatters around the world. What you‚Äôll do:‚Äî Develop and deploy production-quality machine learning frameworks for mobile devices‚Äî Explore and implement challenging state-of-the-art algorithms to move the needle in the machine learning area‚Äî Evaluate the technical tradeoffs of every decision‚Äî Perform code reviews and ensure exceptional code quality‚Äî Work closely with other Snap teams to explore and prototype new product features‚Äî Iterate quickly without compromising quality Minimum qualifications:‚Äî 3+ years of software engineering experience‚Äî Bachelor‚Äôs degree in a technical field such as computer science or equivalent experience Preferred qualifications:‚Äî Experience working with machine learning frameworks such as PyTorch, TensorFlow, Caffe2 or related frameworks‚Äî Experience developing and optimizing real-time software for mobile applications‚Äî M.S. degree in computer science or related field‚Äî Strong understanding of machine learning approaches and algorithms‚Äî Expertise in any of the following fields: segmentation, object detection, classification, deep learning model optimizations (e.g. pruning, quantization, distillation)‚Äî Ability to proactively learn new concepts and apply them at work","Machine Learning, CV@Snap Inc.",https://jobs.dou.ua/companies/snap/vacancies/127214/, Kyiv,"Machine Learning, CV",01 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/a-listware/,A-listware,"{""Required skills"": [""3+ years of professional data science experience;Good knowledge in Deep Learning and with Tensorflow;Strong analytical mindset, curiosity to dig into data and find insights;Vast experience with deep learning techniques, data engineering and end-2-end AI projects;Strong problem-solving skills with an emphasis on research and it\u2019s translation into production;Master\u2019s degree or better in computer sciences, Analytics, Systems Eng., Statistics or related field;English"", ""Intermediate+""], ""As a plus"": [""Good knowledge of Python, PySpark.""], ""We offer"": [""5-day working week, 8-hour working day, flexible schedule;All UA public holidays are days off;Vacation and sick leave are covered by the company;Health care program (activated after the probation period);Office in the city center;English classes.""], ""Project description"": [""The customer company is a new startup in the privacy domain, backed up by industry leaders. This solution reduces both time and cost by offering high quality synthetic data for testing, development and AI modeling, while meeting privacy compliance requirements.""]}",,"Required skills 3+ years of professional data science experience;Good knowledge in Deep Learning and with Tensorflow;Strong analytical mindset, curiosity to dig into data and find insights;Vast experience with deep learning techniques, data engineering and end-2-end AI projects;Strong problem-solving skills with an emphasis on research and it‚Äôs translation into production;Master‚Äôs degree or better in computer sciences, Analytics, Systems Eng., Statistics or related field;English ‚Äî Intermediate+ As a plus Good knowledge of Python, PySpark. We offer 5-day working week, 8-hour working day, flexible schedule;All UA public holidays are days off;Vacation and sick leave are covered by the company;Health care program (activated after the probation period);Office in the city center;English classes. Project description The customer company is a new startup in the privacy domain, backed up by industry leaders. This solution reduces both time and cost by offering high quality synthetic data for testing, development and AI modeling, while meeting privacy compliance requirements.",Middle+/Senior Data Scientist@A-listware,https://jobs.dou.ua/companies/a-listware/vacancies/133616/," Kharkiv, remote",Middle+/Senior Data Scientist,01 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/yellow-stone/,Yellow Stone,{},,"–ö–æ–º–ø–∞–Ω–∏—è Yellow Stone –∏—â–µ—Ç –æ–ø—ã—Ç–Ω–æ–≥–æ Data Scientist! Yellow Stone ‚Äî –ø—Ä–æ–¥—É–∫—Ç–æ–≤–∞—è IT-–∫–æ–º–ø–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞–∑—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è –∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è –ø—Ä–æ–¥–≤–∏–∂–µ–Ω–∏—è –∏ –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –±–∏–∑–Ω–µ—Å–∞ –≤ –∏–≥—Ä–æ–≤–æ–π –∏–Ω–¥—É—Å—Ç—Ä–∏–∏. –ü—Ä–æ–¥—É–∫—Ç –∫–æ–º–ø–∞–Ω–∏–∏ ‚Äî —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–ª—è –±–∏–∑–Ω–µ—Å-—Ä–µ—à–µ–Ω–∏–π, –ø–æ—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è –ø–æ –º–æ–¥—É–ª—å–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–æ–∑–¥–∞–≤–∞—Ç—å –Ω–æ–≤—ã–µ –∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ B2B –ø—Ä–æ–¥—É–∫—Ç—ã –Ω–∞—à–∏—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ –∑–∞—Ç—Ä–∞—Ç–∞–º–∏ –≤—Ä–µ–º–µ–Ω–∏ –∏ —Ä–µ—Å—É—Ä—Å–æ–≤. –ß—Ç–æ –Ω–∞–º –≤–∞–∂–Ω–æ:–æ–ø—ã—Ç —Ä–∞–±–æ—Ç—ã –≤ —Ä–æ–ª–∏ Data scientist –æ—Ç 2—Ö –ª–µ—Ç;–º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –≤—ã—Å—à–µ–µ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ (—Å–∏—Å—Ç–µ–º–Ω—ã–π –∞–Ω–∞–ª–∏–∑, —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∞—è –∫–∏–±–µ—Ä–Ω–µ—Ç–∏–∫–∞, —Ñ–∏–Ω–∞–Ω—Å—ã, –º–µ—Ö–∞–Ω–∏–∫–æ-–º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π);–∑–Ω–∞–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è;–æ–ø—ã—Ç –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è –ø–æ–ª–Ω–æ–≥–æ —Ü–∏–∫–ª–∞ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –º–æ–¥–µ–ª–∏: –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö, –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∑–Ω–∞—á–∏–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤, –æ—Ü–µ–Ω–∫–∞ —Ç–æ—á–Ω–æ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞, –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –Ω–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ –¥–ª—è –±–∏–∑–Ω–µ—Å-—Ü–µ–ª–µ–π.–æ–ø—ã—Ç –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ R –∏–ª–∏ Python. –ë—É–¥—É—â–∏–µ –∑–∞–¥–∞—á–∏:–ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è, –∫–ª—é—á–µ–≤—ã—Ö —Å–æ–±—ã—Ç–∏–π –≤ –∂–∏–∑–Ω–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –Ω–∞ —Å–∞–π—Ç–µ;—Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–π –±–∞–∑—ã;–ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π;–æ—Ü–µ–Ω–∫–∞ —Ç–æ—á–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –∏ –∞–Ω–∞–ª–∏–∑ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –≤–Ω–µ–¥—Ä–µ–Ω–∏—è. –ß—Ç–æ –∫–æ–º–ø–∞–Ω–∏—è –≥–æ—Ç–æ–≤–∞ —Ç–µ–±–µ –ø—Ä–µ–¥–ª–æ–∂–∏—Ç—å:—Å–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏ –∏ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–∞ —Ä–æ—Å—Ç–∞;–º–æ–ª–æ–¥–æ–π –∏ –¥—Ä—É–∂–Ω—ã–π –∫–æ–ª–ª–µ–∫—Ç–∏–≤, –∫–æ–º–∞–Ω–¥—É –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª–æ–≤;–æ–ø–ª–∞—á–∏–≤–∞–µ–º—ã–π –æ—Ç–ø—É—Å–∫, –æ–ø–ª–∞—á–∏–≤–∞–µ–º—ã–π –æ—Ç–ø—É—Å–∫ –ø–æ –±–æ–ª–µ–∑–Ω–∏;–ø—Ä–∏—è—Ç–Ω—ã–π —Å–æ—Ü.–ø–∞–∫–µ—Ç (–∫–æ–º–ø–µ–Ω—Å–∞—Ü–∏—è 50% –∑–∞—Ç—Ä–∞—Ç –Ω–∞ –∫—É—Ä—Å—ã –∏–Ω–æ—Å—Ç—Ä–∞–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞, —Å–ø–æ—Ä—Ç–∞ –∏ –º–µ–¥. —Å—Ç—Ä–∞—Ö–æ–≤–∫–∏);–æ–±—É—á–µ–Ω–∏–µ, —É—á–∞—Å—Ç–∏–µ –≤ –∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏—è—Ö;–∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–∞—è –∑–∞—Ä–∞–±–æ—Ç–Ω–∞—è –ø–ª–∞—Ç–∞ —Å –ø—Ä–∏–≤—è–∑–∫–æ–π –∫ –∫—É—Ä—Å—É —É.–µ. (–æ–≥–æ–≤–∞—Ä–∏–≤–∞–µ—Ç—Å—è –ø–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º —Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏—è –∏ –æ—Ü–µ–Ω–∫–∏ —É—Ä–æ–≤–Ω—è –∫–∞–Ω–¥–∏–¥–∞—Ç–∞).–∫–æ–º—Ñ–æ—Ä—Ç–∞–±–µ–ª—å–Ω—ã–π –æ—Ñ–∏—Å –≤ —Ü–µ–Ω—Ç—Ä–µ –≥–æ—Ä–æ–¥–∞ (–ø–æ —É–ª. –ë.–•–º–µ–ª—å–Ω–∏—Ü–∫–æ–≥–æ).",Data Scientist@Yellow Stone,https://jobs.dou.ua/companies/yellow-stone/vacancies/127888/, Kyiv,Data Scientist,01 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/unitask/,Unitask Group Ukraine,"{""Required skills"": [""Education and Professional experience Requirements""], ""As a plus"": [""* Bachelor degree software development, web development, or any other related field* Experience in data migration development in large scale projects"", ""Preferred Familiar with SQL Queries* Dealing with stakeholders over the world, identifying the business needs and bringing them to life* Knowledge of business process, data management, data flows, data quality standards, processes and proficiency with data analysis and data quality tools preferred * Work with technical skills: Oracle, PL/SQL; SAP S/4HANA; VBA* English"", ""Upper-Intermediate""], ""We offer"": [""Skills and Capabilities""], ""Responsibilities"": [""* Team player* Leadership* Scrum experience* Analytical mindset* Proactive & self-driven* Readiness to work with US time zones (for ex until 10 PM UKR time)""], ""Project description"": [""* 5+ years of experience in data migration projects.* Ability to integrate directly with the customer and understand the requirements.* SAP experience"", ""preferred.""]}",,"Required skills Education and Professional experience Requirements * Bachelor degree software development, web development, or any other related field* Experience in data migration development in large scale projects ‚Äî Preferred Familiar with SQL Queries* Dealing with stakeholders over the world, identifying the business needs and bringing them to life* Knowledge of business process, data management, data flows, data quality standards, processes and proficiency with data analysis and data quality tools preferred * Work with technical skills: Oracle, PL/SQL; SAP S/4HANA; VBA* English ‚Äî Upper-Intermediate Skills and Capabilities * Team player* Leadership* Scrum experience* Analytical mindset* Proactive & self-driven* Readiness to work with US time zones (for ex until 10 PM UKR time) As a plus * 5+ years of experience in data migration projects.* Ability to integrate directly with the customer and understand the requirements.* SAP experience ‚Äî preferred. We offer * Career and professional development of your personal strength in the international group* Fun & creative work environment in a fast-growing company * Opportunity to choose days of remote work * Comfortable office in Kyiv at Obolonska Naberezhnaya St., 20 (Golf Center)* Paid Time Off (Vacation, Sick & Public Holidays)* English classes * Democratic banking policy ‚Äî you can cooperate with us through any legal Ukrainian bank you choose* We speak both English and Russian ‚Äî we‚Äôll help you improve your English on the job Responsibilities * We are looking for a person who can deal with our internal customers, who can work independently, and is a self-starter.* Contribute collecting requirements for Data Integrity and Post Load programs from Stakeholders (Business and Functional teams)* Develop Data Integrity and Post Load programs* Deliver in a timely manner* Run Internal Tests* Collaborate with stakeholders to get program approval Project description Unitask Group provides solutions for the management of information, knowledge and related processes, enhancing organizational infrastructure and control to bring a competitive edge, a boost in growth and higher profit. We are proud of our finest IT talents who drives our clients to success! Since its establishment in 1994, Unitask has maintained a high level of professionalism, quality solutions and excellent service and is considered a world leader in Oracle business solutions. We invite you to join the team of professionals on our exciting journey into the excellence, be a part of our vision and share in our success. We are looking for creative and skilled individuals like you!",Business Analyst (Data migration)@Unitask Group Ukraine,https://jobs.dou.ua/companies/unitask/vacancies/120983/," Kyiv, remote",Business Analyst (Data migration),01 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/ibox/,IBox,"{""Required skills"": [""\u0417\u043d\u0430\u043d\u0438\u0435 \u0442\u0435\u043e\u0440\u0438\u0438 \u0440\u0435\u043b\u044f\u0446\u0438\u043e\u043d\u043d\u044b\u0445 \u0431\u0430\u0437 \u0434\u0430\u043d\u043d\u044b\u0445 \u0438 \u043a\u043e\u043d\u0446\u0435\u043f\u0446\u0438\u0438 \u0445\u0440\u0430\u043d\u0438\u043b\u0438\u0449\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 (DWH);"", ""\u0417\u043d\u0430\u043d\u0438\u0435 Oracle SQL \u0438 PL/SQL (\u0437\u0430\u043f\u0440\u043e\u0441\u044b, \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u044f, \u0444\u0443\u043d\u043a\u0446\u0438\u0438, \u0438 \u043f\u0440\u043e\u0446\u0435\u0434\u0443\u0440\u044b);"", ""\u041e\u043f\u044b\u0442 \u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u043d\u0430 Oracle DB \u043d\u0430 \u043f\u043e\u0437\u0438\u0446\u0438\u044f\u0445 DB Developer, DWH Developer, DB Engineer, SQL/Oracle Developer, DBA, Data/SQL Analyst, \u0438\u043b\u0438 \u0430\u043d\u0430\u043b\u043e\u0433\u0438\u0447\u043d\u044b\u0445 (\u0433\u043e\u0442\u043e\u0432\u044b \u0440\u0430\u0441\u0441\u043c\u0430\u0442\u0440\u0438\u0432\u0430\u0442\u044c \u043a\u0430\u043d\u0434\u0438\u0434\u0430\u0442\u043e\u0432 \u0441 \u044d\u043a\u0432\u0438\u0432\u0430\u043b\u0435\u043d\u0442\u043d\u044b\u043c \u043e\u043f\u044b\u0442\u043e\u043c \u043d\u0430 \u0434\u0440\u0443\u0433\u0438\u0445 \u0421\u0423\u0411\u0414);"", ""\u041e\u043f\u044b\u0442 \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u0438 \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438 \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u0432.""], ""As a plus"": [""\u041e\u043f\u044b\u0442 \u0432\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438/\u0440\u0430\u0431\u043e\u0442\u044b \u0441 \u043a\u0430\u043a\u043e\u0439-\u043b\u0438\u0431\u043e BI-\u0441\u0438\u0441\u0442\u0435\u043c\u043e\u0439 (Oracle BI , Power BI, Tableau, Qlik Sense, DOMO, \u0438\u043b\u0438 \u0434\u0440\u0443\u0433\u0438\u0445);"", ""\u0417\u043d\u0430\u043d\u0438\u0435 R \u0438\u043b\u0438 Python.""], ""We offer"": [""\u041c\u043e\u043b\u043e\u0434\u043e\u0439 \u0438 \u0434\u0440\u0443\u0436\u043d\u044b\u0439 \u043a\u043e\u043b\u043b\u0435\u043a\u0442\u0438\u0432, \u0430\u0442\u043c\u043e\u0441\u0444\u0435\u0440\u0443 \u043f\u043e\u0434\u0434\u0435\u0440\u0436\u043a\u0438 \u0438 \u0443\u0432\u0430\u0436\u0435\u043d\u0438\u044f;"", ""\u0413\u0438\u0431\u043a\u0438\u0439 \u0433\u0440\u0430\u0444\u0438\u043a \u0440\u0430\u0431\u043e\u0442\u044b \u043f\u043d-\u043f\u0442, \u043d\u0430\u0447\u0430\u043b\u043e \u0440\u0430\u0431\u043e\u0442\u044b \u0441 8:00 \u0434\u043e 10:00, \u043e\u043a\u043e\u043d\u0447\u0430\u043d\u0438\u0435 \u0441 17:00 \u0434\u043e 19:00"", ""\u0420\u0430\u0431\u043e\u0442\u0430 \u0432 \u0441\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u043c \u0438 \u043a\u043e\u043c\u0444\u043e\u0440\u0442\u043d\u043e\u043c \u043e\u0444\u0438\u0441\u0435 10 \u043c\u0438\u043d \u0445\u043e\u0434\u044c\u0431\u044b \u043e\u0442 \u043c.\u041b\u0443\u043a\u044c\u044f\u043d\u043e\u0432\u0441\u043a\u0430\u044f"", ""\u0414\u0440\u0443\u0433\u0438\u0435 \u043f\u043b\u044e\u0448\u043a\u0438 \u0432 \u043e\u0444\u0438\u0441\u0435 (\u0447\u0430\u0439 / \u043a\u043e\u0444\u0435, \u0442\u0435\u043d\u043d\u0438\u0441, PlayStation, \u0439\u043e\u0433\u0430, \u0430\u043d\u0433\u043b\u0438\u0439\u0441\u043a\u0438\u0439 \u044f\u0437\u044b\u043a, \u043a\u043e\u0440\u043f\u043e\u0440\u0430\u0442\u0438\u0432\u043d\u044b\u0435 \u043c\u0435\u0440\u043e\u043f\u0440\u0438\u044f\u0442\u0438\u044f \u0438 \u0442.\u0434.)"", ""\u0423\u0440\u043e\u0432\u0435\u043d\u044c \u0437\u0430\u0440\u0430\u0431\u043e\u0442\u043d\u043e\u0439 \u043f\u043b\u0430\u0442\u044b \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0435\u0442\u0441\u044f \u043f\u043e \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0430\u043c \u0441\u043e\u0431\u0435\u0441\u0435\u0434\u043e\u0432\u0430\u043d\u0438\u044f.""], ""Responsibilities"": [""\u0421\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u0434\u0430\u043d\u043d\u044b\u0445 \u0432 \u0445\u0440\u0430\u043d\u0438\u043b\u0438\u0449\u0435 (DWH) \u043d\u0430 Oracle Database;"", ""\u041f\u043e\u0434\u0434\u0435\u0440\u0436\u043a\u0430 \u0438 \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u044f \u0441\u0443\u0449\u0435\u0441\u0442\u0432\u0443\u044e\u0449\u0435\u0439 \u0431\u0430\u0437\u044b SQL \u0438 PL/SQL \u043a\u043e\u0434\u0430;"", ""\u0421\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u0438 \u0440\u0435\u0434\u0430\u043a\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u0434\u0430\u0448\u0431\u043e\u0440\u0434\u043e\u0432 \u0432 BI \u0441\u0438\u0441\u0442\u0435\u043c\u0435 (Oracle BI);"", ""\u0412\u044b\u0433\u0440\u0443\u0437\u043a\u0438 \u0438 \u0430\u043d\u0430\u043b\u0438\u0437 \u0434\u0430\u043d\u043d\u044b\u0445 \u043f\u043e \u0437\u0430\u043f\u0440\u043e\u0441\u0430\u043c \u0434\u0440\u0443\u0433\u0438\u0445 \u043e\u0442\u0434\u0435\u043b\u043e\u0432 (\u0442\u0438\u043a\u0435\u0442\u0430\u043c \u0432 JIRA).""]}",,"Required skills ‚Ä¢ –ó–Ω–∞–Ω–∏–µ —Ç–µ–æ—Ä–∏–∏ —Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—ã—Ö –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö –∏ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ –¥–∞–Ω–Ω—ã—Ö (DWH);‚Ä¢ –ó–Ω–∞–Ω–∏–µ Oracle SQL –∏ PL/SQL (–∑–∞–ø—Ä–æ—Å—ã, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è, —Ñ—É–Ω–∫—Ü–∏–∏, –∏ –ø—Ä–æ—Ü–µ–¥—É—Ä—ã);‚Ä¢ –û–ø—ã—Ç —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –Ω–∞ Oracle DB –Ω–∞ –ø–æ–∑–∏—Ü–∏—è—Ö DB Developer, DWH Developer, DB Engineer, SQL/Oracle Developer, DBA, Data/SQL Analyst, –∏–ª–∏ –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã—Ö (–≥–æ—Ç–æ–≤—ã —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—Ç—å –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ —Å —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω—ã–º –æ–ø—ã—Ç–æ–º –Ω–∞ –¥—Ä—É–≥–∏—Ö –°–£–ë–î);‚Ä¢ –û–ø—ã—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∑–∞–ø—Ä–æ—Å–æ–≤. As a plus ‚Ä¢ –û–ø—ã—Ç –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏/—Ä–∞–±–æ—Ç—ã —Å –∫–∞–∫–æ–π-–ª–∏–±–æ BI-—Å–∏—Å—Ç–µ–º–æ–π (Oracle BI , Power BI, Tableau, Qlik Sense, DOMO, –∏–ª–∏ –¥—Ä—É–≥–∏—Ö);‚Ä¢ –ó–Ω–∞–Ω–∏–µ R –∏–ª–∏ Python. We offer ‚Ä¢ –ú–æ–ª–æ–¥–æ–π –∏ –¥—Ä—É–∂–Ω—ã–π –∫–æ–ª–ª–µ–∫—Ç–∏–≤, –∞—Ç–º–æ—Å—Ñ–µ—Ä—É –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –∏ —É–≤–∞–∂–µ–Ω–∏—è;‚Ä¢ –ì–∏–±–∫–∏–π –≥—Ä–∞—Ñ–∏–∫ —Ä–∞–±–æ—Ç—ã –ø–Ω-–ø—Ç, –Ω–∞—á–∞–ª–æ —Ä–∞–±–æ—Ç—ã —Å 8:00 –¥–æ 10:00, –æ–∫–æ–Ω—á–∞–Ω–∏–µ —Å 17:00 –¥–æ 19:00‚Ä¢ –†–∞–±–æ—Ç–∞ –≤ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–º –∏ –∫–æ–º—Ñ–æ—Ä—Ç–Ω–æ–º –æ—Ñ–∏—Å–µ 10 –º–∏–Ω —Ö–æ–¥—å–±—ã –æ—Ç –º.–õ—É–∫—å—è–Ω–æ–≤—Å–∫–∞—è‚Ä¢ –î—Ä—É–≥–∏–µ –ø–ª—é—à–∫–∏ –≤ –æ—Ñ–∏—Å–µ (—á–∞–π / –∫–æ—Ñ–µ, —Ç–µ–Ω–Ω–∏—Å, PlayStation, –π–æ–≥–∞, –∞–Ω–≥–ª–∏–π—Å–∫–∏–π —è–∑—ã–∫, –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã–µ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è –∏ —Ç.–¥.)‚Ä¢ –£—Ä–æ–≤–µ–Ω—å –∑–∞—Ä–∞–±–æ—Ç–Ω–æ–π –ø–ª–∞—Ç—ã –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –ø–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º —Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏—è. Responsibilities ‚Ä¢ –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –¥–∞–Ω–Ω—ã—Ö –≤ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ (DWH) –Ω–∞ Oracle Database;‚Ä¢ –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –±–∞–∑—ã SQL –∏ PL/SQL –∫–æ–¥–∞;‚Ä¢ –°–æ–∑–¥–∞–Ω–∏–µ –∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞—à–±–æ—Ä–¥–æ–≤ –≤ BI —Å–∏—Å—Ç–µ–º–µ (Oracle BI);‚Ä¢ –í—ã–≥—Ä—É–∑–∫–∏ –∏ –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö –ø–æ –∑–∞–ø—Ä–æ—Å–∞–º –¥—Ä—É–≥–∏—Ö –æ—Ç–¥–µ–ª–æ–≤ (—Ç–∏–∫–µ—Ç–∞–º –≤ JIRA).",PL/SQL Developer@IBox,https://jobs.dou.ua/companies/ibox/vacancies/124825/, Kyiv,PL/SQL Developer,01 October 2020,$700‚Äì1500,2020-10-13,,dou
https://jobs.dou.ua/companies/itera-consulting-group-ukraine/,Itera,"{""Required skills"": [""Professional requirements:"", ""At least 3+ years\u2019 experience within software development / Data Engineering"", ""Experience with Python"", ""Familiarity with ETL / SQL / Docker"", ""Experience in taking larger projects from prototypes to production (ie know the requirements for \u201cproduction quality\u201d)"", ""Understanding of the concept \u201cdata platform\u201d, ie both technology (cloud and distributed solutions) and business model"", ""Experience of direct communication with customer and requirements clarification"", ""English: Intermediate or higher. You must be able to talk to Norwegian manager directly."", ""High-level of responsibility and loyalty""], ""As a plus"": [""Will be a plus:"", ""Previous experience at Scandinavian market"", ""Previous experience in Oil & Gas projects"", ""Ability to build trust with the major international data providers by understanding data and use cases""], ""We offer"": [""Company offers:"", ""Competitive compensation"", ""Paid vacation, 100% paid sick leave (+3 paid sick days without sick list)"", ""Medical insurance with sport and stomatology programs"", ""Free English and Norwegian Courses"", ""Comfortable office in the center of Kyiv with excellent office facilities"", ""Friendly team of high experienced specialists"", ""Trainings and travel abroad, certifications covered by the company""], ""Responsibilities"": [""Tasks and responsibilities:"", ""Able to work independently"", ""Communicate with customer, gather technical requirements"", ""Implement software development solution and modules""], ""Project description"": [""Itera is looking for a Senior Data Engineer who will work on the big Norwegian industrial IOT data platform provider. Clients data platform aggregates and processes data from millions of industrial sensors. Combine that data with 3D-models and drawings, P&IDs, and historical data. Current goal for our client is to build a team which going to work on customization of existed solution for oil & gas industry. We are looking for an open-minded person, self-going and proactive who confess continuous self-development and able to solve challenging tasks. We need someone that can understand the customer\u2019s goals and find relevant and proper solutions without continuous supervision.""]}",,"Required skills Professional requirements:‚Ä¢ At least 3+ years‚Äô experience within software development / Data Engineering‚Ä¢ Experience with Python‚Ä¢ Familiarity with ETL / SQL / Docker‚Ä¢ Experience in taking larger projects from prototypes to production (ie know the requirements for ‚Äúproduction quality‚Äù)‚Ä¢ Understanding of the concept ‚Äúdata platform‚Äù, ie both technology (cloud and distributed solutions) and business model‚Ä¢ Experience of direct communication with customer and requirements clarification‚Ä¢ English: Intermediate or higher. You must be able to talk to Norwegian manager directly.‚Ä¢ High-level of responsibility and loyalty As a plus Will be a plus:‚Ä¢ Previous experience at Scandinavian market ‚Ä¢ Previous experience in Oil & Gas projects ‚Ä¢ Ability to build trust with the major international data providers by understanding data and use cases We offer Company offers:‚Ä¢ Competitive compensation‚Ä¢ Paid vacation, 100% paid sick leave (+3 paid sick days without sick list)‚Ä¢ Medical insurance with sport and stomatology programs‚Ä¢ Free English and Norwegian Courses‚Ä¢ Comfortable office in the center of Kyiv with excellent office facilities‚Ä¢ Friendly team of high experienced specialists‚Ä¢ Trainings and travel abroad, certifications covered by the company Responsibilities Tasks and responsibilities:‚Ä¢ Able to work independently ‚Ä¢ Communicate with customer, gather technical requirements‚Ä¢ Implement software development solution and modules Project description Itera is looking for a Senior Data Engineer who will work on the big Norwegian industrial IOT data platform provider. Clients data platform aggregates and processes data from millions of industrial sensors. Combine that data with 3D-models and drawings, P&IDs, and historical data. Current goal for our client is to build a team which going to work on customization of existed solution for oil & gas industry. We are looking for an open-minded person, self-going and proactive who confess continuous self-development and able to solve challenging tasks. We need someone that can understand the customer‚Äôs goals and find relevant and proper solutions without continuous supervision.",Senior Data Engineer with Python@Itera,https://jobs.dou.ua/companies/itera-consulting-group-ukraine/vacancies/130055/," Kyiv, Lviv",Senior Data Engineer with Python,01 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/competera/,Competera,{},,"Competera uses Big Data and Deep Learning to change the way retailers do pricing. We are known for both cutting-edge math ‚Äòunder the hood‚Äô and for deep expertise in the pricing domain. We are now looking for a Data Engineer to change the way we cope with data flows while delivering the product to the end-users. The role is all about:‚Ä¢ Existing data flow monitoring and support‚Ä¢ New ETL pipelines creating‚Ä¢ Bottlenecks identification and optimization Starter-kit needed to join the board:‚Ä¢ Brilliant SQL knowledge: your SQL queries are to be effective and elegant‚Ä¢ Good knowledge of core Python, followed by hands-on experience with it‚Ä¢ Experience in optimizing SQL queries Pleasant extras:‚Ä¢ Experience in building data pipelines, ETL design‚Ä¢ Familiarity with Google Cloud Platform‚Ä¢ Familiarity with Apache Airflow‚Ä¢ Experience with different SQL/NoSQL databases You‚Äôre gonna love it, and here‚Äôs why:‚Ä¢ Meaningful work in an agile team ever-open to the experiments‚Ä¢ Want to learn? Competera loves that and is eager to cover 60% of your training/courses fee‚Ä¢ Fair payout with regular performance-based reviews and stock options plan for top performers‚Ä¢ Freedom to choose between a pet-friendly coworking and a home office‚Ä¢ Paid vacation & sick leaves (20 business days each) + 15 days off‚Ä¢ Partial medical insurance coverage Let‚Äôs price the world together!",Data Engineer@Competera,https://jobs.dou.ua/companies/competera/vacancies/133577/," Kyiv, remote",Data Engineer,01 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/svitla-systems-inc/,"Svitla Systems, Inc.",{},,"Svitla Systems Inc. is looking for Senior Data Engineer for a full-time position (40 hours per week) in Ukraine. Our client is reimagining how the world moves freight. They are a fast-growing tech startup based in Pittsburgh that is working on building the next era of fleet management, dispatch, and infrastructure technologies for the transportation and service industries. In a few short years, they‚Äôve grown from an idea to create smart driver safety tracking to becoming a full-blown fleet management and telematics company that has successfully deployed to thousands of drivers across the country with some of the nation‚Äôs top 100 fleets. They provide mission critical software and applications that power the transportation industry 24/7. This means client has a huge responsibility to create rock solid technology and products that are fault tolerant and tested extremely thoroughly. 5+ years of experience working as Data Engineer.Experience in creating Data oriented/Big data architectures from scratch.Working experience with one of script languages for Data Pipelines development: Node.js, Python. (preferably with both).Experience with Data oriented Amazon Cloud Services: S3, Dynamo DB, Kinesis, EMR + Spark, Dynamo DB, Redshift (as a plus).Experience with ETL modelling for data pipelines. Healthy, Diverse Teams Breed Innovation. Our client is proud to be an equal opportunity employer. We deeply believe that diverse backgrounds and experiences make better products, and we seek to attract talent from all walks of life.The team is smart, friendly and passionate, and we value a healthy work environment to foster personal development and opportunities to move within our small, but quickly growing organization. 1. Experience in development Streaming Data Pipelines.2. Experience in AWS Serverless stack with data pipelines ‚Äî Lambda, Step Functions. Competitive compensation plan that takes skills and experience into consideration.Annual performance appraisals.Possibility to choose your workspace either remote or combination of your home and one of our development offices. Flexible working hours and adjustable work/life balance. Projects that use advanced, cutting-edge technologies.Competitive bonuses for a personal recommendation of new employees.Vacation time, sick-leaves, national holidays, family supplementary days off.Comprehensive medical insurance including dental services, massages, and sports activities.Support for a healthy lifestyle, compensation of running events.A personal loan budget is available for long-term personnel.Partial compensation of conferences, courses and English classes.Free meetups, webinars, and conferences organized by Svitla.Birthday presents for personnel and New Year gifts for children.Fun summer and winter corporate parties and memorable anniversary presents.","Senior Data Engineer@Svitla Systems, Inc.",https://jobs.dou.ua/companies/svitla-systems-inc/vacancies/133575/," Kyiv, Kharkiv, Lviv, remote",Senior Data Engineer,01 October 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/kuna-systems/,Kuna Systems,"{""Required skills"": [""Proficiency in Python or other equivalent statistics and Machine Learning tools"", ""Hands-on experience working in Computer Vision and Machine Learning using models and methods such as CNN, LSTM, GAN etc."", ""Understanding of benchmarking datasets in Computer Vision"", ""Strong ability in problem solving and driving for results"", ""Experience with related AWS services and large datasets"", ""Strong personal interest in learning, researching, and creating new technologies with high customer impact"", ""Experience with defining organizational research and development practices in an industry setting""], ""As a plus"": [""Experience ML-based approaches to Computer Vision (object detection, segmentation, re-identification)"", ""Knowledge of classical (non-ML) CV algorithms is a plus""], ""We offer"": [""You\u2019ll be working with an incredibly talented team of engineers who\u2019ve built some amazing things. You\u2019ll also be working with some pretty savvy business folks who\u2019ve founded successful companies. Ask us about drones, Mars rovers, Emmy Awards, and making the Super Bowl broadcast possible. We have a lot to offer, and we\u2019re willing to bet you do too.""], ""Project description"": [""We let the employees keep whatever schedule that works well for them.Our overseas employees typically keep schedules that allow a good overlap daily with our US employees (who tend to work 10:30 AM"", ""6:30 PM PST). Any meetings are usually in the earlier part of the day, allowing for this flexibility."", ""Flexible schedule for annual vacation leave"", ""Candidates with demonstrated technical strength in the interview above, will take on a small project, working with one of us to solve a specific problem. We pay for the candidate\u2019s time at this stage.""]}",,"Required skills ‚Äî Proficiency in Python or other equivalent statistics and Machine Learning tools‚Äî Hands-on experience working in Computer Vision and Machine Learning using models and methods such as CNN, LSTM, GAN etc.‚Äî Understanding of benchmarking datasets in Computer Vision‚Äî Strong ability in problem solving and driving for results‚Äî Experience with related AWS services and large datasets‚Äî Strong personal interest in learning, researching, and creating new technologies with high customer impact‚Äî Experience with defining organizational research and development practices in an industry setting As a plus ‚Äî Experience ML-based approaches to Computer Vision (object detection, segmentation, re-identification)‚Äî Knowledge of classical (non-ML) CV algorithms is a plus We offer You‚Äôll be working with an incredibly talented team of engineers who‚Äôve built some amazing things. You‚Äôll also be working with some pretty savvy business folks who‚Äôve founded successful companies. Ask us about drones, Mars rovers, Emmy Awards, and making the Super Bowl broadcast possible. We have a lot to offer, and we‚Äôre willing to bet you do too. ‚Äî We let the employees keep whatever schedule that works well for them.Our overseas employees typically keep schedules that allow a good overlap daily with our US employees (who tend to work 10:30 AM ‚Äî 6:30 PM PST). Any meetings are usually in the earlier part of the day, allowing for this flexibility.‚Äî Flexible schedule for annual vacation leave‚Äî Candidates with demonstrated technical strength in the interview above, will take on a small project, working with one of us to solve a specific problem. We pay for the candidate‚Äôs time at this stage. Project description We‚Äôre Kuna, a San Francisco Bay Area startup, and we‚Äôre building an intelligent, cloud-connected home automation platform. After a stint in YCombinator, the world‚Äôs most prestigious startup incubator, and a successful crowdfunding campaign, we‚Äôre growing and we‚Äôre growing fast. We‚Äôre building out our engineering team and laying the groundwork for our next generation of products. Our main focus on security products using AI for residential and commercial markets. We are looking for a Senior Machine Learning Engineer (AI) who can evaluate and recommend tools, technologies and processes to ensure the highest quality product platform. Our Software Stack‚Äî Python/Django Server backend‚Äî AWS Opsworks, Cloudformation, Terraform‚Äî Java (some)‚Äî C on ARM Linux firmware‚Äî Android Java‚Äî iOS Swift, Objective C‚Äî Javascript ‚Äî React and ReduxNote: for our new products, we allow the flexibility of choosing new frameworks",Senior Machine Learning Engineer@Kuna Systems,https://jobs.dou.ua/companies/kuna-systems/vacancies/133544/," Kyiv, remote",Senior Machine Learning Engineer,30 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/artkai/,Artkai,"{""Required skills"": [""Good knowledge of SparkGood knowledge of SQLHands on experience with Scala and Java, Python, Shell (as plus)Hands on experience with Hadoop ecosystem: HDFS, Yarn, LivyKnowledge of AWS: S3, EC2, EM""], ""We offer"": [""Work in a cool multinational team* \u0421omfortable office conditions including all modern conveniences* Remote option (fully or partially)* Good management* Paid vacation and sick leave""], ""Responsibilities"": [""Contribute in all phases of the development lifecycle;Write well designed, testable, efficient code;Ensure designs are in compliance with specifications.Interact with team, arrange meetings to find out and clarify the project\u2019s requirements;Implementation of requirements based on scala\\spark.""], ""Project description"": [""The aim of the Project is transformation of data on watching of television programming, partition of data according to television programming and duration of advertising.""]}",,"Required skills Good knowledge of SparkGood knowledge of SQLHands on experience with Scala and Java, Python, Shell (as plus)Hands on experience with Hadoop ecosystem: HDFS, Yarn, LivyKnowledge of AWS: S3, EC2, EM We offer Work in a cool multinational team* –°omfortable office conditions including all modern conveniences* Remote option (fully or partially)* Good management* Paid vacation and sick leave Responsibilities Contribute in all phases of the development lifecycle;Write well designed, testable, efficient code;Ensure designs are in compliance with specifications.Interact with team, arrange meetings to find out and clarify the project‚Äôs requirements;Implementation of requirements based on scala\spark. Project description The aim of the Project is transformation of data on watching of television programming, partition of data according to television programming and duration of advertising.",Big Data developer@Artkai,https://jobs.dou.ua/companies/artkai/vacancies/133473/," Kyiv, Dnipro, remote",Big Data developer,30 September 2020,$2500‚Äì4000,2020-10-13,,dou
https://jobs.dou.ua/companies/tilting-point/,Tilting Point,"{""Required skills"": [""\u25cf Minimum bachelor\u2019s in Computer Science\u25cf 2+ years of experience as a Data Engineer working on big data analytics\u25cf Excellent knowledge of Python and SQL\u25cf Good knowledge of Scala and Spark\u25cf Good knowledge of Apache Airflow\u25cf You are fluent in English (written and oral), additional languages a plus\u25cf Experience with integration of data from multiple data sources and APIs\u25cf Knowledge of various ETL techniques and frameworks\u25cf Ability to collaborate with colleagues across different disciplines/locations""], ""As a plus"": [""\u25cf Have experience/exposure to mobile User Acquisition and Marketing\u25cf Worked in the gaming industry and knowledge game specific data\u25cf Comfortable with AWS cloud (S3, EC2, Redshift, EKS(Kubernetes))\u25cf Knowledge of existing third party analytics visualization software (Amplitude, Looker, Tableau)""], ""We offer"": [""\u25cf Startup environment"", ""where each individual makes a large impact\u25cf Ability to own technical direction of various products and systems\u25cf Work with great people on great games that reach millions of people each month""], ""Responsibilities"": [""Responsibilities include, but are not limited to the following, on an as-needed basis:\u25cf Work with data analysts, game producers, and customer service to help define technology needs for the logging, processing, storage and presentation of data in a manner that delivers business value\u25cf Design, implement, maintain tools and reports for User Acquisition and LiveOps teams from start to finish\u25cf Work with various internal teams to satisfy analytical needs by building both large systematic reports and small custom pieces\u25cf Build data expertise and own data quality for a variety of products""], ""Project description"": [""Tilting Point is looking for a Data Engineer to join our team in Kyiv. As part of the team, you will work on collecting, storing, processing, and analyzing huge sets of game data. The primary focus will be in owning a complete project and developing tools and systems for internal stakeholders for improving User Acquisition and LiveOps. Opportunity to combine data engineering, full stack development and data science. You will interface with various internal teams to understand their data needs and provide high quality data and insights which drive business value.""]}",,"Required skills ‚óè Minimum bachelor‚Äôs in Computer Science‚óè 2+ years of experience as a Data Engineer working on big data analytics‚óè Excellent knowledge of Python and SQL‚óè Good knowledge of Scala and Spark‚óè Good knowledge of Apache Airflow‚óè You are fluent in English (written and oral), additional languages a plus‚óè Experience with integration of data from multiple data sources and APIs‚óè Knowledge of various ETL techniques and frameworks‚óè Ability to collaborate with colleagues across different disciplines/locations As a plus ‚óè Have experience/exposure to mobile User Acquisition and Marketing‚óè Worked in the gaming industry and knowledge game specific data‚óè Comfortable with AWS cloud (S3, EC2, Redshift, EKS(Kubernetes))‚óè Knowledge of existing third party analytics visualization software (Amplitude, Looker, Tableau) We offer ‚óè Startup environment ‚Äî where each individual makes a large impact‚óè Ability to own technical direction of various products and systems‚óè Work with great people on great games that reach millions of people each month Responsibilities Responsibilities include, but are not limited to the following, on an as-needed basis:‚óè Work with data analysts, game producers, and customer service to help define technology needs for the logging, processing, storage and presentation of data in a manner that delivers business value‚óè Design, implement, maintain tools and reports for User Acquisition and LiveOps teams from start to finish‚óè Work with various internal teams to satisfy analytical needs by building both large systematic reports and small custom pieces‚óè Build data expertise and own data quality for a variety of products Project description Tilting Point is looking for a Data Engineer to join our team in Kyiv. As part of the team, you will work on collecting, storing, processing, and analyzing huge sets of game data. The primary focus will be in owning a complete project and developing tools and systems for internal stakeholders for improving User Acquisition and LiveOps. Opportunity to combine data engineering, full stack development and data science. You will interface with various internal teams to understand their data needs and provide high quality data and insights which drive business value.",Data Engineer@Tilting Point,https://jobs.dou.ua/companies/tilting-point/vacancies/106765/, Kyiv,Data Engineer,30 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/megogonet-/,MEGOGO,{},,"–ù–∞—à –∏–¥–µ–∞–ª—å–Ω—ã–π –∫–∞–Ω–¥–∏–¥–∞—Ç:‚Äî –ò–º–µ–µ—Ç –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –æ–ø—ã—Ç –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏—è –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö SQL –∏ noSQL (MySQL, MongoDB).‚Äî –£–º–µ–µ—Ç –≤ –¥–∏–∑–∞–π–Ω –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö, —Å–æ–∑–¥–∞–Ω–∏–µ –æ–±—â–µ–π –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã –≤–æ–∫—Ä—É–≥ –±–∞–∑, —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é.‚Äî –°–ø–æ—Å–æ–±–µ–Ω –æ–±–µ—Å–ø–µ—á–∏—Ç—å –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å, –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –∏ —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö.‚Äî –ó–Ω–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è, –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∏ –±—ç–∫–∞–ø–∏—Ä–æ–≤–∞–Ω–∏—è –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö.‚Äî –°–ø–æ—Å–æ–±–µ–Ω –ø–æ–º–æ—á—å —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫—É —Å –¥–∏–∑–∞–π–Ω–æ–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è –ø–æ —á–∞—Å—Ç–∏ —Ä–∞–±–æ—Ç—ã —Å –±–∞–∑–æ–π. –ë—É–¥–µ—Ç –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ–º:‚Äî –û–ø—ã—Ç —Ä–∞–±–æ—Ç—ã —Å Java/Scala/PHP —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∞–º–∏.‚Äî –û–ø—ã—Ç —Ä–∞–±–æ—Ç—ã —Å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–º–∏ —Ñ–∞–π–ª–æ–≤—ã–º–∏, –±–ª–æ—á–Ω—ã–º–∏ –∏–ª–∏ –æ–±—ä–µ–∫—Ç–Ω—ã–º–∏ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞–º–∏: GlusterFS, Cinder, Ceph –∏ —Ç.–¥..‚Äî –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –æ–ø—ã—Ç –≤ –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–∏ –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∫ PCI DSS compliant —Å–æ—Å—Ç–æ—è–Ω–∏—é ‚Äî –±–æ–ª—å—à–æ–π –ø–ª—é—Å. –ß—Ç–æ –º—ã –∂–¥–µ–º:‚Äî –ö–æ–º–ø–µ—Ç–µ–Ω—Ç–Ω–æ–≥–æ –∏–Ω–∂–µ–Ω–µ—Ä–∞ –Ω–∞—à–∏—Ö –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö, –∫–æ—Ç–æ—Ä—ã–π –æ–±–µ—Å–ø–µ—á–∏—Ç –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å, –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å, –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –∏ –æ—Ç–∫–∞–∑–æ—É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å.‚Äî –ö–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç–∞ –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–æ–≤, —Ç–∞–±–ª–∏—Ü, –∏–Ω–¥–µ–∫—Å–æ–≤, –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π. ‚Äî –ú—ã —Å—Ç–∞—Ä–∞–µ–º—Å—è –¥–∏–∑–∞–π–Ω–∏—Ç—å —Å—Ä–µ–¥—ã –∏ —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á–∞–º –∫–æ–ª–ª–µ–∫—Ç–∏–≤–Ω–æ. –í—Å–µ –º–æ–≥—É—Ç, –∏ –¥–∞–∂–µ –¥–æ–ª–∂–Ω—ã —É—á–∞—Å—Ç–≤–æ–≤–∞—Ç—å –ß—Ç–æ –º—ã –ø—Ä–µ–¥–ª–∞–≥–∞–µ–º:‚Äî –†–∞–±–æ—Ç—É –≤ —Å—Ç–∞–±–∏–ª—å–Ω–æ–π –∫–æ–º–ø–∞–Ω–∏–∏ ‚Äî –º—ã –±–æ–ª–µ–µ –≤–æ—Å—å–º–∏ –ª–µ—Ç –Ω–∞ –º–µ–¥–∏–∞—Ä—ã–Ω–∫–µ.‚Äî –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –ø–æ—É—á–∞—Å—Ç–≤–æ–≤–∞—Ç—å –≤ —Å–æ–∑–¥–∞–Ω–∏–∏ –≤–∏–¥–µ–æ—Å–µ—Ä–≤–∏—Å–∞ –±—É–¥—É—â–µ–≥–æ.‚Äî –ö–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å –∑–∞—Ä–∞–±–æ—Ç–Ω–æ–π –ø–ª–∞—Ç—ã.‚Äî –ö—Ä—É—Ç—ã–µ –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤—ã. –ù–∞–ø—Ä–∏–º–µ—Ä, –≤—Å–µ–π –∫–æ–º–∞–Ω–¥–æ–π –Ω–∞ Atlas Weekend:)‚Äî –û—Ç–Ω–æ—à–µ–Ω–∏—è, –ø–æ—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ –Ω–∞ –¥–æ–≤–µ—Ä–∏–∏.‚Äî –ê–∫—Ç–∏–≤–Ω—ã–π –æ—Ç–¥—ã—Ö: –∏–≥—Ä–∞–π –≤ —Ñ—É—Ç–±–æ–ª –∏–ª–∏ –Ω–∞—Å—Ç–æ–ª—å–Ω—ã–π —Ç–µ–Ω–Ω–∏—Å. –£ –Ω–∞—Å –µ—Å—Ç—å —Å–≤–æ—è —Ñ—É—Ç–±–æ–ª—å–Ω–∞—è –∫–æ–º–∞–Ω–¥–∞, –∞ –Ω–∞—à —Ç—Ä–µ–Ω–µ—Ä –ø–æ –ø–∏–Ω–≥-–ø–æ–Ω–≥—É –≤—Å–µ–º—É –Ω–∞—É—á–∏—Ç, –æ–Ω –≤—Å–µ–≥–¥–∞ –≤ –æ—Ñ–∏—Å–µ. –í—ã–±–∏—Ä–∞–π, —á—Ç–æ —Ç–µ–±–µ –ø–æ –¥—É—à–µ, –∏ —É—á–∞—Å—Ç–≤—É–π –≤ —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏—è—Ö! –ï—Å–ª–∏ –æ—á–µ–Ω—å —Ö–æ—á–µ—Ç—Å—è, –¥–∞–∂–µ –µ—Å—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –∏—Å–ø–æ–ª–Ω–∏—Ç—å —Å–≤–æ—é –¥–µ—Ç—Å–∫—É—é –º–µ—á—Ç—É, –µ—Å–ª–∏ —Ö–æ—Ç–µ–ª–æ—Å—å —Å—Ç–∞—Ç—å —Å–ø–æ—Ä—Ç–∏–≤–Ω—ã–º –∫–æ–º–º–µ–Ω—Ç–∞—Ç–æ—Ä–æ–º.‚Äî –í—Å–µ–≥–¥–∞ —Å–≤–µ–∂–∏–µ —Ñ—Ä—É–∫—Ç—ã –≤ –æ—Ñ–∏—Å–µ –∏ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–π –±–∞—Ä–∏—Å—Ç–∞.‚Äî –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è —Ä–∞–∑–≤–∏—Ç–∏—è.‚Äî –ë–µ—Å–ø–ª–∞—Ç–Ω—ã–µ —É—Ä–æ–∫–∏ –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ –≤ –æ—Ñ–∏—Å–µ. –ß—É–≤—Å—Ç–≤—É–µ—à—å, —á—Ç–æ —Ç–≤–æ–∏ –Ω–∞–≤—ã–∫–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º? –•–æ—á–µ—à—å —Å—Ç–∞—Ç—å —á–∞—Å—Ç—å—é –Ω–∞—à–µ–π –∫—Ä—É—Ç–æ–π –∫–æ–º–∞–Ω–¥—ã? –°–∫–æ—Ä–µ–µ –æ—Ç–ø—Ä–∞–≤–ª—è–π —Å–≤–æ–µ —Ä–µ–∑—é–º–µ! –û—Ç–≤–µ—Ç–∏–≤ –Ω–∞ –≤–∞–∫–∞–Ω—Å–∏—é –∏ –æ—Ç–ø—Ä–∞–≤–∏–≤ —Å–≤–æ–µ —Ä–µ–∑—é–º–µ –≤ –ö–æ–º–ø–∞–Ω–∏—é (–û–û–û ¬´–ú–ï–ì–û–ì–û¬ª), –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∏ –¥–µ–π—Å—Ç–≤—É—é—â—É—é –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–æ–º –£–∫—Ä–∞–∏–Ω—ã, —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π –Ω–æ–º–µ—Ä 38347009, –∞–¥—Ä–µ—Å –º–µ—Å—Ç–æ–Ω–∞—Ö–æ–∂–¥–µ–Ω–∏—è: –£–∫—Ä–∞–∏–Ω–∞, –≥. –ö–∏–µ–≤, —É–ª. –ù–æ–≤–æ–∫–æ–Ω—Å—Ç–∞–Ω—Ç–∏–Ω–æ–≤—Å–∫–∞—è, 18 –í (–¥–∞–ª–µ–µ ¬´–ö–æ–º–ø–∞–Ω–∏—è¬ª), –≤—ã –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç–µ –∏ —Å–æ–≥–ª–∞—à–∞–µ—Ç–µ—Å—å —Å —Ç–µ–º, —á—Ç–æ –ö–æ–º–ø–∞–Ω–∏—è –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤–∞—à–∏ –ª–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ –≤ –≤–∞—à–µ–º —Ä–µ–∑—é–º–µ, –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å –ó–∞–∫–æ–Ω–æ–º –£–∫—Ä–∞–∏–Ω—ã ¬´–û –∑–∞—â–∏—Ç–µ –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö¬ª –∏ –ø—Ä–∞–≤–∏–ª–∞–º–∏ GDPR.",DBA (Data Base Administrator)@MEGOGO,https://jobs.dou.ua/companies/megogonet-/vacancies/122867/, Kyiv,DBA (Data Base Administrator),30 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/yaypay/,YayPay by Quadient,"{""Required skills"": [""YayPay is looking for a Data Analyst. YayPay operates with an array of internal data from different departments and projects, which needs to be collected, interpreted and analyzed. We expect you to work with the following data (this list is not exhaustive and can be updated):"", ""Product Usage and Adoption"", ""Processed Payment"", ""Customer Success and implementations"", ""Customer CSAT/NPS scores"", ""Customer Churn"", ""Departamental KPIs""], ""As a plus"": [""You need to be fluent with the data in any way shape or form, and have experience with the following tools:"", ""Grafana"", ""SaleForce"", ""Spreadsheets (Google or Microsoft)"", ""MySQL Database""], ""We offer"": [""Gainsight PX"", ""ReTool"", ""Jira"", ""PowerBI (or similar)""], ""Responsibilities"": [""A high-level (A1) corporate insurance from UNIQATax compensationEquipment (laptops, screens, etc.)The company encourages all kinds of professional development (incl. books, training, workshops, etc.). We will cover 100% of the price of the educational literature and 60% of the conference (training etc.) cost.""], ""Project description"": [""Working with this data and appropriate departments, you will be responsible for the following:"", ""Collecting and interpreting data"", ""Analyzing Data, looking for insights"", ""Reporting the results back to the relevant members of the business"", ""Identifying patterns and trends in data sets"", ""By request, providing relevant datasets and analytics to support management decision making"", ""Defining new data collection and analysis processes"", ""Implementing Data Collection Automation"", ""Building Reporting Dashboards in the company tools"", ""Collecting and automating KPI collection, reporting on KPIs""]}",,"Required skills YayPay is looking for a Data Analyst. YayPay operates with an array of internal data from different departments and projects, which needs to be collected, interpreted and analyzed. We expect you to work with the following data (this list is not exhaustive and can be updated):‚Äî Product Usage and Adoption‚Äî Processed Payment ‚Äî Customer Success and implementations ‚Äî Customer CSAT/NPS scores‚Äî Customer Churn ‚Äî Departamental KPIs You need to be fluent with the data in any way shape or form, and have experience with the following tools:‚Äî Grafana‚Äî SaleForce‚Äî Spreadsheets (Google or Microsoft)‚Äî MySQL Database As a plus ‚Äî Gainsight PX‚Äî ReTool‚Äî Jira‚Äî PowerBI (or similar) We offer A high-level (A1) corporate insurance from UNIQATax compensationEquipment (laptops, screens, etc.)The company encourages all kinds of professional development (incl. books, training, workshops, etc.). We will cover 100% of the price of the educational literature and 60% of the conference (training etc.) cost. Responsibilities Working with this data and appropriate departments, you will be responsible for the following:‚Äî Collecting and interpreting data‚Äî Analyzing Data, looking for insights‚Äî Reporting the results back to the relevant members of the business‚Äî Identifying patterns and trends in data sets‚Äî By request, providing relevant datasets and analytics to support management decision making‚Äî Defining new data collection and analysis processes‚Äî Implementing Data Collection Automation‚Äî Building Reporting Dashboards in the company tools‚Äî Collecting and automating KPI collection, reporting on KPIs Project description YayPay (A Quadient Company) one of the fastest growing Fintech products that creates modern accounts receivables and payments solutions for mid-market and enterprise finance teams through Artificial Intelligence and Machine Learning. YayPay‚Äôs Smart AR platform gives finance teams more insight and more control over customer and credit relationships, more working capital availability, deeper insight into their revenues, and closer relationships with their customers. We are experiencing very high demand for our product due to higher fundamental focuses on key areas of working capital optimisation, the need for remote work collaboration and fostering strong buyer and supplier relationships. Recognised as an industry leader by IDC, we deliver innovative products and services for our customers by rigorously applying the principles that shape our culture at Quadient.",Data Analyst@YayPay by Quadient,https://jobs.dou.ua/companies/yaypay/vacancies/133446/," Kyiv, Dnipro, remote",Data Analyst,30 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/rakuten/,Rakuten,{},,"Rakuten Americas is looking for a talented Data Scientist to join our amazing Machine Learning team in Ukraine. Rakuten Americas is a global leader in internet services, empowering individuals, communities, businesses and society. Our 20+ businesses span e-commerce, digital content, communications and data analytics, bringing the joy of discovery to millions of members around the world. We‚Äôve brought together a unique set of internet services that are changing the way retailers and marketers do business. With businesses such as Rakuten Marketing, Rakuten Rewards and Rakuten Intelligence, we can offer marketers the platform and data intelligence to reach consumers at the moment they want to make a purchase. Our Data Science group consists of ~40 Data Scientists and Engineers, part of them based in Ukraine. We apply SOTA and classical machine learning techniques to Rakuten data to improve the product, internal processes and personalization for users, which in turn improves profitability of Rakuten business units. Our products are composed of many microservices. We own the products we deliver and maintain them. We follow DevOps approach when developers are responsible for code delivery with help of existing tools created by a dedicated Infra team. As a team is distributed, you can work remotely or from our Odessa/Kiev offices. We cover english classes, health insurance, sport activities, certifications/trainings and participation in conferences. ‚Ä¢BS or MS in a technical field ‚Ä¢2+ years of experience in machine learning, the experience in at least one of next domains will be a plus: NLP, Recommendation Systems, Predictive Analytics, Knowledge graphs ‚Ä¢Expertise in Python and SQL ‚Ä¢Strong CS fundamentals, such as algorithms and data structures ‚Ä¢Experience with cloud computing stacks such as Amazon Web Services preferred ‚Ä¢Excellent written and verbal communication skills ‚Ä¢Enthusiasm for working hard and having fun in a dynamic environment ‚Ä¢Prepare the required datasets for modeling purposes ‚Ä¢Evaluate the models and perform investigative error analyses ‚Ä¢Report and provide feedback on the findings of analyses to team members ‚Ä¢Research and experiment with different machine learning algorithms and techniques to solve business problems ‚Ä¢Work with engineers to make sure the engines scale well on high volumes of data",Data Scientist@Rakuten,https://jobs.dou.ua/companies/rakuten/vacancies/129697/," Odesa, remote",Data Scientist,30 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/ciklum/,Ciklum,{},,"On behalf of Ciklum Digital, Ciklum is looking for a Middle Data Engineer to join the UA team on a full-time basis. You will join a highly motivated team and will be working on a modern solution for our existing client. We are looking for technology experts who want to make an impact on new business by applying best practices and taking ownership. Project description:Our Client is a technology company that wants to make the world a better place through telemedicine, enabling health care providers to conduct secure video visits with their patients. Soon our Client will have a million users spanning the globe who then visit with tens of millions of patients. It hubs in Charleston SC, Salt Lake City UT, Rochester NY, Kyiv, and London. Our Client wants to deliver the best possible telemedicine experience for their users and their patient population.",Middle Data Engineer for Ciklum Digital@Ciklum,https://jobs.dou.ua/companies/ciklum/vacancies/133396/, Lviv,Middle Data Engineer for Ciklum Digital,29 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/proxet/,Proxet,"{""Required skills"": [""Job SummaryWe are looking for a Senior Data Engineer who will be a significant part of creating a data platform for a new product, especially for subsystems related to data scraping.""], ""We offer"": [""Requirements:"", ""A clear understanding of ETL and data curation"", ""Strong experience and understanding of relational and distributed databases (including internals) (e.g. PostgreSQL, MySQL, Cassandra, etc.)"", ""Experience with scraping systems (Scrapy)"", ""Proven experience with Spark/PySpark, AirFlow"", ""Strong experience with AWS"", ""Strong expertise in Python, SQL"", ""Experience with continuous integration, test automation, and deployment"", ""Experience with data monitoring and tracing in distributed and service-oriented systems"", ""Experience in distributed systems design and best practices"", ""Understanding of integration with BI tools"", ""Advanced / fluent written and spoken English""], ""Responsibilities"": [""We offer multiple benefits, that include:"", ""Challenging work in an international professional environment"", ""The long-standing team as this is for a long term project"", ""Competitive salary"", ""Flexible work-from-home & remote work policy"", ""Mastering English language with a native speaker"", ""40-hours working week with flexible working hours"", ""PE accounting and support"", ""20 paid vacation days per year"", ""14 paid sick leaves per year"", ""Collaborative friendly team environment""], ""Project description"": [""Responsibilities:"", ""Work with the Data Engineering team to design the new data platform"", ""Closely work with the data scraping pipelines"", ""Manage and guide software engineers during the development"", ""Systematically work on formalizing business processes, customer acquisition lifecycles, and the overall data framework"", ""Actively participate in the development""]}",,"Required skills Job SummaryWe are looking for a Senior Data Engineer who will be a significant part of creating a data platform for a new product, especially for subsystems related to data scraping. Requirements:‚Äî A clear understanding of ETL and data curation‚Äî Strong experience and understanding of relational and distributed databases (including internals) (e.g. PostgreSQL, MySQL, Cassandra, etc.)‚Äî Experience with scraping systems (Scrapy)‚Äî Proven experience with Spark/PySpark, AirFlow‚Äî Strong experience with AWS‚Äî Strong expertise in Python, SQL‚Äî Experience with continuous integration, test automation, and deployment‚Äî Experience with data monitoring and tracing in distributed and service-oriented systems‚Äî Experience in distributed systems design and best practices‚Äî Understanding of integration with BI tools‚Äî Advanced / fluent written and spoken English We offer We offer multiple benefits, that include:‚Äî Challenging work in an international professional environment‚Äî The long-standing team as this is for a long term project‚Äî Competitive salary‚Äî Flexible work-from-home & remote work policy‚Äî Mastering English language with a native speaker‚Äî 40-hours working week with flexible working hours‚Äî PE accounting and support‚Äî 20 paid vacation days per year‚Äî 14 paid sick leaves per year‚Äî Collaborative friendly team environment Responsibilities Responsibilities:‚Äî Work with the Data Engineering team to design the new data platform‚Äî Closely work with the data scraping pipelines‚Äî Manage and guide software engineers during the development‚Äî Systematically work on formalizing business processes, customer acquisition lifecycles, and the overall data framework‚Äî Actively participate in the development Project description About the project: Thrasio is one of the fastest-growing acquirers of Amazon third-party FBA brands. Amazon has become the most ubiquitous part of the modern shopping experience and our company is positioned to be a leader in this vibrant and evolving marketplace. Thrasio was established in 2018 with a clear mission: to become the largest, most profitable seller on Amazon. Since then, our growth has been dramatic; we have over $100MM in committed capital, with an impressive group of investors supporting our success.",Data Engineer with Scraping skills@Proxet,https://jobs.dou.ua/companies/proxet/vacancies/118640/," Kyiv, remote",Data Engineer with Scraping skills,29 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/datarobot/,DataRobot,"{""Required skills"": [""4-5+ years of real-world business experience in a data science roleEnglish speaking a mustHands-on experience building and implementing predictive models using machine learning algorithmsStrong customer interaction experienceStrong project management skillsExcellent organizational, communication, writing and interpersonal skillsFamiliarity with a variety of technical tools for the manipulation of datasetsFluency with scripting (Python / R)15-40% travel""], ""As a plus"": [""Familiarity with consultative sales process in the analytics marketplaceFamiliarity with Hadoop and related Big Data technologiesFamiliarity with data prep tools like Alteryx or TrifactaExperience dealing with complex customer organizationsDeep experience with specific industries (e.g. banking, healthcare, insurance) or specific problem types (e.g. time-series, optimization)Experience in MLOps"", ""model deployment, monitoring, validation and operationalisation""], ""We offer"": [""Customer Facing Data Scientists (CFDSs) are critical to making our customers successful. An ideal CFDS candidate should have strong fundamentals of applied data science in business setting, and should enjoy communicating and evangelizing data science solutions to business stakeholders.""], ""Responsibilities"": [""You will be based in one of our Ukraine offices and primarily interact with clients and the team on videoconference, but you will also sometimes visit clients on-site.""], ""Project description"": [""Product:Representing the DataRobot product from a technical standpoint to customers"", ""including demonstrations, conducting proof-of-concept trials, helping clients evaluate success criteria, and training usersProviding the customer\u2019s point of view to DataRobot\u2019s Product team, informing the direction of future product feature development.""]}",,"Required skills 4-5+ years of real-world business experience in a data science roleEnglish speaking a mustHands-on experience building and implementing predictive models using machine learning algorithmsStrong customer interaction experienceStrong project management skillsExcellent organizational, communication, writing and interpersonal skillsFamiliarity with a variety of technical tools for the manipulation of datasetsFluency with scripting (Python / R)15-40% travel As a plus Familiarity with consultative sales process in the analytics marketplaceFamiliarity with Hadoop and related Big Data technologiesFamiliarity with data prep tools like Alteryx or TrifactaExperience dealing with complex customer organizationsDeep experience with specific industries (e.g. banking, healthcare, insurance) or specific problem types (e.g. time-series, optimization)Experience in MLOps ‚Äî model deployment, monitoring, validation and operationalisation We offer Customer Facing Data Scientists (CFDSs) are critical to making our customers successful. An ideal CFDS candidate should have strong fundamentals of applied data science in business setting, and should enjoy communicating and evangelizing data science solutions to business stakeholders. You will be based in one of our Ukraine offices and primarily interact with clients and the team on videoconference, but you will also sometimes visit clients on-site. Responsibilities Product:Representing the DataRobot product from a technical standpoint to customers ‚Äî including demonstrations, conducting proof-of-concept trials, helping clients evaluate success criteria, and training usersProviding the customer‚Äôs point of view to DataRobot‚Äôs Product team, informing the direction of future product feature development. Data Science:Enabling customers to solve complex data science problems using DataRobot ‚Äî including problem framing, data preparation, model building, model deployment, model management, and output consumptionIn some cases, executing data science workflows for customersProviding data science knowledge and expertise as a trusted advisor to the client. Project management:Conducting and managing data science projects with the customer‚Äôs vision of success in mindCollaborating with Sales, Field Engineers, and the rest of the DataRobot team to identify the best possible resources to move forward customer‚Äôs projects. LeadershipBuilding a long-term trusted relationship with the customer so that the customers can be led towards successUnderstanding and empathizing with customers‚Äô pain points of building AI solutionsQualifying opportunities where DataRobot can be a suitable fit and thus making DataRobot more efficientPresenting DataRobot in industry conferences as well as creating powerful technical content for marketing purposes. Project description On a day-to-day basis, CFDS works side-by-side with the Sales, Account Management, and Field Engineering teams to help our customers achieve their goals with DataRobot. Internally, CFDS act as the voice of customer to the Product, Engineering, and Marketing teams.",Customer Facing Data Scientist@DataRobot,https://jobs.dou.ua/companies/datarobot/vacancies/133351/," Kyiv, Kharkiv, Lviv, Odesa, Khmelnytskyi",Customer Facing Data Scientist,29 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/softserve/,SoftServe,{},,"WE ARE The largest Big Data & AI Center of Excellence in Eastern Europe, our expertise in Machine Learning, AI, Big Data, IoT, Cloud technologies recognized by Google and Amazon.We are transforming the way thousands of global organizations do business by creating the most innovative solutions using proven architecture design methodologies, innovation frameworks, and experience design approaches. In 2019, our group successfully delivered 50+ discovery & consulting projects and 75+ implementation projects.Together with the SEI, and Carnegie Mellon University, we invented Smart Decisions game, in order to promote architecture design methodologies, intended to facilitate the design of complex architectures.Please read more at smartdecisionsgame.com YOU ARE An expert with ‚Ä¢ Excellent understanding of distributed computing technologies, approaches, and patterns‚Ä¢ Proficiency in one of the following programming languages: Java, Scala, or Python‚Ä¢ Cloud experience which is a big plus point: AWS, GCP or Azure‚Ä¢ Hands-on experience with Hadoop, NoSQL, MPP technologies, processing frameworks, or other Big Data technology areas: data ingestion, consolidation, streaming, or batching‚Ä¢ Skill in at least one of the processing and computation frameworks: Kafka Streams, Storm, Spark, Flink, Beam/DataFlow, Akka, etc.‚Ä¢ Experience in at least one of the RDBMS or NoSQL engines: PostgreSQL, MySQL, Cassandra, HBase, Elasticsearch, Redis, MongoDB, Impala, Kudu, etc.‚Ä¢ Background of implementing Data Lakes, Data Warehousing, or analytics systems is a big advantage YOU WANT TO WORK WITH Opportunity to ‚Ä¢ Create sustainable Big Data and AI solutions‚Ä¢ Evaluate cutting-edge Big Data technologies, implement PoCs and MVPs‚Ä¢ Learn new technologies and obtain certifies‚Ä¢ Learn how to design architectures using proven methodologies by SEI, Carnegie Mellon‚Ä¢ Become a top expert, or certified Architect‚Ä¢ Gain deep expertise in one of the clouds or multiple clouds TOGETHER WE WILL ‚Ä¢ Deliver discovery and consulting projects, such as solution design, technology assessment and architecture evaluation together with CoE Lead Architects‚Ä¢ Implement full-scale high-performance Data Platforms and decision-support systems‚Ä¢ Utilize rapid prototyping techniques to accelerate the implementation of new technical solutions‚Ä¢ Adopt cutting-edge technologies on a challenging project‚Ä¢ Provide high-value services to different range of companies: from startups to Fortune 100‚Ä¢ Engage new clients and work closely with the Sales teamOur benefits ‚Ä¢ Assimilate best practices from experts, working in the team of top-notch Architects‚Ä¢ Work closely or be a part of Google or Amazon professional services‚Ä¢ Being a part of Center of Excellence, you will have a chance to address different business and technology challenges, drive multiple projects and initiatives‚Ä¢ Boost your communication and leadership skills by obtaining experience on different projects‚Ä¢ Attend and speak at international events","Middle/Senior Big Data Engineer (Python+GCP), ID¬†56083@SoftServe",https://jobs.dou.ua/companies/softserve/vacancies/133336/," Kharkiv, remote","Middle/Senior Big Data Engineer (Python+GCP), ID¬†56083",29 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/datarobot/,DataRobot,"{""Required skills"": [""A passion for automating everythingA passion for collaborating and tearing down communication silos3+ years of experience in DevOps focused on Big Data environments3+ years of experience scripting in Bash, Java, Scala, Python or similar3+ years of experience with Linux (Ubuntu, RedHat or similar)Experience with Docker and/or container orchestration (Docker, Kubernetes, Mesos, or similar)Experience in configuring and setting up Kubernetes and Hadoop clustersExperience in setting up Kerberos authentication and impersonationExperience with Hadoop ecosystem/Hadoop stack (Hadoop, Spark, Hive, etc)Experience with Kubernetes ecosystem/Kubernetes stack (Kubeflow, EKS)Good coding skills. In the interview process, you will be evaluated on your performance in a number of coding and design scenarios"", ""be prepared to think!Good writing and communication skills""], ""As a plus"": [""Experience with launching/managing computing resources in AWS, Azure or similarExperience with CI/CD tools (Jenkins, Team City, Ansible, etcd, Terraform or similar)Familiar with DevOps methodologiesFamiliarity with both Cloud Deployment and On-Premise Release WorkflowsApplication-level metrics familiarity (ELK stack, Instana, Grafana, Prometheus, Tracing)Have experience creating automated build pipelines""], ""Responsibilities"": [""As a DevOps Engineer, you will build out a Data Prep & ETL system inside DataRobot utilizing distributed frameworks such as Spark, Hadoop, and Kubernetes. You will work on availability, security, scalability, and resiliency of solutions focused on data processing. You will also be responsible for the design and integration of pipelines for all kinds of automated testing.""], ""Project description"": [""Ideally, a candidate can bring new ideas from concept to implementation, write quality, testable code, and participate in design/development discussions. We value engineers who are familiar with DevOps tools and practices, who do not believe that any problem is too hard, and who are willing and eager to chase problems down no matter where they lead.""]}",,"Required skills A passion for automating everythingA passion for collaborating and tearing down communication silos3+ years of experience in DevOps focused on Big Data environments3+ years of experience scripting in Bash, Java, Scala, Python or similar3+ years of experience with Linux (Ubuntu, RedHat or similar)Experience with Docker and/or container orchestration (Docker, Kubernetes, Mesos, or similar)Experience in configuring and setting up Kubernetes and Hadoop clustersExperience in setting up Kerberos authentication and impersonationExperience with Hadoop ecosystem/Hadoop stack (Hadoop, Spark, Hive, etc)Experience with Kubernetes ecosystem/Kubernetes stack (Kubeflow, EKS)Good coding skills. In the interview process, you will be evaluated on your performance in a number of coding and design scenarios ‚Äî be prepared to think!Good writing and communication skills As a plus Experience with launching/managing computing resources in AWS, Azure or similarExperience with CI/CD tools (Jenkins, Team City, Ansible, etcd, Terraform or similar)Familiar with DevOps methodologiesFamiliarity with both Cloud Deployment and On-Premise Release WorkflowsApplication-level metrics familiarity (ELK stack, Instana, Grafana, Prometheus, Tracing)Have experience creating automated build pipelines Responsibilities As a DevOps Engineer, you will build out a Data Prep & ETL system inside DataRobot utilizing distributed frameworks such as Spark, Hadoop, and Kubernetes. You will work on availability, security, scalability, and resiliency of solutions focused on data processing. You will also be responsible for the design and integration of pipelines for all kinds of automated testing. Ideally, a candidate can bring new ideas from concept to implementation, write quality, testable code, and participate in design/development discussions. We value engineers who are familiar with DevOps tools and practices, who do not believe that any problem is too hard, and who are willing and eager to chase problems down no matter where they lead. Project description The Global Data Domain at DataRobot helps make AI incredibly easy to build by surfacing and connecting users to all enterprise data, and enabling data prep at modeling and prediction time. In this role, you‚Äôll help enable our users to easily interact with the data required to train models and generate predictions. Additionally, you‚Äôll help design solutions that foster collaboration amongst distributed teams of engineers, data scientists, and beyond.","DevOps Engineer, Data Management@DataRobot",https://jobs.dou.ua/companies/datarobot/vacancies/125367/," Kyiv, Lviv","DevOps Engineer, Data Management",29 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/wix/,WIX.com,{},,"A top-notch data-driven company. We‚Äôre growing so fast (180M users) that we‚Äôre collecting more data than we can monetize. Our Data Science group consists of ~50 people, of which there are 20+ data scientists, and others (engineering, curation and labeling) that help us make our projects successful and impactful for Wix. We apply SOTA Machine Learning techniques to Wix‚Äôs data to improve the product, internal processes and personalization for users, which in turn improves our profitability. We‚Äôre looking for a Data Science Team Leader who is passionate about data science and analytical problem-solving to join our Kyiv office. An extremely talented data scientist with at least 5 years‚Äô experience with data science, machine learning, and/or deep learning and at least 2 years‚Äô experience managing a successful team of data scientists. You have a deep understanding of classical ML & DL algs for a wide spectrum of problems and domains: Computer Vision, NLP, Recommendation Systems, Supervised & Unsupervised Learning. You have the technical know-how to advise team members, the managerial skills to prioritize and advance projects and the interpersonal skills to communicate data science to non-technical management. You also have hands-on machine learning experience with Python. You‚Äôre willing to work hands-on, potentially working on your own project while managing the team. Have an M.Sc or PhD in Math, Statistics, Computer Science, Physics, or an equivalent field.Are familiar with Deep Learning frameworks and applications Get to work on extremely diverse problems and domains.Manage and mentor a team of talented data scientists with experience in various machine learning applications.Serve as the technical expert of the team, providing high level guidance on all projects.Direct and define projects based on the business requirements of various product managers.Conduct advanced analysis, then design and code appropriate solutions using machine-learning, and communicating with all business partners.Be responsible for building relationships with stakeholders in other business units, to identify aspects of the business/product that can benefit from data science and machine learning.",Data Science Team Leader@WIX.com,https://jobs.dou.ua/companies/wix/vacancies/124350/, Kyiv,Data Science Team Leader,29 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/grid-dynamics/,Grid Dynamics,{},,"Our customer is one of the world‚Äôs largest technology companies based in Silicon Valley with operations all over the world. On this project, we are working with bleeding-edge big data technologies to develop a high-performance data analytics platform, which handles petabytes of data. We are looking for an enthusiastic and technology-proficient Big Data Developer, who is eager to participate in design and implementation of a top-notch big data solution that will be deployed at a massive scale. Responsibilities:‚Äî Participate in the design and development of a big data analytics application‚Äî Design, support and continuously enhance the project code base, continuous integration pipeline, etc.‚Äî Write complex ETL processes and frameworks for analytics and data management‚Äî Implement large-scale near real-time streaming data processing pipelines‚Äî Work with a team of industry experts on cutting-edge big data technologies to develop solutions for deployment at massive scale Requirements:‚Äî Strong knowledge of Scala‚Äî In-depth knowledge of Hadoop and Spark, experience with data mining and stream processing technologies (Kafka, Spark Streaming, Akka Streams)‚Äî Understanding of the best practices in data quality and quality engineering‚Äî Experience with version control systems, Git in particular ‚Äî Ability to quickly learn new tools and technologies Will be a plus:‚Äî Knowledge of Unix-based operating systems (bash/ssh/ps/grep etc.)‚Äî Experience with Github-based development processes‚Äî Experience with JVM build systems (SBT, Maven, Gradle) We offer:‚Äî Opportunity to work on bleeding-edge projects‚Äî Work with a highly motivated and dedicated team‚Äî Competitive salary‚Äî Flexible schedule‚Äî Benefits program‚Äî Social package ‚Äî medical insurance, sports‚Äî Corporate social events‚Äî Professional development opportunities‚Äî Opportunity for long business trips to the US and possibility for relocation About us:Grid Dynamics is the engineering services company known for transformative, mission-critical cloud solutions for retail, finance and technology sectors. We architected some of the busiest e-commerce services on the Internet and have never had an outage during the peak season. Founded in 2006 and headquartered in San Ramon, California with offices throughout the US and Eastern Europe, we focus on big data analytics, scalable omnichannel services, DevOps, and cloud enablement.",Big Data Engineer with Scala and Spark@Grid Dynamics,https://jobs.dou.ua/companies/grid-dynamics/vacancies/129968/, Kyiv,Big Data Engineer with Scala and Spark,29 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/genesis-technology-partners/,Genesis,{},,"Genesis is one of the largest IT companies in Ukraine with more than 1500 people in 9 countries, who create products for 200 million users monthly. We are the most high-loaded company in the country and one of the largest partner of Facebook, Google, Snapchat and Apple in the CEE region. Our team is one of the best high-tech teams in Eastern Europe. Genesis was recognised by DOU.UA as the Best IT Employer in Ukraine in the category 800 ‚Äî 1500 employees over the last years. We received very high ratings across all major evaluation categories such as professional growth, compensation, working conditions, communication with management and colleagues, etc. What We‚Äôll Expect From You:‚Ä¢ 1-3 years of experience doing quantitative product/marketing analysis;‚Ä¢ 1-3 years of initiating and driving projects to completion with minimal guidance;‚Ä¢ Highly skilled in data visualization tools such as Tableau;‚Ä¢ Experience using product analytics tools like Grafana, Google Analytics;‚Ä¢ Proficiency in using SQL;‚Ä¢ Proficiency in using Python. Who You Are:‚Ä¢ You have a passion for creating and supporting new great products;‚Ä¢ You are highly motivated and hard-working as well as curious and creative at problem-solving;‚Ä¢ You have strong verbal and written communication skills;‚Ä¢ You thrive on collaboration, working side by side with people of all backgrounds and disciplines;‚Ä¢ Analytical mindset; ability to structure information and dive deeper when it is needed. What You‚Äôll Do:‚Ä¢ Passion for digital advertising;‚Ä¢ Be organised and self-motivated;‚Ä¢ Strong analytical skills and ability to make data-driven decisions;‚Ä¢ Entrepreneurial Mindset;‚Ä¢ English ‚Äî Upper Intermediate or higher. Genesis is a unique place for the development and growth with:‚Ä¢ Expertise in the development of high-loaded products in international markets;‚Ä¢ Unique opportunities for learning including various training and seminars within the company, access to a valuable and extensive book library, English courses and participation in key IT industry events worldwide;‚Ä¢ Perfect working conditions: an excellent office in a 5 minutes‚Äô walk from Taras Shevchenko metro station, free food and drinks in the office, medical insurance. The company has professional coaches in football, basketball and running and supports various sports initiatives (swim races, marathons, etc).",Middle/Senior Data Analyst@Genesis,https://jobs.dou.ua/companies/genesis-technology-partners/vacancies/121242/, Kyiv,Middle/Senior Data Analyst,29 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/genesis-technology-partners/,Genesis,{},,"Genesis is one of the largest IT companies in Ukraine with more than 1500 people in 9 countries, who create products for 200 million users monthly. We are the most high-loaded company in the country and one of the largest partner of Facebook, Google, Snapchat and Apple in the CEE region. Our team is one of the best high-tech teams in Eastern Europe. Genesis was recognised by DOU.UA as the Best IT Employer in Ukraine in the category 800 ‚Äî 1500 employees over the last years. We received very high ratings across all major evaluation categories such as professional growth, compensation, working conditions, communication with management and colleagues, etc. So, what You‚Äôll Be Doing:‚Ä¢ Metrics‚Äô monitoring work:‚Ä¢ daily all metrics‚Äô check;‚Ä¢ finding the causes of metrics‚Äô changes.‚Ä¢ Monitoring the integrity and quality of data, which is written to analytics systems.‚Ä¢ Customer‚Äôs management, ideas for growth:‚Ä¢ monitoring of performance indicators;‚Ä¢ building reports for clients;‚Ä¢ ideas for A / B testing. What We‚Äôll Expect from You:‚Ä¢ You have more than a 2 years of working experience in the product company as an analyst;‚Ä¢ You are able to quickly analyze data and make decisions (with a deadline);‚Ä¢ Logical thinking + mathematics‚Äô skills;‚Ä¢ High level of SQL;‚Ä¢ Experience with BI systems (Tableau will be an advantage). Genesis is a unique place for the development and growth with:‚Ä¢ Expertise in the development of high-loaded products in international markets;‚Ä¢ Unique opportunities for learning including various training and seminars within the company, access to a valuable and extensive book library, English courses and participation in key IT industry events worldwide;‚Ä¢ Perfect working conditions: an excellent office in a 5 minutes‚Äô walk from Taras Shevchenko metro station, free food and drinks in the office, medical insurance. The company has professional coaches in football, basketball and running and supports various sports initiatives (swim races, marathons, etc).",Middle Data Analyst@Genesis,https://jobs.dou.ua/companies/genesis-technology-partners/vacancies/123666/, Kyiv,Middle Data Analyst,29 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/unicheck/,Unicheck,"{""Required skills"": [""Previous experience in Software Engineering."", ""5+ years of experience in NLP and 3+ with Python."", ""Good practical and theoretical knowledge of Machine Learning and Deep Learning."", ""Understanding of classic text processing and embeddings."", ""Understanding of unsupervised learning algorithms and Machine Learning metrics."", ""Experience with scikit-learn and any of Keras / Tensorflow / PyTorch."", ""Knowledge of SQL and experience with relational or NoSQL databases."", ""Experience with Git, Linux."", ""Good communication and teamwork skills."", ""Good written and spoken English.""], ""We offer"": [""Professional growth opportunity."", ""Generous holiday allowance."", ""Free English classes."", ""Young and friendly team."", ""Close-knit family atmosphere."", ""Flexible schedule."", ""Comfortable and modern office.""], ""Responsibilities"": [""Lead research projects and specific research areas in the company."", ""Propose new ideas and directions of research, improvements, technical decisions and best practices."", ""Fully cover (develop, maintain and monitor) the entire lifecycle of created models."", ""Propose and realize new ideas to benefit our customers and the company."", ""Create production-ready solutions, dockerize them, launch MVPs, help DevOps Team to deploy them."", ""Share knowledge, ideas and new approaches with team members."", ""Stay up to date with the latest findings in applied data science.""], ""Project description"": [""Unicheck\u2019s R&D team is looking for Middle Data Scientist with NLP background to help us create new solutions for the product.""]}",,"Required skills ‚Äî Previous experience in Software Engineering.‚Äî 5+ years of experience in NLP and 3+ with Python.‚Äî Good practical and theoretical knowledge of Machine Learning and Deep Learning.‚Äî Understanding of classic text processing and embeddings.‚Äî Understanding of unsupervised learning algorithms and Machine Learning metrics.‚Äî Experience with scikit-learn and any of Keras / Tensorflow / PyTorch.‚Äî Knowledge of SQL and experience with relational or NoSQL databases.‚Äî Experience with Git, Linux.‚Äî Good communication and teamwork skills.‚Äî Good written and spoken English. We offer ‚Äî Professional growth opportunity.‚Äî Generous holiday allowance.‚Äî Free English classes.‚Äî Young and friendly team.‚Äî Close-knit family atmosphere.‚Äî Flexible schedule.‚Äî Comfortable and modern office. Responsibilities ‚Äî Lead research projects and specific research areas in the company.‚Äî Propose new ideas and directions of research, improvements, technical decisions and best practices.‚Äî Fully cover (develop, maintain and monitor) the entire lifecycle of created models.‚Äî Propose and realize new ideas to benefit our customers and the company.‚Äî Create production-ready solutions, dockerize them, launch MVPs, help DevOps Team to deploy them.‚Äî Share knowledge, ideas and new approaches with team members.‚Äî Stay up to date with the latest findings in applied data science. Project description Unicheck‚Äôs R&D team is looking for Middle Data Scientist with NLP background to help us create new solutions for the product. Unicheck is an international project developed as a complex plagiarism detection solution for all sorts of academic institutions around the world. Who we are? ‚Äî We are an ever-growing international project with plenty of space for personal and career development.‚Äî Our team builds a community-driven product meaning our product roadmap is being shaped by the actual needs of our users.‚Äî Unicheck is a highly popular anti-plagiarism solution with almost 3 mln, active users, worldwide.‚Äî Unicheck is not solely a plagiarism detection tool. We have several AI-charged products on board aiming at streamlining educators‚Äô routines.‚Äî Everyone‚Äôs opinion matters and is heard. By combining our expertise, we always come to the best possible solution.‚Äî Proactive professionals who are passionate about what they do are at the core of our society and are very welcome to join the team. Why join Unicheck? ‚Äî Opportunities for leveling up your knowledge and skills through challenging projects and tasks;‚Äî The freedom of creativity ‚Äî everyone can offer their own vision of the solution to the problem;‚Äî We invest in your personal and professional development ‚Äî personal development plans, financing courses, and lectures, etc.;‚Äî Initiative is the language we aim to speak ‚Äî speak out, offer a new flow, get your tools in place, implement;‚Äî We are an open and easy-going community of professionals enjoying each other‚Äôs company and inviting you to join.",Senior Data Scientist (NLP)@Unicheck,https://jobs.dou.ua/companies/unicheck/vacancies/121275/, Kyiv,Senior Data Scientist (NLP),28 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/vodafone-ukraine/,Vodafone –£–∫—Ä–∞—ó–Ω–∞,"{""Required skills"": [""\u0417\u043d\u0430\u043d\u043d\u044f \u0442\u0430 \u0440\u043e\u0437\u0443\u043c\u0456\u043d\u043d\u044f \u0442\u0435\u0445\u043d\u043e\u043b\u043e\u0433\u0456\u0439 OLAP"", ""\u0414\u043e\u0441\u0432\u0456\u0434 \u0432 \u0433\u0430\u043b\u0443\u0437\u0456 \u0431\u0443\u0434\u0443\u0432\u0430\u043d\u043d\u044f \u0441\u0445\u043e\u0432\u0438\u0449 \u0434\u0430\u043d\u0438\u0445 \u0442\u0430 \u043f\u0440\u043e\u0446\u0435\u0441\u0456\u0432 \u0432\u0438\u0434\u043e\u0431\u0443\u0442\u043a\u0443, \u043e\u0431\u0440\u043e\u0431\u043a\u0438 \u0442\u0430 \u0442\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0443\u0432\u0430\u043d\u043d\u044f \u0434\u0430\u043d\u0438\u0445"", ""\u0414\u043e\u0441\u0432\u0456\u0434 \u0443 database schema design \u0442\u0430 dimensional data modeling"", ""\u0414\u043e\u0441\u0432\u0456\u0434 \u0442\u0430 \u0433\u043b\u0438\u0431\u043e\u043a\u0456 \u0437\u043d\u0430\u043d\u043d\u044f \u0441\u0438\u0441\u0442\u0435\u043c\u0438 SAS (SAS EG, SAS code, macro language)"", ""\u0414\u043e\u0441\u0432\u0456\u0434 \u0432 \u0440\u043e\u0437\u0440\u043e\u0431\u0446\u0456 ETL \u043f\u0440\u043e\u0446\u0435\u0441\u0456\u0432"", ""\u0417\u043d\u0430\u043d\u043d\u044f \u0442\u0430 \u0434\u043e\u0441\u0432\u0456\u0434 \u0440\u043e\u0431\u043e\u0442\u0438 \u0437 UNIX, Oracle"", ""\u0417\u043d\u0430\u043d\u043d\u044f \u0442\u0435\u0445\u043d\u0456\u0447\u043d\u043e\u0457 \u0430\u043d\u0433\u043b\u0456\u0439\u0441\u044c\u043a\u043e\u0457 \u043c\u043e\u0432\u0438""], ""As a plus"": [""\u0417\u043d\u0430\u043d\u043d\u044f \u043c\u043e\u0432 \u043f\u0440\u043e\u0433\u0440\u0430\u043c\u0443\u0432\u0430\u043d\u043d\u044f SQL, Python, Shell"", ""\u0414\u043e\u0441\u0432\u0456\u0434 \u0443 \u0440\u043e\u0431\u043e\u0442\u0456 \u0437 \u0421\u0423\u0411\u0414 \u0434\u043b\u044f \u0430\u043d\u0430\u043b\u0456\u0437\u0443 \u0432\u0435\u043b\u0438\u043a\u0438\u0445 \u0434\u0430\u043d\u0438\u0445 HP Vertica"", ""\u0417\u043d\u0430\u043d\u043d\u044f Agile \u0442\u0430 \u043f\u0440\u0438\u043d\u0446\u0438\u043f\u0456\u0432 \u0443\u043f\u0440\u0430\u0432\u043b\u0456\u043d\u043d\u044f \u043f\u0440\u043e\u0435\u043a\u0442\u0430\u043c\u0438, \u0440\u043e\u0437\u0443\u043c\u0456\u043d\u043d\u044f \u043f\u0440\u0435\u0434\u043c\u0435\u0442\u043d\u043e\u0457 \u0433\u0430\u043b\u0443\u0437\u0456 \u043c\u043e\u0431\u0456\u043b\u044c\u043d\u043e\u0433\u043e \u0437\u0432\u2019\u044f\u0437\u043a\u0443""], ""We offer"": [""\u041a\u043e\u043b\u0435\u043a\u0442\u0438\u0432 \u043e\u0434\u043d\u043e\u0434\u0443\u043c\u0446\u0456\u0432, \u043f\u0440\u043e\u0437\u043e\u0440\u0456\u0441\u0442\u044c \u0440\u0456\u0448\u0435\u043d\u044c \u0442\u0430 \u0432\u0456\u0434\u043a\u0440\u0438\u0442\u0456\u0441\u0442\u044c \u0432 \u043a\u043e\u043c\u0443\u043d\u0456\u043a\u0430\u0446\u0456\u0457"", ""\u041e\u0444\u0456\u0446\u0456\u0439\u043d\u0435 \u043f\u0440\u0430\u0446\u0435\u0432\u043b\u0430\u0448\u0442\u0443\u0432\u0430\u043d\u043d\u044f, \u0441\u043e\u0446\u0456\u0430\u043b\u044c\u043d\u0456 \u0433\u0430\u0440\u0430\u043d\u0442\u0456\u0457"", ""\u0413\u043d\u0443\u0447\u043a\u0438\u0439 \u0433\u0440\u0430\u0444\u0456\u043a \u0440\u043e\u0431\u043e\u0442\u0438 \u0442\u0430 \u0432\u0456\u0434\u0434\u0430\u043b\u0435\u043d\u0456 \u0440\u043e\u0431\u043e\u0447\u0456 \u0434\u043d\u0456"", ""31 \u043a\u0430\u043b\u0435\u043d\u0434\u0430\u0440\u043d\u0438\u0439 \u0434\u0435\u043d\u044c \u0432\u0456\u0434\u043f\u0443\u0441\u0442\u043a\u0438"", ""\u041c\u0435\u0434\u0438\u0447\u043d\u0435 \u043a\u043e\u0440\u043f\u043e\u0440\u0430\u0442\u0438\u0432\u043d\u0435 \u0441\u0442\u0440\u0430\u0445\u0443\u0432\u0430\u043d\u043d\u044f \u0442\u0430 \u043e\u0437\u0434\u043e\u0440\u043e\u0432\u043b\u0435\u043d\u043d\u044f"", ""\u0411\u0435\u0437\u043a\u043e\u0448\u0442\u043e\u0432\u043d\u0438\u0439 \u043c\u043e\u0431\u0456\u043b\u044c\u043d\u0438\u0439 \u0437\u0432\u2019\u044f\u0437\u043e\u043a"", ""\u0412\u0438\u0433\u0456\u0434\u043d\u0438\u0439 \u0442\u0430\u0440\u0438\u0444\u043d\u0438\u0439 \u043f\u043b\u0430\u043d \u0432\u0456\u0434 Vodafone \u0434\u043b\u044f \u0441\u0456\u043c\u2019\u0457"", ""\u041f\u0440\u043e\u0444\u0435\u0441\u0456\u0439\u043d\u0435 \u043d\u0430\u0432\u0447\u0430\u043d\u043d\u044f \u0437\u0430 \u0440\u0430\u0445\u0443\u043d\u043e\u043a \u0440\u043e\u0431\u043e\u0442\u043e\u0434\u0430\u0432\u0446\u044f"", ""\u0410\u043a\u0442\u0438\u0432\u043d\u0435 \u043a\u043e\u0440\u043f\u043e\u0440\u0430\u0442\u0438\u0432\u043d\u0435 \u0436\u0438\u0442\u0442\u044f""], ""Responsibilities"": [""\u042f\u043a\u0449\u043e \u0442\u0435\u0431\u0435 \u0437\u0430\u0446\u0456\u043a\u0430\u0432\u0438\u043b\u0430 \u0434\u0430\u043d\u0430 \u043f\u043e\u0437\u0438\u0446\u0456\u044f, \u043d\u0430\u0434\u0441\u0438\u043b\u0430\u0439 \u043d\u0430\u043c \u0441\u0432\u043e\u0454 \u0440\u0435\u0437\u044e\u043c\u0435 \u0437 \u043e\u0447\u0456\u043a\u0443\u0432\u0430\u043d\u043d\u044f\u043c\u0438 \u0437\u0430\u0440\u043e\u0431\u0456\u0442\u043d\u043e\u0457 \u043f\u043b\u0430\u0442\u0438.""], ""Project description"": [""\u0412\u0438\u0437\u043d\u0430\u0447\u0435\u043d\u043d\u044f \u0437\u043e\u0432\u043d\u0456\u0448\u043d\u0456\u0445 \u0442\u0430 \u0432\u043d\u0443\u0442\u0440\u0456\u0448\u043d\u0456\u0445 \u0434\u0436\u0435\u0440\u0435\u043b \u0432\u0435\u043b\u0438\u043a\u0438\u0445 \u0434\u0430\u043d\u0438\u0445 \u0434\u043b\u044f \u043f\u0440\u043e\u0432\u0435\u0434\u0435\u043d\u043d\u044f \u0430\u043d\u0430\u043b\u0456\u0442\u0438\u0447\u043d\u0438\u0445 \u0440\u043e\u0431\u0456\u0442"", ""\u041e\u0442\u0440\u0438\u043c\u0430\u043d\u043d\u044f, \u043e\u0431\u0440\u043e\u0431\u043a\u0430 \u0432\u0435\u043b\u0438\u043a\u0438\u0445 \u043e\u0431\u2019\u0454\u043c\u0456\u0432 \u0434\u0430\u043d\u0438\u0445 \u0456\u0437 \u0433\u0435\u0442\u0435\u0440\u043e\u0433\u0435\u043d\u043d\u0438\u0445 \u0434\u0436\u0435\u0440\u0435\u043b \u0434\u0430\u043d\u0438\u0445, \u0440\u043e\u0437\u0440\u043e\u0431\u043a\u0430 \u043c\u0435\u0442\u043e\u0434\u0456\u0432 \u0442\u0430 \u0440\u0435\u0433\u043b\u0430\u043c\u0435\u043d\u0442\u0456\u0432 \u043d\u0435\u043e\u0431\u0445\u0456\u0434\u043d\u0438\u0445 ETL-\u043f\u0440\u043e\u0446\u0435\u0441\u0456\u0432"", ""\u0420\u043e\u0437\u0440\u043e\u0431\u043a\u0430 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u0434\u0430\u043d\u0438\u0445, \u0430\u0434\u0430\u043f\u0442\u043e\u0432\u0430\u043d\u0438\u0445 \u0434\u043e \u0442\u0435\u0445\u043d\u043e\u043b\u043e\u0433\u0456\u0439 \u0432\u0435\u043b\u0438\u043a\u0438\u0445 \u0434\u0430\u043d\u0438\u0445"", ""\u0420\u043e\u0437\u0440\u043e\u0431\u043a\u0430 \u043d\u0435\u043e\u0431\u0445\u0456\u0434\u043d\u0438\u0445 \u0430\u043d\u0430\u043b\u0456\u0442\u0438\u0447\u043d\u0438\u0445 \u0437\u0432\u0456\u0442\u0456\u0432, \u0440\u043e\u0437\u0440\u0456\u0437\u0456\u0432 \u0456 \u0430\u0433\u0440\u0435\u0433\u0430\u0442\u0456\u0432 \u0432\u0435\u043b\u0438\u043a\u0438\u0445 \u0434\u0430\u043d\u0438\u0445"", ""\u041e\u0446\u0456\u043d\u043a\u0430 \u0432\u0456\u0434\u043f\u043e\u0432\u0456\u0434\u043d\u043e\u0441\u0442\u0456 \u043d\u0430\u0431\u043e\u0440\u0456\u0432 \u0434\u0430\u043d\u0438\u0445 \u043f\u0440\u0435\u0434\u043c\u0435\u0442\u043d\u0456\u0439 \u0433\u0430\u043b\u0443\u0437\u0456 \u0442\u0430 \u0437\u0430\u0434\u0430\u0447\u0430\u043c \u0430\u043d\u0430\u043b\u0456\u0442\u0438\u0447\u043d\u0438\u0445 \u0440\u043e\u0431\u0456\u0442"", ""\u041a\u0435\u0440\u0443\u0432\u0430\u043d\u043d\u044f \u0436\u0438\u0442\u0442\u0454\u0432\u0438\u043c \u0446\u0438\u043a\u043b\u043e\u043c \u0434\u0430\u043d\u0438\u0445 (\u043f\u0440\u043e\u0446\u0435\u0441\u0438 \u043e\u0442\u0440\u0438\u043c\u0430\u043d\u043d\u044f, \u0440\u043e\u0437\u043c\u0456\u0449\u0435\u043d\u043d\u044f, \u0437\u0431\u0435\u0440\u0456\u0433\u0430\u043d\u043d\u044f, \u0440\u043e\u0437\u043f\u043e\u0434\u0456\u043b\u0435\u043d\u043d\u044f, \u043c\u0456\u0433\u0440\u0430\u0446\u0456\u0457, \u0430\u0440\u0445\u0456\u0432\u0443\u0432\u0430\u043d\u043d\u044f\u0442\u0430 \u0432\u0438\u0434\u0430\u043b\u0435\u043d\u043d\u044f \u0432\u0435\u043b\u0438\u043a\u0438\u0445 \u0434\u0430\u043d\u0438\u0445)""]}",,"Required skills ‚Ä¢ –ó–Ω–∞–Ω–Ω—è —Ç–∞ —Ä–æ–∑—É–º—ñ–Ω–Ω—è —Ç–µ—Ö–Ω–æ–ª–æ–≥—ñ–π OLAP‚Ä¢ –î–æ—Å–≤—ñ–¥ –≤ –≥–∞–ª—É–∑—ñ –±—É–¥—É–≤–∞–Ω–Ω—è —Å—Ö–æ–≤–∏—â –¥–∞–Ω–∏—Ö —Ç–∞ –ø—Ä–æ—Ü–µ—Å—ñ–≤ –≤–∏–¥–æ–±—É—Ç–∫—É, –æ–±—Ä–æ–±–∫–∏ —Ç–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º—É–≤–∞–Ω–Ω—è –¥–∞–Ω–∏—Ö‚Ä¢ –î–æ—Å–≤—ñ–¥ —É database schema design —Ç–∞ dimensional data modeling‚Ä¢ –î–æ—Å–≤—ñ–¥ —Ç–∞ –≥–ª–∏–±–æ–∫—ñ –∑–Ω–∞–Ω–Ω—è —Å–∏—Å—Ç–µ–º–∏ SAS (SAS EG, SAS code, macro language)‚Ä¢ –î–æ—Å–≤—ñ–¥ –≤ —Ä–æ–∑—Ä–æ–±—Ü—ñ ETL –ø—Ä–æ—Ü–µ—Å—ñ–≤‚Ä¢ –ó–Ω–∞–Ω–Ω—è —Ç–∞ –¥–æ—Å–≤—ñ–¥ —Ä–æ–±–æ—Ç–∏ –∑ UNIX, Oracle‚Ä¢ –ó–Ω–∞–Ω–Ω—è —Ç–µ—Ö–Ω—ñ—á–Ω–æ—ó –∞–Ω–≥–ª—ñ–π—Å—å–∫–æ—ó –º–æ–≤–∏ As a plus ‚Ä¢ –ó–Ω–∞–Ω–Ω—è –º–æ–≤ –ø—Ä–æ–≥—Ä–∞–º—É–≤–∞–Ω–Ω—è SQL, Python, Shell‚Ä¢ –î–æ—Å–≤—ñ–¥ —É —Ä–æ–±–æ—Ç—ñ –∑ –°–£–ë–î –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É –≤–µ–ª–∏–∫–∏—Ö –¥–∞–Ω–∏—Ö HP Vertica‚Ä¢ –ó–Ω–∞–Ω–Ω—è Agile —Ç–∞ –ø—Ä–∏–Ω—Ü–∏–ø—ñ–≤ —É–ø—Ä–∞–≤–ª—ñ–Ω–Ω—è –ø—Ä–æ–µ–∫—Ç–∞–º–∏, —Ä–æ–∑—É–º—ñ–Ω–Ω—è –ø—Ä–µ–¥–º–µ—Ç–Ω–æ—ó –≥–∞–ª—É–∑—ñ –º–æ–±—ñ–ª—å–Ω–æ–≥–æ –∑–≤‚Äô—è–∑–∫—É We offer ‚Ä¢ –ö–æ–ª–µ–∫—Ç–∏–≤ –æ–¥–Ω–æ–¥—É–º—Ü—ñ–≤, –ø—Ä–æ–∑–æ—Ä—ñ—Å—Ç—å —Ä—ñ—à–µ–Ω—å —Ç–∞ –≤—ñ–¥–∫—Ä–∏—Ç—ñ—Å—Ç—å –≤ –∫–æ–º—É–Ω—ñ–∫–∞—Ü—ñ—ó‚Ä¢ –û—Ñ—ñ—Ü—ñ–π–Ω–µ –ø—Ä–∞—Ü–µ–≤–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è, —Å–æ—Ü—ñ–∞–ª—å–Ω—ñ –≥–∞—Ä–∞–Ω—Ç—ñ—ó‚Ä¢ –ì–Ω—É—á–∫–∏–π –≥—Ä–∞—Ñ—ñ–∫ —Ä–æ–±–æ—Ç–∏ —Ç–∞ –≤—ñ–¥–¥–∞–ª–µ–Ω—ñ —Ä–æ–±–æ—á—ñ –¥–Ω—ñ‚Ä¢ 31 –∫–∞–ª–µ–Ω–¥–∞—Ä–Ω–∏–π –¥–µ–Ω—å –≤—ñ–¥–ø—É—Å—Ç–∫–∏‚Ä¢ –ú–µ–¥–∏—á–Ω–µ –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–µ —Å—Ç—Ä–∞—Ö—É–≤–∞–Ω–Ω—è —Ç–∞ –æ–∑–¥–æ—Ä–æ–≤–ª–µ–Ω–Ω—è‚Ä¢ –ë–µ–∑–∫–æ—à—Ç–æ–≤–Ω–∏–π –º–æ–±—ñ–ª—å–Ω–∏–π –∑–≤‚Äô—è–∑–æ–∫‚Ä¢ –í–∏–≥—ñ–¥–Ω–∏–π —Ç–∞—Ä–∏—Ñ–Ω–∏–π –ø–ª–∞–Ω –≤—ñ–¥ Vodafone –¥–ª—è —Å—ñ–º‚Äô—ó‚Ä¢ –ü—Ä–æ—Ñ–µ—Å—ñ–π–Ω–µ –Ω–∞–≤—á–∞–Ω–Ω—è –∑–∞ —Ä–∞—Ö—É–Ω–æ–∫ —Ä–æ–±–æ—Ç–æ–¥–∞–≤—Ü—è‚Ä¢ –ê–∫—Ç–∏–≤–Ω–µ –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–µ –∂–∏—Ç—Ç—è –Ø–∫—â–æ —Ç–µ–±–µ –∑–∞—Ü—ñ–∫–∞–≤–∏–ª–∞ –¥–∞–Ω–∞ –ø–æ–∑–∏—Ü—ñ—è, –Ω–∞–¥—Å–∏–ª–∞–π –Ω–∞–º —Å–≤–æ—î —Ä–µ–∑—é–º–µ –∑ –æ—á—ñ–∫—É–≤–∞–Ω–Ω—è–º–∏ –∑–∞—Ä–æ–±—ñ—Ç–Ω–æ—ó –ø–ª–∞—Ç–∏. Responsibilities ‚Ä¢ –í–∏–∑–Ω–∞—á–µ–Ω–Ω—è –∑–æ–≤–Ω—ñ—à–Ω—ñ—Ö —Ç–∞ –≤–Ω—É—Ç—Ä—ñ—à–Ω—ñ—Ö –¥–∂–µ—Ä–µ–ª –≤–µ–ª–∏–∫–∏—Ö –¥–∞–Ω–∏—Ö –¥–ª—è –ø—Ä–æ–≤–µ–¥–µ–Ω–Ω—è –∞–Ω–∞–ª—ñ—Ç–∏—á–Ω–∏—Ö —Ä–æ–±—ñ—Ç‚Ä¢ –û—Ç—Ä–∏–º–∞–Ω–Ω—è, –æ–±—Ä–æ–±–∫–∞ –≤–µ–ª–∏–∫–∏—Ö –æ–±‚Äô—î–º—ñ–≤ –¥–∞–Ω–∏—Ö —ñ–∑ –≥–µ—Ç–µ—Ä–æ–≥–µ–Ω–Ω–∏—Ö –¥–∂–µ—Ä–µ–ª –¥–∞–Ω–∏—Ö, —Ä–æ–∑—Ä–æ–±–∫–∞ –º–µ—Ç–æ–¥—ñ–≤ —Ç–∞ —Ä–µ–≥–ª–∞–º–µ–Ω—Ç—ñ–≤ –Ω–µ–æ–±—Ö—ñ–¥–Ω–∏—Ö ETL-–ø—Ä–æ—Ü–µ—Å—ñ–≤‚Ä¢ –†–æ–∑—Ä–æ–±–∫–∞ –º–æ–¥–µ–ª–µ–π –¥–∞–Ω–∏—Ö, –∞–¥–∞–ø—Ç–æ–≤–∞–Ω–∏—Ö –¥–æ —Ç–µ—Ö–Ω–æ–ª–æ–≥—ñ–π –≤–µ–ª–∏–∫–∏—Ö –¥–∞–Ω–∏—Ö‚Ä¢ –†–æ–∑—Ä–æ–±–∫–∞ –Ω–µ–æ–±—Ö—ñ–¥–Ω–∏—Ö –∞–Ω–∞–ª—ñ—Ç–∏—á–Ω–∏—Ö –∑–≤—ñ—Ç—ñ–≤, —Ä–æ–∑—Ä—ñ–∑—ñ–≤ —ñ –∞–≥—Ä–µ–≥–∞—Ç—ñ–≤ –≤–µ–ª–∏–∫–∏—Ö –¥–∞–Ω–∏—Ö‚Ä¢ –û—Ü—ñ–Ω–∫–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–æ—Å—Ç—ñ –Ω–∞–±–æ—Ä—ñ–≤ –¥–∞–Ω–∏—Ö –ø—Ä–µ–¥–º–µ—Ç–Ω—ñ–π –≥–∞–ª—É–∑—ñ —Ç–∞ –∑–∞–¥–∞—á–∞–º –∞–Ω–∞–ª—ñ—Ç–∏—á–Ω–∏—Ö —Ä–æ–±—ñ—Ç‚Ä¢ –ö–µ—Ä—É–≤–∞–Ω–Ω—è –∂–∏—Ç—Ç—î–≤–∏–º —Ü–∏–∫–ª–æ–º –¥–∞–Ω–∏—Ö (–ø—Ä–æ—Ü–µ—Å–∏ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è, —Ä–æ–∑–º—ñ—â–µ–Ω–Ω—è, –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è, —Ä–æ–∑–ø–æ–¥—ñ–ª–µ–Ω–Ω—è, –º—ñ–≥—Ä–∞—Ü—ñ—ó, –∞—Ä—Ö—ñ–≤—É–≤–∞–Ω–Ω—è—Ç–∞ –≤–∏–¥–∞–ª–µ–Ω–Ω—è –≤–µ–ª–∏–∫–∏—Ö –¥–∞–Ω–∏—Ö) Project description –ú–∏ –≤—ñ—Ä–∏–º–æ, —â–æ —Ç–∏ –∑–º–æ–∂–µ—à:‚Ä¢ –í–∑–∞—î–º–æ–¥—ñ—è—Ç–∏ –∑ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞–º–∏ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó —Ç–∞ –≤–Ω—É—Ç—Ä—ñ—à–Ω—ñ–º–∏ —ñ –∑–æ–≤–Ω—ñ—à–Ω—ñ–º–∏ –ø–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫–∞–º–∏ –¥–∞–Ω–∏—Ö —ñ–∑ –≥–µ—Ç–µ—Ä–æ–≥–µ–Ω–Ω–∏—Ö –¥–∂–µ—Ä–µ–ª‚Ä¢ –í–∏–∑–Ω–∞—á–∞—Ç–∏ –≤–∏–º–æ–≥–∏ –¥–æ –ø–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫—ñ–≤ –¥–∞–Ω–∏—Ö‚Ä¢ –ü—Ä–æ–≤–æ–¥–∏—Ç–∏ —ñ–Ω—Ç–µ–≥—Ä–∞—Ü—ñ—é –¥–∂–µ—Ä–µ–ª –≤–µ–ª–∏–∫–∏—Ö –æ–±‚Äô—î–º—ñ–≤ –¥–∞–Ω–∏—Ö‚Ä¢ –ü—Ä–∞—Ü—é–≤–∞—Ç–∏ –∑ MPP –æ–±—á–∏—Å–ª—é–≤–∞–ª—å–Ω–∏–º–∏ —Å–∏—Å—Ç–µ–º–∞–º–∏‚Ä¢ –ü–æ—Å—Ç—ñ–π–Ω–æ –≤–∏–≤—á–∞—Ç–∏ —Å—É—á–∞—Å–Ω—ñ –º–µ—Ç–æ–¥–∏ —Ç–∞ —Ç–µ—Ö–Ω–æ–ª–æ–≥—ñ—ó –¥–æ–±—É–≤–∞–Ω–Ω—è —Ç–∞ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –≤–µ–ª–∏–∫–∏—Ö –¥–∞–Ω–∏—Ö",Data Engineer@Vodafone –£–∫—Ä–∞—ó–Ω–∞,https://jobs.dou.ua/companies/vodafone-ukraine/vacancies/128244/, Kyiv,Data Engineer,28 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/flatlogic-llc/,Flatlogic LLC,{},,"–ù–∞–º –±—ã —Ö–æ—Ç–µ–ª–æ—Å—å, —á—Ç–æ–±—ã –í—ã: ‚Äî —É–º–µ–ª–∏ —Ä–∞–∑—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å ETL –ø—Ä–æ—Ü–µ—Å—Å—ã (pandas, apache airflow, scrapy, postgres);‚Äî –∏–º–µ–ª–∏ 2+ –ª–µ—Ç –æ–ø—ã—Ç–∞ (Python)‚Äî –∏–º–µ–ª–∏ –æ–ø—ã—Ç —Å php‚Äî –∏–º–µ–ª–∏ —Ö–æ—Ä–æ—à–∏–µ –∑–Ω–∞–Ω–∏—è sql‚Äî –∞–Ω–≥–ª–∏–π—Å–∫–∏–π (Advanced+)",Python Developer¬†/ Python Data Engineer@Flatlogic LLC,https://jobs.dou.ua/companies/flatlogic-llc/vacancies/133243/, remote,Python Developer¬†/ Python Data Engineer,28 September 2020,–æ—Ç¬†$1500,2020-10-13,,dou
https://jobs.dou.ua/companies/megogonet-/,MEGOGO,{},,"–ù–∞—à –∏–¥–µ–∞–ª—å–Ω—ã–π –∫–∞–Ω–¥–∏–¥–∞—Ç:–ò—â–µ–º Senior Data Engineer, –∫–æ—Ç–æ—Ä–æ–≥–æ –Ω–µ –ø—É–≥–∞–µ—Ç —Ä–∞–±–æ—Ç–∞ —Å —Å–æ—Ç–Ω—è–º–∏ –º–∏–ª–ª–∏–æ–Ω–æ–≤ —Å–æ–±—ã—Ç–∏–π, –∏ –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å –∑–∞ —Å–æ–∑–¥–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –∏ –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∫–∏ —Å–æ–∑–¥–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏ –ø—Ä–æ–≤–µ–¥–µ–Ω–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –±—É–¥—É—Ç –æ–ø—Ä–µ–¥–µ–ª—è—Ç—å —Ä–∞–∑–≤–∏—Ç–∏–µ –æ–¥–Ω–æ–≥–æ –∏–∑ –∫—Ä—É–ø–Ω–µ–π—à–∏—Ö –≤–µ–¥–µ–æ—Å–µ—Ä–≤–∏—Å–æ–≤ –°–ù–ì. –ú—ã –∏–¥–µ–º –ø–æ –ø—É—Ç–∏ data driven –∏ —É –≤–∞—Å –µ—Å—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —Å–≤–æ–∏ –Ω–∞–≤—ã–∫–∏ –∏ —É–º–µ–Ω–∏—è –≤ –Ω–∞—à–µ–π –∫–æ–º–∞–Ω–¥–µ. –ß—Ç–æ –º—ã –∂–¥–µ–º:5+ –ª–µ—Ç –æ–ø—ã—Ç–∞ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è, —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏, —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è, —Ä–∞–∑–≤—ë—Ä—Ç—ã–≤–∞–Ω–∏—è –∏ –ø–æ–¥–¥–µ—Ä–∂–∫–∏ Data Lake/Data Warehouse —Ä–µ—à–µ–Ω–∏–π.–≠–∫—Å–ø–µ—Ä—Ç–Ω—ã–µ –∑–Ω–∞–Ω–∏—è Python –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ —Å–æ–∑–¥–∞–Ω–∏—è ETL data pipelines.–≠–∫—Å–ø–µ—Ä—Ç–Ω—ã–µ –∑–Ω–∞–Ω–∏—è PySpark: –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è, –∞–≥—Ä–µ–≥–∞—Ü–∏–∏, –æ–∫–æ–Ω–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏, –Ω–∞–ø–∏—Å–∞–Ω–∏–µ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è UDF.–≠–∫—Å–ø–µ—Ä—Ç–Ω—ã–µ –∑–Ω–∞–Ω–∏—è SQL/NoSQL –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö: MySQL, PostgreSQL, MongoDB.–û–ø—ã—Ç –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–π —ç–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏–∏ Big Data —Å—Ç–µ–∫–∞: Hadoop, Hive, Kafka, Spark, Cassandra.–û–ø—ã—Ç —Å–æ–∑–¥–∞–Ω–∏—è –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã—Ö –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏—Ö —Å–∏—Å—Ç–µ–º.–£–º–µ–Ω–∏–µ –Ω–∞—Ö–æ–¥–∏—Ç—å –∏ –±—ã—Å—Ç—Ä—ã–µ —Ä–µ—à–µ–Ω–∏—è –¥–ª—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ –∏ —Å—Ç–∞–±–∏–ª—å–Ω—ã–µ production-ready –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –æ–±—Å—Ç–æ—è—Ç–µ–ª—å—Å—Ç–≤. –ë—É–¥–µ—Ç –ø–ª—é—Å–æ–º:–û–ø—ã—Ç —Ä–∞–±–æ—Ç—ã —Å AWS (EC2, EMR, ECS, Kinesis, S3).–û–ø—ã—Ç –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ Java/Scala.–û–ø—ã—Ç —Ä–∞–±–æ—Ç—ã —Å –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–º–∏ –±–∞–∑–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö: Vertica, Exasol, Teradata, Redshift, BigQuery, Druid, Clickhouse.–ù–∞–≤—ã–∫–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –≤ Docker/Kubernetes –æ–∫—Ä—É–∂–µ–Ω–∏–∏.–ù–µ–∑–∞–±—ã—Ç—ã–µ —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç—Å–∫–∏–µ –∑–Ω–∞–Ω–∏—è –ª–∏–Ω–µ–π–∫–∏, –º–∞—Ç–∞–Ω–∞ –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏.–û–ø—ã—Ç —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–æ–¥—É–∫—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º machine learning. –û–±—è–∑–∞–Ω–Ω–æ—Å—Ç–∏:–°–æ–∑–¥–∞–Ω–∏–µ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è –∏–¥–µ–π –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è data driven –≤ –∫–æ–º–ø–∞–Ω–∏–∏.–†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –∏ –≤—ã–±–æ—Ä —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ä–µ—à–µ–Ω–∏–π.–†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –∏ –ø–æ–¥–¥–µ—Ä–∂–∫–∞ –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ DWH.–†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ ETL data pipelines: –∑–∞–≥—Ä—É–∑–∫–∞ –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤, –æ—á–∏—Å—Ç–∫–∞, –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ, –æ–±–æ–≥–∞—â–µ–Ω–∏–µ –≤–Ω–µ—à–Ω–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏, –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è/–¥–µ–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è, —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å —É—á—ë—Ç–æ–º –Ω—é–∞–Ω—Å–æ–≤ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π, –ø–æ–∫—Ä—ã—Ç–∏–µ —Ç–µ—Å—Ç–∞–º–∏, –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥, —ç–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏—è. –ß—Ç–æ –º—ã –ø—Ä–µ–¥–ª–∞–≥–∞–µ–º:‚Äî –†–∞–±–æ—Ç—É –≤ —Å—Ç–∞–±–∏–ª—å–Ω–æ–π –∫–æ–º–ø–∞–Ω–∏–∏ ‚Äî –º—ã –±–æ–ª–µ–µ –≤–æ—Å—å–º–∏ –ª–µ—Ç –Ω–∞ –º–µ–¥–∏–∞—Ä—ã–Ω–∫–µ.‚Äî –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –ø–æ—É—á–∞—Å—Ç–≤–æ–≤–∞—Ç—å –≤ —Å–æ–∑–¥–∞–Ω–∏–∏ –≤–∏–¥–µ–æ—Å–µ—Ä–≤–∏—Å–∞ –±—É–¥—É—â–µ–≥–æ.‚Äî –ö–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å –∑–∞—Ä–∞–±–æ—Ç–Ω–æ–π –ø–ª–∞—Ç—ã.‚Äî –ö—Ä—É—Ç—ã–µ –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤—ã. –ù–∞–ø—Ä–∏–º–µ—Ä, –≤—Å–µ–π –∫–æ–º–∞–Ω–¥–æ–π –Ω–∞ Atlas Weekend:)‚Äî –û—Ç–Ω–æ—à–µ–Ω–∏—è, –ø–æ—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ –Ω–∞ –¥–æ–≤–µ—Ä–∏–∏.‚Äî –ê–∫—Ç–∏–≤–Ω—ã–π –æ—Ç–¥—ã—Ö: –∏–≥—Ä–∞–π –≤ —Ñ—É—Ç–±–æ–ª –∏–ª–∏ –Ω–∞—Å—Ç–æ–ª—å–Ω—ã–π —Ç–µ–Ω–Ω–∏—Å. –£ –Ω–∞—Å –µ—Å—Ç—å —Å–≤–æ—è —Ñ—É—Ç–±–æ–ª—å–Ω–∞—è –∫–æ–º–∞–Ω–¥–∞, –∞ –Ω–∞—à —Ç—Ä–µ–Ω–µ—Ä –ø–æ –ø–∏–Ω–≥-–ø–æ–Ω–≥—É –≤—Å–µ–º—É –Ω–∞—É—á–∏—Ç, –æ–Ω –≤—Å–µ–≥–¥–∞ –≤ –æ—Ñ–∏—Å–µ. –í—ã–±–∏—Ä–∞–π, —á—Ç–æ —Ç–µ–±–µ –ø–æ –¥—É—à–µ, –∏ —É—á–∞—Å—Ç–≤—É–π –≤ —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏—è—Ö! –ï—Å–ª–∏ –æ—á–µ–Ω—å —Ö–æ—á–µ—Ç—Å—è, –¥–∞–∂–µ –µ—Å—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –∏—Å–ø–æ–ª–Ω–∏—Ç—å —Å–≤–æ—é –¥–µ—Ç—Å–∫—É—é –º–µ—á—Ç—É, –µ—Å–ª–∏ —Ö–æ—Ç–µ–ª–æ—Å—å —Å—Ç–∞—Ç—å —Å–ø–æ—Ä—Ç–∏–≤–Ω—ã–º –∫–æ–º–º–µ–Ω—Ç–∞—Ç–æ—Ä–æ–º.‚Äî –í—Å–µ–≥–¥–∞ —Å–≤–µ–∂–∏–µ —Ñ—Ä—É–∫—Ç—ã –≤ –æ—Ñ–∏—Å–µ –∏ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–π –±–∞—Ä–∏—Å—Ç–∞.‚Äî –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è —Ä–∞–∑–≤–∏—Ç–∏—è.‚Äî –ë–µ—Å–ø–ª–∞—Ç–Ω—ã–µ —É—Ä–æ–∫–∏ –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ –≤ –æ—Ñ–∏—Å–µ. –ß—É–≤—Å—Ç–≤—É–µ—à—å, —á—Ç–æ —Ç–≤–æ–∏ –Ω–∞–≤—ã–∫–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º? –•–æ—á–µ—à—å —Å—Ç–∞—Ç—å —á–∞—Å—Ç—å—é –Ω–∞—à–µ–π –∫—Ä—É—Ç–æ–π –∫–æ–º–∞–Ω–¥—ã? –°–∫–æ—Ä–µ–µ –æ—Ç–ø—Ä–∞–≤–ª—è–π —Å–≤–æ–µ —Ä–µ–∑—é–º–µ! –û—Ç–≤–µ—Ç–∏–≤ –Ω–∞ –≤–∞–∫–∞–Ω—Å–∏—é –∏ –æ—Ç–ø—Ä–∞–≤–∏–≤ —Å–≤–æ–µ —Ä–µ–∑—é–º–µ –≤ –ö–æ–º–ø–∞–Ω–∏—é (–û–û–û ¬´–ú–ï–ì–û–ì–û¬ª), –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∏ –¥–µ–π—Å—Ç–≤—É—é—â—É—é –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–æ–º –£–∫—Ä–∞–∏–Ω—ã, —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π –Ω–æ–º–µ—Ä 38347009, –∞–¥—Ä–µ—Å –º–µ—Å—Ç–æ–Ω–∞—Ö–æ–∂–¥–µ–Ω–∏—è: –£–∫—Ä–∞–∏–Ω–∞, –≥. –ö–∏–µ–≤, —É–ª. –ù–æ–≤–æ–∫–æ–Ω—Å—Ç–∞–Ω—Ç–∏–Ω–æ–≤—Å–∫–∞—è, 18 –í (–¥–∞–ª–µ–µ ¬´–ö–æ–º–ø–∞–Ω–∏—è¬ª), –≤—ã –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç–µ –∏ —Å–æ–≥–ª–∞—à–∞–µ—Ç–µ—Å—å —Å —Ç–µ–º, —á—Ç–æ –ö–æ–º–ø–∞–Ω–∏—è –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤–∞—à–∏ –ª–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ –≤ –≤–∞—à–µ–º —Ä–µ–∑—é–º–µ, –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å –ó–∞–∫–æ–Ω–æ–º –£–∫—Ä–∞–∏–Ω—ã ¬´–û –∑–∞—â–∏—Ç–µ –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö¬ª –∏ –ø—Ä–∞–≤–∏–ª–∞–º–∏ GDPR.",Data Engineer@MEGOGO,https://jobs.dou.ua/companies/megogonet-/vacancies/79414/, Kyiv,Data Engineer,28 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/oftengames/,OftenGames,"{""Required skills"": [""Proven working experience as a Data Analyst in GameDev.Strong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy.Good understanding of Free to Play games principles.Be communicative and act as a team player.Desire to suggest, experiment and invent.""], ""As a plus"": [""Prior experience with social casino games. Upper-intermediate English.""], ""We offer"": [""\u0421ompetitive compensation.Challenging work in a startup environment.40-hours working week with a flexible working schedule.Annual paid vacations (20 working days) and paid sick leave.Opportunity for personal and professional growth.""], ""Responsibilities"": [""Research and find insights based on user behavior patterns.Build automated dashboards for visualizing key metrics and trends to support Product and Marketing functions.Analyze A/B tests.""]}",,"Required skills Proven working experience as a Data Analyst in GameDev.Strong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy.Good understanding of Free to Play games principles.Be communicative and act as a team player.Desire to suggest, experiment and invent. As a plus Prior experience with social casino games. Upper-intermediate English. We offer –°ompetitive compensation.Challenging work in a startup environment.40-hours working week with a flexible working schedule.Annual paid vacations (20 working days) and paid sick leave.Opportunity for personal and professional growth. Responsibilities Research and find insights based on user behavior patterns.Build automated dashboards for visualizing key metrics and trends to support Product and Marketing functions.Analyze A/B tests.",Data Analyst¬†/ Game Economist@OftenGames,https://jobs.dou.ua/companies/oftengames/vacancies/133218/," Kyiv, remote",Data Analyst¬†/ Game Economist,28 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/zfort/,Zfort Group,{},,"Zfort Group is looking for a passionate and experienced Data Analyst to join us. Responsibilities include: ‚Äî Crafting and executing queries upon request for data;‚Äî Presenting information through reports and visualization;‚Äî Defining clear requirements and critically access initial requests;‚Äî Design, build and deploy BI solutions (e.g. reporting tools);‚Äî Maintain and support data analytics platforms (e.g. Microsoft Power BI, Tableu, Domo);‚Äî Conduct testing and troubleshooting;‚Äî Verification of presented data. This role requires:‚Äî Analytical and communication skills;‚Äî Knowledge of SQL queries;‚Äî Background in data and/or business analysis will be a plus;‚Äî Excellent English speaking and writing skills. Experience in working with English speaking customers is required. We offer: ‚Äî 15 days of paid planned leave; ‚Äî 20 days of paid planned leave after 2 years of cooperation;‚Äî 5 paid days per year ‚Äî sick leave;‚Äî 10 paid days per year ‚Äî sick leave after 2 years of cooperation;‚Äî training and certifications (paid by the company);‚Äî spacious and comfortable office, flexible working hours. If you are interested, please let us know job@zfort.com.",Data Analyst@Zfort Group,https://jobs.dou.ua/companies/zfort/vacancies/133185/, Kharkiv,Data Analyst,28 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/socialtech/,SocialTech,{},,"SocialTech‚Äî –≥–ª–æ–±–∞–ª—å–Ω–∞—è IT-–∫–æ–º–ø–∞–Ω–∏—è –≤ –∏–Ω–¥—É—Å—Ç—Ä–∏–∏ Social Discovery. –ú—ã —Ä–∞–∑–≤–∏–≤–∞–µ–º —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–¥—É–∫—Ç—ã, –∫–æ—Ç–æ—Ä—ã–º–∏ –ø–æ–ª—å–∑—É—é—Ç—Å—è –º–∏–ª–ª–∏–æ–Ω—ã –ª—é–¥–µ–π –ø–æ –≤—Å–µ–º—É –º–∏—Ä—É. –£ —Ç–µ–±—è –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–π —Å–∫–ª–∞–¥ —É–º–∞, —Ç—ã –ª—é–±–∏—à—å –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ —Ä–µ—à–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏ ‚Äî –ø—Ä–∏—Å–æ–µ–¥–∏–Ω—è–π—Å—è –∫ –Ω–∞–º –∏ –º—ã –≤–º–µ—Å—Ç–µ –ø—Ä–æ–¥–æ–ª–∂–∏–º —Å—Ç—Ä–æ–∏—Ç—å –æ–¥–Ω—É –∏–∑ —Å–∏–ª—å–Ω–µ–π—à–∏—Ö –∫–æ–º–∞–Ω–¥ –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ –≤ —Å—Ç—Ä–∞–Ω–µ. –ß–µ–º –ø—Ä–µ–¥—Å—Ç–æ–∏—Ç –∑–∞–Ω–∏–º–∞—Ç—å—Å—è:‚óè –ü—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –∏–∑ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ —Ä–∞–∑—Ä–æ–∑–Ω–µ–Ω–Ω—ã—Ö, –∫–æ–Ω—Ç—Ä–∏–Ω—Ç—É–∏—Ç–∏–≤–Ω–æ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. ‚óè –û–ø—Ä–µ–¥–µ–ª—è—Ç—å –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≤ –ø–æ–≤–µ–¥–µ–Ω–∏–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∏ –∏–∑–≤–ª–µ–∫–∞—Ç—å –∏–∑ –Ω–∏—Ö –ø–æ–ª—å–∑—É. ‚óè –†–∞–∑–±–∏—Ä–∞—Ç—å—Å—è, –∑–∞ —Å—á–µ—Ç –∫–∞–∫–∏—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –º—ã –ø–æ–ª—É—á–∞–µ–º –æ—Å–Ω–æ–≤–Ω—É—é —á–∞—Å—Ç—å –ø—Ä–∏–±—ã–ª–∏, —Å–∫–æ–ª—å–∫–æ –∏ –∫–∞–∫ –æ–Ω–∏ –∂–∏–≤—É—Ç –Ω–∞ –ø—Ä–æ–¥—É–∫—Ç–µ, —Ä–∞—Å—Ç–µ—Ç –∏–ª–∏ –ø–∞–¥–∞–µ—Ç –∏—Ö –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å. ‚óè –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ä–µ—à–µ–Ω–∏—è –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è –∑–∞–¥–∞—á: –æ—Ü–µ–Ω–∫–∞ A/B-—Ç–µ—Å—Ç–æ–≤, –Ω–∞–ª–∏–≤–∫–∞ —Ç—Ä–∞—Ñ–∏–∫–∞ –≤ AdWords –∏ —Ç.–¥.‚óè –†–∞–∑—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ —É–∑–∫–∏—Ö –º–µ—Å—Ç –≤ –ø—Ä–æ–¥—É–∫—Ç–æ–≤—ã—Ö –≤–æ—Ä–æ–Ω–∫–∞—Ö.‚óè –ü—Ä–æ–¥—É–º–∞—Ç—å –∫–∞–∫ –æ–±–µ—Å–ø–µ—á–∏—Ç—å —Å–Ω–∏–∂–µ–Ω–∏–µ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏ –∏–ª–∏ –ø–æ–≤—ã—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –º–µ—Ç—Ä–∏–∫.‚óè –ü—Ä–æ–≤–æ–¥–∏—Ç—å —Ñ–∞–∫—Ç–æ—Ä–Ω—ã–π, —Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã–π, –∫–æ–≥–æ—Ä—Ç–Ω—ã–π –∏ –¥—Ä—É–≥–∏–µ –≤–∏–¥—ã –∞–Ω–∞–ª–∏–∑–∞. –ß–µ–≥–æ –≤ —Ä–∞–±–æ—Ç–µ –Ω–µ –±—É–¥–µ—Ç:‚óè ML —Ä–∞–¥–∏ ML. –ú—ã –Ω–µ —Ö–æ—Ç–∏–º —Å—Ç—Ä–æ–∏—Ç—å –∫–æ—Å–º–æ–ª–µ—Ç—ã —Ç–∞–º, –≥–¥–µ –º–æ–∂–Ω–æ –æ–±–æ–π—Ç–∏—Å—å —Å–∞–º–æ–∫–∞—Ç–æ–º (–Ω–æ –∏–Ω–æ–≥–¥–∞ –ø—Ä–∏—Ö–æ–¥–∏—Ç—Å—è!).‚óè –ë—é—Ä–æ–∫—Ä–∞—Ç–∏–∏. –û—Ç —Ç–µ–±—è –º—ã –æ–∂–∏–¥–∞–µ–º:‚óè –£–º–µ–Ω–∏–µ —Ä–∞–±–æ—Ç–∞—Ç—å —Å —Ü–∏—Ñ—Ä–∞–º–∏ –∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º—ã—Å–ª–∏—Ç—å —Å–∏—Å—Ç–µ–º–Ω–æ.‚óè –ñ–µ–ª–∞–Ω–∏–µ –±—ã—Ç—å —ç–∫—Å–ø–µ—Ä—Ç–æ–º —Å–≤–æ–µ–≥–æ –¥–µ–ª–∞ –∏ –ø—Ä–∏–º–µ–Ω—è—Ç—å —Å–≤–æ–∏ –Ω–∞–≤—ã–∫–∏ —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å—é.‚óè –£–≤–µ—Ä–µ–Ω–Ω–æ–µ –≤–ª–∞–¥–µ–Ω–∏–µ SQL.‚óè –û–ø—ã—Ç —Ä–∞–±–æ—Ç—ã —Å Python (pandas, seaborn) / R (tidyverse, ggplot).‚óè –û–ø—ã—Ç —Å Tableau / PowerBI / QlikView –±—É–¥–µ—Ç –ø–ª—é—Å–æ–º. –ß—Ç–æ –µ—â–µ –º—ã –º–æ–∂–µ–º –ø—Ä–µ–¥–ª–æ–∂–∏—Ç—å:‚óè –£—é—Ç–Ω—ã–π –æ—Ñ–∏—Å, —Å—Ç—Ä–∞—Ö–æ–≤–∫–∞, –∑–∞–≤—Ç—Ä–∞–∫–∏ / –æ–±–µ–¥—ã / —Å–Ω–µ–∫–∏.‚óè –ö–æ–º–ø–µ–Ω—Å–∞—Ü–∏—é –ª—é–±—ã—Ö —É—á–µ–±–Ω—ã—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤, –∑–∞–Ω—è—Ç–∏–π —Å–ø–æ—Ä—Ç–æ–º –∏ —É—á–∞—Å—Ç–∏—è –≤ –∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏—è—Ö.‚óè –°–ø–ª–æ—á–µ–Ω–Ω—ã–π –∏ –¥—Ä—É–∂–Ω—ã–π –∫–æ–ª–ª–µ–∫—Ç–∏–≤.",Data Analyst (Scientist)@SocialTech,https://jobs.dou.ua/companies/socialtech/vacancies/123061/, Kyiv,Data Analyst (Scientist),28 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/daxx-group/,Daxx,"{""Required skills"": [""Excellent knowledge of Java"", ""Expert in JDBC connectors"", ""AWS EMR (Elastic MapReduce)"", ""Understanding of the CI/CD concept"", ""Presto and Drill deep knowledge"", ""Distributed Computing experience"", ""Master Degree (Computer Science or Computer Science Engineering)"", ""Upper-Intermediate English language (written and spoken)""], ""As a plus"": [""Experience with Atlassian products""], ""We offer"": [""Direct cooperation with the customer"", ""Dedicated HR/ Client Manager"", ""Regular performance reviews"", ""Competitive salary, medical insurance, 20 working vacation days"", ""Regular corporate events, team buildings, etc.""], ""Responsibilities"": [""Currently, the AgileLab team developing software for a big energetic company. We are looking for a Big Data Developer who will develop a Drill plugin together with two teammates. This plugin will be used to connect to microservices, AWS S3, 3rd party databases, data catalogs and etc.""], ""Project description"": [""AgileLab is an Italian R&D company focused on the development of Big Data and AI Applications. They operate in different business sectors and work with well-known big companies on the Italian market (e.g. Unicredit, Brembo).""]}",,"Required skills ‚Äî Excellent knowledge of Java‚Äî Expert in JDBC connectors‚Äî AWS EMR (Elastic MapReduce)‚Äî Understanding of the CI/CD concept‚Äî Presto and Drill deep knowledge‚Äî Distributed Computing experience‚Äî Master Degree (Computer Science or Computer Science Engineering)‚Äî Upper-Intermediate English language (written and spoken) As a plus ‚Äî Experience with Atlassian products We offer ‚Äî Direct cooperation with the customer‚Äî Dedicated HR/ Client Manager‚Äî Regular performance reviews‚Äî Competitive salary, medical insurance, 20 working vacation days‚Äî Regular corporate events, team buildings, etc. Responsibilities Currently, the AgileLab team developing software for a big energetic company. We are looking for a Big Data Developer who will develop a Drill plugin together with two teammates. This plugin will be used to connect to microservices, AWS S3, 3rd party databases, data catalogs and etc. Project description AgileLab is an Italian R&D company focused on the development of Big Data and AI Applications. They operate in different business sectors and work with well-known big companies on the Italian market (e.g. Unicredit, Brembo). AgileLab provides essentially BigData services or platforms, IoT environments, analytical platforms, etc. They are very specialized in BigData and machine learning. AgileLab develops a high- and low-level architecture of the solution, appoint a Team Leader and build teams according to the client‚Äôs goals from scratch.",Senior Big Data (Java) Developer for AgileLab@Daxx,https://jobs.dou.ua/companies/daxx-group/vacancies/133166/," Kyiv, Kharkiv, Lviv, Dnipro",Senior Big Data (Java) Developer for AgileLab,28 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/hey-machine-learning/,Hey Machine Learning,"{""Required skills"": [""\u0443\u0432\u0435\u0440\u0435\u043d\u043d\u043e\u0435 \u0437\u043d\u0430\u043d\u0438\u0435 Python"", ""\u043e\u0442 \u0434\u0432\u0443\u0445 \u043b\u0435\u0442 \u043e\u043f\u044b\u0442\u0430 \u0440\u0430\u0431\u043e\u0442\u044b \u0432 \u043e\u0431\u043b\u0430\u0441\u0442\u0438 \u043c\u0430\u0448\u0438\u043d\u043d\u043e\u0433\u043e \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f"", ""\u0443\u043c\u0435\u043d\u0438\u0435 \u0440\u0435\u0448\u0430\u0442\u044c \u0437\u0430\u0434\u0430\u0447\u0438 \u043a\u043e\u043c\u043f\u044c\u044e\u0435\u0442\u0440\u043d\u043e\u0433\u043e \u0437\u0440\u0435\u043d\u0438\u044f \u0438 NLP""], ""As a plus"": [""\u0437\u043d\u0430\u043d\u0438\u0435 C++"", ""\u043c\u0438\u043d\u0438\u043c\u0430\u043b\u044c\u043d\u044b\u0439 \u043e\u043f\u044b\u0442 \u0432\u0435\u0431-\u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u043a\u0438""], ""We offer"": [""\u0438\u043d\u0442\u0435\u0440\u0435\u0441\u043d\u044b\u0435 \u043f\u0440\u043e\u0435\u043a\u0442\u044b \u0432 \u043e\u0431\u043b\u0430\u0441\u0442\u0438 \u043a\u043e\u043c\u043f\u044c\u044e\u0442\u0435\u0440\u043d\u043e\u0433\u043e \u0437\u0440\u0435\u043d\u0438\u044f \u0438 NLP"", ""\u043a\u043e\u043c\u0444\u043e\u0440\u0442\u043d\u044b\u0439 \u043e\u0444\u0438\u0441 \u0432 \u0446\u0435\u043d\u0442\u0440\u0435 \u0425\u0430\u0440\u044c\u043a\u043e\u0432\u0430 \u0432 \u0434\u0432\u0443\u0445 \u043c\u0438\u043d\u0443\u0442\u0430\u0445 \u043e\u0442 \u043c. \u0410\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u043e\u0440\u0430 \u0411\u0435\u043a\u0435\u0442\u043e\u0432\u0430"", ""\u0447\u0430\u0439, \u043a\u043e\u0444\u0435, \u043f\u0435\u0447\u0435\u043d\u044c\u0435"", ""\u0417\u041f \u043e\u0431\u0441\u0443\u0436\u0434\u0430\u0435\u0442\u0441\u044f \u043f\u043e \u0438\u0442\u043e\u0433\u0443 \u0441\u043e\u0431\u0435\u0441\u0435\u0434\u043e\u0432\u0430\u043d\u0438\u044f""], ""Responsibilities"": [""\u043f\u0438\u0441\u0430\u0442\u044c \u043a\u043e\u0434"", ""\u044d\u0441\u0442\u0438\u043c\u0435\u0439\u0442\u0438\u0442\u044c \u043d\u043e\u0432\u044b\u0435 \u043f\u0440\u043e\u0435\u043a\u0442\u044b"", ""\u0431\u044b\u0442\u044c \u043d\u0430\u0441\u0442\u0430\u0432\u043d\u0438\u043a\u043e\u043c \u0434\u043b\u044f \u043c\u043e\u043b\u043e\u0434\u044b\u0445 \u043a\u043e\u043b\u043b\u0435\u0433""], ""Project description"": [""\u041f\u0440\u043e\u0435\u043a\u0442\u044b \u0432 \u043e\u0431\u043b\u0430\u0441\u0442\u0438 \u043a\u043e\u043c\u043f\u044c\u044e\u0442\u0435\u0440\u043d\u043e\u0433\u043e \u0437\u0440\u0435\u043d\u0438\u044f \u0438 NLP""]}",,"Required skills ‚Äî —É–≤–µ—Ä–µ–Ω–Ω–æ–µ –∑–Ω–∞–Ω–∏–µ Python‚Äî –æ—Ç –¥–≤—É—Ö –ª–µ—Ç –æ–ø—ã—Ç–∞ —Ä–∞–±–æ—Ç—ã –≤ –æ–±–ª–∞—Å—Ç–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è‚Äî —É–º–µ–Ω–∏–µ —Ä–µ—à–∞—Ç—å –∑–∞–¥–∞—á–∏ –∫–æ–º–ø—å—é–µ—Ç—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –∏ NLP As a plus ‚Äî –∑–Ω–∞–Ω–∏–µ C++‚Äî –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –æ–ø—ã—Ç –≤–µ–±-—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ We offer ‚Äî –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–µ –ø—Ä–æ–µ–∫—Ç—ã –≤ –æ–±–ª–∞—Å—Ç–∏ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –∏ NLP‚Äî –∫–æ–º—Ñ–æ—Ä—Ç–Ω—ã–π –æ—Ñ–∏—Å –≤ —Ü–µ–Ω—Ç—Ä–µ –•–∞—Ä—å–∫–æ–≤–∞ –≤ –¥–≤—É—Ö –º–∏–Ω—É—Ç–∞—Ö –æ—Ç –º. –ê—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä–∞ –ë–µ–∫–µ—Ç–æ–≤–∞‚Äî —á–∞–π, –∫–æ—Ñ–µ, –ø–µ—á–µ–Ω—å–µ‚Äî –ó–ü –æ–±—Å—É–∂–¥–∞–µ—Ç—Å—è –ø–æ –∏—Ç–æ–≥—É —Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏—è Responsibilities ‚Äî –ø–∏—Å–∞—Ç—å –∫–æ–¥‚Äî —ç—Å—Ç–∏–º–µ–π—Ç–∏—Ç—å –Ω–æ–≤—ã–µ –ø—Ä–æ–µ–∫—Ç—ã‚Äî –±—ã—Ç—å –Ω–∞—Å—Ç–∞–≤–Ω–∏–∫–æ–º –¥–ª—è –º–æ–ª–æ–¥—ã—Ö –∫–æ–ª–ª–µ–≥ Project description –ü—Ä–æ–µ–∫—Ç—ã –≤ –æ–±–ª–∞—Å—Ç–∏ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –∏ NLP",Machine Learning/Data Science engineer@Hey Machine Learning,https://jobs.dou.ua/companies/hey-machine-learning/vacancies/133160/, Kharkiv,Machine Learning/Data Science engineer,27 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/data-science-ua/,Data Science UA,"{""Required skills"": [""We\u2019re looking for a Senior level team leader with experience in working in an agile environment withcontinuous integration tools like Github.You also:\u25cf Preferably a master\u2019s degree in CS. Bachelor\u2019s degree also considered\u25cf 5+ years overall working with JS\u25cf 3+ years of experience as full stack engineer\u25cf Proven leadership experience\u25cf Knowledge set: JS, react, MobX, ExpressJS, NodeJS, TypeScript, D3js, DB, SQL, expert onVanilla JS, NPM Package experience, BE and FE experience in building complex systems""], ""As a plus"": [""\u25cf TDD mindset\u25cf Infrastructure\u25cf CI/CD understanding\u25cf Docker understanding\u25cf Kubernetes experience as a plus / Singularity\u25cf Prometheus""], ""We offer"": [""\u25cf Flexible working\u25cf Sharing culture\u25cf Diversity\u25cf Days off""], ""Responsibilities"": [""What you will be doing:""], ""Project description"": [""\u25cf You will help architect and build innovative products to enhance and extend search data\u25cf You will lead a team of Full Stack Application Engineers\u25cf You will experiment with new tools and technologies to produce cutting-edge solutions tobusiness problems\u25cf You will lead a pro-active, results-oriented agile team\u25cf You will be responsible for the line management of the team and will provide technical andmanagement support/coaching to continuously improve the team capabilities and grow theindividual team members skill sets\u25cf Your role is to work closely with the product lead for Sense to plan and build a world classSaas platform\u25cf You will be collaborating with the other engineering team leads to identify dependenciesacross teams and work with product and PMO leads to successfully plan roadmap delivery asa group""]}",,"Required skills We‚Äôre looking for a Senior level team leader with experience in working in an agile environment withcontinuous integration tools like Github.You also:‚óè Preferably a master‚Äôs degree in CS. Bachelor‚Äôs degree also considered‚óè 5+ years overall working with JS‚óè 3+ years of experience as full stack engineer‚óè Proven leadership experience‚óè Knowledge set: JS, react, MobX, ExpressJS, NodeJS, TypeScript, D3js, DB, SQL, expert onVanilla JS, NPM Package experience, BE and FE experience in building complex systems As a plus ‚óè TDD mindset‚óè Infrastructure‚óè CI/CD understanding‚óè Docker understanding‚óè Kubernetes experience as a plus / Singularity‚óè Prometheus We offer ‚óè Flexible working‚óè Sharing culture‚óè Diversity‚óè Days off Responsibilities What you will be doing: ‚óè You will help architect and build innovative products to enhance and extend search data‚óè You will lead a team of Full Stack Application Engineers‚óè You will experiment with new tools and technologies to produce cutting-edge solutions tobusiness problems‚óè You will lead a pro-active, results-oriented agile team‚óè You will be responsible for the line management of the team and will provide technical andmanagement support/coaching to continuously improve the team capabilities and grow theindividual team members skill sets‚óè Your role is to work closely with the product lead for Sense to plan and build a world classSaas platform‚óè You will be collaborating with the other engineering team leads to identify dependenciesacross teams and work with product and PMO leads to successfully plan roadmap delivery asa group Project description We are looking for a Team Lead that will work as a part of a team of multi-disciplined engineers. This is product-based company. The team is responsible for providing a full-stack self-service solution to clients and partners for planning, activation, measurement, and insights, tapping into the best of company‚Äôs proprietary data and analysis",Technical Lead (Node/React)@Data Science UA,https://jobs.dou.ua/companies/data-science-ua/vacancies/129589/, Kyiv,Technical Lead (Node/React),27 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/cqg/,CQG,"{""Required skills"": [""Experience with databases with high-transaction volume in a fast-paced, globally distributed highly-available (24\u00d77) production environment is required."", ""Advanced understanding and development experience with NoSQL Cassandra/Datastax DB is required."", ""Hands-on experience in setting up a Cassandra Database in an enterprise, Design and Support data migration on a worldwide multi-datacenter cluster is required."", ""Experience with backup & recovery, security, monitoring, performance tuning and managing Cassandra cluster is required."", ""Strong understanding of complex inner workings of Cassandra, such as the gossip protocol, hinted handoffs, read repairs, Merkle trees is required."", ""Strong understanding and experience with JVM tuning."", ""Five years\u2019 experience using Linux with scripting or python is required."", ""Proficiency in English written and oral communication is required.""], ""As a plus"": [""Ansible experience is strongly desired."", ""Demonstrated knowledge of Server Administration, Security Administration, Network Administration and Application Development is preferred."", ""Excellent problem-solving skills, a self-starting attitude and good communication skills are essential"", ""Must be able to work in a team-oriented environment and as a member of the production support team"", ""Must demonstrate leadership skills without requiring positional authority""], ""We offer"": [""CQG provides a variety of benefits to enhance your personal and financial well-being. Employees and eligible dependents may participate in the following:"", ""Career and professional opportunities;"", ""Work in multicultural environment;"", ""Completely \u201cWhite\u201d and competitive salary;"", ""Employment in accordance with Ukrainian labor legislation;"", ""Full medical insurance for employee and family (dental insurance is included);""], ""Responsibilities"": [""Keep abreast of Data Base technologies and provide Designs, guidance and support forother DBA\u2019s in order to build, deploy and support our production, stage, test,and development operations of database back end systems while focusing on monitoring,disaster recovery, security, integrity, stability, and scalability"", ""Responsible for performance tuning and optimization of production, stage, test, anddevelopment operations of database back end systems including monitoring,disaster recovery, security, integrity, stability, and scalability"", ""Advanced troubleshooting including diagnostics andresolution of issues"", ""Automate day to day management functions"", ""Communicate issues that arise from day to daymanagement functions"", ""Design or validate designs of custom databasesystems"", ""Responsible for code, script, documentation andother artifact generation of custom database systems"", ""Mentor and advise on best practices for enterprisedata management to the team"", ""Off hours support including being on call whenneeded""], ""Project description"": [""Summary:Responsible for implementation, development, management, support, upgrade and monitoring of database environments supporting CQG applications, infrastructure systems and services. Define enterprise architecture and mentor the architecture team in the development of architecture and design of multi-tier and federated back end database systems and clients for CQG applications. Lead development activities revolving around DB\u2019s by providing mentoring, support, and decisions for database administrators, developers, and other internal customers.""]}",,"Required skills ‚Ä¢ Experience with databases with high-transaction volume in a fast-paced, globally distributed highly-available (24√ó7) production environment is required.‚Ä¢ Advanced understanding and development experience with NoSQL Cassandra/Datastax DB is required.‚Ä¢ Hands-on experience in setting up a Cassandra Database in an enterprise, Design and Support data migration on a worldwide multi-datacenter cluster is required.‚Ä¢ Experience with backup & recovery, security, monitoring, performance tuning and managing Cassandra cluster is required.‚Ä¢ Strong understanding of complex inner workings of Cassandra, such as the gossip protocol, hinted handoffs, read repairs, Merkle trees is required.‚Ä¢ Strong understanding and experience with JVM tuning.‚Ä¢ Five years‚Äô experience using Linux with scripting or python is required.‚Ä¢ Proficiency in English written and oral communication is required. As a plus ‚Ä¢ Ansible experience is strongly desired.‚Ä¢ Demonstrated knowledge of Server Administration, Security Administration, Network Administration and Application Development is preferred.‚Ä¢ Excellent problem-solving skills, a self-starting attitude and good communication skills are essential‚Ä¢ Must be able to work in a team-oriented environment and as a member of the production support team‚Ä¢ Must demonstrate leadership skills without requiring positional authority We offer CQG provides a variety of benefits to enhance your personal and financial well-being. Employees and eligible dependents may participate in the following:‚Ä¢ Career and professional opportunities;‚Ä¢ Work in multicultural environment;‚Ä¢ Completely ‚ÄúWhite‚Äù and competitive salary;‚Ä¢ Employment in accordance with Ukrainian labor legislation;‚Ä¢ Full medical insurance for employee and family (dental insurance is included); Responsibilities ‚Ä¢ Keep abreast of Data Base technologies and provide Designs, guidance and support forother DBA‚Äôs in order to build, deploy and support our production, stage, test,and development operations of database back end systems while focusing on monitoring,disaster recovery, security, integrity, stability, and scalability ‚Ä¢ Responsible for performance tuning and optimization of production, stage, test, anddevelopment operations of database back end systems including monitoring,disaster recovery, security, integrity, stability, and scalability ‚Ä¢ Advanced troubleshooting including diagnostics andresolution of issues ‚Ä¢ Automate day to day management functions ‚Ä¢ Communicate issues that arise from day to daymanagement functions ‚Ä¢ Design or validate designs of custom databasesystems ‚Ä¢ Responsible for code, script, documentation andother artifact generation of custom database systems ‚Ä¢ Mentor and advise on best practices for enterprisedata management to the team ‚Ä¢ Off hours support including being on call whenneeded Project description Summary:Responsible for implementation, development, management, support, upgrade and monitoring of database environments supporting CQG applications, infrastructure systems and services. Define enterprise architecture and mentor the architecture team in the development of architecture and design of multi-tier and federated back end database systems and clients for CQG applications. Lead development activities revolving around DB‚Äôs by providing mentoring, support, and decisions for database administrators, developers, and other internal customers. About us CQG, Inc. was founded more than 30 years ago in the United States. We are one of the world leaders in the delivery of market data and financial information, analysis technologies and stock trading. More than 500 people work in 15 offices around the world ‚Äî from Denver and Chicago to Sydney and Singapore. The development offices are located in Denver, Moscow, Kiev, Samara, Yerevan and Zelenograd. Kiev office was opened in 2004 and now has about 30 people. Our friendly team for more than 10 years working on server solutions for stock trading, client applications for the analysis of stock quotes and many others. We‚Äôre looking for experienced motivated experts who will be interested in joining our team.",Senior DBA &¬†Data Architect (Cassandra)@CQG,https://jobs.dou.ua/companies/cqg/vacancies/60961/, Kyiv,Senior DBA &¬†Data Architect (Cassandra),26 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/windsor-ai/,Windsor.ai,"{""Required skills"": [""Python, R, Bash, Linux, Postgres, Pandas""], ""As a plus"": [""We are looking for an experienced engineer to join our team. We use docker and bitbucket pipelines for continuous integration.Interest in joining a startup and contribute to its growth and building a good product.""], ""We offer"": [""Analytics and visualisation skills, data-science.Scientific and curious mindset.Good programming practices, writing tests etc.Docker experience""], ""Responsibilities"": [""Great team and the opportunity to be part of one of the fastest growing european Analytics data companies.""], ""Project description"": [""Opportunity to travel and meet clients if desired.""]}",,"Required skills Python, R, Bash, Linux, Postgres, Pandas We are looking for an experienced engineer to join our team. We use docker and bitbucket pipelines for continuous integration.Interest in joining a startup and contribute to its growth and building a good product. As a plus Analytics and visualisation skills, data-science.Scientific and curious mindset.Good programming practices, writing tests etc.Docker experience We offer Great team and the opportunity to be part of one of the fastest growing european Analytics data companies. Opportunity to travel and meet clients if desired. Responsibilities -optimise data-architectures and pipelines-Improve algorithms, both R and python-Develop AI models for advertising Project description We pull data from many different marketing platforms. We join all this data and do attribution modelling. Then we propose better budget allocations to the clients. Some of the Worlds biggest advertisers use our platform. We are looking for skilled and talented engineers who want to be part of a fast-growing start-up for the long term.","Data Engineer/Scientist, Marketing data analytics SAAS company@Windsor.ai",https://jobs.dou.ua/companies/windsor-ai/vacancies/75673/," Kyiv, remote","Data Engineer/Scientist, Marketing data analytics SAAS company",26 September 2020,$2500‚Äì3500,2020-10-13,,dou
https://jobs.dou.ua/companies/grid-dynamics/,Grid Dynamics,{},,"As a leader of the Data Science team, you will run the development of next-generation platforms for customers micro-segmentation, clusterization, behavior analysis, and prediction as well as building up new improved recommendation systems for the customers.You will use your expertise to design data science models and frameworks, data processing pipelines, and lead efforts of the ML models productization. You will use project management skills to manage the deliverables and ensure all stakeholder work is complete on time. You will also manage stakeholder communications and present your project across a wide spectrum of management levels. Lead Data Scientist is supposed to handle complex problems independently and demonstrate analytical thinking. At this position, you should be able to make judgments and recommendations based on the analysis and interpretation of data. The position requires excellent communication skills and experience working directly with technical teams as well as with business stakeholders. Being able to present findings in a meaningful and clear way is a must. Responsibilities: ‚Äî Lead a team of skilled data scientists and data engineers ‚Äî Drive the collection, cleaning, processing, and analysis of new and existing data sources.‚Äî Communicate with business stakeholders to clarify their requirements and present the teamwork results.‚Äî Learn & stay current on developments in one or more analytics domains: Optimization, Machine Learning, Deep Learning / AI, Simulation, etc. Requirements: ‚Äî Data scientist with 5+ years of experience ‚Äî Solid understanding of Statistics, Machine Learning and Deep Learning‚Äî Hands-on experience in Python‚Äî Experience with Recommender Systems (Content-Based, Collaborative Filtering, Hybrid, Market Basket Analysis, Repeat Purchase)‚Äî Experience with look-alike modeling and sequential-input models, RNNs (LSTM, GRU, etc.)‚Äî Expertise in building, productionising and scaling analytics solutions for big data problems‚Äî Experience with SQL databases‚Äî Familiarity with cloud ML Platforms (Google AI Platform, Amazon Sagemaker, MS Azure AI Platform) is a plus.‚Äî Hands-on experience with data preparation, cleansing, feature engineering, and visualization‚Äî Great communication and presentation skills. Proven ability to present progress made by the team to senior business management and the project stakeholders‚Äî Understanding of and experience with customer intelligence & marketing domains a plus‚Äî Experience in working across different global cultures a plus We offer: ‚Äî Opportunity to work on bleeding-edge projects‚Äî Work with a highly motivated and dedicated team‚Äî Flexible schedule‚Äî Medical insurance‚Äî Benefits program‚Äî Corporate social events‚Äî Professional development opportunities About us: Grid Dynamics is the engineering services company known for transformative, mission-critical cloud solutions for retail, finance and technology sectors. We architected some of the busiest e-commerce services on the Internet and have never had an outage during the peak season. Founded in 2006 and headquartered in San Ramon, California with offices throughout the US and Eastern Europe, we focus on big data analytics, scalable omnichannel services, DevOps, and cloud enablement.",Senior Data Scientist@Grid Dynamics,https://jobs.dou.ua/companies/grid-dynamics/vacancies/133140/," Kyiv, Kharkiv, Lviv",Senior Data Scientist,25 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/fozzy/,"Fozzy Group, –ö–æ—Ä–ø–æ—Ä–∞—Ü—ñ—è","{""Required skills"": [""\u043e\u0442 3+ \u043b\u0435\u0442 \u043e\u043f\u044b\u0442\u0430 \u0440\u0430\u0431\u043e\u0442\u044b \u0432 \u0440\u043e\u043b\u0438 Computer Vision Engineer"", ""\u043e\u043f\u044b\u0442 \u0432 \u0431\u043e\u043b\u044c\u0448\u0438\u043d\u0441\u0442\u0432\u0435 \u0438\u0437 \u043f\u0435\u0440\u0435\u0447\u0438\u0441\u043b\u0435\u043d\u043d\u044b\u0445 \u0437\u0430\u0434\u0430\u0447:depth estimation, 3d reconstruction, object detection, face and emotions recognition, SLAM, feature extraction"", ""\u0432\u044b\u0441\u043e\u043a\u0438\u0439 \u0443\u0440\u043e\u0432\u0435\u043d\u044c \u0432\u043b\u0430\u0434\u0435\u043d\u0438\u044f Python 3"", ""\u043e\u043f\u044b\u0442 \u043f\u0440\u0438\u043c\u0435\u043d\u0435\u043d\u0438\u044f DNN \u0430\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440, \u043f\u043e\u043d\u0438\u043c\u0430\u043d\u0438\u0435 \u0438\u0445 \u043e\u0441\u043e\u0431\u0435\u043d\u043d\u043e\u0441\u0442\u0435\u0439 \u0438 \u0441\u0444\u0435\u0440 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u044f"", ""\u0432\u043b\u0430\u0434\u0435\u043d\u0438\u0435 ML/CV \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0430\u043c\u0438 OpenCV, Pandas, Numpy, Tensorflow/Pytorch"", ""R&D: \u0443\u043c\u0435\u043d\u0438\u0435 \u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c \u0441 \u043d\u0430\u0443\u0447\u043d\u044b\u043c\u0438 \u0441\u0442\u0430\u0442\u044c\u044f\u043c\u0438, \u043e\u043f\u044b\u0442 \u0441\u0430\u043c\u043e\u0441\u0442\u043e\u044f\u0442\u0435\u043b\u044c\u043d\u044b\u0445 \u0438\u0441\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u043d\u0438\u0439""], ""As a plus"": [""\u041a\u043e\u043c\u043f\u044c\u044e\u0442\u0435\u0440\u043d\u043e\u0435 \u0441\u0442\u0435\u0440\u0435\u043e\u0437\u0440\u0435\u043d\u0438\u0435, \u0444\u043e\u0442\u043e\u0433\u0440\u0430\u043c\u043c\u0435\u0442\u0440\u0438\u044f, \u043f\u0440\u043e\u0435\u043a\u0442\u0438\u0432\u043d\u0430\u044f \u0433\u0435\u043e\u043c\u0435\u0442\u0440\u0438\u044f"", ""\u0412\u044b\u0441\u0448\u0435\u0435 \u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u0435 \u043f\u043e \u0444\u0438\u0437-\u043c\u0430\u0442 \u043d\u0430\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u044e"", ""\u0420\u0430\u0431\u043e\u0442\u0430 \u0441 IP \u043a\u0430\u043c\u0435\u0440\u0430\u043c\u0438 \u0432\u0438\u0434\u0435\u043e\u043d\u0430\u0431\u043b\u044e\u0434\u0435\u043d\u0438\u044f \u0438 \u0437\u043d\u0430\u043d\u0438\u0435 camera models(pinhole, fisheye, etc)"", ""\u041e\u043f\u044b\u0442 \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u044f \u0441\u0435\u0440\u0432\u0438\u0441\u043e\u0432, \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f Flask/Django \u0438 Docker."", ""\u041e\u043f\u044b\u0442 \u0440\u0430\u0431\u043e\u0442\u044b \u0441 Redis/Kafka/Rabbitmq"", ""\u041d\u0430\u0432\u044b\u043a\u0438 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u044f MySQL/PostgreSQL \u0438 MongoDB""], ""We offer"": [""\u0418\u043d\u0442\u0435\u0440\u0435\u0441\u043d\u044b\u0435 \u0438 \u0440\u0430\u0437\u043d\u043e\u043e\u0431\u0440\u0430\u0437\u043d\u044b\u0435 \u043f\u0440\u043e\u0435\u043a\u0442\u044b"", ""\u0421\u043f\u0440\u0430\u0432\u0435\u0434\u043b\u0438\u0432\u0430\u044f \u0438 \u0441\u0432\u043e\u0435\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u0430\u044f \u0437\u0430\u0440\u043f\u043b\u0430\u0442\u0430"", ""\u041f\u0440\u043e\u0444\u0435\u0441\u0441\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u0443\u044e \u043a\u043e\u043c\u0430\u043d\u0434\u0443 \u0443\u043c\u043d\u044b\u0445, \u043b\u044e\u0431\u043e\u0437\u043d\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u0445 \u043a\u043e\u043b\u043b\u0435\u0433"", ""\u041f\u043b\u043e\u0449\u0430\u0434\u043a\u0430 \u0434\u043b\u044f \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0438 \u0441\u0430\u043c\u044b\u0445 \u0441\u043c\u0435\u043b\u044b\u0445 \u0433\u0438\u043f\u043e\u0442\u0435\u0437"", ""\u0411\u044e\u0434\u0436\u0435\u0442 \u043d\u0430 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u043d\u043e\u0432\u043e\u043c\u0443, (\u043a\u0443\u0440\u0441\u044b, \u043a\u043e\u043d\u0444\u0435\u0440\u0435\u043d\u0446\u0438\u0438 \u0438 \u0442.\u0434.)"", ""5 \u0434\u043d\u0435\u0432\u043d\u0430\u044f \u0440\u0430\u0431\u043e\u0447\u0430\u044f \u043d\u0435\u0434\u0435\u043b\u044f \u0441 \u0433\u0438\u0431\u043a\u0438\u043c \u0433\u0440\u0430\u0444\u0438\u043a\u043e\u043c"", ""\u0412\u043e\u0437\u043c\u043e\u0436\u043d\u043e\u0441\u0442\u044c \u0443\u0434\u0430\u043b\u0435\u043d\u043d\u043e\u0439 \u0440\u0430\u0431\u043e\u0442\u044b"", ""\u0434\u043e\u0431\u0440\u043e\u0432\u043e\u043b\u044c\u043d\u043e\u0435 \u043c\u0435\u0434\u0438\u0446\u0438\u043d\u0441\u043a\u043e\u0435 \u0441\u0442\u0440\u0430\u0445\u043e\u0432\u0430\u043d\u0438\u0435"", ""\u0441\u043a\u0438\u0434\u043a\u0438 \u0434\u043b\u044f \u0441\u043e\u0442\u0440\u0443\u0434\u043d\u0438\u043a\u043e\u0432 \u0441\u0435\u0442\u0438 (\u0440\u0435\u0441\u0442\u043e\u0440\u0430\u043d\u044b, \u0430\u043f\u0442\u0435\u043a\u0438, \u0441\u0443\u043f\u0435\u0440\u043c\u0430\u0440\u043a\u0435\u0442\u044b \u0438 \u0442\u0443\u0440\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u043f\u043e\u0435\u0437\u0434\u043a\u0438)"", ""\u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0438\u0435 \u0434\u0440\u0435\u0441\u0441-\u043a\u043e\u0434\u0430"", ""\u041a\u043e\u043c\u0444\u043e\u0440\u0442\u043d\u044b\u0439 \u0438 \u0441\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0439 \u043e\u0444\u0438\u0441 \u043d\u0430 \u043b\u0435\u0432\u043e\u043c \u0431\u0435\u0440\u0435\u0433\u0443 \u0414\u043d\u0435\u043f\u0440\u0430, \u0440\u044f\u0434\u043e\u043c \u0441 \u043c\u043e\u0441\u0442\u043e\u043c \u041f\u0430\u0442\u043e\u043d\u0430, (\u0422\u0426 Silver Breeze)""], ""Responsibilities"": [""\u0418\u0441\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u044c state of the art Computer Vision \u0442\u0435\u0445\u043d\u043e\u043b\u043e\u0433\u0438\u0438 \u043f\u0440\u0438\u043c\u0435\u043d\u0438\u043c\u044b\u0435 \u0432 \u0440\u0438\u0442\u0435\u0439\u043b\u0435"", ""\u0421\u0442\u0440\u043e\u0438\u0442\u044c, \u043e\u0431\u0443\u0447\u0430\u0442\u044c, \u0442\u0435\u0441\u0442\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0433\u043b\u0443\u0431\u043e\u043a\u0438\u0435 \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u044b\u0435 \u0441\u0435\u0442\u0438"", ""\u041e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439"", ""\u041e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u044f \u0441\u0438\u0441\u0442\u0435\u043c \u0441 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435\u043c \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u043e\u0432 \u043a\u043b\u0430\u0441\u0441\u0438\u0447\u0435\u0441\u043a\u043e\u0433\u043e \u043c\u0430\u0448\u0438\u043d\u043d\u043e\u0433\u043e \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f"", ""\u0421\u043e\u0437\u0434\u0430\u043d\u0438\u0435 API \u0434\u043b\u044f \u0434\u043e\u0441\u0442\u0430\u0432\u043a\u0438 \u043e\u0431\u0443\u0447\u0435\u043d\u043d\u044b\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439""], ""Project description"": [""\u041c\u044b \u0438\u0449\u0435\u043c \u0432 \u043d\u0430\u0448\u0443 \u043a\u043e\u043c\u0430\u043d\u0434\u0443 \u0441\u043f\u0435\u0446\u0438\u0430\u043b\u0438\u0441\u0442\u0430 \u043f\u043e \u043a\u043e\u043c\u043f\u044c\u044e\u0442\u0435\u0440\u043d\u043e\u043c\u0443 \u0437\u0440\u0435\u043d\u0438\u044e \u0438 \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u044b\u043c \u0441\u0435\u0442\u044f\u043c. \u0422\u043e\u0433\u043e, \u043a\u0442\u043e \u0433\u043e\u0442\u043e\u0432 \u043f\u043e\u043c\u043e\u0447\u044c \u043d\u0430\u043c \u0441\u043e\u0437\u0434\u0430\u0442\u044c \u0438\u043d\u043d\u043e\u0432\u0430\u0446\u0438\u043e\u043d\u043d\u044b\u0435 \u0438 \u044d\u0444\u0444\u0435\u043a\u0442\u0438\u0432\u043d\u044b\u0435 \u0438\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442\u044b \u0434\u043b\u044f \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u044f \u0432 \u043e\u0434\u043d\u043e\u043c \u0438\u0437 \u043a\u0440\u0443\u043f\u043d\u0435\u0439\u0448\u0438\u0445 \u0420\u0438\u0442\u0435\u0439\u043b\u0435\u0440\u043e\u0432 \u0423\u043a\u0440\u0430\u0438\u043d\u044b \u0438 \u043d\u0435 \u0442\u043e\u043b\u044c\u043a\u043e.""]}",,"Required skills ‚Äî –æ—Ç 3+ –ª–µ—Ç –æ–ø—ã—Ç–∞ —Ä–∞–±–æ—Ç—ã –≤ —Ä–æ–ª–∏ Computer Vision Engineer‚Äî –æ–ø—ã—Ç –≤ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–µ –∏–∑ –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á:depth estimation, 3d reconstruction, object detection, face and emotions recognition, SLAM, feature extraction‚Äî –≤—ã—Å–æ–∫–∏–π —É—Ä–æ–≤–µ–Ω—å –≤–ª–∞–¥–µ–Ω–∏—è Python 3‚Äî –æ–ø—ã—Ç –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è DNN –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä, –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∏—Ö –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–µ–π –∏ —Å—Ñ–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è‚Äî –≤–ª–∞–¥–µ–Ω–∏–µ ML/CV –±–∏–±–ª–∏–æ—Ç–µ–∫–∞–º–∏ OpenCV, Pandas, Numpy, Tensorflow/Pytorch‚Äî R&D: —É–º–µ–Ω–∏–µ —Ä–∞–±–æ—Ç–∞—Ç—å —Å –Ω–∞—É—á–Ω—ã–º–∏ —Å—Ç–∞—Ç—å—è–º–∏, –æ–ø—ã—Ç —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω—ã—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π As a plus ‚Äî –ö–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–µ —Å—Ç–µ—Ä–µ–æ–∑—Ä–µ–Ω–∏–µ, —Ñ–æ—Ç–æ–≥—Ä–∞–º–º–µ—Ç—Ä–∏—è, –ø—Ä–æ–µ–∫—Ç–∏–≤–Ω–∞—è –≥–µ–æ–º–µ—Ç—Ä–∏—è‚Äî –í—ã—Å—à–µ–µ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –ø–æ —Ñ–∏–∑-–º–∞—Ç –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—é‚Äî –†–∞–±–æ—Ç–∞ —Å IP –∫–∞–º–µ—Ä–∞–º–∏ –≤–∏–¥–µ–æ–Ω–∞–±–ª—é–¥–µ–Ω–∏—è –∏ –∑–Ω–∞–Ω–∏–µ camera models(pinhole, fisheye, etc)‚Äî –û–ø—ã—Ç —Å–æ–∑–¥–∞–Ω–∏—è —Å–µ—Ä–≤–∏—Å–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—è Flask/Django –∏ Docker.‚Äî –û–ø—ã—Ç —Ä–∞–±–æ—Ç—ã —Å Redis/Kafka/Rabbitmq‚Äî –ù–∞–≤—ã–∫–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è MySQL/PostgreSQL –∏ MongoDB We offer ‚Ä¢ –ò–Ω—Ç–µ—Ä–µ—Å–Ω—ã–µ –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –ø—Ä–æ–µ–∫—Ç—ã‚Ä¢ –°–ø—Ä–∞–≤–µ–¥–ª–∏–≤–∞—è –∏ —Å–≤–æ–µ–≤—Ä–µ–º–µ–Ω–Ω–∞—è –∑–∞—Ä–ø–ª–∞—Ç–∞‚Ä¢ –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—É—é –∫–æ–º–∞–Ω–¥—É —É–º–Ω—ã—Ö, –ª—é–±–æ–∑–Ω–∞—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–ª–ª–µ–≥‚Ä¢ –ü–ª–æ—â–∞–¥–∫–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–∞–º—ã—Ö —Å–º–µ–ª—ã—Ö –≥–∏–ø–æ—Ç–µ–∑‚Ä¢ –ë—é–¥–∂–µ—Ç –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ–º—É, (–∫—É—Ä—Å—ã, –∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏–∏ –∏ —Ç.–¥.)‚Ä¢ 5 –¥–Ω–µ–≤–Ω–∞—è —Ä–∞–±–æ—á–∞—è –Ω–µ–¥–µ–ª—è —Å –≥–∏–±–∫–∏–º –≥—Ä–∞—Ñ–∏–∫–æ–º‚Ä¢ –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å —É–¥–∞–ª–µ–Ω–Ω–æ–π —Ä–∞–±–æ—Ç—ã‚Ä¢ –¥–æ–±—Ä–æ–≤–æ–ª—å–Ω–æ–µ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–µ —Å—Ç—Ä–∞—Ö–æ–≤–∞–Ω–∏–µ‚Ä¢ —Å–∫–∏–¥–∫–∏ –¥–ª—è —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–æ–≤ —Å–µ—Ç–∏ (—Ä–µ—Å—Ç–æ—Ä–∞–Ω—ã, –∞–ø—Ç–µ–∫–∏, —Å—É–ø–µ—Ä–º–∞—Ä–∫–µ—Ç—ã –∏ —Ç—É—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –ø–æ–µ–∑–¥–∫–∏)‚Ä¢ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –¥—Ä–µ—Å—Å-–∫–æ–¥–∞‚Ä¢ –ö–æ–º—Ñ–æ—Ä—Ç–Ω—ã–π –∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π –æ—Ñ–∏—Å –Ω–∞ –ª–µ–≤–æ–º –±–µ—Ä–µ–≥—É –î–Ω–µ–ø—Ä–∞, —Ä—è–¥–æ–º —Å –º–æ—Å—Ç–æ–º –ü–∞—Ç–æ–Ω–∞, (–¢–¶ Silver Breeze) Responsibilities ‚Äî –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å state of the art Computer Vision —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –ø—Ä–∏–º–µ–Ω–∏–º—ã–µ –≤ —Ä–∏—Ç–µ–π–ª–µ‚Äî –°—Ç—Ä–æ–∏—Ç—å, –æ–±—É—á–∞—Ç—å, —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –≥–ª—É–±–æ–∫–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏‚Äî –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π‚Äî –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–æ–≥–æ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è‚Äî –°–æ–∑–¥–∞–Ω–∏–µ API –¥–ª—è –¥–æ—Å—Ç–∞–≤–∫–∏ –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π Project description –ú—ã –∏—â–µ–º –≤ –Ω–∞—à—É –∫–æ–º–∞–Ω–¥—É —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–∞ –ø–æ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–º—É –∑—Ä–µ–Ω–∏—é –∏ –Ω–µ–π—Ä–æ–Ω–Ω—ã–º —Å–µ—Ç—è–º. –¢–æ–≥–æ, –∫—Ç–æ –≥–æ—Ç–æ–≤ –ø–æ–º–æ—á—å –Ω–∞–º —Å–æ–∑–¥–∞—Ç—å –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–µ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –æ–¥–Ω–æ–º –∏–∑ –∫—Ä—É–ø–Ω–µ–π—à–∏—Ö –†–∏—Ç–µ–π–ª–µ—Ä–æ–≤ –£–∫—Ä–∞–∏–Ω—ã –∏ –Ω–µ —Ç–æ–ª—å–∫–æ.","Senior Computer Vision Engineer@Fozzy Group, –ö–æ—Ä–ø–æ—Ä–∞—Ü—ñ—è",https://jobs.dou.ua/companies/fozzy/vacancies/133123/, Kyiv,Senior Computer Vision Engineer,25 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/amazingappstech/,Amazing Apps,{},,"–ú—ã –≤ –ø–æ–∏—Å–∫–µ –æ–ø—ã—Ç–Ω–æ–≥–æ Product Data Analyst, –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–Ω–æ–≥–æ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è —Å–∏—Å—Ç–µ–º—ã –æ–ø–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∫–æ–º–∞–Ω–¥ –∏ –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π –Ω–∞ –∏—Ö –æ—Å–Ω–æ–≤–∞–Ω–∏–∏. –û –Ω–∞—Å:‚Ä¢ Data –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è –¥–µ—Å—è—Ç–∫–∞ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π —Å –º–∏–ª–ª–∏–æ–Ω–∞–º–∏ –∑–∞–≥—Ä—É–∑–æ–∫‚Ä¢ –ú–æ–¥–µ–ª–∏ –ø—Ä–æ–≥–Ω–æ–∑–∞ LTV –∏ ROI‚Ä¢ –°–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ ETL –∏–∑ –≤—Å–µ—Ö —Å–∏—Å—Ç–µ–º –∏ –º–µ–¥–∏–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤‚Ä¢ –ü–æ—Å—Ç—É–ø–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ —Ä–µ–∂–∏–º–µ –æ–Ω–ª–∞–π–Ω‚Ä¢ –ü–æ—Å—Ç–æ—è–Ω–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å –¥–∞–Ω–Ω—ã—Ö‚Ä¢ –¶–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π DWH‚Ä¢ –¢–µ–∫—É—â–∞—è –∫–æ–º–∞–Ω–¥–∞: Data Engineer, WEB Analyst, Marketing Analyst + –æ—Ç–∫—Ä—ã—Ç—ã –ø–æ–∑–∏—Ü–∏–∏ Data Analyst, Data Engineer, Data Quality Analyst –ö–ª—é—á–µ–≤—ã–µ –∑–∞–¥–∞—á–∏:‚Ä¢ –ü—Ä–æ—Ä–∞–±–æ—Ç–∫–∞ –∏–≤–µ–Ω—Ç–æ–≤ –ø–æ–¥ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è, —É–Ω–∏—Ñ–∏–∫–∞—Ü–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ø–æ–¥—Ö–æ–¥–æ–≤ (—Å–æ–≤–º–µ—Å—Ç–Ω–æ —Å –ø—Ä–æ–¥—É–∫—Ç–æ–º)‚Ä¢ –°–∞–ø–ø–æ—Ä—Ç –∫–æ–º–∞–Ω–¥—ã –º–æ–Ω–µ—Ç–∏–∑–∞—Ü–∏–∏ –≤ –ø—Ä–æ–≤–µ–¥–µ–Ω–∏–∏ –∏ –æ—Ü–µ–Ω–∫–µ AB —Ç–µ—Å—Ç–æ–≤‚Ä¢ –ó–∞–≤–µ–¥–µ–Ω–∏–µ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è —Ü–µ–Ω–æ–≤—ã—Ö –ø—Ä–æ–¥—É–∫—Ç–æ–≤. –î–æ—Ä–∞–±–æ—Ç–∫–∞ –º–µ—Ç–æ–¥–∏–∫–∏ —Ä–∞—Å—á–µ—Ç–∞ LTV.‚Ä¢ –°–æ–∑–¥–∞–Ω–∏–µ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –æ—Ç—á–µ—Ç–æ–≤ –ø–æ –º–µ—Ç—Ä–∏–∫–∞–º –ø—Ä–æ–¥—É–∫—Ç–æ–≤ + –ø–æ—Å—Ç—Ä–µ–ª–∏–∑–Ω—ã–π –∞–Ω–∞–ª–∏–∑‚Ä¢ –°–∞–ø–ø–æ—Ä—Ç –∫–æ–º–∞–Ω–¥—ã QA –≤ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ —Ä–µ–ª–∏–∑–æ–≤ ‚Ä¢ –ó–∞–≤–µ–¥–µ–Ω–∏–µ –∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –∞–∫–∫–∞—É–Ω—Ç–æ–≤ –≤ —Å–∏—Å—Ç–µ–º–∞—Ö –∞–Ω–∞–ª–∏—Ç–∏–∫–∏, —Ä–µ–≥—É–ª—è—Ä–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞‚Ä¢ –£—á–∞—Å—Ç–∏–µ –≤ —Ä–∞–±–æ—á–∏—Ö –≥—Ä—É–ø–ø–∞—Ö –ø–æ GDPR –ù–∞–≤—ã–∫–∏ –∏ –æ–ø—ã—Ç:‚Ä¢ –û–ø—ã—Ç —Ä–∞–±–æ—Ç—ã —Å mobile –ø—Ä–æ–¥—É–∫—Ç–∞–º–∏ –≤ —Ä–æ–ª–∏ product analyst –∏–ª–∏ data analyst‚Ä¢ –û–ø—ã—Ç —Ä–∞–±–æ—Ç—ã —Å –ø–æ–¥–ø–∏—Å–æ—á–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏, —Ç—Ä–∏–∞–ª–∞–º–∏, —Ä–µ–∫–ª–∞–º–Ω–æ–π –º–æ–Ω–µ—Ç–∏–∑–∞—Ü–∏–µ–π, –ø–æ–Ω–∏–º–∞–Ω–∏–µ LT/LTV‚Ä¢ –ó–Ω–∞–Ω–∏–µ SQL, –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–µ –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –≤–æ—Ä–æ–Ω–æ–∫, –∫–æ–≥–æ—Ä—Ç, —Ä–µ—Ç–µ–Ω—à–Ω–∞ –Ω–∞ —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö‚Ä¢ –û–ø—ã—Ç —Ä–∞–±–æ—Ç—ã —Å BI –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏ (—Å–µ–π—á–∞—Å –∏—Å–ø–æ–ª—å–∑—É–µ–º google data studio) ‚Ä¢ –û–ø—ã—Ç —Ä–∞–±–æ—Ç—ã —Å –º–æ–±–∏–ª—å–Ω–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–æ–π Firebase/Facebook/Amplitude/Metrika‚Ä¢ –û–ø—ã—Ç —Ä–∞–±–æ—Ç—ã —Å GA/GTM‚Ä¢ –ü–æ–Ω–∏–º–∞–Ω–∏–µ –≤–∑–∞–∏–º–æ—Å–≤—è–∑–µ–π –≤ –¥–∞–Ω–Ω—ã—Ö; —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å —Ä–∞—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å –∞–Ω–æ–º–∞–ª–∏–∏; —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞—Ç—å –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –ë—É–¥–µ—Ç –ø–ª—é—Å–æ–º:‚Ä¢ –ó–Ω–∞–Ω–∏–µ Python/R (—Ä–∞–±–æ—Ç–∞ —Å api, –ë–î, –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö)‚Ä¢ –û–ø—ã—Ç —Ä–∞–±–æ—Ç—ã —Å MMP (appsflyer/adjust), –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–π, —Ä–∞–±–æ—Ç–∞ —Å API‚Ä¢ –û–ø—ã—Ç —Ä–∞–±–æ—Ç—ã —Å –ø—Ä–æ–¥—É–∫—Ç–æ–≤—ã–º–∏ –ê–ë —Ç–µ—Å—Ç–∞–º–∏ –í –Ω–∞—à–µ–π –∫–æ–º–∞–Ω–¥–µ –º—ã —Ü–µ–Ω–∏–º:‚Ä¢ –ü—Ä–æ–∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏ –æ—Ä–∏–µ–Ω—Ç–∞—Ü–∏—é –Ω–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç. –¢—ã –∑–∞–¥–∞–µ—à—å –≤–æ–ø—Ä–æ—Å—ã –∏ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—à—å —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã, —Å—Ç—Ä–µ–º–∏—à—å—Å—è –∫ —É–ª—É—á—à–µ–Ω–∏—é —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –∏ –ø–æ–¥—Ö–æ–¥–æ–≤‚Ä¢ –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—É—é –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏—é –≤ –∫–æ–º–∞–Ω–¥–µ. –í—ã–ø–æ–ª–Ω—è–µ—à—å –¥–æ–≥–æ–≤–æ—Ä–µ–Ω–Ω–æ—Å—Ç–∏, –∫–æ–º–º—É–Ω–∏—Ü–∏—Ä—É–µ—à—å —Å–≤–æ–µ–≤—Ä–µ–º–µ–Ω–Ω–æ, –ø–æ–Ω—è—Ç–Ω–æ, —á–µ—Ç–∫–æ –∏ –ª–∞–∫–æ–Ω–∏—á–Ω–æ, –¥–∞–µ—à—å –±–æ–ª—å—à–µ –¥–µ—Ç–∞–ª–µ–π, –µ—Å–ª–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ, –≥–æ—Ç–æ–≤ –ø–æ–º–æ—á—å –≤ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–∏ –æ–±—â–µ–π —Ü–µ–ª–∏ –ú—ã –ø—Ä–µ–¥–ª–∞–≥–∞–µ–º:‚Ä¢ –ì–∏–±–∫–∏–π –≥—Ä–∞—Ñ–∏–∫ —Ä–∞–±–æ—Ç—ã: –≤—Ä–µ–º—è –Ω–∞—á–∞–ª–∞ —Ä–∞–±–æ—á–µ–≥–æ –¥–Ω—è ‚Äî —Å 8 –¥–æ 11 —á–∞—Å–æ–≤ —É—Ç—Ä–∞. –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å —Ä–∞–±–æ—Ç–∞—Ç—å —É–¥–∞–ª–µ–Ω–Ω–æ –Ω–µ—Å–∫–æ–ª—å–∫–æ –¥–Ω–µ–π –≤ –º–µ—Å—è—Ü‚Ä¢ Work-life balance: —É –Ω–∞—Å –¥–∏–Ω–∞–º–∏—á–Ω–æ, –Ω–æ –º—ã –≤—Å–µ–≥–¥–∞ –Ω–∞—Ö–æ–¥–∏–º –≤—Ä–µ–º—è –Ω–∞ –æ—Ç–¥—ã—Ö. –í —Ç–≤–æ–µ–º —Ä–∞—Å–ø–æ—Ä—è–∂–µ–Ω–∏–∏ 20 —Ä–∞–±–æ—á–∏—Ö –¥–Ω–µ–π –æ–ø–ª–∞—á–∏–≤–∞–µ–º–æ–≥–æ –æ—Ç–ø—É—Å–∫–∞‚Ä¢ –ú–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–µ —Å—Ç—Ä–∞—Ö–æ–≤–∞–Ω–∏–µ —Å –ø–µ—Ä–≤–æ–≥–æ —Ä–∞–±–æ—á–µ–≥–æ –¥–Ω—è –∏ –æ–ø–ª–∞—á–∏–≤–∞–µ–º—ã–µ –±–æ–ª—å–Ω–∏—á–Ω—ã–µ‚Ä¢ –û–±–µ–¥—ã –≤ –æ—Ñ–∏—Å–µ –∑–∞ —Å—á–µ—Ç –∫–æ–º–ø–∞–Ω–∏–∏ —Å –ø–µ—Ä–≤–æ–≥–æ —Ä–∞–±–æ—á–µ–≥–æ –¥–Ω—è‚Ä¢ –û–ø–ª–∞—á–∏–≤–∞–µ–º–æ–µ –æ–±—É—á–µ–Ω–∏–µ: –≤–Ω–µ—à–Ω–∏–µ —Ç—Ä–µ–Ω–∏–Ω–≥–∏ –∏ –∫—É—Ä—Å—ã, –±–æ–ª—å—à–∞—è –æ–Ω–ª–∞–π–Ω –∏ –æ—Ñ—Ñ–ª–∞–π–Ω –±–∏–±–ª–∏–æ—Ç–µ–∫–∞, meetups –∏ —Å–µ–º–∏–Ω–∞—Ä—ã ‚Äî –Ω–∞–º –≤–∞–∂–Ω–æ, —á—Ç–æ–±—ã –ª—é–¥–∏ –≤ –∫–æ–º–ø–∞–Ω–∏–∏ —Ä–∞–∑–≤–∏–≤–∞–ª–∏—Å—å –∏ –Ω–µ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å—Å—è –Ω–∞ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–æ–º‚Ä¢ –î–æ—Å—Ç–æ–π–Ω–æ–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á–∏—Ç—å—Å—è –Ω–∞ —Ä–∞–∑–≤–∏—Ç–∏–∏ –ø—Ä–æ–µ–∫—Ç–∞ –∏ —Å–≤–æ–µ–º –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–º —Ä–æ—Å—Ç–µ‚Ä¢ –ö–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã–µ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è –∏ —Ç–∏–º–±–∏–ª–¥–∏–Ω–≥–∏, –Ω–∞ –∫–æ—Ç–æ—Ä—ã—Ö –º—ã –ø—Ä–∞–∑–¥–Ω—É–µ–º –ø–æ–±–µ–¥—ã –∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –Ω–∞—à–∏—Ö –∫–æ–º–∞–Ω–¥ –∏ –∫–æ–º–ø–∞–Ω–∏–∏ –í–æ—Ç –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –∏–∑ –Ω–∞—à–∏—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π:Yoga Go ‚Äî apple.co/2OXPszb / bit.ly/androidYogaGoMuscle Booster ‚Äî apple.co/2LyMBA1 / bit.ly/androidMBapp ‚Ä¢ –ù–∞—à–∏ –ø—Ä–æ–¥—É–∫—Ç—ã ‚Äî –ª–∏–¥–µ—Ä—ã –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ Health and Fitness –ø–æ –≤—Å–µ–º—É –º–∏—Ä—É. –ú—ã –≥–æ—Ä–¥–∏–º—Å—è —Ç–µ–º, —á—Ç–æ –∑–∞ 8 –º–µ—Å—è—Ü–µ–≤ —Å–æ–∑–¥–∞–ª–∏ –∏ –≤—ã–≤–µ–ª–∏ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ Muscle Booster –≤ –¢–û–ü-1 –≤ –º–∏—Ä–µ –≤ —Å–≤–æ–µ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏. –ù–∞—à–∞ —Ü–µ–ª—å ‚Äî –≤—ã–≤–µ—Å—Ç–∏ –≤—Å–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è –≤ –ª–∏–¥–µ—Ä—ã‚Ä¢ –£ –Ω–∞—Å –º–æ–∂–Ω–æ —Ä–µ–∞–ª–∏–∑–æ–≤—ã–≤–∞—Ç—å –∏–¥–µ–∏ –±–µ–∑ –º–∏–∫—Ä–æ–º–µ–Ω–µ–¥–∂–º–µ–Ω—Ç–∞ –∏ –±—é—Ä–æ–∫—Ä–∞—Ç–∏–∏, –∏ –ø–æ—á—É–≤—Å—Ç–≤–æ–≤–∞—Ç—å —Å–≤–æ–±–æ–¥—É —Ç–≤–æ—Ä—á–µ—Å—Ç–≤–∞. –ò—Å—Å–ª–µ–¥—É–π –≤–∞—Ä–∏–∞–Ω—Ç—ã –≥–ª—É–±–∂–µ, —Å–ª—É—à–∞–π –∫–æ–ª–ª–µ–≥, –∞—Ä–≥—É–º–µ–Ω—Ç–∏—Ä—É–π –∏ –ø—Ä–∏–Ω–∏–º–∞–π —Ä–µ—à–µ–Ω–∏—è –†–µ–∫—Ä—É—Ç–º–µ–Ω—Ç –ø—Ä–æ—Ü–µ—Å—Å:‚≠ïÔ∏è –ó–Ω–∞–∫–æ–º—Å—Ç–≤–æ —Å —Ä–µ–∫—Ä—É—Ç–µ—Ä–æ–º ----> ‚≠ïÔ∏è –¢–µ—Å—Ç–æ–≤–æ–µ –∑–∞–¥–∞–Ω–∏–µ ----> ‚≠ïÔ∏è –§–∏–Ω–∞–ª—å–Ω–æ–µ –∏–Ω—Ç–µ—Ä–≤—å—é",Product Data Analyst@Amazing Apps,https://jobs.dou.ua/companies/amazingappstech/vacancies/133092/, Kyiv,Product Data Analyst,25 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/amazingappstech/,Amazing Apps,{},,"–ú—ã –≤ –ø–æ–∏—Å–∫–µ –æ–ø—ã—Ç–Ω–æ–≥–æ Data Engineer, –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–Ω–æ–≥–æ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è —Å–∏—Å—Ç–µ–º—ã –æ–ø–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö, —Å–∏—Å—Ç–µ–º—ã –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∏ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è –∑–¥–æ—Ä–æ–≤—å—è —Å–∏—Å—Ç–µ–º—ã. –ú–µ–Ω–µ–¥–∂–º–µ–Ω—Ç DWH + data lake —Ç–∞–∫–∂–µ —è–≤–ª—è–µ—Ç—Å—è –æ–¥–Ω–æ–π –∏–∑ –∫–ª—é—á–µ–≤—ã—Ö –∑–∞–¥–∞—á —Ä–æ–ª–∏. –û –Ω–∞—Å:‚Ä¢ Data –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è –¥–µ—Å—è—Ç–∫–∞ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π —Å –º–∏–ª–ª–∏–æ–Ω–∞–º–∏ –∑–∞–≥—Ä—É–∑–æ–∫‚Ä¢ –ú–æ–¥–µ–ª–∏ –ø—Ä–æ–≥–Ω–æ–∑–∞ LTV –∏ ROI‚Ä¢ –°–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ ETL –∏–∑ –≤—Å–µ—Ö —Å–∏—Å—Ç–µ–º –∏ –º–µ–¥–∏–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤‚Ä¢ –ü–æ—Å—Ç—É–ø–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ —Ä–µ–∂–∏–º–µ –æ–Ω–ª–∞–π–Ω‚Ä¢ –ü–æ—Å—Ç–æ—è–Ω–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å –¥–∞–Ω–Ω—ã—Ö‚Ä¢ –¶–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π DWH‚Ä¢ –¢–µ–∫—É—â–∞—è –∫–æ–º–∞–Ω–¥–∞: Data Engineer, WEB Analyst, Marketing Analyst + –æ—Ç–∫—Ä—ã—Ç—ã –ø–æ–∑–∏—Ü–∏–∏ Data Analyst, Data Quality Analyst, Product Analyst –ö–ª—é—á–µ–≤—ã–µ –∑–∞–¥–∞—á–∏:‚Ä¢ –ü–æ—Å—Ç–∞–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö, python ‚Äî –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å API FB, GA, —Ä–µ–∫–ª–∞–º–Ω—ã—Ö —Å–µ—Ç–µ–π, AF –∏ –¥—Ä—É–≥–∏—Ö‚Ä¢ –î–æ—Å—Ç–∞–≤–∫–∞ –≤ DWH –∏–≤–µ–Ω—Ç —Å—Ç—Ä–∏–º–æ–≤ (AWS sdk-pinpoint-firehose-ec2-redshift)‚Ä¢ –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Ä–µ—Å—É—Ä—Å–æ–≤ EC2 –∏–Ω—Å—Ç–∞–Ω—Å–æ–≤ (win, ubuntu) –¥–ª—è –∑–∞–ø—É—Å–∫–∞ cron –∑–∞–¥–∞—á‚Ä¢ –í–µ–¥–µ–Ω–∏–µ –∫–æ–¥–æ–≤–æ–π –±–∞–∑—ã, —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Å–∫—Ä–∏–ø—Ç–æ–≤‚Ä¢ –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –∏ –¥–æ—Ä–∞–±–æ—Ç–∫–∞ (SQS, lambda) –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –≤—Ö–æ–¥—è—â–∏–º–∏ –ø–æ—Ç–æ–∫–∞–º–∏‚Ä¢ –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∏ ETL (grafana, graylog, sqs, DB resources, ec2 resources) ‚Ä¢ Account management ‚Äî —Å–æ–∑–¥–∞–Ω–∏–µ –∏ –≤–µ–¥–µ–Ω–∏–µ apps/tokens, –¥–æ—Å—Ç—É–ø–æ–≤, –±–æ—Ç –∞–∫–∫–∞—É–Ω—Ç–æ–≤, —Å–µ—Ä–≤–∏—Å –∞–∫–∫–∞—É–Ω—Ç–æ–≤ –∏ —Ç.–¥. –ù–∞–≤—ã–∫–∏ –∏ –æ–ø—ã—Ç:‚Ä¢ –û–ø—ã—Ç —Ä–∞–±–æ—Ç—ã c –æ–±–ª–∞—á–Ω–æ–π –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π, –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω–æ AWS (lambda, sqs, redshift, glue)‚Ä¢ –ó–Ω–∞–Ω–∏–µ Python, –æ–ø—ã—Ç –∏–º–ø–æ—Ä—Ç–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ API (FB, GA, —Ä–µ–∫–ª–∞–º–Ω—ã–µ —Å–µ—Ç–∏), web scraping –∏ —Ç.–¥.‚Ä¢ –û–ø—ã—Ç —Ä–∞–±–æ—Ç—ã —Å VCS‚Ä¢ –°–∏–ª—å–Ω—ã–π SQL (–æ–∫–æ–Ω–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏), –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ë–î‚Ä¢ –ê–Ω–≥–ª–∏–π—Å–∫–∏–π —è–∑—ã–∫ –Ω–∞ —É—Ä–æ–≤–Ω–µ —á—Ç–µ–Ω–∏—è –∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –ë—É–¥–µ—Ç –ø–ª—é—Å–æ–º:‚Ä¢ –û–ø—ã—Ç –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏—è –ë–î‚Ä¢ –û–ø—ã—Ç —Ä–∞–±–æ—Ç—ã —Å Jenkins, Graylog, Vault‚Ä¢ –û–ø—ã—Ç —Ä–∞–±–æ—Ç—ã —Å –ø–æ—Ç–æ–∫–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏‚Ä¢ –û–ø—ã—Ç —Ä–∞–±–æ—Ç—ã —Å data lakes –í –Ω–∞—à–µ–π –∫–æ–º–∞–Ω–¥–µ –º—ã —Ü–µ–Ω–∏–º:‚Ä¢ –ü—Ä–æ–∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏ –æ—Ä–∏–µ–Ω—Ç–∞—Ü–∏—é –Ω–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç. –¢—ã –∑–∞–¥–∞–µ—à—å –≤–æ–ø—Ä–æ—Å—ã –∏ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—à—å —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã, —Å—Ç—Ä–µ–º–∏—à—å—Å—è –∫ —É–ª—É—á—à–µ–Ω–∏—é —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –∏ –ø–æ–¥—Ö–æ–¥–æ–≤‚Ä¢ –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—É—é –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏—é –≤ –∫–æ–º–∞–Ω–¥–µ. –í—ã–ø–æ–ª–Ω—è–µ—à—å –¥–æ–≥–æ–≤–æ—Ä–µ–Ω–Ω–æ—Å—Ç–∏, –∫–æ–º–º—É–Ω–∏—Ü–∏—Ä—É–µ—à—å —Å–≤–æ–µ–≤—Ä–µ–º–µ–Ω–Ω–æ, –ø–æ–Ω—è—Ç–Ω–æ, —á–µ—Ç–∫–æ –∏ –ª–∞–∫–æ–Ω–∏—á–Ω–æ, –¥–∞–µ—à—å –±–æ–ª—å—à–µ –¥–µ—Ç–∞–ª–µ–π, –µ—Å–ª–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ, –≥–æ—Ç–æ–≤ –ø–æ–º–æ—á—å –≤ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–∏ –æ–±—â–µ–π —Ü–µ–ª–∏ –ú—ã –ø—Ä–µ–¥–ª–∞–≥–∞–µ–º:‚Ä¢ –ì–∏–±–∫–∏–π –≥—Ä–∞—Ñ–∏–∫ —Ä–∞–±–æ—Ç—ã: –≤—Ä–µ–º—è –Ω–∞—á–∞–ª–∞ —Ä–∞–±–æ—á–µ–≥–æ –¥–Ω—è ‚Äî —Å 8 –¥–æ 11 —á–∞—Å–æ–≤ —É—Ç—Ä–∞. –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å —Ä–∞–±–æ—Ç–∞—Ç—å —É–¥–∞–ª–µ–Ω–Ω–æ –Ω–µ—Å–∫–æ–ª—å–∫–æ –¥–Ω–µ–π –≤ –º–µ—Å—è—Ü‚Ä¢ Work-life balance: —É –Ω–∞—Å –¥–∏–Ω–∞–º–∏—á–Ω–æ, –Ω–æ –º—ã –≤—Å–µ–≥–¥–∞ –Ω–∞—Ö–æ–¥–∏–º –≤—Ä–µ–º—è –Ω–∞ –æ—Ç–¥—ã—Ö. –í —Ç–≤–æ–µ–º —Ä–∞—Å–ø–æ—Ä—è–∂–µ–Ω–∏–∏ 20 —Ä–∞–±–æ—á–∏—Ö –¥–Ω–µ–π –æ–ø–ª–∞—á–∏–≤–∞–µ–º–æ–≥–æ –æ—Ç–ø—É—Å–∫–∞‚Ä¢ –ú–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–µ —Å—Ç—Ä–∞—Ö–æ–≤–∞–Ω–∏–µ —Å –ø–µ—Ä–≤–æ–≥–æ —Ä–∞–±–æ—á–µ–≥–æ –¥–Ω—è –∏ –æ–ø–ª–∞—á–∏–≤–∞–µ–º—ã–µ –±–æ–ª—å–Ω–∏—á–Ω—ã–µ‚Ä¢ –û–±–µ–¥—ã –≤ –æ—Ñ–∏—Å–µ –∑–∞ —Å—á–µ—Ç –∫–æ–º–ø–∞–Ω–∏–∏ —Å –ø–µ—Ä–≤–æ–≥–æ —Ä–∞–±–æ—á–µ–≥–æ –¥–Ω—è‚Ä¢ –û–ø–ª–∞—á–∏–≤–∞–µ–º–æ–µ –æ–±—É—á–µ–Ω–∏–µ: –≤–Ω–µ—à–Ω–∏–µ —Ç—Ä–µ–Ω–∏–Ω–≥–∏ –∏ –∫—É—Ä—Å—ã, –±–æ–ª—å—à–∞—è –æ–Ω–ª–∞–π–Ω –∏ –æ—Ñ—Ñ–ª–∞–π–Ω –±–∏–±–ª–∏–æ—Ç–µ–∫–∞, meetups –∏ —Å–µ–º–∏–Ω–∞—Ä—ã ‚Äî –Ω–∞–º –≤–∞–∂–Ω–æ, —á—Ç–æ–±—ã –ª—é–¥–∏ –≤ –∫–æ–º–ø–∞–Ω–∏–∏ —Ä–∞–∑–≤–∏–≤–∞–ª–∏—Å—å –∏ –Ω–µ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å—Å—è –Ω–∞ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–æ–º‚Ä¢ –î–æ—Å—Ç–æ–π–Ω–æ–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á–∏—Ç—å—Å—è –Ω–∞ —Ä–∞–∑–≤–∏—Ç–∏–∏ –ø—Ä–æ–µ–∫—Ç–∞ –∏ —Å–≤–æ–µ–º –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–º —Ä–æ—Å—Ç–µ‚Ä¢ –ö–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã–µ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è –∏ —Ç–∏–º–±–∏–ª–¥–∏–Ω–≥–∏, –Ω–∞ –∫–æ—Ç–æ—Ä—ã—Ö –º—ã –ø—Ä–∞–∑–¥–Ω—É–µ–º –ø–æ–±–µ–¥—ã –∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –Ω–∞—à–∏—Ö –∫–æ–º–∞–Ω–¥ –∏ –∫–æ–º–ø–∞–Ω–∏–∏ –í–æ—Ç –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –∏–∑ –Ω–∞—à–∏—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π:Yoga Go ‚Äî apple.co/2OXPszb / bit.ly/androidYogaGoMuscle Booster ‚Äî apple.co/2LyMBA1 / bit.ly/androidMBapp ‚Ä¢ –ù–∞—à–∏ –ø—Ä–æ–¥—É–∫—Ç—ã ‚Äî –ª–∏–¥–µ—Ä—ã –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ Health and Fitness –ø–æ –≤—Å–µ–º—É –º–∏—Ä—É. –ú—ã –≥–æ—Ä–¥–∏–º—Å—è —Ç–µ–º, —á—Ç–æ –∑–∞ 8 –º–µ—Å—è—Ü–µ–≤ —Å–æ–∑–¥–∞–ª–∏ –∏ –≤—ã–≤–µ–ª–∏ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ Muscle Booster –≤ –¢–û–ü-1 –≤ –º–∏—Ä–µ –≤ —Å–≤–æ–µ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏. –ù–∞—à–∞ —Ü–µ–ª—å ‚Äî –≤—ã–≤–µ—Å—Ç–∏ –≤—Å–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è –≤ –ª–∏–¥–µ—Ä—ã‚Ä¢ –£ –Ω–∞—Å –º–æ–∂–Ω–æ —Ä–µ–∞–ª–∏–∑–æ–≤—ã–≤–∞—Ç—å –∏–¥–µ–∏ –±–µ–∑ –º–∏–∫—Ä–æ–º–µ–Ω–µ–¥–∂–º–µ–Ω—Ç–∞ –∏ –±—é—Ä–æ–∫—Ä–∞—Ç–∏–∏, –∏ –ø–æ—á—É–≤—Å—Ç–≤–æ–≤–∞—Ç—å —Å–≤–æ–±–æ–¥—É —Ç–≤–æ—Ä—á–µ—Å—Ç–≤–∞. –ò—Å—Å–ª–µ–¥—É–π –≤–∞—Ä–∏–∞–Ω—Ç—ã –≥–ª—É–±–∂–µ, —Å–ª—É—à–∞–π –∫–æ–ª–ª–µ–≥, –∞—Ä–≥—É–º–µ–Ω—Ç–∏—Ä—É–π –∏ –ø—Ä–∏–Ω–∏–º–∞–π —Ä–µ—à–µ–Ω–∏—è –†–µ–∫—Ä—É—Ç–º–µ–Ω—Ç –ø—Ä–æ—Ü–µ—Å—Å:‚≠ïÔ∏è –ó–Ω–∞–∫–æ–º—Å—Ç–≤–æ —Å —Ä–µ–∫—Ä—É—Ç–µ—Ä–æ–º ----> ‚≠ïÔ∏è –¢–µ—Å—Ç–æ–≤–æ–µ –∑–∞–¥–∞–Ω–∏–µ ----> ‚≠ïÔ∏è –§–∏–Ω–∞–ª—å–Ω–æ–µ –∏–Ω—Ç–µ—Ä–≤—å—é",Data Engineer@Amazing Apps,https://jobs.dou.ua/companies/amazingappstech/vacancies/133087/, Kyiv,Data Engineer,25 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/macpaw/,MacPaw,"{""Required skills"": [""We are looking for an experienced Data Engineer to scale the team. Together with 2 Data Engineers, you will partner closely with all MacPaw products and teams to solve ongoing business problems.""], ""Project description"": [""We aim to develop smart data direction in the company, and now we start getting more requests for data engineering solutions from our teams (products and services).""]}",,"Required skills We are looking for an experienced Data Engineer to scale the team. Together with 2 Data Engineers, you will partner closely with all MacPaw products and teams to solve ongoing business problems. We aim to develop smart data direction in the company, and now we start getting more requests for data engineering solutions from our teams (products and services). In this role, you will work closely with Product Analysts and Data Scientists to understand the data needs of different stakeholders across MacPaw, provide proactive solutions, and enable our teams to extract insights and value from data. You will also help us integrate different data sources, improve our data system‚Äôs efficiency, reliability, and latency, help automate data pipelines, and improve our data model and overall architecture. Our position is a good match for someone keenly aware of and motivated by driving business value. As part of this team, you will pitch ideas and quickly see the impact your work makes. ‚Äî Experience with message brokers‚Äî Practical skills with Apache BigData ecosystem‚Äî Knowledge of building cross-team solutions (Data Scientist, DevOps, Analyst) ‚Äî Outstanding team‚Äî Opportunity to improve process and implement your ideas‚Äî Great conditions for education and development within the company (MacPaw Labs days, conferences, workshops, trainings, etc.)‚Äî –°are about your health (insurance, office gym, paid sick leaves, etc.) ‚Äî Work-rest balance support (meditation/sleeping room, 20 vacation days, etc.)‚Äî UX driven office, equipment of your choice‚Äî English courses‚Äî 2 anti-stress cats Project description We are a growing team of developers, designers, and IT-professionals who love what they do. If you are as passionate about making good products, we‚Äôll be thrilled to have you on board.We value happiness and satisfaction above all, so we try our best to enjoy full comfort at work. You‚Äôll have no fixed hours or task overloads here. Creativity, inspiration, solidarity ‚Äî that‚Äôs what MacPaw is about.",Data Engineer@MacPaw,https://jobs.dou.ua/companies/macpaw/vacancies/133086/, Kyiv,Data Engineer,25 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/proxify/,Proxify,{},,"Proxify is a Sweden IT company, experiencing intense growth. We match remote IT professionals with IT companies in Sweden and abroad. The difference with us is that we like to make sure that the remote workers we present are the very best in their field. We like to make sure we get it the right first time, every time! We are growing fast and currently looking for a Senior Data Scientist to join our team. What you‚Äôll do:You as a Data Scientist in the team will try to make sense out of a vast number of data sources that have been ingested as part of the Discovery phase. Most likely you will use a graph to find patterns and identify high-value opportunities for enterprises. Given that this will be a greenfield proof of concept product, you will have the opportunity to choose your preferred stack and have a pivotal role in shaping the company‚Äôs‚Äô future. Who are you:‚Äî You have studied Econometrics, Macroeconomics, Statistics, Engineering, or any other relevant field; ‚Äî You are curious, open-minded and enjoy teamwork in a fast-changing environment; ‚Äî You are interested or have experience with business strategy and exploratory analyses.‚Äî has 4+ years of working experience as a Data Scientist; ‚Äî has 3+ years of experience with Python;‚Äî 0,5+ years with Neo4J;‚Äî Upper-intermediate (or higher) English level; What we offer: üíª100% remote work (work from where you want);üí™We pay for overtime (over 8 hours);üèÑüèª‚Äç‚ôÇÔ∏èBusiness trips to Sweden at company expense (if and when necessary);üëåüèªThe ability to change the project to another one;üíµCompetitive salary/hour with a potential bonus scheme;üßòüèª‚Äç‚ôÇÔ∏èVery flexible working schedule;üöÄOpportunities for professional development and personal growth;üêïPet-friendly office, if you need to work from the office, you can come with your little friend.",Senior Data Scientist@Proxify,https://jobs.dou.ua/companies/proxify/vacancies/133082/, remote,Senior Data Scientist,25 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/amazingappstech/,Amazing Apps,{},,"–ú—ã –≤ –ø–æ–∏—Å–∫–µ Data Quality Analyst, –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–Ω–æ–≥–æ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è —Å–∏—Å—Ç–µ–º—ã –æ–ø–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö, —Å–∏—Å—Ç–µ–º—ã –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∏ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è –∑–¥–æ—Ä–æ–≤—å—è —Å–∏—Å—Ç–µ–º—ã. –û –Ω–∞—Å:‚Ä¢ Data –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è –¥–µ—Å—è—Ç–∫–∞ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π —Å –º–∏–ª–ª–∏–æ–Ω–∞–º–∏ –∑–∞–≥—Ä—É–∑–æ–∫‚Ä¢ –ú–æ–¥–µ–ª–∏ –ø—Ä–æ–≥–Ω–æ–∑–∞ LTV –∏ ROI‚Ä¢ –°–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ ETL –∏–∑ –≤—Å–µ—Ö —Å–∏—Å—Ç–µ–º –∏ –º–µ–¥–∏–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤‚Ä¢ –ü–æ—Å—Ç—É–ø–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ —Ä–µ–∂–∏–º–µ –æ–Ω–ª–∞–π–Ω‚Ä¢ –ü–æ—Å—Ç–æ—è–Ω–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å –¥–∞–Ω–Ω—ã—Ö‚Ä¢ –¢–µ–∫—É—â–∞—è –∫–æ–º–∞–Ω–¥–∞: Data Engineer, WEB Analyst, Marketing Analyst + –æ—Ç–∫—Ä—ã—Ç—ã –ø–æ–∑–∏—Ü–∏–∏ Data Analyst, Data Engineer, Product Analyst –ö–ª—é—á–µ–≤—ã–µ –∑–∞–¥–∞—á–∏:‚Ä¢ –†–∞–±–æ—Ç–∞ —Å –¥–∞–Ω–Ω—ã–º–∏ –æ–± –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è—Ö, –ø–æ—Å—Ç—É–ø–∞—é—â–∏–º–∏ –≤ –ë–î‚Ä¢ –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø–æ–ª–Ω–æ—Ç—ã –∏ —Å–≤–æ–µ–≤—Ä–µ–º–µ–Ω–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö, –∏—Ö —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –º–µ–∂–¥—É —Å–∏—Å—Ç–µ–º–∞–º–∏‚Ä¢ –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–∞–º–∏ –ë–î (—Å–æ–≤–º–µ—Å—Ç–Ω–æ —Å data engineer)‚Ä¢ –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∏ ETL (grafana, graylog, sqs, DB resources, ec2 resources) —Å–æ–≤–º–µ—Å—Ç–Ω–æ —Å Data engineer‚Ä¢ Account management ‚Äî —Å–æ–∑–¥–∞–Ω–∏–µ –∏ –≤–µ–¥–µ–Ω–∏–µ apps/tokens, –¥–æ—Å—Ç—É–ø–æ–≤, –±–æ—Ç –∞–∫–∫–∞—É–Ω—Ç–æ–≤, —Å–µ—Ä–≤–∏—Å –∞–∫–∫–∞—É–Ω—Ç–æ–≤, etc –ù–∞–≤—ã–∫–∏ –∏ –æ–ø—ã—Ç:‚Ä¢ –û–ø—ã—Ç —Ä–∞–±–æ—Ç—ã –∞–Ω–∞–ª–∏—Ç–∏–∫–æ–º –æ—Ç 1 –≥–æ–¥–∞‚Ä¢ –•–æ—Ä–æ—à–µ–µ –∑–Ω–∞–Ω–∏–µ SQL, –∂–µ–ª–∞—Ç–µ–ª—å–Ω–æ –µ–∂–µ–¥–Ω–µ–≤–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤ —Ä–∞–±–æ—Ç–µ‚Ä¢ –ó–Ω–∞–Ω–∏–µ Python (—Ä–∞–±–æ—Ç–∞ —Å api, –ë–î, –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö)‚Ä¢ –û–ø—ã—Ç —Ä–∞–±–æ—Ç—ã —Å BI –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏ (—Å–µ–π—á–∞—Å –∏—Å–ø–æ–ª—å–∑—É–µ–º google data studio) –∏ —Å–æ–∑–¥–∞–Ω–∏—è –æ—Ç—á–µ—Ç–æ–≤ –≤ –Ω–∏—Ö‚Ä¢ –ü–æ–Ω–∏–º–∞–Ω–∏–µ –≤–∑–∞–∏–º–æ—Å–≤—è–∑–µ–π –≤ –¥–∞–Ω–Ω—ã—Ö; —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å —Ä–∞—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å –∞–Ω–æ–º–∞–ª–∏–∏ –ë—É–¥–µ—Ç –ø–ª—é—Å–æ–º:‚Ä¢ –û–ø—ã—Ç —Ä–µ—à–µ–Ω–∏—è ETL –∑–∞–¥–∞—á‚Ä¢ –û–ø—ã—Ç —Ä–∞–±–æ—Ç—ã —Å Grafana‚Ä¢ –û–ø—ã—Ç —Ä–∞–±–æ—Ç—ã —Å realtime –¥–∞–Ω–Ω—ã–º–∏ –∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏‚Ä¢ –û–ø—ã—Ç —Ä–∞–±–æ—Ç—ã —Å mobile –ø—Ä–æ–¥—É–∫—Ç–∞–º–∏ –í –Ω–∞—à–µ–π –∫–æ–º–∞–Ω–¥–µ –º—ã —Ü–µ–Ω–∏–º:‚Ä¢ –ü—Ä–æ–∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏ –æ—Ä–∏–µ–Ω—Ç–∞—Ü–∏—é –Ω–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç. –¢—ã –∑–∞–¥–∞–µ—à—å –≤–æ–ø—Ä–æ—Å—ã –∏ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—à—å —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã, —Å—Ç—Ä–µ–º–∏—à—å—Å—è –∫ —É–ª—É—á—à–µ–Ω–∏—é —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –∏ –ø–æ–¥—Ö–æ–¥–æ–≤‚Ä¢ –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—É—é –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏—é –≤ –∫–æ–º–∞–Ω–¥–µ. –í—ã–ø–æ–ª–Ω—è–µ—à—å –¥–æ–≥–æ–≤–æ—Ä–µ–Ω–Ω–æ—Å—Ç–∏, –∫–æ–º–º—É–Ω–∏—Ü–∏—Ä—É–µ—à—å —Å–≤–æ–µ–≤—Ä–µ–º–µ–Ω–Ω–æ, –ø–æ–Ω—è—Ç–Ω–æ, —á–µ—Ç–∫–æ –∏ –ª–∞–∫–æ–Ω–∏—á–Ω–æ, –¥–∞–µ—à—å –±–æ–ª—å—à–µ –¥–µ—Ç–∞–ª–µ–π, –µ—Å–ª–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ, –≥–æ—Ç–æ–≤ –ø–æ–º–æ—á—å –≤ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–∏ –æ–±—â–µ–π —Ü–µ–ª–∏ –ú—ã –ø—Ä–µ–¥–ª–∞–≥–∞–µ–º:‚Ä¢ –ì–∏–±–∫–∏–π –≥—Ä–∞—Ñ–∏–∫ —Ä–∞–±–æ—Ç—ã: –≤—Ä–µ–º—è –Ω–∞—á–∞–ª–∞ —Ä–∞–±–æ—á–µ–≥–æ –¥–Ω—è ‚Äî —Å 8 –¥–æ 11 —á–∞—Å–æ–≤ —É—Ç—Ä–∞. –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å —Ä–∞–±–æ—Ç–∞—Ç—å —É–¥–∞–ª–µ–Ω–Ω–æ –Ω–µ—Å–∫–æ–ª—å–∫–æ –¥–Ω–µ–π –≤ –º–µ—Å—è—Ü‚Ä¢ Work-life balance: —É –Ω–∞—Å –¥–∏–Ω–∞–º–∏—á–Ω–æ, –Ω–æ –º—ã –≤—Å–µ–≥–¥–∞ –Ω–∞—Ö–æ–¥–∏–º –≤—Ä–µ–º—è –Ω–∞ –æ—Ç–¥—ã—Ö. –í —Ç–≤–æ–µ–º —Ä–∞—Å–ø–æ—Ä—è–∂–µ–Ω–∏–∏ 20 —Ä–∞–±–æ—á–∏—Ö –¥–Ω–µ–π –æ–ø–ª–∞—á–∏–≤–∞–µ–º–æ–≥–æ –æ—Ç–ø—É—Å–∫–∞‚Ä¢ –ú–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–µ —Å—Ç—Ä–∞—Ö–æ–≤–∞–Ω–∏–µ —Å –ø–µ—Ä–≤–æ–≥–æ —Ä–∞–±–æ—á–µ–≥–æ –¥–Ω—è –∏ –æ–ø–ª–∞—á–∏–≤–∞–µ–º—ã–µ –±–æ–ª—å–Ω–∏—á–Ω—ã–µ‚Ä¢ –û–±–µ–¥—ã –≤ –æ—Ñ–∏—Å–µ –∑–∞ —Å—á–µ—Ç –∫–æ–º–ø–∞–Ω–∏–∏ —Å –ø–µ—Ä–≤–æ–≥–æ —Ä–∞–±–æ—á–µ–≥–æ –¥–Ω—è‚Ä¢ –û–ø–ª–∞—á–∏–≤–∞–µ–º–æ–µ –æ–±—É—á–µ–Ω–∏–µ: –≤–Ω–µ—à–Ω–∏–µ —Ç—Ä–µ–Ω–∏–Ω–≥–∏ –∏ –∫—É—Ä—Å—ã, –±–æ–ª—å—à–∞—è –æ–Ω–ª–∞–π–Ω –∏ –æ—Ñ—Ñ–ª–∞–π–Ω –±–∏–±–ª–∏–æ—Ç–µ–∫–∞, meetups –∏ —Å–µ–º–∏–Ω–∞—Ä—ã ‚Äî –Ω–∞–º –≤–∞–∂–Ω–æ, —á—Ç–æ–±—ã –ª—é–¥–∏ –≤ –∫–æ–º–ø–∞–Ω–∏–∏ —Ä–∞–∑–≤–∏–≤–∞–ª–∏—Å—å –∏ –Ω–µ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å—Å—è –Ω–∞ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–æ–º‚Ä¢ –î–æ—Å—Ç–æ–π–Ω–æ–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á–∏—Ç—å—Å—è –Ω–∞ —Ä–∞–∑–≤–∏—Ç–∏–∏ –ø—Ä–æ–µ–∫—Ç–∞ –∏ —Å–≤–æ–µ–º –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–º —Ä–æ—Å—Ç–µ‚Ä¢ –ö–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã–µ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è –∏ —Ç–∏–º–±–∏–ª–¥–∏–Ω–≥–∏, –Ω–∞ –∫–æ—Ç–æ—Ä—ã—Ö –º—ã –ø—Ä–∞–∑–¥–Ω—É–µ–º –ø–æ–±–µ–¥—ã –∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –Ω–∞—à–∏—Ö –∫–æ–º–∞–Ω–¥ –∏ –∫–æ–º–ø–∞–Ω–∏–∏ –í–æ—Ç –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –∏–∑ –Ω–∞—à–∏—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π:Yoga Go ‚Äî apple.co/2OXPszb / bit.ly/androidYogaGoMuscle Booster ‚Äî apple.co/2LyMBA1 / bit.ly/androidMBapp ‚Ä¢ –ù–∞—à–∏ –ø—Ä–æ–¥—É–∫—Ç—ã ‚Äî –ª–∏–¥–µ—Ä—ã –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ Health and Fitness –ø–æ –≤—Å–µ–º—É –º–∏—Ä—É. –ú—ã –≥–æ—Ä–¥–∏–º—Å—è —Ç–µ–º, —á—Ç–æ –∑–∞ 8 –º–µ—Å—è—Ü–µ–≤ —Å–æ–∑–¥–∞–ª–∏ –∏ –≤—ã–≤–µ–ª–∏ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ Muscle Booster –≤ –¢–û–ü-1 –≤ –º–∏—Ä–µ –≤ —Å–≤–æ–µ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏. –ù–∞—à–∞ —Ü–µ–ª—å ‚Äî –≤—ã–≤–µ—Å—Ç–∏ –≤—Å–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è –≤ –ª–∏–¥–µ—Ä—ã‚Ä¢ –£ –Ω–∞—Å –º–æ–∂–Ω–æ —Ä–µ–∞–ª–∏–∑–æ–≤—ã–≤–∞—Ç—å –∏–¥–µ–∏ –±–µ–∑ –º–∏–∫—Ä–æ–º–µ–Ω–µ–¥–∂–º–µ–Ω—Ç–∞ –∏ –±—é—Ä–æ–∫—Ä–∞—Ç–∏–∏, –∏ –ø–æ—á—É–≤—Å—Ç–≤–æ–≤–∞—Ç—å —Å–≤–æ–±–æ–¥—É —Ç–≤–æ—Ä—á–µ—Å—Ç–≤–∞. –ò—Å—Å–ª–µ–¥—É–π –≤–∞—Ä–∏–∞–Ω—Ç—ã –≥–ª—É–±–∂–µ, —Å–ª—É—à–∞–π –∫–æ–ª–ª–µ–≥, –∞—Ä–≥—É–º–µ–Ω—Ç–∏—Ä—É–π –∏ –ø—Ä–∏–Ω–∏–º–∞–π —Ä–µ—à–µ–Ω–∏—è –†–µ–∫—Ä—É—Ç–º–µ–Ω—Ç –ø—Ä–æ—Ü–µ—Å—Å:‚≠ïÔ∏è –ó–Ω–∞–∫–æ–º—Å—Ç–≤–æ —Å —Ä–µ–∫—Ä—É—Ç–µ—Ä–æ–º ----> ‚≠ïÔ∏è –¢–µ—Å—Ç–æ–≤–æ–µ –∑–∞–¥–∞–Ω–∏–µ ----> ‚≠ïÔ∏è –§–∏–Ω–∞–ª—å–Ω–æ–µ –∏–Ω—Ç–µ—Ä–≤—å—é",Data Quality Analyst@Amazing Apps,https://jobs.dou.ua/companies/amazingappstech/vacancies/133070/, Kyiv,Data Quality Analyst,25 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/epicentr-m/,Epicentr M,"{""Required skills"": [""\u0415\u043f\u0456\u0446\u0435\u043d\u0442\u0440 \u041c\u0430\u0440\u043a\u0435\u0442\u043b\u043f\u0435\u0439\u0441"", ""\u0446\u0435 \u0443\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430 \u043f\u0440\u043e\u0434\u0443\u043a\u0442\u043e\u0432\u0430 \u043a\u043e\u043c\u043f\u0430\u043d\u0456\u044f, \u0449\u043e \u0441\u0442\u0432\u043e\u0440\u044e\u0454 \u043c\u0430\u0439\u0434\u0430\u043d\u0447\u0438\u043a \u0434\u043b\u044f \u043c\u0430\u043b\u043e\u0433\u043e \u0442\u0430 \u0441\u0435\u0440\u0435\u0434\u043d\u044c\u043e\u0433\u043e \u0431\u0456\u0437\u043d\u0435\u0441\u0443 \u0432 \u0441\u0435\u0433\u043c\u0435\u043d\u0442\u0456 DIY \u0442\u0430 \u0442\u043e\u0432\u0430\u0440\u0456\u0432 \u0434\u043b\u044f \u0434\u043e\u043c\u0443. \u041e\u0444\u043b\u0430\u0439\u043d \u0447\u0430\u0441\u0442\u0438\u043d\u0430 \u043d\u0430\u0448\u043e\u0457 \u043a\u043e\u043c\u043f\u0430\u043d\u0456\u0457"", ""\u0446\u0435 \u043d\u0430\u0439\u0431\u0456\u043b\u044c\u0448\u0430 \u043c\u0435\u0440\u0435\u0436\u0430 \u0442\u043e\u0440\u0433\u043e\u0432\u0438\u0445 \u0446\u0435\u043d\u0442\u0440\u0456\u0432 \u0443 \u043a\u0440\u0430\u0457\u043d\u0456 Epicetrk.ua"", ""\u0446\u0435 \u0431\u0456\u043b\u044c\u0448 \u044f\u043a 3 \u043c\u043b\u043d. \u043f\u0435\u0440\u0435\u0433\u043b\u044f\u0434\u0456\u0432 \u0442\u043e\u0432\u0430\u0440\u0456\u0432 \u0442\u0430 \u0431\u043b\u0438\u0437\u044c\u043a\u043e 20 \u043c\u043b\u043d. \u043f\u043e\u0434\u0456\u0439 \u0432 \u0434\u0435\u043d\u044c.\u0426\u0435 \u0442\u0435\u0440\u0430\u0431\u0430\u0439\u0442\u0438 \u0434\u0430\u043d\u0438\u0445 \u044f\u043a\u0456 \u043c\u0438 \u0432\u0436\u0435 \u0437\u0431\u0438\u0440\u0430\u0454\u043c\u043e, \u0442\u0430 \u044f\u043a\u0456 \u0449\u0435 \u043f\u043e\u0442\u0440\u0456\u0431\u043d\u043e \u043f\u043e\u0447\u0430\u0442\u0438 \u0437\u0431\u0438\u0440\u0430\u0442\u0438. \u0426\u0435 \u043e\u0444\u043b\u0430\u0439\u043d \u0442\u0430 \u043e\u043d\u043b\u0430\u0439\u043d \u043f\u0440\u043e\u0434\u0430\u0436\u0456 \u043e\u0434\u043d\u043e\u0433\u043e \u0439 \u0442\u043e\u0433\u043e \u0441\u0430\u043c\u043e\u0433\u043e \u043f\u043e\u043a\u0443\u043f\u0446\u044f, \u0449\u043e \u0432\u0447\u043e\u0440\u0430 \u0434\u043e\u0434\u0430\u0432 \u0443 \u043a\u043e\u0448\u0438\u043a \u043c\u043e\u0431\u0456\u043b\u044c\u043d\u0438\u0439 \u0442\u0435\u043b\u0435\u0444\u043e\u043d, \u0430 \u0441\u044c\u043e\u0433\u043e\u0434\u043d\u0456 \u043a\u0443\u043f\u0438\u0432 \u0432 \u043e\u0444\u043b\u0430\u0439\u043d\u0456 Siltek P-15 2 mm 25 (\u043c\u0438 \u0442\u0435\u0436 \u0433\u0443\u0433\u043b\u0438\u043b\u0438).\u0426\u0435 \u043c\u0456\u0441\u0446\u0435 \u0434\u0435 \u043a\u0443\u043f\u0443\u044e\u0442\u044c \u0433\u0443\u0431\u043d\u0443 \u043f\u043e\u043c\u0430\u0434\u0443 \u0442\u0430 \u0446\u0435\u043c\u0435\u043d\u0442 \u0432 \u043e\u0434\u043d\u043e\u043c\u0443 \u0437\u0430\u043c\u043e\u0432\u043b\u0435\u043d\u043d\u0456 \u0456 \u0432 \u0446\u044c\u043e\u043c\u0443 \u0454 \u043b\u043e\u0433\u0456\u043a\u0430 \u0442\u0430 \u0441\u0442\u0456\u0439\u043a\u0438\u0439 \u043f\u0430\u0442\u0435\u0440\u043d \u043f\u043e\u0432\u0435\u0434\u0456\u043d\u043a\u0438.\u0412 \u043d\u0430\u0441 \u0434\u0456\u0439\u0441\u043d\u043e \u043f\u0440\u0438\u043a\u043e\u043b\u044c\u043d\u043e) \u0429\u043e \u043c\u0438 \u0437\u0430\u0434\u0443\u043c\u0430\u043b\u0438?""], ""We offer"": [""\u0417\u0430\u0440\u0430\u0437 \u043f\u0435\u0440\u0435\u0434 \u043d\u0430\u0448\u043e\u044e \u043a\u043e\u043c\u0430\u043d\u0434\u043e\u044e \u0441\u0442\u043e\u0457\u0442\u044c \u043a\u0456\u043b\u044c\u043a\u0430 \u0432\u0435\u043b\u0438\u0447\u0435\u0437\u043d\u0438\u0445 \u0437\u0430\u0434\u0430\u0447:"", ""\u0417\u0440\u043e\u0431\u0438\u0442\u0438 \u043a\u0440\u0443\u0442\u0438\u0439 \u043f\u043e\u0432\u043d\u043e\u0442\u0435\u043a\u0441\u0442\u043e\u0432\u0438\u0439 \u043f\u043e\u0448\u0443\u043a"", ""\u0420\u043e\u0437\u0440\u043e\u0431\u0438\u0442\u0438 \u0441\u0438\u0441\u0442\u0435\u043c\u0443 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0456\u0439 \u0442\u043e\u0432\u0430\u0440\u0456\u0432 \u0434\u043b\u044f \u043a\u043e\u0436\u043d\u043e\u0433\u043e \u043f\u043e\u043a\u0443\u043f\u0446\u044f. \u0406 \u0439\u043e\u0433\u043e \u0434\u0440\u0443\u0436\u0438\u043d\u0438. \u0406 \u043a\u043e\u0442\u0430."", ""\u0421\u0442\u0432\u043e\u0440\u0438\u0442\u0438 \u0441\u0438\u0441\u0442\u0435\u043c\u0443 \u043a\u043e\u043c\u043f\u043b\u0435\u043a\u0441\u043d\u043e\u0457 \u0430\u043d\u0430\u043b\u0456\u0442\u0438\u043a\u0438, \u044f\u043a\u0430 \u043f\u043e\u0447\u043d\u0435\u0442\u044c\u0441\u044f \u0432 \u043c\u043e\u043c\u0435\u043d\u0442 \u043a\u043e\u043b\u0438 \u043a\u043e\u0440\u0438\u0441\u0442\u0443\u0432\u0430\u0447 \u043c\u0456\u0433 \u043f\u043e\u0431\u0430\u0447\u0438\u0442\u0438 \u043d\u0430\u0448 \u0431\u0430\u043d\u043d\u0435\u0440 \u043d\u0430 \u043c\u0430\u0440\u0448\u0440\u0443\u0442\u0446\u0456 \u0432 \u041a\u043e\u0432\u0435\u043b\u0456 \u0442\u0430 \u0437\u0430\u043a\u0456\u043d\u0447\u0443\u044e\u0447\u0438 \u0447\u0430\u0439\u043e\u0432\u0438\u043c\u0438 \u043a\u0443\u0440\u2019\u0454\u0440\u0443.""], ""Responsibilities"": [""\u0425\u0442\u043e \u043d\u0430\u043c \u0442\u0440\u0435\u0431\u0430:\u0417\u0430\u0440\u0430\u0437 \u043c\u0438 \u0448\u0443\u043a\u0430\u0454\u043c\u043e \u043b\u0456\u0434\u0430 Data Science \u043a\u043e\u043c\u0430\u043d\u0434\u0438. \u0421\u0432\u0456\u0442\u0438\u043b\u043e Tensorflow \u0442\u0430 \u0441\u0443\u043f\u0435\u0440\u0441\u0430\u043c\u043e\u0441\u0442\u0456\u0439\u043d\u043e\u0433\u043e \u0440\u043e\u0437\u0440\u043e\u0431\u043d\u0438\u043a\u0430, \u044f\u043a\u0438\u0439 \u0432\u0456\u0434\u0447\u0443\u0432\u0430\u0454 \u0432 \u0441\u043e\u0431\u0456 \u0441\u0438\u043b\u0438, \u0430 \u0449\u0435 \u043a\u0440\u0430\u0449\u0435, \u043c\u0430\u0454 \u0434\u043e\u0441\u0432\u0456\u0434, \u0434\u043b\u044f \u043f\u043e\u0431\u0443\u0434\u043e\u0432\u0438 \u043f\u0456\u0434\u0440\u043e\u0437\u0434\u0456\u043b\u0443 Data Science.P.S.: \u0427\u0435\u0441\u043d\u043e, \u0432 \u043d\u0430\u0441 \u043f\u043e\u043a\u0438 \u0449\u043e \u043d\u0435\u043c\u0430\u0454 \u0442\u0435\u043e\u0440\u0435\u0442\u0438\u0447\u043d\u0438\u0445 \u0437\u0430\u0434\u0430\u0447, \u0442\u0456\u043b\u044c\u043a\u0438 \u043f\u0440\u0438\u043a\u043b\u0430\u0434\u043d\u0456.""]}",,"Required skills –ï–ø—ñ—Ü–µ–Ω—Ç—Ä –ú–∞—Ä–∫–µ—Ç–ª–ø–µ–π—Å ‚Äî —Ü–µ —É–∫—Ä–∞—ó–Ω—Å—å–∫–∞ –ø—Ä–æ–¥—É–∫—Ç–æ–≤–∞ –∫–æ–º–ø–∞–Ω—ñ—è, —â–æ —Å—Ç–≤–æ—Ä—é—î –º–∞–π–¥–∞–Ω—á–∏–∫ –¥–ª—è –º–∞–ª–æ–≥–æ —Ç–∞ —Å–µ—Ä–µ–¥–Ω—å–æ–≥–æ –±—ñ–∑–Ω–µ—Å—É –≤ —Å–µ–≥–º–µ–Ω—Ç—ñ DIY —Ç–∞ —Ç–æ–≤–∞—Ä—ñ–≤ –¥–ª—è –¥–æ–º—É. –û—Ñ–ª–∞–π–Ω —á–∞—Å—Ç–∏–Ω–∞ –Ω–∞—à–æ—ó –∫–æ–º–ø–∞–Ω—ñ—ó ‚Äî —Ü–µ –Ω–∞–π–±—ñ–ª—å—à–∞ –º–µ—Ä–µ–∂–∞ —Ç–æ—Ä–≥–æ–≤–∏—Ö —Ü–µ–Ω—Ç—Ä—ñ–≤ —É –∫—Ä–∞—ó–Ω—ñ Epicetrk.ua ‚Äî —Ü–µ –±—ñ–ª—å—à —è–∫ 3 –º–ª–Ω. –ø–µ—Ä–µ–≥–ª—è–¥—ñ–≤ —Ç–æ–≤–∞—Ä—ñ–≤ —Ç–∞ –±–ª–∏–∑—å–∫–æ 20 –º–ª–Ω. –ø–æ–¥—ñ–π –≤ –¥–µ–Ω—å.–¶–µ —Ç–µ—Ä–∞–±–∞–π—Ç–∏ –¥–∞–Ω–∏—Ö —è–∫—ñ –º–∏ –≤–∂–µ –∑–±–∏—Ä–∞—î–º–æ, —Ç–∞ —è–∫—ñ —â–µ –ø–æ—Ç—Ä—ñ–±–Ω–æ –ø–æ—á–∞—Ç–∏ –∑–±–∏—Ä–∞—Ç–∏. –¶–µ –æ—Ñ–ª–∞–π–Ω —Ç–∞ –æ–Ω–ª–∞–π–Ω –ø—Ä–æ–¥–∞–∂—ñ –æ–¥–Ω–æ–≥–æ –π —Ç–æ–≥–æ —Å–∞–º–æ–≥–æ –ø–æ–∫—É–ø—Ü—è, —â–æ –≤—á–æ—Ä–∞ –¥–æ–¥–∞–≤ —É –∫–æ—à–∏–∫ –º–æ–±—ñ–ª—å–Ω–∏–π —Ç–µ–ª–µ—Ñ–æ–Ω, –∞ —Å—å–æ–≥–æ–¥–Ω—ñ –∫—É–ø–∏–≤ –≤ –æ—Ñ–ª–∞–π–Ω—ñ Siltek P-15 2 mm 25 (–º–∏ —Ç–µ–∂ –≥—É–≥–ª–∏–ª–∏).–¶–µ –º—ñ—Å—Ü–µ –¥–µ –∫—É–ø—É—é—Ç—å –≥—É–±–Ω—É –ø–æ–º–∞–¥—É —Ç–∞ —Ü–µ–º–µ–Ω—Ç –≤ –æ–¥–Ω–æ–º—É –∑–∞–º–æ–≤–ª–µ–Ω–Ω—ñ —ñ –≤ —Ü—å–æ–º—É —î –ª–æ–≥—ñ–∫–∞ —Ç–∞ —Å—Ç—ñ–π–∫–∏–π –ø–∞—Ç–µ—Ä–Ω –ø–æ–≤–µ–¥—ñ–Ω–∫–∏.–í –Ω–∞—Å –¥—ñ–π—Å–Ω–æ –ø—Ä–∏–∫–æ–ª—å–Ω–æ) –©–æ –º–∏ –∑–∞–¥—É–º–∞–ª–∏? –ó–∞—Ä–∞–∑ –ø–µ—Ä–µ–¥ –Ω–∞—à–æ—é –∫–æ–º–∞–Ω–¥–æ—é —Å—Ç–æ—ó—Ç—å –∫—ñ–ª—å–∫–∞ –≤–µ–ª–∏—á–µ–∑–Ω–∏—Ö –∑–∞–¥–∞—á:‚Äî –ó—Ä–æ–±–∏—Ç–∏ –∫—Ä—É—Ç–∏–π –ø–æ–≤–Ω–æ—Ç–µ–∫—Å—Ç–æ–≤–∏–π –ø–æ—à—É–∫‚Äî –†–æ–∑—Ä–æ–±–∏—Ç–∏ —Å–∏—Å—Ç–µ–º—É —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ–π —Ç–æ–≤–∞—Ä—ñ–≤ –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –ø–æ–∫—É–ø—Ü—è. –Ü –π–æ–≥–æ –¥—Ä—É–∂–∏–Ω–∏. –Ü –∫–æ—Ç–∞.‚Äî –°—Ç–≤–æ—Ä–∏—Ç–∏ —Å–∏—Å—Ç–µ–º—É –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ—ó –∞–Ω–∞–ª—ñ—Ç–∏–∫–∏, —è–∫–∞ –ø–æ—á–Ω–µ—Ç—å—Å—è –≤ –º–æ–º–µ–Ω—Ç –∫–æ–ª–∏ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á –º—ñ–≥ –ø–æ–±–∞—á–∏—Ç–∏ –Ω–∞—à –±–∞–Ω–Ω–µ—Ä –Ω–∞ –º–∞—Ä—à—Ä—É—Ç—Ü—ñ –≤ –ö–æ–≤–µ–ª—ñ —Ç–∞ –∑–∞–∫—ñ–Ω—á—É—é—á–∏ —á–∞–π–æ–≤–∏–º–∏ –∫—É—Ä‚Äô—î—Ä—É. –•—Ç–æ –Ω–∞–º —Ç—Ä–µ–±–∞:–ó–∞—Ä–∞–∑ –º–∏ —à—É–∫–∞—î–º–æ –ª—ñ–¥–∞ Data Science –∫–æ–º–∞–Ω–¥–∏. –°–≤—ñ—Ç–∏–ª–æ Tensorflow —Ç–∞ —Å—É–ø–µ—Ä—Å–∞–º–æ—Å—Ç—ñ–π–Ω–æ–≥–æ —Ä–æ–∑—Ä–æ–±–Ω–∏–∫–∞, —è–∫–∏–π –≤—ñ–¥—á—É–≤–∞—î –≤ —Å–æ–±—ñ —Å–∏–ª–∏, –∞ —â–µ –∫—Ä–∞—â–µ, –º–∞—î –¥–æ—Å–≤—ñ–¥, –¥–ª—è –ø–æ–±—É–¥–æ–≤–∏ –ø—ñ–¥—Ä–æ–∑–¥—ñ–ª—É Data Science.P.S.: –ß–µ—Å–Ω–æ, –≤ –Ω–∞—Å –ø–æ–∫–∏ —â–æ –Ω–µ–º–∞—î —Ç–µ–æ—Ä–µ—Ç–∏—á–Ω–∏—Ö –∑–∞–¥–∞—á, —Ç—ñ–ª—å–∫–∏ –ø—Ä–∏–∫–ª–∞–¥–Ω—ñ. –í –∫–∞–Ω–¥–∏–¥–∞—Ç–∞—Ö —à—É–∫–∞—î–º–æ –Ω–∞—Å—Ç—É–ø–Ω–µ :‚Äî –î–æ—Å–≤—ñ–¥ –≤—ñ–¥ 3-—Ö —Ä–æ–∫—ñ–≤ —Ç–∞ –≥–æ—Ç–æ–≤—ñ –ø—Ä–æ–µ–∫—Ç–∏ —É –ø–æ—Ä—Ç—Ñ–æ–ª—ñ–æ‚Äî –°–∞–º–æ—Å—Ç—ñ–π–Ω—ñ—Å—Ç—å —Ç–∞ –≤–º—ñ–Ω–Ω—è –ø–æ—è—Å–Ω–∏—Ç–∏ —á–æ–º—É —â–æ—Å—å –ø—Ä–∞—Ü—é—î —Å–∞–º–µ —Ç–∞–∫, –∞ –Ω–µ —ñ–Ω–∞–∫—à–µ‚Äî –í–∏—Å–æ–∫–∏–π —Ä—ñ–≤–µ–Ω—å –∫–æ–º—É–Ω—ñ–∫–∞—Ü—ñ–π–Ω–∏—Ö –Ω–∞–≤–∏—á–æ–∫. –¢–∏ –∂ –ª—ñ–¥, —è–∫ –Ω—ñ —è–∫)‚Äî –ú–∞—Ç–µ–º–∞—Ç–∏—á–Ω–∞ –±–∞–∑–∞: —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞, —Ç–µ–æ—Ä—ñ—è –≤—ñ—Ä–æ–≥—ñ–¥–Ω–æ—Å—Ç–µ–π, –≥–∞—Ä–Ω—ñ –∑–Ω–∞–Ω–Ω—è –∞–ª–≥–æ—Ä–∏—Ç–º—ñ–≤‚Äî –ó–Ω–∞–Ω–Ω—è –∫–ª–∞—Å–∏—á–Ω–∏—Ö –∞–ª–≥–æ—Ä–∏—Ç–º—ñ–≤ ML‚Äî –ì–∞—Ä–Ω—ñ –∑–Ω–∞–Ω–Ω—è Python (C++)‚Äî PyTorch/Tensorflow (Keras)‚Äî Elastic Search. –¢–æ–º—É —â–æ –≤ –Ω–∞—Å –Ω–∞ –Ω—å–æ–º—É –ø–æ—à—É–∫‚Äî –ë—É—Ç–∏ –±–æ–∂–µ–Ω—å–∫–æ—é SQL‚Äî –í–º—ñ–Ω–Ω—è —à—É–∫–∞—Ç–∏ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ –Ω–∞ —Å–≤–æ—ó –ø–∏—Ç–∞–Ω–Ω—è —Ç–∞ —à–≤–∏–¥–∫–æ –≤—á–∏—Ç–∏—Å—å‚Äî –î–æ—Å–≤—ñ–¥ —Ä–æ–±–æ—Ç–∏ –∑ WEB, –∫–æ–ª–∏ –≤ —Ç–µ–ª–µ 100500 —ñ–≤–µ–Ω—Ç—ñ–≤ –≤ —Å–µ–∫—É–Ω–¥–∏‚Äî Docker‚Äî –í–º—ñ–Ω–Ω—è –ø–∏—Å–∞—Ç–∏ –∑—Ä–æ–∑—É–º—ñ–ª—ñ –¥–ª—è —ñ–Ω—à–∏—Ö –ª—é–¥–µ–π –ª–æ–≥–∏ —Å–≤–æ—ó—Ö —Å–µ—Ä–≤—ñ—Å—ñ–≤‚Äî –ó–Ω–∞–Ω–Ω—è ETL —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ñ–≤‚Äî RabbitMQ‚Äî MongoDB‚Äî Big Query\ClickHouse\Amazon Redshift–ü—Ä–∏—Ö–æ–¥—å. –ë—É–¥–µ –∫—Ä—É—Ç–æ ;) We offer ‚óè –û—Ñ—ñ—Ü—ñ–π–Ω–µ –ø—Ä–∞—Ü–µ–≤–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è;‚óè –û–ø–ª–∞—Ç—É –ª—ñ–∫–∞—Ä–Ω—è–Ω–∏—Ö —ñ –≤—ñ–¥–ø—É—Å—Ç–∫—É 24 –∫–∞–ª–µ–Ω–¥–∞—Ä–Ω–∏—Ö –¥–Ω—ñ;‚óè –ö–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω—É —ñ –≤—á–∞—Å–Ω—É –∑–∞—Ä–æ–±—ñ—Ç–Ω—é –ø–ª–∞—Ç—É;‚óè –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∏–π 8 –≥–æ–¥–∏–Ω–Ω–∏–π —Ä–æ–±–æ—á–∏–π –¥–µ–Ω—å;‚óè –û—Ñ—ñ—Å –Ω–∞ –ë—Ä–∞—Ç–∏—Å–ª–∞–≤—Å—å–∫—ñ–π. Responsibilities –ó–∞—Ä–∞–∑ –ø–µ—Ä–µ–¥ –Ω–∞—à–æ—é –∫–æ–º–∞–Ω–¥–æ—é —Å—Ç–æ—ó—Ç—å –∫—ñ–ª—å–∫–∞ –≤–µ–ª–∏—á–µ–∑–Ω–∏—Ö –∑–∞–¥–∞—á:‚óè –ó—Ä–æ–±–∏—Ç–∏ –∫—Ä—É—Ç–∏–π –ø–æ–≤–Ω–æ—Ç–µ–∫—Å—Ç–æ–≤–∏–π –ø–æ—à—É–∫‚óè –†–æ–∑—Ä–æ–±–∏—Ç–∏ —Å–∏—Å—Ç–µ–º—É —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ–π —Ç–æ–≤–∞—Ä—ñ–≤ –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –ø–æ–∫—É–ø—Ü—è. –Ü –π–æ–≥–æ –¥—Ä—É–∂–∏–Ω–∏. –Ü –∫–æ—Ç–∞.‚óè –°—Ç–≤–æ—Ä–∏—Ç–∏ —Å–∏—Å—Ç–µ–º—É –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ—ó –∞–Ω–∞–ª—ñ—Ç–∏–∫–∏, —è–∫–∞ –ø–æ—á–Ω–µ—Ç—å—Å—è –≤ –º–æ–º–µ–Ω—Ç –∫–æ–ª–∏ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á –º—ñ–≥ –ø–æ–±–∞—á–∏—Ç–∏ –Ω–∞—à –±–∞–Ω–Ω–µ—Ä –Ω–∞ –º–∞—Ä—à—Ä—É—Ç—Ü—ñ –≤ –ö–æ–≤–µ–ª—ñ —Ç–∞ –∑–∞–∫—ñ–Ω—á—É—é—á–∏ —á–∞–π–æ–≤–∏–º–∏ –∫—É—Ä‚Äô—î—Ä—É.",Lead Data Science@Epicentr M,https://jobs.dou.ua/companies/epicentr-m/vacancies/133045/, Kyiv,Lead Data Science,25 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/mgid/,MGID,{},,"MGID ‚Äî –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω–∞—è –∫–æ–º–ø–∞–Ω–∏—è-–ª–∏–¥–µ—Ä –Ω–∞ —Ä—ã–Ω–∫–µ –Ω–∞—Ç–∏–≤–Ω–æ–π —Ä–µ–∫–ª–∞–º—ã, –æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è –≤ 2008 –≥–æ–¥—É. –ü–ª–∞—Ç—Ñ–æ—Ä–º–∞ MGID –ø–æ–º–æ–≥–∞–µ—Ç –º–µ–¥–∏–∞ –º–æ–Ω–µ—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∞—É–¥–∏—Ç–æ—Ä–∏—é, –∞ –±—Ä–µ–Ω–¥–∞–º ‚Äî –¥–æ–Ω–µ—Å—Ç–∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ —Ä–µ–∫–ª–∞–º–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ —Å–≤–æ–∏–º –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—è–º. MGID –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è: –æ—Ç –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ —Ä–µ–∫–ª–∞–º–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏, –¥–æ –µ–µ –≤–Ω–µ–¥—Ä–µ–Ω–∏—è –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏. –ù–∞—à–∏ –∫–ª–∏–µ–Ω—Ç—ã ‚Äî –∫—Ä—É–ø–Ω—ã–µ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –±—Ä–µ–Ω–¥—ã: Renault, Domino‚Äôs, airbnb, PizzaHut, Qatar Airlines –∏ –º–Ω–æ–≥–æ –¥—Ä—É–≥–∏—Ö –∫–æ–º–ø–∞–Ω–∏–π, –≤–∫–ª—é—á–∞—è –º–µ–¥–∏–∞—Ö–æ–ª–¥–∏–Ω–≥–∏ –∏ —Å–µ—Ç–µ–≤—ã–µ –∞–≥–µ–Ω—Ç—Å—Ç–≤–∞. MGID ‚Äî —ç—Ç–æ:‚Äî –û–¥–Ω–∞ –∏–∑ —Å–∞–º—ã—Ö –±–æ–ª—å—à–∏—Ö martech-–∫–æ–º–ø–∞–Ω–∏–π –Ω–∞ —É–∫—Ä–∞–∏–Ω—Å–∫–æ–º —Ä—ã–Ω–∫–µ;‚Äî –°–æ–±—Å—Ç–≤–µ–Ω–Ω–∞—è Highload –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç 185 –º–ª—Ä–¥ –∫–æ–Ω—Ç–µ–Ω—Ç–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –¥–ª—è 850 –º–ª–Ω —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–æ—Å–µ—Ç–∏—Ç–µ–ª–µ–π –Ω–∞ –±–æ–ª–µ–µ —á–µ–º 60 —è–∑—ã–∫–∞—Ö;‚Äî –ü–æ–±–µ–¥–∏—Ç–µ–ª—å –º–Ω–æ–≥–æ—á–∏—Å–ª–µ–Ω–Ω—ã—Ö –ø—Ä–µ–º–∏–π –∑–∞ –∏–Ω–Ω–æ–≤–∞—Ü–∏–∏ –∏ –∫–∞—á–µ—Å—Ç–≤–æ –ø—Ä–æ–¥—É–∫—Ç–∞ –≤ —Å—Ñ–µ—Ä–µ AdTech;‚Äî –®—Ç–∞—Ç –±–æ–ª–µ–µ 500 —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–æ–≤, —Ä–∞–±–æ—Ç–∞—é—â–∏—Ö –≤ –æ—Ñ–∏—Å–∞—Ö –≤ –°–®–ê, –ï–≤—Ä–æ–ø–µ –∏ –ê–∑–∏–∏;‚Äî –ù–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–µ –∏–Ω–Ω–æ–≤–∞—Ü–∏–∏, –æ–±–º–µ–Ω –æ–ø—ã—Ç–æ–º –∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞–º–∏ –º–µ–∂–¥—É —Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã–º–∏ –∫–æ–º–∞–Ω–¥–∞–º–∏. –ó–Ω–∞–Ω–∏—è –∏ –Ω–∞–≤—ã–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω—ã:‚Äî –î–∏–ø–ª–æ–º –±–∞–∫–∞–ª–∞–≤—Ä–∞ –≤ –æ–±–ª–∞—Å—Ç–∏ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–π –∏–Ω–∂–µ–Ω–µ—Ä–∏–∏ –∏–ª–∏ —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω—ã–π –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –æ–ø—ã—Ç;‚Äî 5 –ª–µ—Ç –æ–ø—ã—Ç–∞ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–æ–π –≤—ã—Å–æ–∫–æ—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–Ω—ã—Ö –ø—Ä–æ–¥—É–∫—Ç–æ–≤ –¥–ª—è –≤–µ–± –≤ —Ä–æ–ª–∏ –ø—Ä–æ–¥—É–∫—Ç–æ–≤–æ–≥–æ/–ø—Ä–æ–µ–∫—Ç–Ω–æ–≥–æ –º–µ–Ω–µ–¥–∂–µ—Ä–∞, —Ç–∏–º –∏–ª–∏ —Ç–µ—Ö –ª–∏–¥–∞;‚Äî –û–ø—ã—Ç –∞–Ω–∞–ª–∏–∑–∞ –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–æ–≤ –¥–∞–Ω–Ω—ã—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ (MapReduce, Hadoop, Clickhouse, Vertica etc);‚Äî –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –æ–ø—ã—Ç –≤ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–∏ –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–º –∞–Ω–∞–ª–∏–∑–µ;‚Äî –ì–ª—É–±–æ–∫–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Ç–æ–≥–æ, –∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç –ò–Ω—Ç–µ—Ä–Ω–µ—Ç —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏, —Å–æ—Ü–∏–∞–ª—å–Ω–æ –∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏. –ò–Ω—Ç–µ—Ä–µ—Å –∫ –∏–Ω–Ω–æ–≤–∞—Ü–∏—è–º –∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è–º;‚Äî –û–ø—ã—Ç —Ö–æ—Ç—è –±—ã –≤ –æ–¥–Ω–æ–π –∏–∑ —Å–ª–µ–¥—É—é—â–∏—Ö –æ–±–ª–∞—Å—Ç–µ–π: –æ–Ω–ª–∞–π–Ω —Ä–µ–∫–ª–∞–º–∞, –±–æ—Ä—å–±–∞ —Å –Ω–∞–∫—Ä—É—Ç–∫–æ–π, SEO –∏ —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω–∞—è –∫–æ–º–º–µ—Ä—Ü–∏—è;‚Äî –•–æ—Ä–æ—à–∏–µ –∫–æ–º–º—É–Ω–∏–∫–∞—Ç–∏–≤–Ω—ã–µ –Ω–∞–≤—ã–∫–∏. –ß–µ–º –ø—Ä–µ–¥—Å—Ç–æ–∏—Ç –∑–∞–Ω–∏–º–∞—Ç—å—Å—è:‚Äî –ö—É—Ä–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–æ–µ–∫—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–º–æ–≥—É—Ç –∑–∞—â–∏—Ç–∏—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π MGID –æ—Ç –Ω–µ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω–≤–µ–Ω—Ç–∞—Ä—è, –Ω–µ–ø—Ä–∏–µ–º–ª–µ–º–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏ –Ω–∞–∫—Ä—É—Ç–∫–∏;‚Äî –û–ø—Ä–µ–¥–µ–ª—è—Ç—å –∏ —Ä–∞–∑—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã–µ —Ä–µ—à–µ–Ω–∏—è –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –≤—ã—è–≤–ª–µ–Ω–∏—è –∏ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –Ω–∞–∫—Ä—É—Ç–∫–∏ –∏ –ø–æ–ª—É—á–µ–Ω–∏—è –Ω–µ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω–≤–µ–Ω—Ç–∞—Ä—è —Ä–µ–∫–ª–∞–º–æ–¥–∞—Ç–µ–ª—è–º–∏;‚Äî –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç—å –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π/–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –Ω–æ–≤—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤, –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏, —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ –∏ –¥–æ–≤–æ–¥–∏—Ç—å —É—Ç–∏–ª–∏–∑–∞—Ü–∏—é —ç—Ç–∏—Ö –Ω–æ–≤—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤ –¥–æ –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω–æ–≥–æ –≤–Ω–µ–¥—Ä–µ–Ω–∏—è;‚Äî –ü—Ä–æ–∞–∫—Ç–∏–≤–Ω–æ –≤—ã—è–≤–ª—è—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –∏ —É–ª—É—á—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏;‚Äî –ü—Ä–æ–≤–æ–¥–∏—Ç—å –≤—Å–µ–æ–±—ä–µ–º–ª—é—â–∏–π –∞–Ω–∞–ª–∏–∑ —Ä–µ—à–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é –∫—Ä–æ—Å—Å-–∫–æ–º–∞–Ω–¥–Ω–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è. –ú—ã –ø—Ä–µ–¥–ª–∞–≥–∞–µ–º:‚Äî –ù–æ–≤—ã–π –æ—Ñ–∏—Å –≤ –ë–¶ ¬´–ú–∞—Ä–º–µ–ª–∞–¥¬ª;‚Äî –ö—É—Ä—Å—ã –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ —Å –Ω–æ—Å–∏—Ç–µ–ª–µ–º;‚Äî –ì–∏–±–∫–∏–π –ø–æ–¥—Ö–æ–¥ –∫ –≥—Ä–∞—Ñ–∏–∫—É. –î–ª—è –Ω–∞—Å –≥–ª–∞–≤–Ω–æ–µ ‚Äî —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏ –≤–æ–≤–ª–µ—á–µ–Ω–Ω–æ—Å—Ç—å;‚Äî –ö–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–æ–µ —É—á–∞—Å—Ç–∏–µ –≤ —Å–ø–æ—Ä—Ç–∏–≤–Ω—ã—Ö, —ç–∫–æ- –∏ —Å–æ—Ü–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–æ–µ–∫—Ç–∞—Ö. –ö–∞–∫ —ç—Ç–æ –±—ã–≤–∞–µ—Ç ‚Äî —Ç—É—Ç www.facebook.com/MGID.inside‚Äî –ü–∞–∫–µ—Ç –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π —Å—Ç—Ä–∞—Ö–æ–≤–∫–∏ —É—Ä–æ–≤–Ω—è ¬´–ü—Ä–µ–º–∏—É–º¬ª.",Head of¬†Fraud Protection Unit (AdTech)@MGID,https://jobs.dou.ua/companies/mgid/vacancies/133044/, Kyiv,Head of¬†Fraud Protection Unit (AdTech),25 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/socialtech/,SocialTech,{},,"–ü—Ä–æ –ø—Ä–æ–¥—É–∫—Ç–†—ñ–∫ —Ç–æ–º—É –º–∏ –∑–∞–ø—É—Å—Ç–∏–ª–∏ RnD –Ω–∞–ø—Ä—è–º–æ–∫ –≤ –æ–¥–Ω—ñ–π –∑ –Ω–∞–π–±—ñ–ª—å—à–∏—Ö –ø—Ä–æ–¥—É–∫—Ç–æ–≤–∏—Ö IT-–∫–æ–º–ø–∞–Ω—ñ–π –£–∫—Ä–∞—ó–Ω–∏. –ú–µ—Ç–∞ ‚Äî –Ω–∞ –æ—Å–Ω–æ–≤—ñ –≥–æ–ª–æ–≤–Ω–æ–≥–æ –±—ñ–∑–Ω–µ—Å—É –∫–æ–º–ø–∞–Ω—ñ—ó –ø–æ–±—É–¥—É–≤–∞—Ç–∏ –ø—Ä–æ–¥—É–∫—Ç –∑ –≤—ñ–¥–º—ñ–Ω–Ω–æ—é –±—ñ–∑–Ω–µ—Å-–º–æ–¥–µ–ª–ª—é.–ó–∞ —Ü–µ–π —á–∞—Å –º–∏ –ø—ñ–¥—Ç–≤–µ—Ä–¥–∏–ª–∏ —Ü—ñ–Ω–Ω—ñ—Å–Ω—É –≥—ñ–ø–æ—Ç–µ–∑—É –Ω–æ–≤–æ–≥–æ –ø—Ä–æ–¥—É–∫—Ç—É: –Ω–∞–≤—á–∏–ª–∏—Å—è –æ–∫—É–ø–∞—Ç–∏ –¥–µ—Å–∫—Ç–æ–ø –≤–µ—Ä—Å—ñ—é —Å–∞–π—Ç—É —ñ –Ω–∞ –ø–æ—Ä–æ–∑—ñ –ø–æ–±—É–¥–æ–≤–∏ –ø–æ–∑–∏—Ç–∏–≤–Ω–æ—ó —é–Ω—ñ—Ç-–µ–∫–æ–Ω–æ–º—ñ–∫–∏ –Ω–∞ –º–æ–±—ñ–ª—å–Ω—ñ–π –≤–µ—Ä—Å—ñ—ó.–ù–∞—à –Ω–∞—Å—Ç—É–ø–Ω–∏–π –µ—Ç–∞–ø ‚Äî –º–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è. –£–∂–µ –∑–∞—Ä–∞–∑ –≤—Ö–æ–¥–∏–º–æ –≤ —Ç–æ–ø 100 –∫–æ–º–ø–∞–Ω—ñ–π –£–∫—Ä–∞—ó–Ω–∏ –∑–∞ –æ–±—Å—è–≥–∞–º–∏ –∑–∞–∫—É–ø—ñ–≤–ª—ñ —Ç—Ä–∞—Ñ—ñ–∫—É. –î–æ –∫—ñ–Ω—Ü—è –Ω–∞—Å—Ç—É–ø–Ω–æ–≥–æ —Ä–æ–∫—É —É–≤—ñ–π–¥–µ–º–æ –≤ —Ç–æ–ø 5.–¢—ñ–ª—å–∫–∏ –¥–æ –∫—ñ–Ω—Ü—è —Ü—å–æ–≥–æ —Ä–æ–∫—É –≤–∏—Ä–æ—Å—Ç–µ–º–æ –ø–æ –≤–∏—Ä—É—á—Ü—ñ –≤ ~ 20x.–ü—Ä–æ –∫–æ–º–∞–Ω–¥—É–ö–æ–º–∞–Ω–¥–∞ —Å–∫–ª–∞–¥–∞—î—Ç—å—Å—è –∑ –¥–µ—Å—è—Ç–∏ —Ñ–∞—Ö—ñ–≤—Ü—ñ–≤ Senior-—Ä—ñ–≤–Ω—è. –ù–µ–≤–µ–ª–∏–∫–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –ª—é–¥–µ–π –∑–∞–±–µ–∑–ø–µ—á—É—î –∫–æ–º–∞–Ω–¥–Ω—É –≥–Ω—É—á–∫—ñ—Å—Ç—å —ñ –≤–∏—Å–æ–∫–∏–π —Ñ–æ–∫—É—Å: –∫–æ–∂–µ–Ω –ø—Ä–∞—Ü—é—î –Ω–∞–¥ —Å–≤–æ—î—é –∑–æ–Ω–æ—é –≤—ñ–¥–ø–æ–≤—ñ–¥–∞–ª—å–Ω–æ—Å—Ç—ñ –≤ –¥–æ—Å—è–≥–Ω–µ–Ω–Ω—ñ –∑–∞–≥–∞–ª—å–Ω–∏—Ö —Ü—ñ–ª–µ–π.–ø–µ—Ä–µ–≤–∞–≥–∏:‚óè –º—ñ–Ω—ñ–º—É–º –±—é—Ä–æ–∫—Ä–∞—Ç—ñ—ó, –º–∞–∫—Å–∏–º—É–º —Å–∞–º–æ—Å—Ç—ñ–π–Ω–æ—Å—Ç—ñ.‚óè —Å–≤–æ–±–æ–¥–∞ –¥—ñ–π —ñ –≤–∏—Å–æ–∫–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥–∞–ª—å–Ω—ñ—Å—Ç—å.‚óè –≥–ª–æ–±–∞–ª—å–Ω—ñ —Ä—ñ—à–µ–Ω–Ω—è –ø—Ä–∏–π–º–∞—î–º–æ —Ä–∞–∑–æ–º; –ª–æ–∫–∞–ª—å–Ω—ñ ‚Äî –∫–æ–∂–µ–Ω —Å–∞–º. –Ø–∫—â–æ —ñ —Ç–µ–±–µ –¥—Ä–∞–π–≤–∏—Ç—å —à–≤–∏–¥–∫—ñ—Å—Ç—å, –≤—ñ–¥–ø–æ–≤—ñ–¥–∞–ª—å–Ω—ñ—Å—Ç—å —ñ –ø—Ä—è–º–∏–π –≤–ø–ª–∏–≤ –Ω–∞ –±—ñ–∑–Ω–µ—Å-—Ä—ñ—à–µ–Ω–Ω—è, —Ç–æ–¥—ñ –¥–∞–≤–∞–π –∑–Ω–∞–π–æ–º–∏—Ç–∏—Å—è!–ê —â–µ —Ç–∏ –∑–Ω–∞—î—à –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ –Ω–∞ –Ω–∞—Å—Ç—É–ø–Ω—ñ –∑–∞–ø–∏—Ç–∞–Ω–Ω—è:‚óè –Ø–∫ –æ—Ü—ñ–Ω–∏—Ç–∏ —è–∫—ñ—Å—Ç—å —Ç—Ä–∞—Ñ—ñ–∫—É –≤ –ø–µ—Ä—à—É –¥–æ–±—É –≤ —Ç—Ä–∞–Ω–∑–∞–∫—Ü—ñ–π–Ω–æ–º—É –ø—Ä–µ–º—ñ—É–º –±—ñ–∑–Ω–µ—Å—ñ (–∫—Ä–∏–≤–∞ –ø—Ä–∏–±—É—Ç–∫–æ–≤–æ—Å—Ç—ñ —Ä–æ–∑—Ç—è–≥–Ω—É—Ç–∞ –Ω–∞ —Ä–æ–∫–∏,% –∫–æ–Ω–≤–µ—Ä—Å—ñ—ó –≤ –ø–ª–∞—Ç–Ω–∏–∫–∞ –Ω–∏–∑—å–∫–∏–π)?‚óè –Ø–∫ —à–≤–∏–¥–∫–æ –º–æ–∂–Ω–∞ –ø—Ä–∏–π–º–∞—Ç–∏ —Ä—ñ—à–µ–Ω–Ω—è —â–æ–¥–æ –≤—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è —Ä–µ–∫–ª–∞–º–∏ / –ø—ñ–¥–≤–∏—â–µ–Ω–Ω—è –±—é–¥–∂–µ—Ç—ñ–≤?‚óè –Ø–∫ –æ—Ü—ñ–Ω—é–≤–∞—Ç–∏ A/B —Ç–µ—Å—Ç –Ω–∞ —Å—Ç–∞—Ä–∏—Ö –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤ –∑ —Ä—ñ–∑–Ω–æ—é –¥–æ–≤–∂–∏–Ω–æ—é –∂–∏—Ç—Ç—è / —à–≤–∏–¥–∫—ñ—Å—Ç—é —Å–ø–æ–∂–∏–≤–∞–Ω–Ω—è –∫–æ–Ω—Ç–µ–Ω—Ç—É?‚óè –Ø–∫ –≤–∏–∑–Ω–∞—á–∏—Ç–∏ —Ü—ñ–ª—å–æ–≤–æ–≥–æ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞ –ø–æ –π–æ–≥–æ –ø–æ–≤–µ–¥—ñ–Ω–∫–æ–≤–∏–º —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º? –î–ª—è —Ü—å–æ–≥–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—à:‚óè –ê–Ω–∞–ª—ñ—Ç–∏–∫—É –º–∞—Ä–∫–µ—Ç–∏–Ω–≥—É / –ø—Ä–æ–¥–∞–∂—ñ–≤ (–≤–æ—Ä–æ–Ω–∫–∏, AO, –ø—Ä–æ–≥–Ω–æ–∑–∏ –æ–±—Å—è–≥—ñ–≤, —ñ —Ç.–¥.)‚óè –î–æ–±—Ä–µ –∑–Ω–∞–Ω–Ω—è A/B, A/B/n, A/A —Ç–µ—Å—Ç—ñ–≤ —ñ —ó—Ö –æ—Ü—ñ–Ω–∫–∞.‚óè SQL, Excel, R / Python, Tableau / Power BI / etc. –ü—Ä–∏ —Ü—å–æ–º—É —Ç–∏:‚óè –ø—Ä–æ–∞–∫—Ç–∏–≤–Ω–∏–π (–Ω–µ –±–æ—ó—à—Å—è –ø—Ä–æ—è–≤–ª—è—Ç–∏ —ñ–Ω—ñ—Ü—ñ–∞—Ç–∏–≤—É —ñ –ø—Ä–æ–ø–æ–Ω—É–≤–∞—Ç–∏ –Ω–æ–≤—ñ –Ω–µ—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ñ —Ä—ñ—à–µ–Ω–Ω—è).‚óè –ø—Ä–∞—Ü—é—î—à –Ω–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç (¬´–±–∞—á—É –º–µ—Ç—É ‚Äî –Ω–µ –±–∞—á—É –ø–µ—Ä–µ—à–∫–æ–¥¬ª).‚óè –∞–º–±—ñ—Ç–Ω–∏–π (—Ö–æ—á–µ—à —Å—Ç–∞—Ç–∏ –Ω–∞–π–∫—Ä–∞—â–∏–º —É –≤—Å—å–æ–º—É, —â–æ —Ä–æ–±–∏—à).‚óè –ø–æ–∑–∏—Ç–∏–≤–Ω–æ –º–∏—Å–ª–∏—à (¬´—Ü–µ –Ω–µ –ø—Ä–æ–±–ª–µ–º–∞ ‚Äî —Ü–µ –º–æ–∂–ª–∏–≤—ñ—Å—Ç—å¬ª).‚óè –≤—ñ–¥–∫—Ä–∏—Ç–∏–π –¥–æ –Ω–æ–≤–æ–≥–æ (–≤–∏–∑–Ω–∞—î—à —Å–≤–æ—ó –ø–æ–º–∏–ª–∫–∏ —ñ –Ω–µ—Å–µ—à –≤—ñ–¥–ø–æ–≤—ñ–¥–∞–ª—å–Ω—ñ—Å—Ç—å –∑–∞ –Ω–∏—Ö). –°–ø—Ä–æ–±—É—î–º–æ ¬´–∑–¥–∏–≤—É–≤–∞—Ç–∏¬ª —É–º–æ–≤–∞–º–∏:‚óè –ì–Ω—É—á–∫–∏–π –≥—Ä–∞—Ñ—ñ–∫.‚óè –ë–µ–∑–∫–æ—à—Ç–æ–≤–Ω—ñ —Å–Ω—ñ–¥–∞–Ω–∫–∏, –æ–±—ñ–¥–∏ —Ç–∞ —Å–Ω–µ–∫–∏.‚óè –ü—Ä–æ—Ñ–µ—Å—ñ–π–Ω–∏–π —Ä–æ–∑–≤–∏—Ç–æ–∫.‚óè –ö–æ–º–ø–µ–Ω—Å–∞—Ü—ñ—è —Å–ø–æ—Ä—Ç–∏–≤–Ω–∏—Ö –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–µ–π (—î –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–∞ –∫–æ–º–∞–Ω–¥–∞ –∑ —Ñ—É—Ç–±–æ–ª—É).‚óè –°—Ç—Ä–∞—Ö–æ–≤–∏–π –º–µ–¥–∏—á–Ω–∏–π –ø–æ–ª—ñ—Å –¥–ª—è –≤—Å—ñ—Ö —Å–ø—ñ–≤—Ä–æ–±—ñ—Ç–Ω–∏–∫—ñ–≤ —Ä—ñ–≤–Ω—è ¬´Elite¬ª.‚óè –ö–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–∏–π –ª—ñ–∫–∞—Ä.‚óè –î–æ—Å—Ç—É–ø –¥–æ –∫–æ—Ä–∏—Å–Ω–æ—ó –ª—ñ—Ç–µ—Ä–∞—Ç—É—Ä–∏, —Ç—Ä–µ–Ω—ñ–Ω–≥—ñ–≤, —Å–µ–º—ñ–Ω–∞—Ä—ñ–≤.‚óè –£—á–∞—Å—Ç—å –≤ –∫–ª—é—á–æ–≤–∏—Ö –∑–∞—Ö–æ–¥–∞—Ö IT —ñ–Ω–¥—É—Å—Ç—Ä—ñ—ó (—è–∫ –≤ –£–∫—Ä–∞—ó–Ω—ñ, —Ç–∞–∫ —ñ –∑–∞ –∫–æ—Ä–¥–æ–Ω–æ–º).‚óè –ü—Ä–∞—Ü—é—î–º–æ –ø–æ –§–û–ü—É, 3 –≥—Ä—É–ø–∞. –ë—É—Ö–≥–∞–ª—Ç–µ—Ä –≤—ñ–¥–∫—Ä–∏–≤–∞—î, –≤–µ–¥–µ —ñ –∫–æ–Ω—Å—É–ª—å—Ç—É—î –§–û–ü.‚óè –†–æ–±–æ—Ç–∞ –≤ –∫—Ä—É—Ç—ñ–π –∫–æ–º–∞–Ω–¥—ñ. –ú–∏ –æ—Å–æ–±–ª–∏–≤—É —É–≤–∞–≥—É –ø—Ä–∏–¥—ñ–ª—è—î–º–æ –ø–æ—à—É–∫—É —Ç–∞–ª–∞–Ω—Ç—ñ–≤ —ñ —ó—Ö —Ä–æ–∑–≤–∏—Ç–∫—É, —Ç–æ–º—É –≤—Å—ñ —Å–ø—ñ–≤—Ä–æ–±—ñ—Ç–Ω–∏–∫–∏ –∞–∫—Ç–∏–≤–Ω—ñ —ñ –º–æ—Ç–∏–≤–æ–≤–∞–Ω—ñ –Ω–∞ –Ω–µ—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–µ —Ä—ñ—à–µ–Ω–Ω—è –∑–∞–¥–∞—á.‚óè –ù–æ–≤–∏–π –≤–µ–ª–∏–∫–∏–π —ñ –ø—Ä–æ—Å—Ç–æ—Ä–∏–π –æ—Ñ—ñ—Å –Ω–∞ –ü–æ–¥–æ–ª—ñ (–≤—É–ª. –ö–∏—Ä–∏–ª—ñ–≤—Å—å–∫–∞, 40–ê. 5 —Ö–≤–∏–ª–∏–Ω –≤—ñ–¥ –º.–¢–∞—Ä–∞—Å–∞ –®–µ–≤—á–µ–Ω–∫–æ), PlayStation 4 Pro —ñ —ñ–Ω—à—ñ ¬´–ø–ª—é—à–∫–∏¬ª. –¢–∏—Å–Ω–∏ Apply to Position —ñ –ø—Ä–∏—î–¥–Ω—É–π—Å—è –¥–æ –∫–æ–º–∞–Ω–¥–∏!–Ø–∫—â–æ –∑–Ω–∞—î—à –≥–∞—Ä–Ω—É –∫–∞–Ω–¥–∏–¥–∞—Ç—É—Ä—É, –ø–µ—Ä–µ—Ö–æ–¥—å –∑–∞ –ø–æ—Å–∏–ª–∞–Ω–Ω—è–º bit.ly/33WU6Z8 —Ç–∞ –æ—Ç—Ä–∏–º–∞–π –ø—Ä–∏—î–º–Ω–∏–π –±–æ–Ω—É—Å –∑–∞ —É—Å–ø—ñ—à–Ω—É —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—é.",Senior Data Analyst@SocialTech,https://jobs.dou.ua/companies/socialtech/vacancies/129201/, Kyiv,Senior Data Analyst,25 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/smartiway/,Smartiway,"{""Required skills"": [""1+ \u0433\u043e\u0434\u0430 \u043e\u043f\u044b\u0442\u0430 \u0440\u0430\u0431\u043e\u0442\u044b \u0432 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u0441\u043a\u043e\u0440\u043c\u0435\u0439\u043a\u0435\u0440\u0430 \u0438\u043b\u0438 \u043d\u0430 \u0430\u043d\u0430\u043b\u043e\u0433\u0438\u0447\u043d\u043e\u0439 \u043f\u043e\u0437\u0438\u0446\u0438\u0438\u041c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u043f\u043e\u0434\u0433\u043e\u0442\u043e\u0432\u043a\u0430, \u0437\u043d\u0430\u043d\u0438\u0435 \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u043a\u0438, \u0442\u0435\u043e\u0440\u0438\u0438 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0438 \u0438 \u0441\u043c\u0435\u0436\u043d\u044b\u0445 \u043e\u0431\u043b\u0430\u0441\u0442\u0435\u0439\u0417\u043d\u0430\u043d\u0438\u0435 \u043c\u0435\u0442\u043e\u0434\u043e\u0432 \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0433\u043e \u0430\u043d\u0430\u043b\u0438\u0437\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 \u0438 \u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u044f \u043c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439\u0417\u043d\u0430\u043d\u0438\u0435 SQL \u043d\u0430 \u0443\u0440\u043e\u0432\u043d\u0435 \u043d\u0430\u043f\u0438\u0441\u0430\u043d\u0438\u044f \u0441\u043b\u043e\u0436\u043d\u044b\u0445 \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u0432\u041e\u043f\u044b\u0442 \u0440\u0430\u0431\u043e\u0442\u044b \u0441 R/Python\u041f\u0440\u0430\u043a\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u043e\u043f\u044b\u0442 \u043f\u0440\u0438\u043c\u0435\u043d\u0435\u043d\u0438\u044f \u043c\u0435\u0442\u043e\u0434\u043e\u0432 \u043c\u0430\u0448\u0438\u043d\u043d\u043e\u0433\u043e \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f (\u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044f, \u0434\u0435\u0440\u0435\u0432\u044c\u044f \u0440\u0435\u0448\u0435\u043d\u0438\u0439, \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u044b\u0435 \u0441\u0435\u0442\u0438, SVM \u0438 \u0442.\u0434.)\u0421\u043f\u043e\u0441\u043e\u0431\u043d\u043e\u0441\u0442\u044c \u0447\u0435\u0442\u043a\u043e \u0444\u043e\u0440\u043c\u0443\u043b\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0441\u0432\u043e\u0438 \u043c\u044b\u0441\u043b\u0438 \u0438 \u043d\u0430\u0433\u043b\u044f\u0434\u043d\u043e \u0434\u0435\u043c\u043e\u043d\u0441\u0442\u0440\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b \u0441\u0432\u043e\u0435\u0439 \u0440\u0430\u0431\u043e\u0442\u044b\u0410\u043d\u0430\u043b\u0438\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u0441\u043a\u043b\u0430\u0434 \u0443\u043c\u0430 \u0438 \u0431\u044b\u0441\u0442\u0440\u0430\u044f \u043e\u0431\u0443\u0447\u0430\u0435\u043c\u043e\u0441\u0442\u044c\u041c\u043d\u043e\u0433\u043e\u0437\u0430\u0434\u0430\u0447\u043d\u043e\u0441\u0442\u044c""], ""As a plus"": [""3+\u0433\u043e\u0434\u0430 \u043e\u043f\u044b\u0442\u0430 \u0440\u0430\u0431\u043e\u0442\u044b \u0432 \u0434\u0430\u043d\u043d\u043e\u0439 \u0441\u0444\u0435\u0440\u0435\u041e\u043f\u044b\u0442 \u043f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f (C/C++/C#/Java/PHP)\u041e\u043f\u044b\u0442 \u0440\u0430\u0431\u043e\u0442\u044b \u0441 \u0430\u0432\u0442\u043e\u043c\u0430\u0442\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u043c\u0438 \u0441\u0438\u0441\u0442\u0435\u043c\u0430\u043c\u0438 \u043e\u0442\u0447\u0435\u0442\u043d\u043e\u0441\u0442\u0438\u041f\u043e\u043d\u0438\u043c\u0430\u043d\u0438\u0435 \u043f\u0440\u0438\u043d\u0446\u0438\u043f\u043e\u0432 \u043a\u0440\u0435\u0434\u0438\u0442\u043e\u0432\u0430\u043d\u0438\u044f \u0438 \u043a\u0440\u0435\u0434\u0438\u0442\u043d\u043e\u0433\u043e \u0441\u043a\u043e\u0440\u0438\u043d\u0433\u0430\u041e\u043f\u044b\u0442 \u0440\u0430\u0431\u043e\u0442\u044b \u0432 \u0441\u0444\u0435\u0440\u0435 \u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u043f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u043d\u043e\u0433\u043e \u043e\u0431\u0435\u0441\u043f\u0435\u0447\u0435\u043d\u0438\u044f\u041e\u043f\u044b\u0442 \u0440\u0430\u0431\u043e\u0442\u044b \u0441 \u00ab\u0431\u043e\u043b\u044c\u0448\u0438\u043c\u0438\u00bb \u0438 \u043d\u0435\u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u043c\u0438 \u0434\u0430\u043d\u043d\u044b\u043c\u0438""], ""We offer"": [""\u0412\u044b\u0441\u043e\u043a\u043e\u0438\u043d\u0442\u0435\u043d\u0441\u0438\u0432\u043d\u0430\u044f \u00ab\u043f\u0440\u043e\u043a\u0430\u0447\u043a\u0430\u00bb \u0438 \u043f\u043e\u0432\u044b\u0448\u0435\u043d\u0438\u0435 \u043a\u0432\u0430\u043b\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438 \u0432 \u0440\u0430\u0437\u0432\u0438\u0432\u0430\u044e\u0449\u0435\u0439\u0441\u044f \u043a\u043e\u043c\u043f\u0430\u043d\u0438\u0438\u0421\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435 \u0442\u0435\u0445\u043d\u043e\u043b\u043e\u0433\u0438\u0438 \u0441\u0442\u0435\u043a\u0430\u041f\u043e\u043b\u043d\u043e\u0441\u0442\u044c\u044e \u0430\u0432\u0442\u043e\u043c\u0430\u0442\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u0430\u044f \u043f\u043b\u0430\u0442\u0444\u043e\u0440\u043c\u0430 \u0434\u043b\u044f \u043f\u0440\u0438\u043d\u044f\u0442\u0438\u044f \u0440\u0435\u0448\u0435\u043d\u0438\u0439 \u0438 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u0441 \u0437\u0430\u043f\u0430\u0442\u0435\u043d\u0442\u043e\u0432\u0430\u043d\u043d\u043e\u0439 \u0441\u0438\u0441\u0442\u0435\u043c\u043e\u0439 \u0440\u0435\u0439\u0442\u0438\u043d\u0433\u043e\u0432\u0430\u043d\u0438\u044f \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u043c\u0430\u0448\u0438\u043d\u043d\u043e\u0433\u043e \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u0438 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u043e\u0432 \u0418\u0418Scrum-based framework (Agile, Kanban, Scrumban)\u041c\u0435\u0434\u0438\u0446\u0438\u043d\u0441\u043a\u0430\u044f \u0441\u0442\u0440\u0430\u0445\u043e\u0432\u043a\u0430\u0424\u0438\u0442\u043d\u0435\u0441 \u0437\u0430\u043b\u0415\u0436\u0435\u043c\u0435\u0441\u044f\u0447\u043d\u044b\u0435 \u043a\u043e\u043c\u0430\u043d\u0434\u043d\u044b\u0435 \u0432\u0435\u0447\u0435\u0440\u0438\u043d\u043a\u0438 \u0441 \u0431\u0430\u0440\u0431\u0435\u043a\u044e \u0438 \u0430\u043a\u0442\u0438\u0432\u043d\u044b\u043c \u043e\u0442\u0434\u044b\u0445\u043e\u043c\u041f\u044f\u0442\u043d\u0438\u0447\u043d\u044b\u0439 \u043f\u043e\u043a\u0435\u0440 (\u043f\u0438\u0432\u043e, \u043f\u0438\u0446\u0446\u0430, \u0441\u043a\u043e\u0442\u0447 / \u0432\u0438\u0441\u043a\u0438)\u041a\u043e\u0444\u0435 \u043c\u0430\u0448\u0438\u043d\u0430, \u0437\u0430\u043a\u0443\u0441\u043a\u0438, \u0444\u0440\u0443\u043a\u0442\u044b""], ""Responsibilities"": [""\u0420\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u043a\u0430/\u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u044f/\u0430\u043d\u0430\u043b\u0438\u0437 \u0440\u0430\u0431\u043e\u0442\u044b \u0441\u0438\u0441\u0442\u0435\u043c\u044b \u043f\u0440\u0438\u043d\u044f\u0442\u0438\u044f \u0440\u0435\u0448\u0435\u043d\u0438\u0439\u041f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u0435 \u0441\u043a\u043e\u0440\u0438\u043d\u0433\u043e\u0432\u044b\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u0440\u0430\u0437\u043d\u043e\u0439 \u043d\u0430\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u043d\u043e\u0441\u0442\u0438\u0420\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u043e\u0442\u0447\u0435\u0442\u043d\u043e\u0441\u0442\u0438\u041e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u044f \u0438 \u0430\u0432\u0442\u043e\u043c\u0430\u0442\u0438\u0437\u0430\u0446\u0438\u0438 \u043f\u043e\u0432\u0441\u0435\u0434\u043d\u0435\u0432\u043d\u044b\u0445 \u0437\u0430\u0434\u0430\u0447 \u0432 \u043a\u043e\u043c\u043f\u0430\u043d\u0438\u0438 (\u043c\u0430\u0440\u043a\u0435\u0442\u0438\u043d\u0433, \u043a\u043e\u043b\u043b\u0435\u043a\u0448\u0435\u043d, \u043f\u043e\u0434\u0434\u0435\u0440\u0436\u043a\u0430 \u043a\u043b\u0438\u0435\u043d\u0442\u043e\u0432) \u0441\u0440\u0435\u0434\u0441\u0442\u0432\u0430\u043c\u0438 \u043c\u0430\u0448\u0438\u043d\u043d\u043e\u0433\u043e \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u0438 \u0430\u043d\u0430\u043b\u0438\u0437\u0430 \u0434\u0430\u043d\u043d\u044b\u0445\u0422\u043e\u0447\u0435\u0447\u043d\u044b\u0439 \u0430\u043d\u0430\u043b\u0438\u0437 \u0438 \u0438\u0441\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u043d\u0438\u044f\u041d\u0435\u043e\u0431\u044b\u0447\u043d\u044b\u0435, \u043d\u043e \u0438\u043d\u0442\u0435\u0440\u0435\u0441\u043d\u044b\u0435 \u0438 \u043d\u0435 \u043e\u0434\u043d\u043e\u043e\u0431\u0440\u0430\u0437\u043d\u044b\u0435 \u0437\u0430\u0434\u0430\u0447\u0438 \u0433\u0430\u0440\u0430\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u044b.""], ""Project description"": [""\u041e \u043f\u0440\u043e\u0435\u043a\u0442\u0435\u041f\u0440\u043e\u0435\u043a\u0442, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u043c\u044b \u0441\u0442\u0430\u0440\u0430\u0435\u043c\u0441\u044f \u0441\u0434\u0435\u043b\u0430\u0442\u044c \u0434\u043b\u044f \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u043e \u0443\u0434\u043e\u0431\u043d\u044b\u043c \u0438 \u0432\u044b\u0433\u043e\u0434\u043d\u044b\u043c, \u0431\u043b\u0430\u0433\u043e\u0434\u0430\u0440\u044f \u0442\u0435\u0445\u043d\u043e\u043b\u043e\u0433\u0438\u0447\u043d\u043e\u0441\u0442\u0438, 100% \u0430\u0432\u0442\u043e\u043c\u0430\u0442\u0438\u0437\u0430\u0446\u0438\u0438 \u0438 \u043c\u0433\u043d\u043e\u0432\u0435\u043d\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u043d\u044f\u0442\u0438\u044f \u0440\u0435\u0448\u0435\u043d\u0438\u0439, \u0441\u0430\u043c\u043e\u0439 \u043d\u0438\u0437\u043a\u043e\u0439 % \u0441\u0442\u0430\u0432\u043a\u043e\u0439 \u043f\u043e \u0440\u044b\u043d\u043a\u0443, \u0441 \u043e\u0447\u0435\u043d\u044c \u0433\u0438\u0431\u043a\u0438\u043c\u0438 \u0438 \u043b\u043e\u044f\u043b\u044c\u043d\u044b\u043c\u0438 \u0443\u0441\u043b\u043e\u0432\u0438\u044f\u043c\u0438 \u0432 \u043c\u043e\u043c\u0435\u043d\u0442 \u043f\u0440\u043e\u0441\u0440\u043e\u0447\u043a\u0438. \u041d\u0430\u0448 \u043f\u0440\u043e\u0434\u0443\u043a\u0442 \u043f\u0435\u0440\u0432\u044b\u0439 \u0432 \u0441\u0444\u0435\u0440\u0435 \u043a\u0440\u0435\u0434\u0438\u0442\u043e\u0432\u0430\u043d\u0438\u044f \u0441\u043e\u0437\u0434\u0430\u043d mobile only, \u0446\u0435\u043b\u044c\u044e \u043a\u043e\u0442\u043e\u0440\u043e\u0433\u043e \u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u0432\u043e\u0441\u043f\u0438\u0442\u0430\u043d\u0438\u0435 \u0441\u043e\u0446\u0438\u0430\u043b\u044c\u043d\u043e\u0439 \u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u0441\u0442\u0438 \u043e\u0431\u0449\u0435\u0441\u0442\u0432\u0430. \u041e\u0441\u043d\u043e\u0432\u043d\u0430\u044f \u0437\u0430\u0434\u0430\u0447\u0430 \u0432 \u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u043a\u0435 \u0438 \u043f\u043e\u0434\u0434\u0435\u0440\u0436\u043a\u0435 \u043f\u0440\u043e\u0434\u0443\u043a\u0442\u0430"", ""simple & smart.\u0411\u044d\u043a \u0438 \u0444\u0440\u043e\u043d\u0442 \u0447\u0430\u0441\u0442\u0438 \u0441\u0438\u0441\u0442\u0435\u043c\u044b \u0440\u0430\u0437\u0440\u0430\u0431\u0430\u0442\u044b\u0432\u0430\u044e\u0442\u0441\u044f \u043e\u0434\u043d\u043e\u0439 \u043a\u043e\u043c\u0430\u043d\u0434\u043e\u0439, \u0447\u0442\u043e \u0433\u0430\u0440\u0430\u043d\u0442\u0438\u0440\u0443\u0435\u0442 \u0431\u0435\u0441\u0448\u043e\u0432\u043d\u0443\u044e \u0438\u043d\u0442\u0435\u0433\u0440\u0430\u0446\u0438\u044e \u0438 \u0448\u0438\u0440\u043e\u0447\u0430\u0439\u0448\u0438\u0435 \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e\u0441\u0442\u0438 \u043f\u043e \u043d\u0430\u043a\u043e\u043f\u043b\u0435\u043d\u0438\u044e \u0438 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u044e \u0434\u0430\u043d\u043d\u044b\u0445, \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u044b\u0445 \u0432 \u043f\u0440\u043e\u0446\u0435\u0441\u0441\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u044f \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u044f, \u0430 \u0442\u0430\u043a\u0436\u0435 \u043f\u043e\u0437\u0432\u043e\u043b\u0438\u0442, \u043f\u0440\u0438 \u0436\u0435\u043b\u0430\u043d\u0438\u0438, \u0440\u0430\u0441\u0448\u0438\u0440\u0438\u0442\u044c \u043f\u0440\u043e\u0444\u0435\u0441\u0441\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u044b\u0435 \u043d\u0430\u0432\u044b\u043a\u0438 \u0432 \u0441\u043c\u0435\u0436\u043d\u044b\u0445 \u043e\u0431\u043b\u0430\u0441\u0442\u044f\u0445.""]}",,"Required skills 1+ –≥–æ–¥–∞ –æ–ø—ã—Ç–∞ —Ä–∞–±–æ—Ç—ã –≤ –∫–∞—á–µ—Å—Ç–≤–µ —Å–∫–æ—Ä–º–µ–π–∫–µ—Ä–∞ –∏–ª–∏ –Ω–∞ –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ–π –ø–æ–∑–∏—Ü–∏–∏–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞, –∑–Ω–∞–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏, —Ç–µ–æ—Ä–∏–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –∏ —Å–º–µ–∂–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π–ó–Ω–∞–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö –∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π–ó–Ω–∞–Ω–∏–µ SQL –Ω–∞ —É—Ä–æ–≤–Ω–µ –Ω–∞–ø–∏—Å–∞–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤–û–ø—ã—Ç —Ä–∞–±–æ—Ç—ã —Å R/Python–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –æ–ø—ã—Ç –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –º–µ—Ç–æ–¥–æ–≤ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è (—Ä–µ–≥—Ä–µ—Å—Å–∏—è, –¥–µ—Ä–µ–≤—å—è —Ä–µ—à–µ–Ω–∏–π, –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏, SVM –∏ —Ç.–¥.)–°–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å —á–µ—Ç–∫–æ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å —Å–≤–æ–∏ –º—ã—Å–ª–∏ –∏ –Ω–∞–≥–ª—è–¥–Ω–æ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–≤–æ–µ–π —Ä–∞–±–æ—Ç—ã–ê–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–π —Å–∫–ª–∞–¥ —É–º–∞ –∏ –±—ã—Å—Ç—Ä–∞—è –æ–±—É—á–∞–µ–º–æ—Å—Ç—å–ú–Ω–æ–≥–æ–∑–∞–¥–∞—á–Ω–æ—Å—Ç—å As a plus 3+–≥–æ–¥–∞ –æ–ø—ã—Ç–∞ —Ä–∞–±–æ—Ç—ã –≤ –¥–∞–Ω–Ω–æ–π —Å—Ñ–µ—Ä–µ–û–ø—ã—Ç –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è (C/C++/C#/Java/PHP)–û–ø—ã—Ç —Ä–∞–±–æ—Ç—ã —Å –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ —Å–∏—Å—Ç–µ–º–∞–º–∏ –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç–∏–ü–æ–Ω–∏–º–∞–Ω–∏–µ –ø—Ä–∏–Ω—Ü–∏–ø–æ–≤ –∫—Ä–µ–¥–∏—Ç–æ–≤–∞–Ω–∏—è –∏ –∫—Ä–µ–¥–∏—Ç–Ω–æ–≥–æ —Å–∫–æ—Ä–∏–Ω–≥–∞–û–ø—ã—Ç —Ä–∞–±–æ—Ç—ã –≤ —Å—Ñ–µ—Ä–µ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–≥–æ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è–û–ø—ã—Ç —Ä–∞–±–æ—Ç—ã —Å ¬´–±–æ–ª—å—à–∏–º–∏¬ª –∏ –Ω–µ—Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ We offer –í—ã—Å–æ–∫–æ–∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–∞—è ¬´–ø—Ä–æ–∫–∞—á–∫–∞¬ª –∏ –ø–æ–≤—ã—à–µ–Ω–∏–µ –∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏–∏ –≤ —Ä–∞–∑–≤–∏–≤–∞—é—â–µ–π—Å—è –∫–æ–º–ø–∞–Ω–∏–∏–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ —Å—Ç–µ–∫–∞–ü–æ–ª–Ω–æ—Å—Ç—å—é –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–ª—è –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å –∑–∞–ø–∞—Ç–µ–Ω—Ç–æ–≤–∞–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º–æ–π —Ä–µ–π—Ç–∏–Ω–≥–æ–≤–∞–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –ò–òScrum-based framework (Agile, Kanban, Scrumban)–ú–µ–¥–∏—Ü–∏–Ω—Å–∫–∞—è —Å—Ç—Ä–∞—Ö–æ–≤–∫–∞–§–∏—Ç–Ω–µ—Å –∑–∞–ª–ï–∂–µ–º–µ—Å—è—á–Ω—ã–µ –∫–æ–º–∞–Ω–¥–Ω—ã–µ –≤–µ—á–µ—Ä–∏–Ω–∫–∏ —Å –±–∞—Ä–±–µ–∫—é –∏ –∞–∫—Ç–∏–≤–Ω—ã–º –æ—Ç–¥—ã—Ö–æ–º–ü—è—Ç–Ω–∏—á–Ω—ã–π –ø–æ–∫–µ—Ä (–ø–∏–≤–æ, –ø–∏—Ü—Ü–∞, —Å–∫–æ—Ç—á / –≤–∏—Å–∫–∏)–ö–æ—Ñ–µ –º–∞—à–∏–Ω–∞, –∑–∞–∫—É—Å–∫–∏, —Ñ—Ä—É–∫—Ç—ã Responsibilities –†–∞–∑—Ä–∞–±–æ—Ç–∫–∞/–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è/–∞–Ω–∞–ª–∏–∑ —Ä–∞–±–æ—Ç—ã —Å–∏—Å—Ç–µ–º—ã –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å–∫–æ—Ä–∏–Ω–≥–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞–∑–Ω–æ–π –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ—Å—Ç–∏–†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç–∏–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∏ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –ø–æ–≤—Å–µ–¥–Ω–µ–≤–Ω—ã—Ö –∑–∞–¥–∞—á –≤ –∫–æ–º–ø–∞–Ω–∏–∏ (–º–∞—Ä–∫–µ—Ç–∏–Ω–≥, –∫–æ–ª–ª–µ–∫—à–µ–Ω, –ø–æ–¥–¥–µ—Ä–∂–∫–∞ –∫–ª–∏–µ–Ω—Ç–æ–≤) —Å—Ä–µ–¥—Å—Ç–≤–∞–º–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏ –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö–¢–æ—á–µ—á–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è–ù–µ–æ–±—ã—á–Ω—ã–µ, –Ω–æ –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–µ –∏ –Ω–µ –æ–¥–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –∑–∞–¥–∞—á–∏ –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω—ã. Project description –û –ø—Ä–æ–µ–∫—Ç–µ–ü—Ä–æ–µ–∫—Ç, –∫–æ—Ç–æ—Ä—ã–π –º—ã —Å—Ç–∞—Ä–∞–µ–º—Å—è —Å–¥–µ–ª–∞—Ç—å –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —É–¥–æ–±–Ω—ã–º –∏ –≤—ã–≥–æ–¥–Ω—ã–º, –±–ª–∞–≥–æ–¥–∞—Ä—è —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–Ω–æ—Å—Ç–∏, 100% –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –∏ –º–≥–Ω–æ–≤–µ–Ω–Ω–æ—Å—Ç–∏ –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π, —Å–∞–º–æ–π –Ω–∏–∑–∫–æ–π % —Å—Ç–∞–≤–∫–æ–π –ø–æ —Ä—ã–Ω–∫—É, —Å –æ—á–µ–Ω—å –≥–∏–±–∫–∏–º–∏ –∏ –ª–æ—è–ª—å–Ω—ã–º–∏ —É—Å–ª–æ–≤–∏—è–º–∏ –≤ –º–æ–º–µ–Ω—Ç –ø—Ä–æ—Å—Ä–æ—á–∫–∏. –ù–∞—à –ø—Ä–æ–¥—É–∫—Ç –ø–µ—Ä–≤—ã–π –≤ —Å—Ñ–µ—Ä–µ –∫—Ä–µ–¥–∏—Ç–æ–≤–∞–Ω–∏—è —Å–æ–∑–¥–∞–Ω mobile only, —Ü–µ–ª—å—é –∫–æ—Ç–æ—Ä–æ–≥–æ —è–≤–ª—è–µ—Ç—Å—è –≤–æ—Å–ø–∏—Ç–∞–Ω–∏–µ —Å–æ—Ü–∏–∞–ª—å–Ω–æ–π –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏ –æ–±—â–µ—Å—Ç–≤–∞. –û—Å–Ω–æ–≤–Ω–∞—è –∑–∞–¥–∞—á–∞ –≤ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ –∏ –ø–æ–¥–¥–µ—Ä–∂–∫–µ –ø—Ä–æ–¥—É–∫—Ç–∞ ‚Äî simple & smart.–ë—ç–∫ –∏ —Ñ—Ä–æ–Ω—Ç —á–∞—Å—Ç–∏ —Å–∏—Å—Ç–µ–º—ã —Ä–∞–∑—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è –æ–¥–Ω–æ–π –∫–æ–º–∞–Ω–¥–æ–π, —á—Ç–æ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç –±–µ—Å—à–æ–≤–Ω—É—é –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é –∏ —à–∏—Ä–æ—á–∞–π—à–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –ø–æ –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—é –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –¥–∞–Ω–Ω—ã—Ö, –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è, –∞ —Ç–∞–∫–∂–µ –ø–æ–∑–≤–æ–ª–∏—Ç, –ø—Ä–∏ –∂–µ–ª–∞–Ω–∏–∏, —Ä–∞—Å—à–∏—Ä–∏—Ç—å –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–µ –Ω–∞–≤—ã–∫–∏ –≤ —Å–º–µ–∂–Ω—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö.",Data Scientist¬†/ Risk analyst@Smartiway,https://jobs.dou.ua/companies/smartiway/vacancies/133035/, Kyiv,Data Scientist¬†/ Risk analyst,25 September 2020,$1000‚Äì2000,2020-10-13,,dou
https://jobs.dou.ua/companies/justanswer/,JustAnswer,"{""Required skills"": [""Educational background in Economics/Finance/Math/Statistics/Engineering"", ""Proficiency in analysis and business modeling using Excel"", ""Understanding of statistics"", ""Business thinking"", ""Ability to coordinate multiple tasks, set and negotiate priorities"", ""Experience with web analytics, including a good understanding of tools and processes to collect and report on all aspects of web analytics (e.g. Google Analytics, Omniture) would be a plus"", ""Basic to mid-level proficiency in data extraction using SQL"", ""Advanced English level (written/spoken)""], ""We offer"": [""Time off throughout the year (paid and unpaid)"", ""Professional development support and encouragement"", ""Resources to help improve your overall well-being"", ""Free membership to dive into JA product""], ""Responsibilities"": [""Work with Product and Analytics leadership to conceive and structure analysis, and deliver highly actionable insights from \u201cdeep dives\u201d into specific areas of our business"", ""Conduct analysis of a tremendous amount of internal & external data to find growth & optimization opportunities for the business"", ""Package and communicate findings and recommendations to a broad audience (including senior leadership)""], ""Project description"": [""JustAnswer is an online marketplace that is revolutionizing professional services by making fast, affordable expert help accessible to people everywhere. Hailed by The Huffington Post as \u201cdestined for glory,\u201d our company is made of smart, hard-working people who know how to have fun and get things done. From doctors, lawyers, and vets to auto mechanics and tech support, the 10,000 experts on JustAnswer have helped over eight million people in 196 countries since 2003. Headquartered in the beautiful Presidio of San Francisco, our mission is simple and inspiring: \u201cWe help people.\u201d JustAnswer, LLC is backed by investors Crosslink Capital, Glynn Capital, and Charles Schwab.""]}",,"Required skills ‚Äî Educational background in Economics/Finance/Math/Statistics/Engineering‚Äî Proficiency in analysis and business modeling using Excel ‚Äî Understanding of statistics ‚Äî Business thinking ‚Äî Ability to coordinate multiple tasks, set and negotiate priorities‚Äî Experience with web analytics, including a good understanding of tools and processes to collect and report on all aspects of web analytics (e.g. Google Analytics, Omniture) would be a plus ‚Äî Basic to mid-level proficiency in data extraction using SQL‚Äî Advanced English level (written/spoken) We offer ‚Äî Time off throughout the year (paid and unpaid)‚Äî Professional development support and encouragement‚Äî Resources to help improve your overall well-being‚Äî Free membership to dive into JA product Responsibilities ‚Äî Work with Product and Analytics leadership to conceive and structure analysis, and deliver highly actionable insights from ‚Äúdeep dives‚Äù into specific areas of our business‚Äî Conduct analysis of a tremendous amount of internal & external data to find growth & optimization opportunities for the business‚Äî Package and communicate findings and recommendations to a broad audience (including senior leadership) Project description JustAnswer is an online marketplace that is revolutionizing professional services by making fast, affordable expert help accessible to people everywhere. Hailed by The Huffington Post as ‚Äúdestined for glory,‚Äù our company is made of smart, hard-working people who know how to have fun and get things done. From doctors, lawyers, and vets to auto mechanics and tech support, the 10,000 experts on JustAnswer have helped over eight million people in 196 countries since 2003. Headquartered in the beautiful Presidio of San Francisco, our mission is simple and inspiring: ‚ÄúWe help people.‚Äù JustAnswer, LLC is backed by investors Crosslink Capital, Glynn Capital, and Charles Schwab.",Data Analyst@JustAnswer,https://jobs.dou.ua/companies/justanswer/vacancies/102522/, Lviv,Data Analyst,25 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/brainstack/,Brainstack_,{},,"Brainstack_ is a team of intelligent, fun-loving people working on their own products that truly have value. Some of our products have already made their name In the US, Europe, Central, and South America, and we keep catching up with new products conquering new territories.We think that creating great products requires absolute teamwork, no matter which position you occupy: every task, every pixel, every code line makes a difference.We‚Äôre looking for Data Analyst. Requirements:‚Äî Technical or Economics Degree.‚Äî 2+ years of professional experience in an analytical role.‚Äî Good theoretical background in statistic, hypothesis testing and familiar with multivariate regression analysis (Logistic Regression, etc.)‚Äî Good Knowledge of Python/R, SQL (MySQL, PostgreSQL).‚Äî Experience with data warehousing and visualization tools.‚Äî Knowledge of web analytics.‚Äî Excel at a high level (pivot tables, connection to external sources).‚Äî Analytical thinking, ability to find patterns in studied data sets, reasons for the deviations of indicators that describe business processes, briefly and accurately focus on the main causes of deviations.‚Äî Intermediate English level or higher. Would be a plus:‚Äî Good understanding of digital marketing, understanding of online; business, knowledge of its main specific indicators.‚Äî Experience in predictive analytics (time-series analysis and forecasting, survival analysis, etc.)‚Äî Experience with cloud computing services (GCP, etc.)‚Äî Experience with payment services/e-commerce/banks/payment providers Responsibilities:‚Äî Business Intelligent development (Creation of an automated reporting In Google Data Studio and Tableau, work with Data Engineers to design DWH scheme and ETL processes).‚Äî Data Analysis to identify the causes of deviations in business metrics, as well as optimization of activities.‚Äî Evaluation of experimental results (A/B tests).‚Äî Forecasting the financial results.‚Äî Participation in business optimization projects. We offer:‚Äî Possibility to be part of the creative environment and make your input to it;‚Äî Variety of social and professional activities;‚Äî Friendly and warm culture;‚Äî PE employment;‚Äî 24 calendar days of vacation paid per year;‚Äî Medical insurance;‚Äî Free English courses;‚Äî Documented sick days paid;‚Äî 3 non-documented sick days paid;‚Äî Fruits/yummies/coffee in the kitchen.",Data Analyst@Brainstack_,https://jobs.dou.ua/companies/brainstack/vacancies/121780/, Kyiv,Data Analyst,24 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/clario/,Clario,{},,"Hi, We are Clario, a consumer-focused cybersecurity company on a mission to change an industry. Over 800 professionals, including 600 digital security experts, with one common goal ‚Äî supporting everyone‚Äôs right to a digital life, secured. We‚Äôre here to create a next-generation digital security solution with a human touch. Join us and help people take back control of their digital privacy and security. Clario Team is looking for a Billing/Data Analyst for full-time work to help build the Clario brand that aims to become the disruptive global leader in digital privacy and security. ‚Ä¢ Analyzing performance of payment processors.‚Ä¢ Research and find insights based on user behavior patterns on payment page.‚Ä¢ Build automated dashboards for visualizing key metrics and trends to support Marketing functions (MySQL, HIVE, Tableau)‚Ä¢ Analyze A/B tests regarding payment page.‚Ä¢ Communicate with stakeholders highlighting problems and opportunities, suggest actions to be taken based on analysis. ‚Ä¢ Superior analytical and problem-solving abilities with keen attention to details‚Ä¢ Self-starter with ability to work in a fast-paced environment and manage tight deadlines‚Ä¢ Strong communication and presentation skills‚Ä¢ Strong SQL knowledge‚Ä¢ Experience of building reports via Tableau‚Ä¢ 2+ years of experience working as a billing or data analyst with IT technical background.‚Ä¢ Understanding payment flow and supporting metrics (chargeback rate, refund rate etc.)‚Ä¢ Ability to work with a large amount of data and figures, IT platforms, systems, tools, and databases‚Ä¢ Knowledge of Hadoop, Hive, and MapReduce would be an advantage‚Ä¢ English level ‚Äî Upper Intermediate +‚Ä¢ University degree in Math, Applied Mathematics, Computer Science or similar beneficial but not mandatory We are not just a company, we are Clario! We put the customer at the heart of all that we do, we achieve our best together, take responsibility, and challenge our limits to create a difference! To apply for this position, please send your CV (in English only) with a detailed description of your career, experience, skills, and projects. We guarantee the privacy of any information received",Billing/Data Analyst@Clario,https://jobs.dou.ua/companies/clario/vacancies/132961/, Kyiv,Billing/Data Analyst,24 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/lohika-systems/,Lohika,"{""Required skills"": [""5+ years with BigData technologies and tools.3+ years of Python development experience (Python, Pyspark)Strong hands-on experience with SparkHandling scalability issues, pipeline/cluster optimizationExperience with AWSGood English (oral & written) and communication skills in generalWOULD BE A PLUS:""], ""As a plus"": [""Pandas, databricks/koalas.DevOps experience for Big Data solutions.""], ""We offer"": [""As Senior Engineer you will build an ETL solution in marketing domain, which involves data transformation, applying models and loading for further analysis.""], ""Responsibilities"": [""Design, develop, deliver and operate scalable, high-performance data processing software.Work proactively on the system architecture.Ensure high quality development standards (unit/integration tests, etc.)Collaboration with the product management team to incorporate the needs of our customers.""]}",,"Required skills 5+ years with BigData technologies and tools.3+ years of Python development experience (Python, Pyspark)Strong hands-on experience with SparkHandling scalability issues, pipeline/cluster optimizationExperience with AWSGood English (oral & written) and communication skills in generalWOULD BE A PLUS: As a plus Pandas, databricks/koalas.DevOps experience for Big Data solutions. We offer As Senior Engineer you will build an ETL solution in marketing domain, which involves data transformation, applying models and loading for further analysis. Responsibilities Design, develop, deliver and operate scalable, high-performance data processing software.Work proactively on the system architecture.Ensure high quality development standards (unit/integration tests, etc.)Collaboration with the product management team to incorporate the needs of our customers.",Senior BigData Engineer #7602@Lohika,https://jobs.dou.ua/companies/lohika-systems/vacancies/98673/, Odesa,Senior BigData Engineer #7602,24 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/ciklum/,Ciklum,{},,"On behalf of Seeking Alpha, Ciklum is looking for Senior Data Analyst to join Kyiv team on full-time basis. The company is the premier website for actionable stock market opinion and analysis, and vibrant, intelligent finance discussion. Each month our crowdsource investment analysis draws an audience of 5.2MM+ monthly visitors to our real-time alerts products on email and mobile. We handpick articles from the world‚Äôs top market blogs, money managers, financial experts and investment newsletters, publishing 500 unique article and news updates daily. The company gives a voice to over 5.5MM registered users, including 12,000+ contributors and individuals averaging 130,000+ comments a month, providing access to the nation‚Äôs savviest and inquisitive investors. Our site is the only free, online source for over 5,000 public companies‚Äô quarterly earnings call transcripts, including the S&P 500. The company was named the Most Informative Website by Kiplinger‚Äôs Magazine and has received Forbes‚Äô ‚ÄòBest of the Web‚Äô Award.",Senior Data Analyst for Seeking Alpha (2000041C)@Ciklum,https://jobs.dou.ua/companies/ciklum/vacancies/132899/, Kyiv,Senior Data Analyst for Seeking Alpha (2000041C),24 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/intellias/,Intellias,"{""Required skills"": [""2 years + of experience in the role of Business Analyst on building BI-powered systems;Good knowledge of Microsoft Business Intelligence Stack having Power BI, SSAS, SSIS;Understanding the business requirements and developing data models accordingly by taking care of the resources;Good knowledge of SQL;Expertise in MS Excel: creating formulas, pivot tables, charts, reports, and dashboards;Analytical thinking for translating data into informative visuals and reports;Understanding of statistical analysis methods;Good communication and interpersonal skills, proven ability to communicate decisions clearly to both technical and non-technical staff;Intermediate + level of English.""], ""As a plus"": [""Basic Knowledge and skills for secondary tools: Microsoft Azure, data warehouse, Visual Studio;Degree in business, statistics or mathematics disciplines;Experience of developing custom visuals for Power BI.""], ""We offer"": [""Besides such basics as a competitive salary, comfortable and motivating work environment, here at Intellias we offer:""], ""Responsibilities"": [""For your professional growth"", ""Innovative projects with advanced technologies;Individual approach to professional and career growth (Personal Development Plan);Regular educational events with leading industry experts;English and German courses.""], ""Project description"": [""For your comfort"", ""Flexible working hours;Spacious office with lots of meeting rooms;Relocation program;Kids\u2019 room with professional baby-sitter (offices in Lviv & Kyiv).""]}",,"Required skills 2 years + of experience in the role of Business Analyst on building BI-powered systems;Good knowledge of Microsoft Business Intelligence Stack having Power BI, SSAS, SSIS;Understanding the business requirements and developing data models accordingly by taking care of the resources;Good knowledge of SQL;Expertise in MS Excel: creating formulas, pivot tables, charts, reports, and dashboards;Analytical thinking for translating data into informative visuals and reports;Understanding of statistical analysis methods;Good communication and interpersonal skills, proven ability to communicate decisions clearly to both technical and non-technical staff;Intermediate + level of English. As a plus Basic Knowledge and skills for secondary tools: Microsoft Azure, data warehouse, Visual Studio;Degree in business, statistics or mathematics disciplines;Experience of developing custom visuals for Power BI. We offer Besides such basics as a competitive salary, comfortable and motivating work environment, here at Intellias we offer: For your professional growth ‚ÄîInnovative projects with advanced technologies;Individual approach to professional and career growth (Personal Development Plan);Regular educational events with leading industry experts;English and German courses. For your comfort ‚ÄîFlexible working hours;Spacious office with lots of meeting rooms;Relocation program;Kids‚Äô room with professional baby-sitter (offices in Lviv & Kyiv). For your health ‚Äî3 health packages to choose from ‚Äî medical insurance, sports attendance or mix of both;Annual vitaminization program;Annual vaccination and ophthalmologist check-up. For your leisure ‚ÄîCorporate celebrations and fun activities;On-site massages;Beauty parlor (offices in Lviv & Kyiv). Responsibilities Gathering, analysis of requirements for data processing and reporting, tight communication with stakeholders during reports developing;Developing and maintaining custom Power BI reports and dashboards for our analytics system;Exploration and validation of data from various sources;Reports demo preparation;Connecting to data sources, importing data and transforming data for Business Intelligence;Implementing row level security on data and understanding of application security layer models in Power BI;Making DAX queries in PowerBI desktop;Using advance level calculations on the data set;Monitoring of analytics results, identification and understanding of trends and anomalies. Project description We are looking for an experienced, inspired, and dedicated Business Intelligence Analyst to join our friendly and ambitious team of professionals. The main goal of the cooperation is to assist with the development of a tool aimed at converting corporate data sets into valuable insights that could drive management decisions and increase the efficiency of business processes at Intellias. At present, the engineering efforts are focused on work with the big data analytics that meets the needs of all Intellias departments and integrates with a range of internal systems such as 1C, Pipedrive, Survey Monkey, Jira, and more. All data is stored in DWH in Microsoft Azure. Besides, the team has ambitious plans to implement new cutting-edge data processing approaches such as prediction, deep learning, etc. into the existing system.",Middle data analyst\Power BI¬†developer@Intellias,https://jobs.dou.ua/companies/intellias/vacancies/132897/, Lviv,Middle data analyst\Power BI¬†developer,24 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/metamova/,Metamova,"{""Required skills"": [""1+ years experience with Python, including engineering experience (API solutions etc.)"", ""familiarity with dynamic programming"", ""experience with NLTK, possibly Spacy or StanfordNLP"", ""understanding of linguistics"", ""understanding of statistical and machine learning methods"", ""advanced English""], ""As a plus"": [""We want to see your code / how you think. An active github / medium is a plus!""], ""We offer"": [""Experience with NER systems, co-reference resolution, other information extraction tasksExperience with neural networks, particularly in transfer learningExperience working in an international team developing a client-facing application""], ""Responsibilities"": [""An interesting project with space to innovateA friendly, supportive teamFlexible work hours / work placeGood compensation""], ""Project description"": [""Developing NLP algorithmsEvaluating, debugging and maintaining a NER systemFine-tuning and evaluating classifiersOther information extraction tasks""]}",,"Required skills ‚Äî 1+ years experience with Python, including engineering experience (API solutions etc.)‚Äî familiarity with dynamic programming‚Äî experience with NLTK, possibly Spacy or StanfordNLP‚Äî understanding of linguistics‚Äî understanding of statistical and machine learning methods‚Äî advanced English As a plus We want to see your code / how you think. An active github / medium is a plus! Experience with NER systems, co-reference resolution, other information extraction tasksExperience with neural networks, particularly in transfer learningExperience working in an international team developing a client-facing application We offer An interesting project with space to innovateA friendly, supportive teamFlexible work hours / work placeGood compensation Responsibilities Developing NLP algorithmsEvaluating, debugging and maintaining a NER systemFine-tuning and evaluating classifiersOther information extraction tasks Project description Metamova is looking for an NLP engineer to help with different NLP projects. In particular, right now we need help expanding, improving and maintaining a NER system for a fintech client. Advanced English is important, as well as having a very good grasp on linguistics.",NLP Engineer@Metamova,https://jobs.dou.ua/companies/metamova/vacancies/132884/, Lviv,NLP Engineer,24 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/very-good-group/,Very Good Security,"{""Required skills"": [""5+ years of software development experience- ideally at a product company."", ""3+ years of experience architecting, building and supporting scalable as well as fault-tolerant batch, real-time and/or near-real-time data pipelines."", ""3+ years of experience working with big data ecosystem tools such as Kafka, Protobuf/Thrift, and Spark/Flink/Storm 2.0."", ""3+ years experience with data-flow programming tools such as Apache NiFi, Apache Beam, etc."", ""Strong knowledge of data modeling experience with both relational and NoSQL databases"", ""Hands-on experience with data warehouses, preferably AWS Redshift"", ""Expert knowledge of SQL and Python."", ""Knowledge and practical experience with Docker, Terraform/CloudFormation, and the AWS stack: EC2, Kinesis, Lambda, etc."", ""Ability to work independently to deliver well-designed, high-quality, and testable code on time."", ""English"", ""upper-intermediate/advanced""], ""As a plus"": [""Java or Golang experience"", ""Experience working with CI/CD tools (such as CircleCI, Jenkins, etc.)."", ""Understanding and hands-on experience with Kubernetes"", ""Open source projects on GitHub""], ""We offer"": [""Silicon Valley Experience;"", ""3 weeks of paid vacation and 2 weeks of days off+sick leaves;"", ""Hackers\u2019 days;"", ""Corporate retreats;"", ""Paid lunches and parking;"", ""Covering professional learning: conferences, trainings, and other events;"", ""Sports activities compensation;"", ""English Speaking Club with native speakers;"", ""Medical insurance;"", ""VGS stock options.""], ""Responsibilities"": [""Create and maintain optimal data pipeline architecture."", ""Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS technologies."", ""Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics."", ""Maintain a high culture of code"", ""rigorous testing and automation. Improve test coverage of code that you do not own."", ""Be proactive and innovative. We rely on your feedback to build up the product expertise."", ""Work with stakeholders including the Executive, Product, Data, and Design teams to assist with data-related technical issues and support their data infrastructure needs.""], ""Project description"": [""At Very Good Security (\u201cVGS\u201d) we are on a mission to protect the world\u2019s sensitive data"", ""and we\u2019d love to have you along for this journey. VGS was founded by highly successful repeat entrepreneurs and is backed by world-class investors like Goldman Sachs, Andreessen Horowitz, and Visa. We are building an amazing global team spread across four cities. As a young and growing company, we are laser-focused on delighting our customers and hiring talented and entrepreneurial-minded individuals.""]}",,"Required skills ‚Äî 5+ years of software development experience- ideally at a product company.‚Äî 3+ years of experience architecting, building and supporting scalable as well as fault-tolerant batch, real-time and/or near-real-time data pipelines. ‚Äî 3+ years of experience working with big data ecosystem tools such as Kafka, Protobuf/Thrift, and Spark/Flink/Storm 2.0.‚Äî 3+ years experience with data-flow programming tools such as Apache NiFi, Apache Beam, etc.‚Äî Strong knowledge of data modeling experience with both relational and NoSQL databases‚Äî Hands-on experience with data warehouses, preferably AWS Redshift‚Äî Expert knowledge of SQL and Python. ‚Äî Knowledge and practical experience with Docker, Terraform/CloudFormation, and the AWS stack: EC2, Kinesis, Lambda, etc.‚Äî Ability to work independently to deliver well-designed, high-quality, and testable code on time.‚Äî English ‚Äî upper-intermediate/advanced As a plus ‚Äî Java or Golang experience‚Äî Experience working with CI/CD tools (such as CircleCI, Jenkins, etc.).‚Äî Understanding and hands-on experience with Kubernetes‚Äî Open source projects on GitHub We offer ‚Äî Silicon Valley Experience;‚Äî 3 weeks of paid vacation and 2 weeks of days off+sick leaves;‚Äî Hackers‚Äô days;‚Äî Corporate retreats;‚Äî Paid lunches and parking;‚Äî Covering professional learning: conferences, trainings, and other events;‚Äî Sports activities compensation;‚Äî English Speaking Club with native speakers;‚Äî Medical insurance;‚Äî VGS stock options. Responsibilities ‚Äî Create and maintain optimal data pipeline architecture.‚Äî Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS technologies.‚Äî Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics.‚Äî Maintain a high culture of code ‚Äî rigorous testing and automation. Improve test coverage of code that you do not own.‚Äî Be proactive and innovative. We rely on your feedback to build up the product expertise.‚Äî Work with stakeholders including the Executive, Product, Data, and Design teams to assist with data-related technical issues and support their data infrastructure needs. Project description At Very Good Security (‚ÄúVGS‚Äù) we are on a mission to protect the world‚Äôs sensitive data ‚Äî and we‚Äôd love to have you along for this journey. VGS was founded by highly successful repeat entrepreneurs and is backed by world-class investors like Goldman Sachs, Andreessen Horowitz, and Visa. We are building an amazing global team spread across four cities. As a young and growing company, we are laser-focused on delighting our customers and hiring talented and entrepreneurial-minded individuals. We‚Äôre looking for a Senior Data Engineer with an equal flair for creative problem solving, new technologies enthusiasm, and desire to contribute to product development.",Data Engineer@Very Good Security,https://jobs.dou.ua/companies/very-good-group/vacancies/127646/, Kyiv,Data Engineer,23 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/maritime-capital-llc/vacancies/,All company jobs,"{""Required skills"": [""Maritime Capital is looking for a Quantitative Developer/Researcher in Ukraine to work remotely (an office setup will be provided if necessary). The responsibilities will include development and maintenance of data infrastructure, execution algorithms, performance measurement and new strategy research. The candidate will work directly with the portfolio manager on his projects.""], ""We offer"": [""Education: Bachelor\u2019s degree or higher in Applied Mathematics, Computer Science or similar quantitative field (e.g., statistics, computer science, math\u2019s). Graduate students in quantitative fields are encouraged to apply.""]}",,"Required skills Maritime Capital is looking for a Quantitative Developer/Researcher in Ukraine to work remotely (an office setup will be provided if necessary). The responsibilities will include development and maintenance of data infrastructure, execution algorithms, performance measurement and new strategy research. The candidate will work directly with the portfolio manager on his projects. Education: Bachelor‚Äôs degree or higher in Applied Mathematics, Computer Science or similar quantitative field (e.g., statistics, computer science, math‚Äôs). Graduate students in quantitative fields are encouraged to apply. Experience: At least two to four years of strong Python programming experience (C++ is a plus) in a professional or academic setting following current PEP/Python patterns and standards to produce production quality code. Background in statistics, data science or applied mathematics is required. Skills and Qualifications: ‚Äî Experience with complex software systems, programming in Python and C++ ‚Äî Some experience with statistics and mathematics in an academic or commercial environment ‚Äî Excellent written and verbal communication skills in Russian and English. Tools:‚Äî Python 3, pandas, numpy, statsmodels ‚Äî SQL Databases (Postgres) ‚Äî Integration with C++ from Python ‚Äî Linux environment Compensation: Competitive compensation consistent with the experience and skillset. We offer Compensation: Competitive compensation consistent with the experience and skillset.",Quantitative Developer/Researcher@All company jobs,https://jobs.dou.ua/companies/maritime-capital-llc/vacancies/120932/, remote,Quantitative Developer/Researcher,23 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/newfire-partners/,Newfire Global Partners,"{""Required skills"": [""5+ years of commercial experience with hand-on Data Architecting, Modeling and Warehousing.5+ years experience with a modern Data Warehousing stack including Postgres and other equivalent modern technologiesTrack record of working in Scrum / Agile software teamsProficient in spoken and written EnglishStrong experience with SQL design and architecture best-practices (i.e. third normal form, index management, constraints, and foreign key relationships, etc.).Good understanding of common database administration tasks and concepts such as backup & restore, recovery models, integrity checks, and replication methods.Expertise in using Profiler traces, Extended events and other tools to find poorly performing code.Expertise in query tuning and performance optimization.Experience using data modelling tools like ERWin, Visio, etc.Strong understanding of modern SQL Server DR & scalability features. (i.e. Always On Availability Groups, Resource Governor, etc. preferredStrong SQL abilities and experience with massive relational database systems.Strong knowledge of all traditional Data Warehouse-related components (Sourcing, ETL, Data Modeling, Infrastructure, BI, Reporting) and the modern tools to support those componentsAbility to clearly explain and justify ideas when faced with competing alternativesAbility to design, communicate and apply effective and architectural design patterns across a wide range of technical problemsFamiliarity with GIT and release engineering strategies.""], ""As a plus"": [""Bachelor\u2019s degree or higher in a technical field of studyExperience with AWS-based database systems (RDS/Aurora, Redshift, etc...), No-SQL databases, Operational Data Sources, and Data Lakes is preferred but not required.Strong desire to maintain a high level of professional and technical knowledge by attending educational workshops; reviewing professional publications; establishing personal networks; benchmarking state-of-the-art practices; and participating in professional societies.Familiarity with the MPP Databases (Redshift or Vertica), Hadoop ecosystem, and HQLFamiliarity with continuous delivery and DevOpsFlexibility and creativity in solution design"", ""including leveraging emerging technologies""], ""We offer"": [""Favorite Perks:The clear growth path for every role and regular performance reviewsMeaningful projects with an impact on real-world problemsLots of senior developers and mentorsHigh-trust environment: no time-trackers and flexible schedules""], ""Responsibilities"": [""Other Benefits:Very little bureaucracyPersonal development compensation: from English classes to Udemy courses15 PTO days (20 PTO days after a year in our team) + 8 paid sick daysRelocation support for Lviv & Ivano-Frankivsk officesQuarantine-friendly cozy officesSnack-stocked kitchen and $50 monthly lunch compensationParties, team buildings, and weekly pizza/sushi days""], ""Project description"": [""Work as part of multifunctional teams to own the design and architecture of the logical entity model of the client\u2019s suite of products.Convert logical models into physical data models employing sound database normalization techniques.Create physical database objects like tables and views with appropriate data types, foreign keys, constraints and upfront design and maintenance of proper indexes.Create and maintain easy to follow technical documentation of data models.Serve as a go-to resource for questions related to existing and proposed logical and physical data models.Perform T-SQL code reviews and ensure that new database code meets company standards for readability, reliability, and performance.Assist with resolving the performance of poorly executing stored procedures and queries.Support team initiatives by developing tools and identifying opportunities for process automation; assist in evaluation and selection of standard tools for the department.Conduct Data Warehousing learning sessions for the team to evangelize good design and T-SQL coding practices.Periodically review and manage company T-SQL coding standards.Support building and deploying the infrastructure for ingesting high-volume data from various sources.Supporting developing and maintaining the data-related scripting for build/test/deployment automation. Research individually and in collaboration with other teams on how to solve problems.Partner with team and research, design, test, and evaluate new technologies and services as it applies to data warehousing and architecting.Partner with the team to maintain an organization-wide view of current and future strategy and approach as it applies to data warehousing and architecting.Provide leadership and expertise in the development of standards, architectural governance, design patterns, and practices in data warehousing and architecting.Identify and resolve bottlenecks and bugsSupport with Scrum / Agile software development approach (e.g., sprint, standups, retros, planning, pointing, grooming, etc.""]}",,"Required skills 5+ years of commercial experience with hand-on Data Architecting, Modeling and Warehousing.5+ years experience with a modern Data Warehousing stack including Postgres and other equivalent modern technologiesTrack record of working in Scrum / Agile software teamsProficient in spoken and written EnglishStrong experience with SQL design and architecture best-practices (i.e. third normal form, index management, constraints, and foreign key relationships, etc.).Good understanding of common database administration tasks and concepts such as backup & restore, recovery models, integrity checks, and replication methods.Expertise in using Profiler traces, Extended events and other tools to find poorly performing code.Expertise in query tuning and performance optimization.Experience using data modelling tools like ERWin, Visio, etc.Strong understanding of modern SQL Server DR & scalability features. (i.e. Always On Availability Groups, Resource Governor, etc. preferredStrong SQL abilities and experience with massive relational database systems.Strong knowledge of all traditional Data Warehouse-related components (Sourcing, ETL, Data Modeling, Infrastructure, BI, Reporting) and the modern tools to support those componentsAbility to clearly explain and justify ideas when faced with competing alternativesAbility to design, communicate and apply effective and architectural design patterns across a wide range of technical problemsFamiliarity with GIT and release engineering strategies. As a plus Bachelor‚Äôs degree or higher in a technical field of studyExperience with AWS-based database systems (RDS/Aurora, Redshift, etc...), No-SQL databases, Operational Data Sources, and Data Lakes is preferred but not required.Strong desire to maintain a high level of professional and technical knowledge by attending educational workshops; reviewing professional publications; establishing personal networks; benchmarking state-of-the-art practices; and participating in professional societies.Familiarity with the MPP Databases (Redshift or Vertica), Hadoop ecosystem, and HQLFamiliarity with continuous delivery and DevOpsFlexibility and creativity in solution design ‚Äî including leveraging emerging technologies We offer Favorite Perks:The clear growth path for every role and regular performance reviewsMeaningful projects with an impact on real-world problemsLots of senior developers and mentorsHigh-trust environment: no time-trackers and flexible schedules Other Benefits:Very little bureaucracyPersonal development compensation: from English classes to Udemy courses15 PTO days (20 PTO days after a year in our team) + 8 paid sick daysRelocation support for Lviv & Ivano-Frankivsk officesQuarantine-friendly cozy officesSnack-stocked kitchen and $50 monthly lunch compensationParties, team buildings, and weekly pizza/sushi days Responsibilities Work as part of multifunctional teams to own the design and architecture of the logical entity model of the client‚Äôs suite of products.Convert logical models into physical data models employing sound database normalization techniques.Create physical database objects like tables and views with appropriate data types, foreign keys, constraints and upfront design and maintenance of proper indexes.Create and maintain easy to follow technical documentation of data models.Serve as a go-to resource for questions related to existing and proposed logical and physical data models.Perform T-SQL code reviews and ensure that new database code meets company standards for readability, reliability, and performance.Assist with resolving the performance of poorly executing stored procedures and queries.Support team initiatives by developing tools and identifying opportunities for process automation; assist in evaluation and selection of standard tools for the department.Conduct Data Warehousing learning sessions for the team to evangelize good design and T-SQL coding practices.Periodically review and manage company T-SQL coding standards.Support building and deploying the infrastructure for ingesting high-volume data from various sources.Supporting developing and maintaining the data-related scripting for build/test/deployment automation. Research individually and in collaboration with other teams on how to solve problems.Partner with team and research, design, test, and evaluate new technologies and services as it applies to data warehousing and architecting.Partner with the team to maintain an organization-wide view of current and future strategy and approach as it applies to data warehousing and architecting.Provide leadership and expertise in the development of standards, architectural governance, design patterns, and practices in data warehousing and architecting.Identify and resolve bottlenecks and bugsSupport with Scrum / Agile software development approach (e.g., sprint, standups, retros, planning, pointing, grooming, etc. Project description Our client is a digital health company that partners with employers, brokers, and pharmaceutical companies to dramatically improve people‚Äôs ability to afford and access prescription drugs. With expertise and technology solutions that span across the prescription drug ecosystem, we deliver the outcomes people and businesses need to thrive. We are on a mission to transform the pharmaceutical industry and are backed by leading venture capital firms including Canaan Partners, First Round, New Atlantic Ventures, New Leaf Venture Partners, Tribeca Venture Partners, and McKesson Ventures. In 2018, our client was ranked as one of the fastest-growing technology companies in the U.S. by Deloitte, Crain‚Äôs, and Inc. Learn more about our culture ‚Äî newfireglobal.com/culture",Data Architect /DBA@Newfire Global Partners,https://jobs.dou.ua/companies/newfire-partners/vacancies/132814/, Lviv,Data Architect /DBA,23 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/sbtech/,DraftKings (SBTech),"{""Required skills"": [""1+ years experience of coding in one or more of high level modern programming languages, preferably in C#strong experience with XUnit and NUnitsolid object-oriented programming skillssolid knowledge and experience of QA best practiceknowledge and experience in testing API or web/mobile web applications testingsolid knowledge and experience in test case design end executiongood knowledge of SDLC, Agile methodologies (Scrum, Kanban) and XP practices as wellknowledge in Jenkins jobs configurationability to take a decision in unclear casesproven to be truly Agile-thinking and product-oriented personEnglish B1 or higher""], ""As a plus"": [""experience playing proxy-PO roleexperience with Cucumber is a good advantagefamiliar with online gaming industry and systemsexperience in end-to-end and integration testingexperience using Jira""], ""We offer"": [""Being a part of an international team focused on excellence in product developmentVery good remunerationComprehensive health insurance + gymTeam building and fun activitiesHigh end modern office facilitiesDynamic and friendly work environmentOpen and transparent communicationLong term career growth opportunitiesProfessional growth, extra education opportunities and cross-locations knowledge sharingEnglish classes""], ""Responsibilities"": [""Write test cases for functional requirementsAutomate test cases in C#Continuously improve QA strategyImprove and expand automation test coverageInvestigate, fix and prevent test failuresWork closely with cross-functional teams, technology, and product managers etc. to estimate and implement test plans and cases""], ""Project description"": [""Here we\u2019re working really hard (yet not burning-out-hard), but also learning and developing fast (like Usain-Bolt-fast). We are looking for smart and focused people who represent the team spirit and are trustworthy allies in achieving great results. The business environment provokes us to be dynamic and innovative and that\u2019s exactly what we are. We offer great career opportunities"", ""you simply must seize them.""]}",,"Required skills 1+ years experience of coding in one or more of high level modern programming languages, preferably in C#strong experience with XUnit and NUnitsolid object-oriented programming skillssolid knowledge and experience of QA best practiceknowledge and experience in testing API or web/mobile web applications testingsolid knowledge and experience in test case design end executiongood knowledge of SDLC, Agile methodologies (Scrum, Kanban) and XP practices as wellknowledge in Jenkins jobs configurationability to take a decision in unclear casesproven to be truly Agile-thinking and product-oriented personEnglish B1 or higher As a plus experience playing proxy-PO roleexperience with Cucumber is a good advantagefamiliar with online gaming industry and systemsexperience in end-to-end and integration testingexperience using Jira We offer Being a part of an international team focused on excellence in product developmentVery good remunerationComprehensive health insurance + gymTeam building and fun activitiesHigh end modern office facilitiesDynamic and friendly work environmentOpen and transparent communicationLong term career growth opportunitiesProfessional growth, extra education opportunities and cross-locations knowledge sharingEnglish classes Responsibilities Write test cases for functional requirementsAutomate test cases in C#Continuously improve QA strategyImprove and expand automation test coverageInvestigate, fix and prevent test failuresWork closely with cross-functional teams, technology, and product managers etc. to estimate and implement test plans and cases Project description Here we‚Äôre working really hard (yet not burning-out-hard), but also learning and developing fast (like Usain-Bolt-fast). We are looking for smart and focused people who represent the team spirit and are trustworthy allies in achieving great results. The business environment provokes us to be dynamic and innovative and that‚Äôs exactly what we are. We offer great career opportunities ‚Äî you simply must seize them.",Mid AQA (Sport Data)@DraftKings (SBTech),https://jobs.dou.ua/companies/sbtech/vacancies/132796/, Kyiv,Mid AQA (Sport Data),23 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/onseo/,Onseo,"{""Required skills"": [""Onseo"", ""is an outsourcing company with offices in Kyiv, Vinnytsia, and Khmelnytskyi.For now, we are looking for a Data Engineer who can help us to do our day to day tasks better according to the best practices, cutting edge technologies and out of the box thinking!""], ""As a plus"": [""Required skills"", ""2+ years of programming experience in Python"", ""Strong database, DWH or BI background"", ""Experience of work with SQL and NoSQL databases"", ""Knowledge of Spark and ETL theory"", ""Experience working with Databricks, PySpark, Spark SQL, Delta Lake"", ""Experience building and optimizing data pipelines, architectures, and data sets"", ""English of an upper-intermediate level""], ""We offer"": [""Hands-on experience in Software Engineering"", ""Understanding and some experience in Airflow"", ""Experience with Devtodev"", ""Practical experience with Tableau"", ""Knowledge of AWS""], ""Responsibilities"": [""Challenging tasks, opportunity to influence the quality of the product"", ""Really good team and professional management"", ""Interesting long-term project"", ""Individual program of development"", ""Work in a comfortable office"", ""Career planning and regular performance reviews"", ""20 working days of paid vacation, paid sick leave"", ""Medical insurance, sports compensation"", ""Free English lessons""], ""Project description"": [""Develop ETL pipelines with Apache Spark, Databricks and Delta Lake"", ""Monitor and troubleshoot performance issues on the enterprise data pipelines and the data lake"", ""Partner with product and analytics teams to define and execute data acquisition, transformation, processing and make data actionable for operational and analytics initiatives"", ""Learn new technologies and apply the knowledge in production systems"", ""Create prototypes and proofs of concept for iterative development""]}",,"Required skills Onseo ‚Äî is an outsourcing company with offices in Kyiv, Vinnytsia, and Khmelnytskyi.For now, we are looking for a Data Engineer who can help us to do our day to day tasks better according to the best practices, cutting edge technologies and out of the box thinking! Required skills‚Ä¢ 2+ years of programming experience in Python‚Ä¢ Strong database, DWH or BI background‚Ä¢ Experience of work with SQL and NoSQL databases‚Ä¢ Knowledge of Spark and ETL theory‚Ä¢ Experience working with Databricks, PySpark, Spark SQL, Delta Lake‚Ä¢ Experience building and optimizing data pipelines, architectures, and data sets‚Ä¢ English of an upper-intermediate level As a plus ‚Ä¢ Hands-on experience in Software Engineering‚Ä¢ Understanding and some experience in Airflow‚Ä¢ Experience with Devtodev‚Ä¢ Practical experience with Tableau‚Ä¢ Knowledge of AWS We offer ‚Ä¢ Challenging tasks, opportunity to influence the quality of the product‚Ä¢ Really good team and professional management‚Ä¢ Interesting long-term project‚Ä¢ Individual program of development‚Ä¢ Work in a comfortable office‚Ä¢ Career planning and regular performance reviews‚Ä¢ 20 working days of paid vacation, paid sick leave‚Ä¢ Medical insurance, sports compensation‚Ä¢ Free English lessons Responsibilities ‚Ä¢ Develop ETL pipelines with Apache Spark, Databricks and Delta Lake‚Ä¢ Monitor and troubleshoot performance issues on the enterprise data pipelines and the data lake‚Ä¢ Partner with product and analytics teams to define and execute data acquisition, transformation, processing and make data actionable for operational and analytics initiatives‚Ä¢ Learn new technologies and apply the knowledge in production systems‚Ä¢ Create prototypes and proofs of concept for iterative development Project description DGN Games is a Social & Mobile game developer, currently operating 2 product lines: (coming soon ‚Äî 2 new games!)Old Vegas Slots ‚Äî the first classic social slots brand, which brings its customers a unique Vegas themed old-school gaming experienceLucky Time Slots ‚Äî A thrilling app for slots players who crave a real casino experience on mobile. DGN Games employs the combined strengths of a multicultural team spread across four continents and bases its continued success on a combination of state-of-the-art technologies, in-depth knowledge of the gaming industry, and of top-notch gaming content.",Middle Data Engineer@Onseo,https://jobs.dou.ua/companies/onseo/vacancies/125900/, Kyiv,Middle Data Engineer,23 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/mgid/,MGID,{},,"MGID ‚Äî –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω–∞—è –∫–æ–º–ø–∞–Ω–∏—è-–ª–∏–¥–µ—Ä –Ω–∞ —Ä—ã–Ω–∫–µ –Ω–∞—Ç–∏–≤–Ω–æ–π —Ä–µ–∫–ª–∞–º—ã, –æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è –≤ 2008 –≥–æ–¥—É. –ü–ª–∞—Ç—Ñ–æ—Ä–º–∞ MGID –ø–æ–º–æ–≥–∞–µ—Ç –º–µ–¥–∏–∞ –º–æ–Ω–µ—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∞—É–¥–∏—Ç–æ—Ä–∏—é, –∞ –±—Ä–µ–Ω–¥–∞–º ‚Äî –¥–æ–Ω–µ—Å—Ç–∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ —Ä–µ–∫–ª–∞–º–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ —Å–≤–æ–∏–º –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—è–º. MGID –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è: –æ—Ç –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ —Ä–µ–∫–ª–∞–º–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏, –¥–æ –µ–µ –≤–Ω–µ–¥—Ä–µ–Ω–∏—è –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏. –ù–∞—à–∏ –∫–ª–∏–µ–Ω—Ç—ã ‚Äî –∫—Ä—É–ø–Ω—ã–µ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –±—Ä–µ–Ω–¥—ã: Renault, Domino‚Äôs, airbnb, PizzaHut, Qatar Airlines –∏ –º–Ω–æ–≥–æ –¥—Ä—É–≥–∏—Ö –∫–æ–º–ø–∞–Ω–∏–π, –≤–∫–ª—é—á–∞—è –º–µ–¥–∏–∞—Ö–æ–ª–¥–∏–Ω–≥–∏ –∏ —Å–µ—Ç–µ–≤—ã–µ –∞–≥–µ–Ω—Ç—Å—Ç–≤–∞. MGID ‚Äî —ç—Ç–æ:‚Äî –û–¥–Ω–∞ –∏–∑ —Å–∞–º—ã—Ö –±–æ–ª—å—à–∏—Ö martech-–∫–æ–º–ø–∞–Ω–∏–π –Ω–∞ —É–∫—Ä–∞–∏–Ω—Å–∫–æ–º —Ä—ã–Ω–∫–µ;‚Äî –°–æ–±—Å—Ç–≤–µ–Ω–Ω–∞—è Highload –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç 185 –º–ª—Ä–¥ –∫–æ–Ω—Ç–µ–Ω—Ç–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –¥–ª—è 850 –º–ª–Ω —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–æ—Å–µ—Ç–∏—Ç–µ–ª–µ–π –Ω–∞ –±–æ–ª–µ–µ —á–µ–º 60 —è–∑—ã–∫–∞—Ö;‚Äî –ü–æ–±–µ–¥–∏—Ç–µ–ª—å –º–Ω–æ–≥–æ—á–∏—Å–ª–µ–Ω–Ω—ã—Ö –ø—Ä–µ–º–∏–π –∑–∞ –∏–Ω–Ω–æ–≤–∞—Ü–∏–∏ –∏ –∫–∞—á–µ—Å—Ç–≤–æ –ø—Ä–æ–¥—É–∫—Ç–∞ –≤ —Å—Ñ–µ—Ä–µ AdTech;‚Äî –®—Ç–∞—Ç –±–æ–ª–µ–µ 500 —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–æ–≤, —Ä–∞–±–æ—Ç–∞—é—â–∏—Ö –≤ –æ—Ñ–∏—Å–∞—Ö –≤ –°–®–ê, –ï–≤—Ä–æ–ø–µ –∏ –ê–∑–∏–∏;‚Äî –ù–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–µ –∏–Ω–Ω–æ–≤–∞—Ü–∏–∏, –æ–±–º–µ–Ω –æ–ø—ã—Ç–æ–º –∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞–º–∏ –º–µ–∂–¥—É —Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã–º–∏ –∫–æ–º–∞–Ω–¥–∞–º–∏. –ó–Ω–∞–Ω–∏—è –∏ –Ω–∞–≤—ã–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω—ã:‚Äî –û–ø—ã—Ç —Ä–∞–±–æ—Ç—ã —Å web (–±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Å–æ–±—ã—Ç–∏–π);‚Äî –£—Å–ø–µ—à–Ω—ã–µ –∫–µ–π—Å—ã —Ä–∞–±–æ—Ç—ã —Å –æ–±–µ–∑–ª–∏—á–µ–Ω–Ω–æ–π –∞—É–¥–∏—Ç–æ—Ä–∏–µ–π (–ø—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π, –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è, look-a-like);‚Äî –ì–ª—É–±–æ–∫–∏–µ –∑–Ω–∞–Ω–∏—è –ø—Ä–∏–Ω—Ü–∏–ø–æ–≤ –∏ –æ—Å–Ω–æ–≤–Ω—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –æ—Ü–µ–Ω–æ–∫ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–µ–π;‚Äî –û–ø—ã—Ç –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –∏ –≤—ã–≤–µ–¥–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –≤ production;‚Äî –û–ø—ã—Ç —Ä–∞–±–æ—Ç—ã —Å –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-—Ä–µ–∫–ª–∞–º–æ–π;‚Äî –†–∞–±–æ—Ç–∞ —Å–æ Spark(PySpark). –ß–µ–º –ø—Ä–µ–¥—Å—Ç–æ–∏—Ç –∑–∞–Ω–∏–º–∞—Ç—å—Å—è:‚Äî –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å;‚Äî –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –ø—Ä–æ—Ñ–∏–ª–µ–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π;‚Äî –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –∞—É–¥–∏—Ç–æ—Ä–∏–∏ –¥–ª—è –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ —Ä–µ–∫–ª–∞–º—ã;‚Äî –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∫–æ–Ω–≤–µ—Ä—Å–∏–æ–Ω–Ω–æ—Å—Ç–∏ –ø–æ–∫–∞–∑–∞–Ω–Ω–æ–π —Ä–µ–∫–ª–∞–º—ã. –ú—ã –ø—Ä–µ–¥–ª–∞–≥–∞–µ–º: ‚Äî –ù–æ–≤—ã–π –æ—Ñ–∏—Å –≤ –ë–¶ ¬´–ú–∞—Ä–º–µ–ª–∞–¥¬ª;‚Äî –ö—É—Ä—Å—ã –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ —Å –Ω–æ—Å–∏—Ç–µ–ª–µ–º;‚Äî –ì–∏–±–∫–∏–π –ø–æ–¥—Ö–æ–¥ –∫ –≥—Ä–∞—Ñ–∏–∫—É. –î–ª—è –Ω–∞—Å –≥–ª–∞–≤–Ω–æ–µ ‚Äî —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏ –≤–æ–≤–ª–µ—á–µ–Ω–Ω–æ—Å—Ç—å;‚Äî –ö–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–æ–µ —É—á–∞—Å—Ç–∏–µ –≤ —Å–ø–æ—Ä—Ç–∏–≤–Ω—ã—Ö, —ç–∫–æ- –∏ —Å–æ—Ü–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–æ–µ–∫—Ç–∞—Ö. –ö–∞–∫ —ç—Ç–æ –±—ã–≤–∞–µ—Ç ‚Äî —Ç—É—Ç www.facebook.com/MGID.inside‚Äî –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –ø–æ—Å–µ—â–∞—Ç—å –ª–µ–∫—Ü–∏–∏ –ê–∫–∞–¥–µ–º–∏–∏ MGID;‚Äî –ü–∞–∫–µ—Ç –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π —Å—Ç—Ä–∞—Ö–æ–≤–∫–∏ —É—Ä–æ–≤–Ω—è ¬´–ü—Ä–µ–º–∏—É–º¬ª.",Senior Data Scientist@MGID,https://jobs.dou.ua/companies/mgid/vacancies/126350/, Kyiv,Senior Data Scientist,23 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/raiffeisen-bank-international-ag/,Raiffeisen Bank International AG,"{""Required skills"": [""Technical education (technical school or suitable university degree)"", ""Practical experience in data integration development (2-3 years are recommended)"", ""Excellent knowledge of SQL and DBMS (preferred PostgreSQL)"", ""Good knowledge in ETL design and implementation (preferable with Pentaho Data Integration)"", ""Experience in Data Warehouse implementation projects is an asset"", ""Good knowledge of operating systems (Unix, Linux, Windows)"", ""Open and team-minded personality and communicative competence"", ""Experience in Scrum projects is an advantage"", ""Very good command of English (mandatory) and German (optional)"", ""Willingness to travel (Austria/Vienna)""], ""We offer"": [""Join our dynamic and motivated team in one of the leading banking groups in Austria and Central and Eastern Europe"", ""Strong support from an international banking & technology team"", ""Competitive Salary (based on EUR NBU rate)"", ""Long-term official employment, sustainable and stable working environment. Sick leave, paid vacation (31 days per year)."", ""Medical insurance"", ""Dedication and commitment to develop and educate our employees"", ""Possibility to travel and to contribute to and benefit from international communities"", ""Comfortable work conditions""], ""Responsibilities"": [""Social coding of user stories within a Scrum team"", ""Review code with your peers and improve the overall code expressiveness"", ""Design and implement ETL processes in a continuous delivery context"", ""Enrich the data vault model of the data warehouse"", ""Contribute to the design of the technical solution of the Enterprise Data Warehouse, Reporting System and Analysis System"", ""Implement and ensure software quality goals by providing proper automation test code"", ""Write technical documentation"", ""Provide 3nd level support for developed software components""], ""Project description"": [""Is Software development your passion? Do you want to be part of a team with great software engineers in order to create excellent solutions? With an entrepreneurial attitude, comparable to a team in a start-up company, and with a strong company in the background"", ""Raiffeisen Bank International, we engineer the Business Intelligence System for the Austrian Raiffeisen Bausparkasse by an agile international software engineering team. We don\u2019t just develop, we delivery value as fast as possible based on open standards and open source software.You will work as a team member in the Raiffeisen Bausparkasse Data Group. In your role you will support the product owner and the agile team by creating ETL processes for sourcing new data sources, preparing data marts and developing reports and/or analysis components embedded in a continuous delivery environment. In your daily work will work together closely with the product owner, business and the team to analyse, prototype and develop proper software solutions.""]}",,"Required skills ‚Ä¢ Technical education (technical school or suitable university degree)‚Ä¢ Practical experience in data integration development (2-3 years are recommended)‚Ä¢ Excellent knowledge of SQL and DBMS (preferred PostgreSQL)‚Ä¢ Good knowledge in ETL design and implementation (preferable with Pentaho Data Integration)‚Ä¢ Experience in Data Warehouse implementation projects is an asset‚Ä¢ Good knowledge of operating systems (Unix, Linux, Windows)‚Ä¢ Open and team-minded personality and communicative competence‚Ä¢ Experience in Scrum projects is an advantage‚Ä¢ Very good command of English (mandatory) and German (optional)‚Ä¢ Willingness to travel (Austria/Vienna) We offer ‚Ä¢ Join our dynamic and motivated team in one of the leading banking groups in Austria and Central and Eastern Europe‚Ä¢ Strong support from an international banking & technology team‚Ä¢ Competitive Salary (based on EUR NBU rate)‚Ä¢ Long-term official employment, sustainable and stable working environment. Sick leave, paid vacation (31 days per year).‚Ä¢ Medical insurance‚Ä¢ Dedication and commitment to develop and educate our employees‚Ä¢ Possibility to travel and to contribute to and benefit from international communities‚Ä¢ Comfortable work conditions Responsibilities ‚Ä¢ Social coding of user stories within a Scrum team‚Ä¢ Review code with your peers and improve the overall code expressiveness‚Ä¢ Design and implement ETL processes in a continuous delivery context‚Ä¢ Enrich the data vault model of the data warehouse‚Ä¢ Contribute to the design of the technical solution of the Enterprise Data Warehouse, Reporting System and Analysis System‚Ä¢ Implement and ensure software quality goals by providing proper automation test code‚Ä¢ Write technical documentation‚Ä¢ Provide 3nd level support for developed software components Project description Is Software development your passion? Do you want to be part of a team with great software engineers in order to create excellent solutions? With an entrepreneurial attitude, comparable to a team in a start-up company, and with a strong company in the background ‚Äî Raiffeisen Bank International, we engineer the Business Intelligence System for the Austrian Raiffeisen Bausparkasse by an agile international software engineering team. We don‚Äôt just develop, we delivery value as fast as possible based on open standards and open source software.You will work as a team member in the Raiffeisen Bausparkasse Data Group. In your role you will support the product owner and the agile team by creating ETL processes for sourcing new data sources, preparing data marts and developing reports and/or analysis components embedded in a continuous delivery environment. In your daily work will work together closely with the product owner, business and the team to analyse, prototype and develop proper software solutions.",Senior Data warehouse/BI Developer for Austrian project ( located in¬†Kyiv )@Raiffeisen Bank International AG,https://jobs.dou.ua/companies/raiffeisen-bank-international-ag/vacancies/108286/, Kyiv,Senior Data warehouse/BI Developer for Austrian project ( located in¬†Kyiv ),22 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/everguard-ai/,Everguard.AI,"{""Required skills"": [""Job Qualifications and Skills"", ""1-2 years relevant expertise as data engineer or similar role"", ""Python knowledge"", ""Basic bash scripting"", ""Experience in data versioning"", ""Good written and spoken English.""], ""As a plus"": [""Nice-to-have skills:"", ""Video streaming technologies (ffmpeg/gstreamer)"", ""Experience in implementation of Machine Learning pipelines"", ""Experience in pixel-wise and LIDAR data labeling"", ""Experience in Linux"", ""AWS"", ""Docker""], ""We offer"": [""Competative salary"", ""Cozy office (one of the best co-working in Kyiv)"", ""Challenge tasks"", ""Growth opportunity"", ""Benefit package""], ""Responsibilities"": [""Responsibilities and Duties"", ""Develop scripts for data preparation."", ""Automation of datasets management."", ""Sample labeling of small portions of data (bounding boxes, keypoints, pixel-wise, LIDAR)."", ""Data mining and cleaning."", ""Create reports based on the labeling results.""], ""Project description"": [""Everguard is on a mission to make the world\u2019s industrial environments safer with technology solutions, and to drive a paradigm shift in safety from reactive to proactive approaches to reduce the risk of industrial accidents. Our team in Ukraine is developing the future of industrial safety based on machine learning, computer vision, and IoT.""]}",,"Required skills Job Qualifications and Skills‚Äî 1-2 years relevant expertise as data engineer or similar role‚Äî Python knowledge‚Äî Basic bash scripting‚Äî Experience in data versioning‚Äî Good written and spoken English. As a plus Nice-to-have skills:‚Äî Video streaming technologies (ffmpeg/gstreamer)‚Äî Experience in implementation of Machine Learning pipelines‚Äî Experience in pixel-wise and LIDAR data labeling‚Äî Experience in Linux‚Äî AWS‚Äî Docker We offer ‚Äî Competative salary‚Äî Cozy office (one of the best co-working in Kyiv)‚Äî Challenge tasks‚Äî Growth opportunity‚Äî Benefit package Responsibilities Responsibilities and Duties‚Äî Develop scripts for data preparation. ‚Äî Automation of datasets management. ‚Äî Sample labeling of small portions of data (bounding boxes, keypoints, pixel-wise, LIDAR). ‚Äî Data mining and cleaning. ‚Äî Create reports based on the labeling results. Project description Everguard is on a mission to make the world‚Äôs industrial environments safer with technology solutions, and to drive a paradigm shift in safety from reactive to proactive approaches to reduce the risk of industrial accidents. Our team in Ukraine is developing the future of industrial safety based on machine learning, computer vision, and IoT.",Data Engineer@Everguard.AI,https://jobs.dou.ua/companies/everguard-ai/vacancies/132735/, Kyiv,Data Engineer,22 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/data-science-ua/,Data Science UA,"{""Required skills"": [""Job Qualifications and Skills"", ""1-2 years relevant expertise as data engineer or similar role"", ""Python knowledge"", ""Basic bash scripting"", ""Experience in data versioning"", ""Good written and spoken English.""], ""As a plus"": [""Nice-to-have skills:"", ""Video streaming technologies (ffmpeg/gstreamer)"", ""Experience in implementation of Machine Learning pipelines"", ""Experience in pixel-wise and LIDAR data labeling"", ""Experience in Linux"", ""AWS"", ""Docker""], ""We offer"": [""Competative salary"", ""Cozy office (one of the best co-working in Kyiv)"", ""Challenge tasks"", ""Growth opportunity"", ""Benefit package""], ""Responsibilities"": [""Responsibilities and Duties"", ""Develop scripts for data preparation."", ""Automation of datasets management."", ""Sample labeling of small portions of data (bounding boxes, keypoints, pixel-wise, LIDAR)."", ""Data mining and cleaning."", ""Create reports based on the labeling results.""], ""Project description"": [""Everguard is on a mission to make the world\u2019s industrial environments safer with technology solutions, and to drive a paradigm shift in safety from reactive to proactive approaches to reduce the risk of industrial accidents. Our team in Ukraine is developing the future of industrial safety based on machine learning, computer vision, and IoT.""]}",,"Required skills Job Qualifications and Skills‚Äî 1-2 years relevant expertise as data engineer or similar role‚Äî Python knowledge ‚Äî Basic bash scripting‚Äî Experience in data versioning‚Äî Good written and spoken English. As a plus Nice-to-have skills:‚Äî Video streaming technologies (ffmpeg/gstreamer)‚Äî Experience in implementation of Machine Learning pipelines ‚Äî Experience in pixel-wise and LIDAR data labeling‚Äî Experience in Linux‚Äî AWS‚Äî Docker We offer ‚Äî Competative salary‚Äî Cozy office (one of the best co-working in Kyiv)‚Äî Challenge tasks‚Äî Growth opportunity‚Äî Benefit package Responsibilities Responsibilities and Duties‚Äî Develop scripts for data preparation. ‚Äî Automation of datasets management. ‚Äî Sample labeling of small portions of data (bounding boxes, keypoints, pixel-wise, LIDAR). ‚Äî Data mining and cleaning. ‚Äî Create reports based on the labeling results. Project description Everguard is on a mission to make the world‚Äôs industrial environments safer with technology solutions, and to drive a paradigm shift in safety from reactive to proactive approaches to reduce the risk of industrial accidents. Our team in Ukraine is developing the future of industrial safety based on machine learning, computer vision, and IoT.",Junior/Middle Data Engineer@Data Science UA,https://jobs.dou.ua/companies/data-science-ua/vacancies/132734/, Kyiv,Junior/Middle Data Engineer,22 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/data-science-ua/,Data Science UA,"{""Required skills"": [""2-3+ years of experience in selling IT services (outsourcing/outstaffing) in Europe/US/Israel/UAE markets (experience in APAC will be relevant too)."", ""Proven successful business development track record."", ""Good understanding of IT technologies, IT terminology and markets."", ""Work experience in a software product and/or IT-outsourcing company."", ""Strong communication skills (negotiation, networking, interpersonal, etc.)."", ""Fluent written and verbal English."", ""Excellent presentation skills."", ""Experience in using CRM systems."", ""BS or MS in technical science or management/marketing."", ""Ability to plan and execute independently."", ""Be inspirational, have a lot of energy and drive."", ""Out-of-the-box mindset and ambitions.""], ""As a plus"": [""Basic understanding of Data Science / Machine Learning / Computer vision concepts."", ""Experience in attending international exhibitions and working on a booth.""], ""We offer"": [""Warm and friendly working environment."", ""Competitive compensation."", ""Fully-equipped perfect office space located in the city center (\u201cCreative Quarter\u201d co-working)."", ""Flexible schedule and the ability to work remotely.""], ""Responsibilities"": [""Carry out market research through industry contacts, publications, trade events, news to identify ideas for growth."", ""Define customer acquisition channels."", ""Develop new business with clients from Europe/US/Israel/UAE."", ""Generate, identify and grow leads. Working out the sales funnel from initial lead stage towards closure."", ""Business proposals creation and presentation."", ""Initial negotiation of contract terms together with lawyers."", ""Work with in-house data science team on pre-sales and PoCs."", ""Create marketing campaigns, attend relevant events, conferences and meetups."", ""Linkedin business campaigns and warm calls."", ""Maintain CRM and support of contracts with customers."", ""In long-term"", ""build business development team of professionals.""], ""Project description"": [""You are looking for the job of your dream, and we are looking for the sales of our dream!Data Science UA organizes the biggest (500+ participants) conferences, courses, workshops and meetups about data science/machine learning/artificial intelligence in Ukraine. We\u2019ve grown data science community and built complete ecosystem, which allows us to provide consulting and recruitment services in this cutting edge technology sphere. There is a huge interest in many companies to implement DS/ML/AI, and a very few companies can offer high-quality services in this area. We can, because we know everyone in ML/AI in Ukraine. We are looking for Business Development Manager, who would be able to generate decent amount of leads for us and drive them to the service contracts.""]}",,"Required skills ‚Äî 2-3+ years of experience in selling IT services (outsourcing/outstaffing) in Europe/US/Israel/UAE markets (experience in APAC will be relevant too).‚Äî Proven successful business development track record.‚Äî Good understanding of IT technologies, IT terminology and markets.‚Äî Work experience in a software product and/or IT-outsourcing company.‚Äî Strong communication skills (negotiation, networking, interpersonal, etc.).‚Äî Fluent written and verbal English.‚Äî Excellent presentation skills.‚Äî Experience in using CRM systems.‚Äî BS or MS in technical science or management/marketing.‚Äî Ability to plan and execute independently.‚Äî Be inspirational, have a lot of energy and drive.‚Äî Out-of-the-box mindset and ambitions. As a plus ‚Äî Basic understanding of Data Science / Machine Learning / Computer vision concepts.‚Äî Experience in attending international exhibitions and working on a booth. We offer ‚Äî Warm and friendly working environment.‚Äî Competitive compensation.‚Äî Fully-equipped perfect office space located in the city center (‚ÄúCreative Quarter‚Äù co-working).‚Äî Flexible schedule and the ability to work remotely. Responsibilities ‚Äî Carry out market research through industry contacts, publications, trade events, news to identify ideas for growth.‚Äî Define customer acquisition channels.‚Äî Develop new business with clients from Europe/US/Israel/UAE.‚Äî Generate, identify and grow leads. Working out the sales funnel from initial lead stage towards closure.‚Äî Business proposals creation and presentation.‚Äî Initial negotiation of contract terms together with lawyers.‚Äî Work with in-house data science team on pre-sales and PoCs.‚Äî Create marketing campaigns, attend relevant events, conferences and meetups. ‚Äî Linkedin business campaigns and warm calls.‚Äî Maintain CRM and support of contracts with customers.‚Äî In long-term ‚Äî build business development team of professionals. Project description You are looking for the job of your dream, and we are looking for the sales of our dream!Data Science UA organizes the biggest (500+ participants) conferences, courses, workshops and meetups about data science/machine learning/artificial intelligence in Ukraine. We‚Äôve grown data science community and built complete ecosystem, which allows us to provide consulting and recruitment services in this cutting edge technology sphere. There is a huge interest in many companies to implement DS/ML/AI, and a very few companies can offer high-quality services in this area. We can, because we know everyone in ML/AI in Ukraine. We are looking for Business Development Manager, who would be able to generate decent amount of leads for us and drive them to the service contracts.",IT¬†Sales manager/Business Development Manager@Data Science UA,https://jobs.dou.ua/companies/data-science-ua/vacancies/113276/, Kyiv,IT¬†Sales manager/Business Development Manager,22 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/ring-ukraine/,Ring Ukraine,{},,"We are looking for a team leader/software development manager (SDM) in our Research Organization. As SDM you will be responsible for one or several computer vision features starting from translating high-level product vision to a feasible solution with clear research stages, milestones, and timelines. Your main responsibilities will be technical leadership for the team as well as people management and cross-department communication with Engineering, Data, Product, and TPM organizations. ‚Äî Leading the team of ~4-8 engineers and researchers to advance the algorithmic performance using a mix of state of the art or ‚Äúclassical‚Äù techniques.‚Äî Owning one or several CV features end-to-end starting from non-technical product vision from product team till deploying it to production. ‚Äî Coordinate with data team to select and label relevant data to improve our algorithms and develop metrics by which the feature is evaluated ‚Äî 1+ years experience of a leading research group or a team on a data science project.‚Äî 3+ years experience in research or data analysis.‚Äî Degree in Math, Computer Science, Robotics or a related field‚Äî Experienced software engineer working on problems with advanced algorithms or researcher with a track record of real-world applications involving solid software engineering‚Äî Managed or tech lead small teams‚Äî Strong understanding of deep learning, machine learning, and data analysis concepts. ‚Äî Strong theoretical and practical applied knowledge of deep learning and computer vision algorithms.‚Äî Good knowledge of CS fundamentals, algorithms and data structures‚Äî Good written and spoken English‚Äî Strong experience in Python ‚Äî Experience with DSP, NLP, GAN, Object Classification, detection, segmentation, tracking, Metric Learning, Tracking, Kalman filters.‚Äî Experience in C++; if the candidate is not already an expert in C++, a willingness to gain stronger C++ expertise is required.‚Äî AWS, Docker ‚Äî Opportunity to influence the products‚Äô quality supporting company mission to make neighborhoods safer‚Äî Challenging tasks and professional growth‚Äî Competitive salary and perks‚Äî PE accounting and support‚Äî 18 paid vacation days per year, paid public holidays according to the Ukrainian legislation‚Äî Social package, including gym membership compensation, medical insurance‚Äî Free office meals, fruits, and cookies‚Äî Educational possibilities, knowledge hubs, and free corporate English classes‚Äî Career plan, professional growth, and semiannual performance review.",Lead Research Engineer@Ring Ukraine,https://jobs.dou.ua/companies/ring-ukraine/vacancies/123434/, Kyiv,Lead Research Engineer,22 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/data-science-ua/,Data Science UA,"{""Required skills"": [""What you need to be successful:"", ""Professional experience building applications in Python."", ""Experience with AWS."", ""Experience with current DevOps technologies and practices"", ""Kubernetes, Ansible, Prometheus."", ""Experience setting up logging, monitoring and metrics for applications."", ""Experience with CI/CD frameworks, e.g. Circle CI, BuildKite, Jenkins, TeamCity."", ""Good communications skills, ability to work collaboratively in a team environment, and enjoyment of learning from and teaching other team new skills.""], ""We offer"": [""Would be plus (any of them):"", ""Experience with modern build systems, e.g. Bazel, Maven, Gradle."", ""Experience with JVM.""], ""Responsibilities"": [""Flexible working"", ""We give you the option to Work from Home or better still you can work from our office for 2 weeks in total per year."", ""Sharing culture"", ""If you have learned something new, we welcome you to share it with the team through a short presentation."", ""Diversity"", ""You will be working in global diverse teams with intelligent and like-minded individuals."", ""Days off"", ""You will get 20 days holiday per year plus 2 bonus days off including your birthday and all 11 of the Ukraine public holidays off.""], ""Project description"": [""What you will be doing:"", ""Improve existing tooling, evaluate and developing new ones."", ""Large focus on building scalable solutions that will support the growth of the business."", ""Maximum uptime to both our internal and external customers."", ""Evaluation of AWS-provided tools and services."", ""Improve and maintain current CI/CD framework."", ""Participate in application and infrastructure planning, testing and development."", ""Constantly improve our monitoring and performance of services and tools.""]}",,"Required skills What you need to be successful:‚Ä¢ Professional experience building applications in Python.‚Ä¢ Experience with AWS.‚Ä¢ Experience with current DevOps technologies and practices ‚Äî Kubernetes, Ansible, Prometheus.‚Ä¢ Experience setting up logging, monitoring and metrics for applications.‚Ä¢ Experience with CI/CD frameworks, e.g. Circle CI, BuildKite, Jenkins, TeamCity.‚Ä¢ Good communications skills, ability to work collaboratively in a team environment, and enjoyment of learning from and teaching other team new skills. Would be plus (any of them):‚Ä¢ Experience with modern build systems, e.g. Bazel, Maven, Gradle.‚Ä¢ Experience with JVM. We offer ‚Ä¢ Flexible working ‚Äî We give you the option to Work from Home or better still you can work from our office for 2 weeks in total per year.‚Ä¢ Sharing culture ‚Äî If you have learned something new, we welcome you to share it with the team through a short presentation.‚Ä¢ Diversity ‚Äî You will be working in global diverse teams with intelligent and like-minded individuals.‚Ä¢ Days off ‚Äî You will get 20 days holiday per year plus 2 bonus days off including your birthday and all 11 of the Ukraine public holidays off. Responsibilities What you will be doing:‚Ä¢ Improve existing tooling, evaluate and developing new ones.‚Ä¢ Large focus on building scalable solutions that will support the growth of the business.‚Ä¢ Maximum uptime to both our internal and external customers.‚Ä¢ Evaluation of AWS-provided tools and services.‚Ä¢ Improve and maintain current CI/CD framework.‚Ä¢ Participate in application and infrastructure planning, testing and development.‚Ä¢ Constantly improve our monitoring and performance of services and tools. Project description We are looking for a Tools & Infrastructure Software Engineer for our partner company. This guy will work as a part of a team of multi-disciplined engineers, responsible for enabling the efficient, well-organised and robust flow of data through their systems, all the way from capture through to processing and delivery.",DevOps Engineer (with Python)@Data Science UA,https://jobs.dou.ua/companies/data-science-ua/vacancies/132695/, Kyiv,DevOps Engineer (with Python),22 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/luxoft/,Luxoft,"{""Required skills"": [""Experience with Java"", ""Experience with Spark"", ""Experience with Kafka or any Message broker (Rabbit etc)"", ""Knowledge of design patterns""], ""As a plus"": [""Languages:English: B2 Upper Intermediate""], ""We offer"": [""Cloud Computing Experience (e.g AWS, Azure)"", ""Experience with Hadoop"", ""Experience with Scala""], ""Responsibilities"": [""An exciting and challenging job in a dynamic team"", ""An opportunity to be innovative and to learn"", ""High salary and attractive compensation package""], ""Project description"": [""Design, development and maintenance of Java server side components,"", ""Work closely with technical leads, analysts and developers to design and implement best practice cross project solutions within a structured development process.""]}",,"Required skills ‚Äî Experience with Java‚Äî Experience with Spark‚Äî Experience with Kafka or any Message broker (Rabbit etc)‚Äî Knowledge of design patterns Languages:English: B2 Upper Intermediate As a plus ‚Äî Cloud Computing Experience (e.g AWS, Azure)‚Äî Experience with Hadoop‚Äî Experience with Scala We offer ‚Ä¢ An exciting and challenging job in a dynamic team‚Ä¢ An opportunity to be innovative and to learn‚Ä¢ High salary and attractive compensation package Responsibilities ‚Äî Design, development and maintenance of Java server side components,‚Äî Work closely with technical leads, analysts and developers to design and implement best practice cross project solutions within a structured development process. Project description Our client is a chain of luxury department stores in the USA. The projects are in Supply Chain/Product/Customer domain. You would need to develop enterprise-grade microservices, that serve employee and company business needs. Company has its mature fine-grained engineering standards and complete list of detailed requirements applied to implemented solutions. Any implemented software should have full set of autotests, like unit, integration, system tests out of the box, that‚Äôs why TDD methodologies are used all over the way. Both RPC and messaging communication models are heavily utilized in the solutions. Only modern technology stack is used in implemented software. Kafka, Avro, Spark and AWS like DynamoDB, S3, SQS, etc are widely used across the projects.Customer have fully-fledged CI/CD processes incorporated as well as Kubernetes-over-AWS cluster for container orchestration in production environment. In your daily activity you would have to collaborate with USA engineers and analytics, to share the domain knowledge and best engineering practices, as well as collect functional and non-functional requirements. Since the customer is located in the USA (West coast), so the work schedule is shifted into evening hours (~11-20 is preferable schedule). The team is located in Kyiv and Dnipro.",Java Regular data engineer@Luxoft,https://jobs.dou.ua/companies/luxoft/vacancies/62775/," Kyiv, Dnipro",Java Regular data engineer,22 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/softconstruct-ukraine/,SoftConstruct Ukraine,"{""Required skills"": [""1+ years of experience as BI/Data Analyst;"", ""Hands-on experience with data visualization tools;"", ""Solid data visualization principles understanding;"", ""Business analysis: Marketing, Operations, Finance areas"", ""Data analysis: SQL, basic statistics, EDA, anomaly detection, clustering problems etc."", ""VCS: git, Bitbucket;"", ""OS: Linux.""], ""As a plus"": [""Domain in Gambling area (SportsBook or casino);"", ""Knowledge of R \\Python;"", ""Familiarity with data storage and ETL pipelines;"", ""Hands-on experience with Google Cloud Platform: Cloud Storage, BigQuery etc.""], ""We offer"": [""Interesting and challenging work in a product and data-driven company;"", ""Plenty of opportunities to learn, grow and progress in your career;"", ""We keep work-life balance;"", ""Health insurance;"", ""Paid vacation 24 days and paid sick leave;"", ""Silent office near Lukyanivska metro station;"", ""Participate in R&D projects.""], ""Responsibilities"": [""Data collection and preparation;"", ""Building analytic dashboards (mostly using Metabase, Grafana, Kibana);"", ""Collaboration with the team\u2019s Backend, Data Engineers, FrontEnd, Data Science, Design;"", ""Creation and support of project documentation;"", ""Corporate DWH and data science workbench integration.""], ""Project description"": [""Technologies: SQL, Google BigQuery, Google Colab, Metabase.""]}",,"Required skills ‚Äî 1+ years of experience as BI/Data Analyst;‚Äî Hands-on experience with data visualization tools;‚Äî Solid data visualization principles understanding;‚Äî Business analysis: Marketing, Operations, Finance areas‚Äî Data analysis: SQL, basic statistics, EDA, anomaly detection, clustering problems etc.‚Äî VCS: git, Bitbucket;‚Äî OS: Linux. As a plus ‚Äî Domain in Gambling area (SportsBook or casino);‚Äî Knowledge of R \Python;‚Äî Familiarity with data storage and ETL pipelines;‚Äî Hands-on experience with Google Cloud Platform: Cloud Storage, BigQuery etc. We offer ‚Äî Interesting and challenging work in a product and data-driven company;‚Äî Plenty of opportunities to learn, grow and progress in your career;‚Äî We keep work-life balance;‚Äî Health insurance;‚Äî Paid vacation 24 days and paid sick leave;‚Äî Silent office near Lukyanivska metro station;‚Äî Participate in R&D projects. Responsibilities ‚Äî Data collection and preparation;‚Äî Building analytic dashboards (mostly using Metabase, Grafana, Kibana);‚Äî Collaboration with the team‚Äôs Backend, Data Engineers, FrontEnd, Data Science, Design;‚Äî Creation and support of project documentation;‚Äî Corporate DWH and data science workbench integration. Project description Technologies: SQL, Google BigQuery, Google Colab, Metabase. Data warehouse that delivers the performance, simplicity, concurrency, and affordability for data collecting, rapid analytics, and extracting data-driven insights for business users.Lambda architecture, designed to handle massive quantities of data by taking advantage of both batch and stream-processing methods. This approach to architecture attempts to balance latency, throughput, and fault-tolerance by using batch processing to provide comprehensive and accurate views of batch data, while simultaneously using real-time stream processing to provide views of online data.In many organizations, data consumption processes for timely reporting or crucial analytical requirements are hindered by both delays in query execution and information presentation. The main challenge is to minimize decision delay with help of flexible solutions with open and simplified architecture and high performance based on future-oriented technologies using AI models and forecasting analytics. In numbers:Up to 1 PB of data from 500+ partners in long-term storage30+ M new records dailyLatency for insight based on data delivery time <5 sec",Data Analyst (Big Data Project)@SoftConstruct Ukraine,https://jobs.dou.ua/companies/softconstruct-ukraine/vacancies/132648/, Kyiv,Data Analyst (Big Data Project),22 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/wargaming/,Wargaming.net,{},,"We are looking for Big Data Engineer/ETL Developer for Data Alliance Services/dataWARS department. Wargaming is a well-established game development company famous for its legendary titles such as World of Tanks, World of Warships.Data Alliance Services/dataWARS department is aimed at helping projects to make smart business decisions by integrating and processing of huge amounts of data and providing efficient calculations and reports based on stored information. We operate on global level by assisting WG teams and projects all around the globe. Our team of experienced data-enthusiastic engineers wants to expand by hiring committed and creative specialist. We use cutting edge data processing technologies and improve by constant learning. Team Lead of Big Data, Development. ‚Ä¢ Write code to ingest data from various sources;‚Ä¢ Work with game development studios to log data in a consistent and complete manner;‚Ä¢ Write ETLs to clean/transform data into clear data models;‚Ä¢ Optimize pipelines for performance;‚Ä¢ Implement algorithms at scale as needed;‚Ä¢ Implement new technologies as needed;‚Ä¢ Have fun playing with loads of data. ‚Ä¢ 2+ years total DWH/BI experience (Oracle);‚Ä¢ Expertise in ETL processes design and implementation, data lifecycle management;‚Ä¢ Confident knowledge in data querying and transformation: SQL, Oracle PL/SQL;‚Ä¢ Experience with versioning systems (SVN, git);‚Ä¢ Ability to work with and finalize requestor‚Äôs requirements;‚Ä¢ Communicative level of Russian language. ‚Ä¢ Experience with Hadoop ecosystem (HDFS, Impala/Hive) is a strong plus for a candidate;‚Ä¢ Performance tuning in any DBMS. Wargaming is an award-winning online game developer and publisher headquartered in Nicosia, Cyprus. Delivering legendary games since 1998, Wargaming has grown to become one of the leaders in the free-to-play MMO gaming industry with 4500+ employees and more than 20 offices globally. Over 200 million players enjoy Wargaming‚Äôs titles across all major gaming platforms, including the massively popular World of Tanks and World of Warships. Working in our company means always having interesting challenges and gaining valuable experience while working with top-class experts.Take your place among our passionate and experienced team and bring out the best in yourself at Wargaming! Please see legal.eu.wargaming.net/...‚Äãcandidate-privacy-policy for details on how Wargaming uses your personal data.",Oracle Developer for Big Data (Relocation to¬†Prague)@Wargaming.net,https://jobs.dou.ua/companies/wargaming/vacancies/129409/," Kyiv, Prague (Czech Republic)",Oracle Developer for Big Data (Relocation to¬†Prague),22 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/moneyveo/,Moneyveo,"{""Required skills"": [""Technical skills:"", ""SQL (advance level)."", ""Power BI, R, Phyton (Optional).""], ""As a plus"": [""Must have:"", ""MS SQL (Incremental loading, stored procedures)."", ""Understanding of DWH/BI design concepts."", ""Practical experience in flow-oriented ETL.""], ""We offer"": [""Experience with Google Cloud platformFinancial\\Risk management background""], ""Responsibilities"": [""Gym and sports program (football, volleyball, etc.)."", ""Comfortable, modern office in Pechersk district."", ""Corporate massage therapist."", ""\u0421onferences, courses and trainings (Paid)."", ""Flexible working schedule (09:00\u201311:00 to 18:00\u201320:00)."", ""Pleasant buns in the form of corporate events and teamwork, syrups, fruit, cookies, ice cream."", ""Lounge with exercise machine, wall bars, table football and tennis, Xbox."", ""Vacation days (24 calendar days / year), sick leave.""], ""Project description"": [""Design and creation of financial database (MS SQL)."", ""Ensuring the integrity and relevance of information in databases."", ""Building financial models based on DWH."", ""Building ETL process\\Power BI reports.""]}",,"Required skills Technical skills:‚Äî SQL (advance level).‚Äî Power BI, R, Phyton (Optional). Must have:‚Äî MS SQL (Incremental loading, stored procedures).‚Äî Understanding of DWH/BI design concepts.‚Äî Practical experience in flow-oriented ETL. As a plus Experience with Google Cloud platformFinancial\Risk management background We offer ‚Äî Gym and sports program (football, volleyball, etc.).‚Äî Comfortable, modern office in Pechersk district.‚Äî Corporate massage therapist.‚Äî –°onferences, courses and trainings (Paid).‚Äî Flexible working schedule (09:00‚Äì11:00 to 18:00‚Äì20:00).‚Äî Pleasant buns in the form of corporate events and teamwork, syrups, fruit, cookies, ice cream.‚Äî Lounge with exercise machine, wall bars, table football and tennis, Xbox.‚Äî Vacation days (24 calendar days / year), sick leave. Responsibilities ‚Äî Design and creation of financial database (MS SQL).‚Äî Ensuring the integrity and relevance of information in databases.‚Äî Building financial models based on DWH.‚Äî Building ETL process\Power BI reports. Project description About the product: Moneyveo is a fast growing product company. Our primary product is an online credit system, allowing customer to get a loan online within 10 minutes. About our department:We are a young and growing financial controlling team whose primary goal is to provide data driven business decisions. With our financial expertise we assist sales, marketing and product units in launching profitable businesses & initiatives. We keep track of company‚Äôs performance timely and relevantly. We help to create challenging strategic and operating plans, set and control KPIs. We are constantly in search for improvements and innovations","DB¬†Engineer, Data Analyst@Moneyveo",https://jobs.dou.ua/companies/moneyveo/vacancies/132614/, Kyiv,"DB¬†Engineer, Data Analyst",22 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/wix/,WIX.com,{},,"We‚Äôre looking for a talented data genius to work in our amazing company. It‚Äôs well known that we have a unique atmosphere here at Wix and people really like to work here, probably because the company‚Äôs culture is employee-at-first (after the customer of course :) A Machine Learning practitioner, your arsenal is full of sharp ML tools. You deeply understand classical ML & DL algs for the whole spectrum of problems and domains: Computer Vision, NLP, Recommendation Systems, Supervised & Unsupervised Learning. You‚Äôll need them all! A business mind who can speak the business language, you know how to work across all levels and areas of the company with various stakeholders. You‚Äôre an impressive communicator at all levels. You‚Äôre also a programmer who‚Äôs experienced in coding (preferably Python) your algorithms into production. And it goes without saying that you‚Äôre an awesome human being, the kind of person we want to hang out with in the pub at night (and we do). Reap the full benefits of Wix‚Äôs guilds-companies structure which is a support system that maximize your professional growth.",Senior Data Scientist@WIX.com,https://jobs.dou.ua/companies/wix/vacancies/119756/, Kyiv,Senior Data Scientist,22 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/kyla/vacancies/,All company jobs,"{""Required skills"": [""*** PLEASE, APPLY WITH: ***1. Resume in PDF format2. The date when you can start3. Salary expectation per month (or per hour if you want to work part-time)""], ""As a plus"": [""Required skills:"", ""English at a level where you can comfortably read"", ""you will have to read a lot of medical/scientific literature in English. Good writing, basic speaking"", ""Familiarity with probabilistic and statistical concepts and/or eagerness to learn more"", ""Willingness to study medical literature""], ""We offer"": [""MS/BS Degree in Statistics, Mathematics or equivalent work experience"", ""Medical Doctor completed/in-progress degree"", ""Experience in statistical data analysis and/or science research"", ""Knowledge of data structures and algorithms"", ""Proficiency in one or more: Python, Java, R"", ""Familiarity with graph databases such as neo4j"", ""Experience with Bayesian software such as Netica, Stan, PyMC3""], ""Responsibilities"": [""100% remote job with USA company"", ""Competitive salary""], ""Project description"": [""Research medical literature to build Bayesian Networks holding expert medical knowledge"", ""Collaborate with data scientists, medical doctors, nurses and software engineers"", ""Research and develop novel inference, statistical, and graphical models"", ""Positively impact human lives by developing a platform offering free healthcare""]}",,"Required skills *** PLEASE, APPLY WITH: ***1. Resume in PDF format2. The date when you can start3. Salary expectation per month (or per hour if you want to work part-time) Required skills:‚Äî English at a level where you can comfortably read ‚Äî you will have to read a lot of medical/scientific literature in English. Good writing, basic speaking‚Äî Familiarity with probabilistic and statistical concepts and/or eagerness to learn more‚Äî Willingness to study medical literature As a plus ‚Äî MS/BS Degree in Statistics, Mathematics or equivalent work experience‚Äî Medical Doctor completed/in-progress degree‚Äî Experience in statistical data analysis and/or science research‚Äî Knowledge of data structures and algorithms‚Äî Proficiency in one or more: Python, Java, R‚Äî Familiarity with graph databases such as neo4j‚Äî Experience with Bayesian software such as Netica, Stan, PyMC3 We offer ‚Äî 100% remote job with USA company‚Äî Competitive salary Responsibilities ‚Äî Research medical literature to build Bayesian Networks holding expert medical knowledge ‚Äî Collaborate with data scientists, medical doctors, nurses and software engineers‚Äî Research and develop novel inference, statistical, and graphical models‚Äî Positively impact human lives by developing a platform offering free healthcare Project description California based medical startup is looking for a Jr. Data Scientist to help in designing and implementing our revolutionary AI platform offering free healthcare.",Jr. Data Scientist@All company jobs,https://jobs.dou.ua/companies/kyla/vacancies/110354/?from=first-job, remote,Jr. Data Scientist,22 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/smartcapital-center/,Smart Capital Center,"{""Required skills"": [""OPPORTUNITY""], ""As a plus"": [""Our team shares a passion for data and smart solutions solving hard problems. You will be one of the early employees of a fast-growing startup.""], ""We offer"": [""As an early employee you will have a big impact on how to design the company\u2019s workflows and processes. We are dynamic, flexible and open to new ideas and value and hire for curiosity, integrity, excellence, respect, and contribution.""], ""Responsibilities"": [""This position is in the Ukraine, but can work remotely.""], ""Project description"": [""Our tech stack/environment includes: Hadoop/Spark, Java, Lucene, Elastic, Javascript, Postgres, AWS, Git.""]}",,"Required skills OPPORTUNITY Our team shares a passion for data and smart solutions solving hard problems. You will be one of the early employees of a fast-growing startup. As an early employee you will have a big impact on how to design the company‚Äôs workflows and processes. We are dynamic, flexible and open to new ideas and value and hire for curiosity, integrity, excellence, respect, and contribution. This position is in the Ukraine, but can work remotely. Our tech stack/environment includes: Hadoop/Spark, Java, Lucene, Elastic, Javascript, Postgres, AWS, Git. Required Skills and Qualifications ‚Äî 3+ years of data engineering, data processing experience‚Äî 5+ years of backend engineering experience (Java)‚Äî Experience with Postgres 10+, Lucene & AWS Elasticsearch, cluster and cloud computing (Hadoop, Spark)‚Äî Experienced in creating highly available systems‚Äî Experience in designing architecture and advanced analytics operations‚Äî Strong skills in data analysis, information assessment, and data quality review‚Äî Experience with AWS environment (EC2 instances, Elastic Beanstalk applications) and Git‚Äî English level Intermediate or higher‚Äî A sense of urgency and ownership over the product As a plus ‚Äî Working knowledge, interest and experience with data science / machine learning tools and technologies (NLP, and others)‚Äî High-bandwidth, high-energy, competitive, and creative individual‚Äî Driven to outperform; dedicated to self-improvement‚Äî Experience in a remote SCRUM team involving daily meetings, groomings, plannings, etc We offer ‚Äî Work on product used by worlds largest financial institutions, transforming the US real estate industry‚Äî Remote work and flexible working hours‚Äî An opportunity to work from anywhere ‚Äî we are headquartered in San Francisco with employees in California, Ukraine, Philippines and India‚Äî Compensation in USD, good bonuses, equity in the company‚Äî Business trips to the US‚Äî Great management with no bureaucracy‚Äî Very lean and highly professional team, daily standups include CTO and CEO Responsibilities ‚Äî Build industry-transforming product‚Äî Build data pipelines‚Äî Clean and prepare data, develop data processing algorithms Project description Smart Capital Center (www.SmartCapital.center) operates at the intersection of finance, real estate and tech and provides real-time real-estate property valuations and most affordable financing for real estate property owners. Smart Capital Center uses its proprietary AI technology to analyze unstructured and semi-structured data in financial documents. Additionally, its data platform aggregates hundreds of data-points on every real estate property in the USA. By combining all this data and automation, Smart Capital Center delivers ultra-fast real estate property performance analysis and valuation. Our software is used by leading insurance companies, banks and hedge funds, as well as small real estate investors.",Senior Big Data Engineer@Smart Capital Center,https://jobs.dou.ua/companies/smartcapital-center/vacancies/132599/, remote,Senior Big Data Engineer,22 September 2020,$2500‚Äì4500,2020-10-13,,dou
https://jobs.dou.ua/companies/privatbank/,–ü—Ä–∏–≤–∞—Ç–ë–∞–Ω–∫,"{""Required skills"": [""\u0437\u043d\u0430\u043d\u043d\u044f \u043c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u0447\u043d\u043e\u0457 \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u043a\u0438 \u0442\u0430 \u0442\u0435\u043e\u0440\u0456\u0457 \u0439\u043c\u043e\u0432\u0456\u0440\u043d\u043e\u0441\u0442\u0456 (\u043e\u0431\u043e\u0432\u2019\u044f\u0437\u043a\u043e\u0432\u043e);"", ""\u0434\u043e\u0441\u0432\u0456\u0434 \u043f\u043e\u0431\u0443\u0434\u043e\u0432\u0438 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u043c\u0430\u0448\u0438\u043d\u043d\u043e\u0433\u043e \u043d\u0430\u0432\u0447\u0430\u043d\u043d\u044f \u043d\u0430 Python/R \u043d\u0435 \u043c\u0435\u043d\u0448\u0435 3-\u0445 \u0440\u043e\u043a\u0456\u0432;"", ""\u0437\u043d\u0430\u043d\u043d\u044f SQL (\u0431\u0430\u0436\u0430\u043d\u043e);"", ""\u0440\u0456\u0432\u0435\u043d\u044c \u0432\u043e\u043b\u043e\u0434\u0456\u043d\u043d\u044f \u0430\u043d\u0433\u043b\u0456\u0439\u0441\u044c\u043a\u043e\u044e \u043d\u0435 \u043d\u0438\u0436\u0447\u0435 intermediate;"", ""\u0434\u043e\u0441\u0432\u0456\u0434 \u043f\u043e\u0431\u0443\u0434\u043e\u0432\u0438 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u0442\u0435\u043a\u0441\u0442\u043e\u0432\u043e\u0457 \u0430\u043d\u0430\u043b\u0456\u0442\u0438\u043a\u0438;"", ""\u0431\u0430\u0436\u0430\u043d\u0438\u0439 \u0434\u043e\u0441\u0432\u0456\u0434 \u043f\u043e\u0431\u0443\u0434\u043e\u0432\u0438 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u0434\u043b\u044f \u0444\u0456\u043d\u0430\u043d\u0441\u043e\u0432\u043e\u0433\u043e \u0441\u0435\u043a\u0442\u043e\u0440\u0430.""], ""We offer"": [""\u043f\u0440\u043e\u0444\u0435\u0441\u0456\u0439\u043d\u0435 \u0456 \u043a\u0430\u0440\u2019\u0454\u0440\u043d\u0435 \u0437\u0440\u043e\u0441\u0442\u0430\u043d\u043d\u044f;"", ""\u0448\u0438\u0440\u043e\u043a\u0438\u0439 \u0434\u0456\u0430\u043f\u0430\u0437\u043e\u043d \u0437\u0430\u0434\u0430\u0447 ;"", ""\u0446\u0456\u043a\u0430\u0432\u0456 \u043f\u0440\u043e\u0435\u043a\u0442\u0438 \u0441 \u0437\u0430\u043b\u0443\u0447\u0435\u043d\u043d\u044f\u043c \u043a\u0440\u043e\u0441-\u0444\u0443\u043d\u043a\u0446\u0456\u043e\u043d\u0430\u043b\u044c\u043d\u0438\u0445 \u043a\u043e\u043c\u0430\u043d\u0434 (dev, devOps, Data Science, Data Engineer, DataOps, BA, QA, PM) ;"", ""\u043c\u043e\u0436\u043b\u0438\u0432\u0456\u0441\u0442\u044c \u043f\u0440\u0430\u0446\u044e\u0432\u0430\u0442\u0438 \u0437 \u0432\u0435\u043b\u0438\u043a\u0438\u043c\u0438 \u043c\u0430\u0441\u0438\u0432\u0430\u043c\u0438 \u0434\u0430\u043d\u0438\u0445;"", ""\u043c\u043e\u0436\u043b\u0438\u0432\u0456\u0441\u0442\u044c \u0432\u0438\u043a\u043e\u0440\u0438\u0441\u0442\u043e\u0432\u0443\u0432\u0430\u0442\u0438 \u043f\u043b\u0430\u0442\u0444\u043e\u0440\u043c\u0438 AWS \u0442\u0430 Google Cloud;"", ""\u043c\u043e\u0436\u043b\u0438\u0432\u0456\u0441\u0442\u044c \u0432\u0438\u043a\u043e\u0440\u0438\u0441\u0442\u043e\u0432\u0443\u0432\u0430\u0442\u0438 \u043e\u0441\u0442\u0430\u043d\u043d\u0456 \u0434\u043e\u0441\u044f\u0433\u043d\u0435\u043d\u043d\u044f \u0432 \u043e\u0431\u043b\u0430\u0441\u0442\u0456 \u043c\u0430\u0448\u0438\u043d\u043d\u043e\u0433\u043e \u043d\u0430\u0432\u0447\u0430\u043d\u043d\u044f;"", ""\u043c\u043e\u0436\u043b\u0438\u0432\u0438\u0439 \u0433\u043d\u0443\u0447\u043a\u0438\u0439 \u0433\u0440\u0430\u0444\u0456\u043a \u0440\u043e\u0431\u043e\u0442\u0438.""], ""Responsibilities"": [""\u0441\u0435\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0456\u044f \u043a\u043b\u0456\u0454\u043d\u0442\u0441\u044c\u043a\u043e\u0457 \u0431\u0430\u0437\u0438 \u0434\u043b\u044f \u0440\u0456\u0437\u043d\u0438\u0445 \u043f\u0440\u043e\u0446\u0435\u0441\u0456\u0432 \u0431\u0430\u043d\u043a\u0443;"", ""\u0434\u0456\u0430\u043b\u043e\u0433\u0430\u0445 \\ \u0432\u0456\u0434\u0433\u0443\u043a\u0430\u0445 \\ \u043a\u043e\u043c\u0435\u043d\u0442\u0430\u0440\u044f\u0445;"", ""\u0437\u0430\u0434\u0430\u0447\u0456 \u0437 \u0442\u0435\u043a\u0441\u0442\u043e\u0432\u043e\u0457 \u0430\u043d\u0430\u043b\u0456\u0442\u0438\u043a\u0438: \u0432\u0438\u043a\u043e\u0440\u0438\u0441\u0442\u0430\u043d\u043d\u044f \u0442\u0435\u043c\u0430\u0442\u0438\u0447\u043d\u0438\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u0434\u043b\u044f \u0432\u0438\u0434\u0456\u043b\u0435\u043d\u043d\u044f \u0442\u0435\u043c \u0432 \u0434\u0456\u0430\u043b\u043e\u0433\u0430\u0445 \\ \u0432\u0456\u0434\u0433\u0443\u043a\u0430\u0445 \\ \u043a\u043e\u043c\u0435\u043d\u0442\u0430\u0440\u044f\u0445;"", ""\u043f\u043e\u0431\u0443\u0434\u043e\u0432\u0430 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0456\u0439\u043d\u0438\u0445 \u0441\u0438\u0441\u0442\u0435\u043c \u0434\u043b\u044f \u043e\u043f\u0442\u0438\u043c\u0456\u0437\u0430\u0446\u0456\u0457 \u043f\u0440\u043e\u043f\u043e\u0437\u0438\u0446\u0456\u0439 \u043f\u043e\u0441\u043b\u0443\u0433 \u043a\u043b\u0456\u0454\u043d\u0442\u0430\u043c;"", ""\u043f\u043e\u0448\u0443\u043a \u0430\u043d\u043e\u043c\u0430\u043b\u0456\u0439 \u0443 \u0434\u0430\u043d\u0438\u0445;"", ""\u0440\u043e\u0431\u043e\u0442\u0438 \u0437 \u043f\u0440\u043e\u0433\u043d\u043e\u0437\u0443\u0432\u0430\u043d\u043d\u044f\u043c \u0447\u0430\u0441\u043e\u0432\u0438\u0445 \u0440\u044f\u0434\u0456\u0432;"", ""\u043f\u043e\u0431\u0443\u0434\u043e\u0432\u0430 \u0441\u0438\u0441\u0442\u0435\u043c \u043f\u0440\u043e\u0430\u043a\u0442\u0438\u0432\u043d\u043e\u0433\u043e cache \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0456 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u0456\u0432 ML;"", ""\u0431\u0443\u0434\u043e\u0432\u0430 \u0441\u0438\u0441\u0442\u0435\u043c \u043f\u0440\u043e\u0433\u043d\u043e\u0437\u0443 \u043c\u0430\u0441\u043e\u0432\u0438\u0445 \u0441\u0431\u043e\u0457\u0432 \u0456\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0456\u0439\u043d\u0438\u0445 \u0441\u0438\u0441\u0442\u0435\u043c \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0456 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u0456\u0432 ML;"", ""\u0437\u0430\u0434\u0430\u0447\u0456 \u0437 computer vision: \u0456\u0434\u0435\u043d\u0442\u0438\u0444\u0456\u043a\u0430\u0446\u0456\u044f \u043f\u043e \u0444\u043e\u0442\u043e, \u0440\u043e\u0437\u043f\u0456\u0437\u043d\u0430\u0432\u0430\u043d\u043d\u044f \u0442\u0435\u043a\u0441\u0442\u0443.""], ""Project description"": [""\u0429\u043e \u0441\u0430\u043c\u0435 \u0442\u0440\u0435\u0431\u0430 \u0440\u043e\u0431\u0438\u0442\u0438:"", ""\u0440\u043e\u0437\u0440\u043e\u0431\u043b\u044f\u0442\u0438 \u043f\u0440\u0435\u0434\u0438\u043a\u0442\u0438\u0432\u043d\u0456 \u043c\u043e\u0434\u0435\u043b\u0456 \u0437 \u0432\u0438\u043a\u043e\u0440\u0438\u0441\u0442\u0430\u043d\u043d\u044f\u043c \u043c\u0435\u0442\u043e\u0434\u0456\u0432 \u043c\u0430\u0448\u0438\u043d\u043d\u043e\u0433\u043e \u043d\u0430\u0432\u0447\u0430\u043d\u043d\u044f \u0434\u043b\u044f \u043e\u043f\u0442\u0438\u043c\u0456\u0437\u0430\u0446\u0456\u0457 \u0440\u0456\u0437\u043d\u0438\u0445 \u0431\u0456\u0437\u043d\u0435\u0441-\u043f\u0440\u043e\u0446\u0435\u0441\u0456\u0432 \u0411\u0430\u043d\u043a\u0443;"", ""\u043f\u0440\u043e\u0432\u043e\u0434\u0438\u0442\u0438 \u043e\u0446\u0456\u043d\u043a\u0443 \u0435\u043a\u043e\u043d\u043e\u043c\u0456\u0447\u043d\u043e\u0457 \u0435\u0444\u0435\u043a\u0442\u0438\u0432\u043d\u043e\u0441\u0442\u0456 \u0432\u0438\u043a\u043e\u0440\u0438\u0441\u0442\u0430\u043d\u043d\u044f \u043c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u0447\u043d\u0438\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u0442\u0430 \u0456\u043d\u0448\u0435.""]}",,"Required skills ‚Äî –∑–Ω–∞–Ω–Ω—è –º–∞—Ç–µ–º–∞—Ç–∏—á–Ω–æ—ó —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Ç–∞ —Ç–µ–æ—Ä—ñ—ó –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ (–æ–±–æ–≤‚Äô—è–∑–∫–æ–≤–æ);‚Äî –¥–æ—Å–≤—ñ–¥ –ø–æ–±—É–¥–æ–≤–∏ –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è –Ω–∞ Python/R –Ω–µ –º–µ–Ω—à–µ 3-—Ö —Ä–æ–∫—ñ–≤;‚Äî –∑–Ω–∞–Ω–Ω—è SQL (–±–∞–∂–∞–Ω–æ);‚Äî —Ä—ñ–≤–µ–Ω—å –≤–æ–ª–æ–¥—ñ–Ω–Ω—è –∞–Ω–≥–ª—ñ–π—Å—å–∫–æ—é –Ω–µ –Ω–∏–∂—á–µ intermediate;‚Äî –¥–æ—Å–≤—ñ–¥ –ø–æ–±—É–¥–æ–≤–∏ –º–æ–¥–µ–ª–µ–π —Ç–µ–∫—Å—Ç–æ–≤–æ—ó –∞–Ω–∞–ª—ñ—Ç–∏–∫–∏;‚Äî –±–∞–∂–∞–Ω–∏–π –¥–æ—Å–≤—ñ–¥ –ø–æ–±—É–¥–æ–≤–∏ –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ñ—ñ–Ω–∞–Ω—Å–æ–≤–æ–≥–æ —Å–µ–∫—Ç–æ—Ä–∞. We offer ‚Äî –ø—Ä–æ—Ñ–µ—Å—ñ–π–Ω–µ —ñ –∫–∞—Ä‚Äô—î—Ä–Ω–µ –∑—Ä–æ—Å—Ç–∞–Ω–Ω—è;‚Äî —à–∏—Ä–æ–∫–∏–π –¥—ñ–∞–ø–∞–∑–æ–Ω –∑–∞–¥–∞—á ;‚Äî —Ü—ñ–∫–∞–≤—ñ –ø—Ä–æ–µ–∫—Ç–∏ —Å –∑–∞–ª—É—á–µ–Ω–Ω—è–º –∫—Ä–æ—Å-—Ñ—É–Ω–∫—Ü—ñ–æ–Ω–∞–ª—å–Ω–∏—Ö –∫–æ–º–∞–Ω–¥ (dev, devOps, Data Science, Data Engineer, DataOps, BA, QA, PM) ;‚Äî –º–æ–∂–ª–∏–≤—ñ—Å—Ç—å –ø—Ä–∞—Ü—é–≤–∞—Ç–∏ –∑ –≤–µ–ª–∏–∫–∏–º–∏ –º–∞—Å–∏–≤–∞–º–∏ –¥–∞–Ω–∏—Ö;‚Äî –º–æ–∂–ª–∏–≤—ñ—Å—Ç—å –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –ø–ª–∞—Ç—Ñ–æ—Ä–º–∏ AWS —Ç–∞ Google Cloud;‚Äî –º–æ–∂–ª–∏–≤—ñ—Å—Ç—å –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –æ—Å—Ç–∞–Ω–Ω—ñ –¥–æ—Å—è–≥–Ω–µ–Ω–Ω—è –≤ –æ–±–ª–∞—Å—Ç—ñ –º–∞—à–∏–Ω–Ω–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è;‚Äî –º–æ–∂–ª–∏–≤–∏–π –≥–Ω—É—á–∫–∏–π –≥—Ä–∞—Ñ—ñ–∫ —Ä–æ–±–æ—Ç–∏. Responsibilities ‚Äî —Å–µ–≥–º–µ–Ω—Ç–∞—Ü—ñ—è –∫–ª—ñ—î–Ω—Ç—Å—å–∫–æ—ó –±–∞–∑–∏ –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö –ø—Ä–æ—Ü–µ—Å—ñ–≤ –±–∞–Ω–∫—É;‚Äî –¥—ñ–∞–ª–æ–≥–∞—Ö \ –≤—ñ–¥–≥—É–∫–∞—Ö \ –∫–æ–º–µ–Ω—Ç–∞—Ä—è—Ö;‚Äî –∑–∞–¥–∞—á—ñ –∑ —Ç–µ–∫—Å—Ç–æ–≤–æ—ó –∞–Ω–∞–ª—ñ—Ç–∏–∫–∏: –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è —Ç–µ–º–∞—Ç–∏—á–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –≤–∏–¥—ñ–ª–µ–Ω–Ω—è —Ç–µ–º –≤ –¥—ñ–∞–ª–æ–≥–∞—Ö \ –≤—ñ–¥–≥—É–∫–∞—Ö \ –∫–æ–º–µ–Ω—Ç–∞—Ä—è—Ö;‚Äî –ø–æ–±—É–¥–æ–≤–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ–π–Ω–∏—Ö —Å–∏—Å—Ç–µ–º –¥–ª—è –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó –ø—Ä–æ–ø–æ–∑–∏—Ü—ñ–π –ø–æ—Å–ª—É–≥ –∫–ª—ñ—î–Ω—Ç–∞–º;‚Äî –ø–æ—à—É–∫ –∞–Ω–æ–º–∞–ª—ñ–π —É –¥–∞–Ω–∏—Ö;‚Äî —Ä–æ–±–æ—Ç–∏ –∑ –ø—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è–º —á–∞—Å–æ–≤–∏—Ö —Ä—è–¥—ñ–≤;‚Äî –ø–æ–±—É–¥–æ–≤–∞ —Å–∏—Å—Ç–µ–º –ø—Ä–æ–∞–∫—Ç–∏–≤–Ω–æ–≥–æ cache –Ω–∞ –æ—Å–Ω–æ–≤—ñ –∞–ª–≥–æ—Ä–∏—Ç–º—ñ–≤ ML;‚Äî –±—É–¥–æ–≤–∞ —Å–∏—Å—Ç–µ–º –ø—Ä–æ–≥–Ω–æ–∑—É –º–∞—Å–æ–≤–∏—Ö —Å–±–æ—ó–≤ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ–π–Ω–∏—Ö —Å–∏—Å—Ç–µ–º –Ω–∞ –æ—Å–Ω–æ–≤—ñ –∞–ª–≥–æ—Ä–∏—Ç–º—ñ–≤ ML;‚Äî –∑–∞–¥–∞—á—ñ –∑ computer vision: —ñ–¥–µ–Ω—Ç–∏—Ñ—ñ–∫–∞—Ü—ñ—è –ø–æ —Ñ–æ—Ç–æ, —Ä–æ–∑–ø—ñ–∑–Ω–∞–≤–∞–Ω–Ω—è —Ç–µ–∫—Å—Ç—É. Project description –©–æ —Å–∞–º–µ —Ç—Ä–µ–±–∞ —Ä–æ–±–∏—Ç–∏:‚Äî —Ä–æ–∑—Ä–æ–±–ª—è—Ç–∏ –ø—Ä–µ–¥–∏–∫—Ç–∏–≤–Ω—ñ –º–æ–¥–µ–ª—ñ –∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º –º–µ—Ç–æ–¥—ñ–≤ –º–∞—à–∏–Ω–Ω–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è –¥–ª—è –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó —Ä—ñ–∑–Ω–∏—Ö –±—ñ–∑–Ω–µ—Å-–ø—Ä–æ—Ü–µ—Å—ñ–≤ –ë–∞–Ω–∫—É;‚Äî –ø—Ä–æ–≤–æ–¥–∏—Ç–∏ –æ—Ü—ñ–Ω–∫—É –µ–∫–æ–Ω–æ–º—ñ—á–Ω–æ—ó –µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –º–∞—Ç–µ–º–∞—Ç–∏—á–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π —Ç–∞ —ñ–Ω—à–µ.",Data scientist@–ü—Ä–∏–≤–∞—Ç–ë–∞–Ω–∫,https://jobs.dou.ua/companies/privatbank/vacancies/104661/, Dnipro,Data scientist,21 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/access-softek/,Access Softek,"{""Required skills"": [""Qualifications:"", ""Strong problem-solving skills with an emphasis on product development."", ""Skilled at applying deep neural networks to real-world problems."", ""Experience using statistical computer languages (Python, R, C#) and libraries (Numpy, Pandas, Sklearn, etc.) ."", ""Experience with MS SQL database (querying, prototyping data structures, etc.)."", ""Knowledge of a variety of machine learning techniques and algorithms (deep learning, classification, regression, clustering, etc.) and their real-world advantages/drawbacks."", ""Advanced knowledge of mathematics, statistics, and probability techniques and concepts (properties of distributions, statistical tests, and proper usage, etc.)."", ""Understanding how to train and evaluate Neural Network models, choose hyperparameters and metrics, how to adjust the model for the best performance"", ""Experience with Machine Learning frameworks, such as Keras, TensorFlow, PyTorch, and others."", ""Experience with AWS, Google, and Azure machine learning solutions."", ""Strong ability to implement the developed ML models into an application"", ""Good written and verbal English, for communication with US-based teams."", ""Experience querying databases: MS SQL is preferable."", ""Experience with collaboration processes for large teams, including Git flow, Jira, Confluence etc."", ""A drive to learn and master new technologies and techniques.""], ""We offer"": [""Our benefits:"", ""Work from anywhere in the world!"", ""Senior development team."", ""Long-term employment."", ""Competitive salary."", ""Paid vacation (15 days off in a year) and national holidays."", ""Paid sick leave and internal medical insurance policy."", ""Global corporate events for all employees."", ""Partial or full compensation for training."", ""Provision of computer equipment."", ""Internet compensation (50$ per month).""], ""Responsibilities"": [""Responsibilities:"", ""Strong product-oriented person: you should be able to transform your idea from requirements into an MVP."", ""Develop high-performance ML and DNN models"", ""Mine and analyze data from databases to drive optimization and improvement of product development."", ""Create MVP of your product solutions."", ""Assess the effectiveness and accuracy of data sources and data gathering techniques."", ""Use predictive modeling to increase and optimize customer experience, revenue generation, fraud detection, churn prevention, etc."", ""Develop processes and tools to monitor and analyze model performance and data accuracy.""], ""Project description"": [""\ud83d\udcbb Access Softek is a US-based software development company, started 34 years ago in Berkeley, CA. Today, we have offices opened in New-York city, Chicago, Ohio, Vancouver (Canada) and our head office in California (Berkeley). The Fraud Control team works with US banks and credit unions to reduce the risk of account takeover and fraudulent activity through digital channels. Using machine learning and big data processing, our software identifies \u201cnormal\u201d patterns of user behavior to detect anomalies and force an additional identity verification step for abnormal activity.""]}",,"Required skills Qualifications:‚Äî Strong problem-solving skills with an emphasis on product development.‚Äî Skilled at applying deep neural networks to real-world problems.‚Äî Experience using statistical computer languages (Python, R, C#) and libraries (Numpy, Pandas, Sklearn, etc.) .‚Äî Experience with MS SQL database (querying, prototyping data structures, etc.).‚Äî Knowledge of a variety of machine learning techniques and algorithms (deep learning, classification, regression, clustering, etc.) and their real-world advantages/drawbacks.‚Äî Advanced knowledge of mathematics, statistics, and probability techniques and concepts (properties of distributions, statistical tests, and proper usage, etc.).‚Äî Understanding how to train and evaluate Neural Network models, choose hyperparameters and metrics, how to adjust the model for the best performance‚Äî Experience with Machine Learning frameworks, such as Keras, TensorFlow, PyTorch, and others.‚Äî Experience with AWS, Google, and Azure machine learning solutions.‚Äî Strong ability to implement the developed ML models into an application‚Äî Good written and verbal English, for communication with US-based teams.‚Äî Experience querying databases: MS SQL is preferable.‚Äî Experience with collaboration processes for large teams, including Git flow, Jira, Confluence etc.‚Äî A drive to learn and master new technologies and techniques. We offer Our benefits:‚Äî Work from anywhere in the world!‚Äî Senior development team.‚Äî Long-term employment.‚Äî Competitive salary.‚Äî Paid vacation (15 days off in a year) and national holidays.‚Äî Paid sick leave and internal medical insurance policy.‚Äî Global corporate events for all employees.‚Äî Partial or full compensation for training.‚Äî Provision of computer equipment.‚Äî Internet compensation (50$ per month). Responsibilities Responsibilities:‚Äî Strong product-oriented person: you should be able to transform your idea from requirements into an MVP.‚Äî Develop high-performance ML and DNN models‚Äî Mine and analyze data from databases to drive optimization and improvement of product development.‚Äî Create MVP of your product solutions.‚Äî Assess the effectiveness and accuracy of data sources and data gathering techniques.‚Äî Use predictive modeling to increase and optimize customer experience, revenue generation, fraud detection, churn prevention, etc.‚Äî Develop processes and tools to monitor and analyze model performance and data accuracy. Project description üíª Access Softek is a US-based software development company, started 34 years ago in Berkeley, CA. Today, we have offices opened in New-York city, Chicago, Ohio, Vancouver (Canada) and our head office in California (Berkeley). The Fraud Control team works with US banks and credit unions to reduce the risk of account takeover and fraudulent activity through digital channels. Using machine learning and big data processing, our software identifies ‚Äúnormal‚Äù patterns of user behavior to detect anomalies and force an additional identity verification step for abnormal activity. We are hiring an expert Machine Learning Data Scientist to work with AWS, Google, and Azure ML tools. You will work with large datasets comprised of bank account data, account history, loan records, end-user personal data, and mobile app interactions. The main task is to build and maintain Machine Learning and Neural Network models. You will help to implement those models into the product‚Äôs pipeline to ensure the best accuracy, speed, and performance. You will need to understand how to transform raw data into features/data inputs that will be fed into statistical models. You will need to conduct feature transformation, model selection, and model evaluation, and you will need to be able to analyze big volumes of raw data. The goals of this work include: developing product/loan recommendations, predicting loan risk, identifying good lending options, predicting fraudulent behavior within digital banking channels, etc.","Senior Machine Learning Data Scientist (Remote, Full-time)@Access Softek",https://jobs.dou.ua/companies/access-softek/vacancies/126125/, remote,"Senior Machine Learning Data Scientist (Remote, Full-time)",21 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/data-virtuality-gmbh/,Data Virtuality GmbH,"{""Required skills"": [""SMART. SQL-NATIVE. MULTI-TALENTED...what you should bring from your professional side""], ""As a plus"": [""Understanding of database technology"", ""Experience with Cloud Platforms (AWS, Azure)"", ""Experience with container platforms"", ""Working experience with Linux"", ""Good SQL skills"", ""Ability to debug the source code and to reproduce customer issues on a local environment"", ""Basic understanding of Business intelligence use cases"", ""Minimum 2-3 years experience in software support/consulting or comparable function""], ""We offer"": [""INNOVATIVE. AMBITIOUS. INTERNATIONAL...what you should bring from your personal side.""], ""Responsibilities"": [""Strong communication skills paired with empathy"", ""Fluent in spoken and written English"", ""Self-organized working method"", ""The highest levels of motivation, responsibility, and ambition to proactively support the growth of our company"", ""Analytical and number-based approach"", ""Problem-solving skills"", ""Inquiring mindset""]}",,"Required skills SMART. SQL-NATIVE. MULTI-TALENTED...what you should bring from your professional side ‚Äî Understanding of database technology‚Äî Experience with Cloud Platforms (AWS, Azure)‚Äî Experience with container platforms‚Äî Working experience with Linux‚Äî Good SQL skills‚Äî Ability to debug the source code and to reproduce customer issues on a local environment‚Äî Basic understanding of Business intelligence use cases‚Äî Minimum 2-3 years experience in software support/consulting or comparable function INNOVATIVE. AMBITIOUS. INTERNATIONAL...what you should bring from your personal side. ‚Äî Strong communication skills paired with empathy‚Äî Fluent in spoken and written English‚Äî Self-organized working method‚Äî The highest levels of motivation, responsibility, and ambition to proactively support the growth of our company‚Äî Analytical and number-based approach‚Äî Problem-solving skills‚Äî Inquiring mindset As a plus NICE-TO-HAVE‚Äî Sales Experience‚Äî University Degree We offer BENEFITS:‚Äî Payments in EURO‚Äî Work from everywhere‚Äî Potential relocation to Germany Responsibilities DIVERSIFIED, CHALLENGING, NEVER BORING! TROUBLESHOOTING ‚Äî You are the primary and secondary technical contact for our customers. You are finding the best solutions for upcoming issues either by only answering small questions or by supporting the operation of our software solutions and server. And you are going one step further: by sharing product feedback you will contribute to our product development andmake it better every day. SAAS MONITORING ‚Äî You use the monitoring systems and tools to proactively monitor, identify, and process any incidents by communicating to the customer or fixing the problem. SETUP AND OPERATIONS ‚Äî You have the responsibility to set up, configure, and use cloud management and monitoring tools, as well as managing the cloud environments (backup, etc.). Additionally, you will support the operation of our software and servers and performing software updates and upgrades on Windows and Linux customer machines remote via RDP and SSH. SQL-NATIVE ‚Äî You have a good understanding of SQL engine operation and are able to write, understand and debug the SQL code of our API connectors, modify code of the connectors on the fly and install updates to the customers. In cases, when heavy SQL reworking is required, you reach out to our SQL department with these tasks.",Customer Operations Engineer with SQL expertise¬†‚Äî remote. fulltime. m/f/d@Data Virtuality GmbH,https://jobs.dou.ua/companies/data-virtuality-gmbh/vacancies/132534/, remote,Customer Operations Engineer with SQL expertise¬†‚Äî remote. fulltime. m/f/d,21 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/proxify/,Proxify,{},,"Proxify is a Sweden IT company, experiencing intense growth. We match remote IT professionals with IT companies in Sweden and abroad. The difference with us is that we like to make sure that the remote workers we present are the very best in their field. We like to make sure we get it the right first time, every time! We are growing fast and currently looking for an Azure Data Factory Engineer to join our team.Our client is is a Swedish multinational home appliance manufacturer, headquartered in Stockholm. We are looking for Azure Data Factory Engineer to join us on a remote basis, who will be working in cross-functional teams but deployed to the recently established Data Science department. Requirements:‚ÄîYou have +0,5 years of experience with Azure Data Factory;‚ÄîYou have +1 years of experience with Azure in general;‚ÄîYou follow best practices and conventions;‚ÄîResponsible and able to work with minimal supervision;‚ÄîUpper-intermediate English level;‚ÄîYou can communicate well with both technical and non-technical clients. Nice-to-have: ‚ÄîExperience with Microsoft Power BI‚ÄîTimezone: CET (+/- 3 hours) Responsibilities:‚ÄîBuild data pipelines using Azure Data Factory ‚ÄîWork along-side with Project managers, stakeholders, and Data Engineers/ScientistsAzure Data Factory Engineer will be responsible for building data pipelines in Azure Data Factory for projects across a wide variety of areas in the company: Finance, IoT, Marketing, Operations, etc. What we offerüíª100% remote work (work from where you want);üí™We pay for overtime (over 8 hours);üèÑüèª‚Äç‚ôÇÔ∏èBusiness trips to Sweden at company expense (if and when necessary);üëåüèªThe ability to change the project to another one;üíµCompetitive salary/hour with a potential bonus scheme;üßòüèª‚Äç‚ôÇÔ∏èVery flexible working schedule;üöÄOpportunities for professional development and personal growth;üêïPet-friendly office, if you need to work from the office, you can come with your little friend.",Azure Data Factory Engineer@Proxify,https://jobs.dou.ua/companies/proxify/vacancies/132506/, remote,Azure Data Factory Engineer,21 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/retargetapp/,RetargetApp,{},,"RetargetApp is a marketing solution that helps e-commerce businesses promote their products on Google and Facebook. Currently, we are looking for a Data Engineer to join the team full-time. Our perfect candidate is a team player with experience in data engineering who is ready to help RetargetApp become better by organizing ETL processes. Drop us a line if you think that this job is right for you!",Data Engineer@RetargetApp,https://jobs.dou.ua/companies/retargetapp/vacancies/129087/, Kyiv,Data Engineer,21 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/mgid/,MGID,{},,"MGID was founded in 2008 and is one of the leading companies in native advertising. We enable our media partners to monetize their audience and help brands to promote their services and goods effectively. MGID offers a range of integrated solutions covering the promotion process every step of the way; we offer services ranging from planning out the marketing strategy to its thoughtful implementation and optimization. Our clients include major international brands like Renault, Domino‚Äôs, airbnb, PizzaHut, Qatar Airlines, and many others, including media organizations and web agencies. MGID is:‚Äî One of the largest MarTech-companies in the Ukrainian market;‚Äî A proprietary Highload service that delivers 185 billion advertisements to 850 million unique users in more than 70 languages;‚Äî Handling over 20 billion queries per day (more than 3X the query volume of search globally), our platform operates at an unprecedented scale;‚Äî The winner of multiple AdTech awards for innovation and product quality;‚Äî A workforce of 600+ employees operating from offices in the US, Europe and Asia;‚Äî A passion for cutting-edge technologies and a seamless vertical structure that allows the regional teams to exchange skills and development practices. Primary requirements: ‚Äî 2+ years in Product Management;‚Äî Data-driven decision-making;‚Äî Startup mindset (Fail Fast, Learn Fast);‚Äî Advanced analytical skills;‚Äî Experience in A/B testing;‚Äî Familiarity with a data ecosystem including publicly available data, 3rd party data providers, and 1st and 2nd party sources;‚Äî Expertise with analytics tools (such as SQL, Tableau, or similar);‚Äî Good sense of data privacy and data security;‚Äî Ability to organize work with customers and manage relationships with them;‚Äî Ad-tech ecosystem understanding;‚Äî Comprehensive understanding and application of marketing data and metrics;‚Äî Market and competitors research skills;‚Äî Advanced English. Will be an advantage: ‚Äî Biz Dev experience;‚Äî Experience in digital advertising landscape (ad servers, RTB, DSPs, DMPs);‚Äî Experience in building a DMP. Responsibilities: In this role, you will own the roadmap and execution for various audiences and data products. This can include UI/API experiences for building and managing audiences; data visualizations of audience attributes; data science models and pipelines for data discoverability, data quality, and audience creation; and workflows for activating these audiences across the MGID platform. As a Product Manager for data products, you will be working closely with stakeholders across our UX/UI, Data Partnerships, Data Science, and Engineering Teams ‚Äî helping think through tradeoffs in workflow implementations, visualizations, model approaches, and customer impact. You will work on researching and integrating data from different data sources. Your main responsibilities will be: ‚Äî Build and maintain relationships with key partners in the advertising ecosystem;‚Äî Manage joint product development and rollout with partners;‚Äî Conducting research to deeply understand our customers‚Äô needs;‚Äî Working with the technical department to implement confirmed hypotheses;‚Äî Building strong relationships with media, insights & analytics partners / key vendors across audience building, activation and technology to bring innovations that align with market business initiatives;‚Äî Understanding of how evolving data privacy legislation impacts our business work with our data partners to operate in a compliant manner;‚Äî Work with the Data Science team to transform insights into meaningful product improvements. What we offer: ‚ÄîFriendly team, opportunities to share your knowledge and experience;‚ÄîThe newest office in the business center ‚ÄúMarmalade‚Äù;‚ÄîEnglish courses with a native speaker;‚ÄîFlexible approach to the schedule;‚ÄîCorporate participation in sports, eco-and social projects. How it happens ‚Äî here www.facebook.com/MGID.inside;‚Äî""Premium"" health insurance package.",Data Product Manager@MGID,https://jobs.dou.ua/companies/mgid/vacancies/132486/, Kyiv,Data Product Manager,21 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/right/,Right&Above,"{""Required skills"": [""What You\u2019ll Need:""], ""We offer"": [""Minimum of 4 years\u2019 experience in a software engineering background within Java, Python or Scala""], ""Responsibilities"": [""Ability to translate loosely defined business requirements into specific software tasks that will facilitate company developing a world class data platform.""], ""Project description"": [""Extensive experience in designing and engineering data processing workflows ideally using Apache Airflow""]}",,"Required skills What You‚Äôll Need: Minimum of 4 years‚Äô experience in a software engineering background within Java, Python or Scala Ability to translate loosely defined business requirements into specific software tasks that will facilitate company developing a world class data platform. Extensive experience in designing and engineering data processing workflows ideally using Apache Airflow Strong experience with Big Data technologies such as Spark, EMR, Hadoop, Flink, Beam, Kafka, Hive, Presto, Athena, Impala, Atlas. Deep understanding of data storage technologies for structured and unstructured data. Experience with a cloud-based architecture like AWS, GCS or Azure using Linux as a primary development environment. Expert in data Warehousing Modelling (Kimball/Inmon) and data lake design. The ability to create relationships across the organization and act as an ambassador across the company to promote the data transformation journey we are on. Passion for innovation matched with excellent communication and analytical skills and demonstrated ability to influence strategy outside of your own team. A passion for research, the landscape for data engineering is rapidly evolving and we want to use this to our competitive advantage by using the right tools. We offer What We Offer: ‚Ä¢ Comfortable, well-equipped and freshly renovated offices located on metro Svyatoshin / Zhitomirskaya or metro Livoberezhna‚Ä¢ Work schedule 5 days per week 10 am ‚Äî7 pm‚Ä¢ Flexible schedule is possible and could be discussed ‚Ä¢ The company has successfully survived several crises and we are working!‚Ä¢ Excellent growth and learning opportunities‚Ä¢ Positive and friendly office culture‚Ä¢ No dress code‚Ä¢ Fresh brewed coffee and great tea selection‚Ä¢ Social events. If you found this job and company interesting please: ‚Ä¢ send your CV‚Ä¢ specify your financial expectations‚Ä¢ and English level in a short cover letter Responsibilities Your Opportunity. Work in a lean environment building out the data platform using both traditional data warehousing ideologies while incorporating the latest trends in big data. Automation of innovative algorithms for data analytics, machine learning and artificial intelligence developed by the data science team. Promote the use of Agile within the team to increase overall efficiency and predictability of project delivery. Manage effective communications with key stakeholders ensuring we are using JIRA, Confluence and Portfolio effectively. Design and develop solutions that are core to the company values and future. Help create and maintain the data engineering roadmap in collaboration with team. Mentor and support team to increase their technical knowledge and overall professional development. Promote a culture of collaboration, knowledge sharing, innovation and continuous learning across the team. Development of Engineering brand and overall strategy to attract and maintain the greatest talent. Project description –ü—Ä–æ–¥—É–∫—Ç: —Å–∫–∞–Ω–µ—Ä —É—è–∑–≤–∏–º–æ—Å—Ç–µ–π, –∑–∞–Ω–∏–º–∞–µ—Ç –æ—á–µ–Ω—å –±–æ–ª—å—à—É—é –¥–æ–ª—é —Ä—ã–Ω–∫–∞ –°–®–ê. C–µ—Ä—å–µ–∑–Ω—ã–π –ø—Ä–æ–µ–∫—Ç, –Ω–µ–æ–±—Ö–æ–¥–∏–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π —É—Ä–æ–≤–µ–Ω—å –≤—ã—à–µ —Å—Ä–µ–¥–Ω–µ–≥–æ.",Principal Data Engineer@Right&Above,https://jobs.dou.ua/companies/right/vacancies/126891/," Kyiv, remote",Principal Data Engineer,21 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/maxitech/,Maxitech Software,"{""Required skills"": [""Excellent knowledge of SQL (creation of views, stored procedures, functions, indexes, and relational database management in general)"", ""Understanding the concepts of data profiling and main data quality checks"", ""Understanding of Business Intelligence projects (data integration, data warehousing and visualization)"", ""Basic knowledge of programming and scripting language (preferably Python) to deliver test automation for data consistency and data integrity checks"", ""At least Upper-intermediate English level.""], ""As a plus"": [""Strong financial and business analysis skills"", ""Experience with a whole BI process verification (ETL, DWH, Data Marts, Reporting)"", ""Hands-on knowledge of Python to deliver data quality test automation.""], ""We offer"": [""Unique working environment where you communicate and work directly with client"", ""Being a part of the positive and fun team"", ""Cutting-edge technology stack"", ""Team of strong IT professionals working in a dynamic startup environment"", ""Big amount of knowledge-sharing practices and sessions"", ""Centrally located office near Poshtova Square metro station with great view from our terrace"", ""Competitive salary, reasonable and fair working conditions, flexible schedule"", ""Medical insurance after 3 months cooperation"", ""English classes"", ""Lanches provided by Maxitech"", ""Corporative events.""], ""Responsibilities"": [""Ensure data completeness, consistency and accuracy while performing data checks"", ""Perform data quality validation using SQL for the next data sources: Microsoft SQL Server, MySQL databases"", ""Perform data profiling and regression testing"", ""Perform data integrity checks for REST API data sources and Data Warehouse (DWH)"", ""Validation of the financial metrics calculation using Power BI and Microsoft Excel"", ""Make sure that current Power BI reports updated with new business metrics"", ""Bug reporting via TFS and help with data quality issues communication and troubleshooting"", ""Create and maintain test documentation"", ""Conduct experiments and test with BI Data Model to find gaps and issues"", ""Perform validation of full-cycle Business Intelligence solution"", ""Participate and contribute to CI/CD processes in alignment with DevOps Team"", ""Cooperate with Business Analysts team to assist with Power BI report implementation."", ""Suggest ideas to improve data model, reporting, visualization.""], ""Project description"": [""Maxitech is a R&D office of large international holding, building a portfolio of products linked to each other with the business goals they are addressing. Our project is long term and has stable financing. There are existing solutions for Marketing&Sales departments which are already in production and have active phase of new functions development as well as start-up products.""]}",,"Required skills ‚Äî Excellent knowledge of SQL (creation of views, stored procedures, functions, indexes, and relational database management in general)‚Äî Understanding the concepts of data profiling and main data quality checks‚Äî Understanding of Business Intelligence projects (data integration, data warehousing and visualization)‚Äî Basic knowledge of programming and scripting language (preferably Python) to deliver test automation for data consistency and data integrity checks‚Äî At least Upper-intermediate English level. As a plus ‚Äî Strong financial and business analysis skills‚Äî Experience with a whole BI process verification (ETL, DWH, Data Marts, Reporting)‚Äî Hands-on knowledge of Python to deliver data quality test automation. We offer ‚Äî Unique working environment where you communicate and work directly with client‚Äî Being a part of the positive and fun team‚Äî Cutting-edge technology stack‚Äî Team of strong IT professionals working in a dynamic startup environment‚Äî Big amount of knowledge-sharing practices and sessions‚Äî Centrally located office near Poshtova Square metro station with great view from our terrace‚Äî Competitive salary, reasonable and fair working conditions, flexible schedule‚Äî Medical insurance after 3 months cooperation‚Äî English classes‚Äî Lanches provided by Maxitech‚Äî Corporative events. Responsibilities ‚Äî Ensure data completeness, consistency and accuracy while performing data checks‚Äî Perform data quality validation using SQL for the next data sources: Microsoft SQL Server, MySQL databases‚Äî Perform data profiling and regression testing‚Äî Perform data integrity checks for REST API data sources and Data Warehouse (DWH)‚Äî Validation of the financial metrics calculation using Power BI and Microsoft Excel‚Äî Make sure that current Power BI reports updated with new business metrics‚Äî Bug reporting via TFS and help with data quality issues communication and troubleshooting‚Äî Create and maintain test documentation‚Äî Conduct experiments and test with BI Data Model to find gaps and issues‚Äî Perform validation of full-cycle Business Intelligence solution‚Äî Participate and contribute to CI/CD processes in alignment with DevOps Team ‚Äî Cooperate with Business Analysts team to assist with Power BI report implementation.‚Äî Suggest ideas to improve data model, reporting, visualization. Project description Maxitech is a R&D office of large international holding, building a portfolio of products linked to each other with the business goals they are addressing. Our project is long term and has stable financing. There are existing solutions for Marketing&Sales departments which are already in production and have active phase of new functions development as well as start-up products. Maxitech R&D team‚Äôs target is developing software solutions maximizing the performance of the core business. Current team size is 200+ employees. We are looking for experts ready to bring their knowledge and contribute to building the new, best-in-the class, software product.Our company headquarter is in Israel. We also have offices in 8 locations worldwide. R&D team is fully concentrated in Kyiv.We don‚Äôt do outstaff or outsource, we don‚Äôt have a ‚Äúclient‚Äù and don‚Äôt sell any service. We make qualitative software products to enable the business become more efficient and successful.",Data Analyst for¬†BI project@Maxitech Software,https://jobs.dou.ua/companies/maxitech/vacancies/129122/, Kyiv,Data Analyst for¬†BI project,21 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/squro/,Squro,"{""Required skills"": [""Work experience as a Developer for at least 4 years;"", ""Knowledge of the principles of building DWH data warehouses;"", ""Knowledge of ETL building approaches;"", ""Database Experience (SQL / NoSQL);"", ""Python experience or other programming language;"", ""Working experience with RabbitMQ / Kafka;"", ""Knowledge of web application architecture;"", ""Knowledge of software design patterns, basic algorithms, data structures;"", ""Good knowledge and experience in applying the SOLID principles;"", ""Working experience with version control systems;"", ""Working experience in integrating with third-party services;"", ""Working experience in communicating with the customer and collecting the requirements;"", ""English Intermediate.""], ""As a plus"": [""Working experience with Hadoop;"", ""Working experience with Apache Spark.""], ""We offer"": [""Friendly working environment;"", ""Possibility of personal and professional growth;"", ""Compensation of gym and language courses and professional conferences;"", ""Cozy office in the city center (1 minute from Pecherskaya metro station);"", ""Regular corporate events and other company benefits;"", ""Competitive salary;"", ""Flexible schedule.""], ""Responsibilities"": [""Collecting all the requirements directly from business;"", ""Application development and design of the data warehouse;"", ""Integration and support of developed applications;"", ""Research and analysis of third-party solutions and components;"", ""Work using the best practices: refactoring, code review, continuous delivery, scrum.""], ""Project description"": [""We focus on developing financial high secured web applications, which include a big amount of different transactions, currencies, balances and integrated with other financial services. Now we are looking for a Senior Data Engineer to work on our new project. You will have an opportunity to influence the architecture of the project choosing all the necessary instruments yourself in collaboration with our Team Lead.""]}",,"Required skills ‚Äî Work experience as a Developer for at least 4 years;‚Äî Knowledge of the principles of building DWH data warehouses;‚Äî Knowledge of ETL building approaches;‚Äî Database Experience (SQL / NoSQL);‚Äî Python experience or other programming language;‚Äî Working experience with RabbitMQ / Kafka;‚Äî Knowledge of web application architecture;‚Äî Knowledge of software design patterns, basic algorithms, data structures;‚Äî Good knowledge and experience in applying the SOLID principles;‚Äî Working experience with version control systems;‚Äî Working experience in integrating with third-party services;‚Äî Working experience in communicating with the customer and collecting the requirements;‚Äî English Intermediate. As a plus ‚Äî Working experience with Hadoop;‚Äî Working experience with Apache Spark. We offer ‚Äî Friendly working environment;‚Äî Possibility of personal and professional growth;‚Äî Compensation of gym and language courses and professional conferences;‚Äî Cozy office in the city center (1 minute from Pecherskaya metro station);‚Äî Regular corporate events and other company benefits;‚Äî Competitive salary;‚Äî Flexible schedule. Responsibilities ‚Äî Collecting all the requirements directly from business;‚Äî Application development and design of the data warehouse;‚Äî Integration and support of developed applications;‚Äî Research and analysis of third-party solutions and components;‚Äî Work using the best practices: refactoring, code review, continuous delivery, scrum. Project description We focus on developing financial high secured web applications, which include a big amount of different transactions, currencies, balances and integrated with other financial services. Now we are looking for a Senior Data Engineer to work on our new project. You will have an opportunity to influence the architecture of the project choosing all the necessary instruments yourself in collaboration with our Team Lead.",Senior Data Engineer@Squro,https://jobs.dou.ua/companies/squro/vacancies/109919/, Kyiv,Senior Data Engineer,21 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/zone3000/,ZONE3000,"{""Required skills"": [""Degree in Economics / Statistics / Mathematics"", ""Strong SQL knowledge"", ""Good theoretical background in math and statistic"", ""Understanding predictive analytics"", ""Experience building visualizations in Tableau or other similar tools"", ""Experience working with data received from Google Analytics, Adwords, Facebook, etc."", ""Understanding of LTV / CAC, Conversion Rate, Cohort analysis, Retention Rate"", ""Understanding of digital marketing, especially PPC and SEO fields"", ""Analytical mindset"", ""Upper-intermediate English (written and oral)""], ""We offer"": [""High & competitive salary"", ""Challenging work in an international professional environment"", ""Opportunity to influence software development process, to be the owner of the product in your field of expertise"", ""Opportunity to apply SAFe methodology"", ""Flexible management"", ""Flexible / Casual Leave"", ""Relocation Bonus when moving from a different city / country"", ""Full benefits package: paid vacation and sick leave"", ""Continuous professional development (free internal and external professional trainings)"", ""Free English classes in the company office"", ""Free use of the services provided by Namecheap (for non-commercial purposes)"", ""Quarterly teambuilding activities and company corporate events"", ""RDX gym membership"", ""Coffee, tea, fruits""], ""Responsibilities"": [""Provide Marketing department with deep insights regarding all marketing activities"", ""Support in decision-making"", ""Building forecasts and predictive customer behavioral models"", ""Data analysis to detect the reasons for deviations in the main and secondary metrics"", ""Help maximize revenue by bringing early attention issues or changes in trends related to our products, business practice, processes, finances, and accounting"", ""Ensure that data is tracked in the proper manner"", ""Looking for alternative data sources and tools"", ""Initiate deep digging analysis"", ""Create visualizations and presenting them to the stakeholders""], ""Project description"": [""Data Analyst will become a member of a professional and motivated team. Our main goal is to transform data into knowledge. Data Analyst will work closely with the Marketing department and will help with data analysis, finding patterns, looking for hidden correlations.""]}",,"Required skills ‚Äî Degree in Economics / Statistics / Mathematics‚Äî Strong SQL knowledge‚Äî Good theoretical background in math and statistic‚Äî Understanding predictive analytics‚Äî Experience building visualizations in Tableau or other similar tools‚Äî Experience working with data received from Google Analytics, Adwords, Facebook, etc.‚Äî Understanding of LTV / CAC, Conversion Rate, Cohort analysis, Retention Rate‚Äî Understanding of digital marketing, especially PPC and SEO fields‚Äî Analytical mindset‚Äî Upper-intermediate English (written and oral) We offer ‚Äî High & competitive salary‚Äî Challenging work in an international professional environment‚Äî Opportunity to influence software development process, to be the owner of the product in your field of expertise‚Äî Opportunity to apply SAFe methodology‚Äî Flexible management‚Äî Flexible / Casual Leave‚Äî Relocation Bonus when moving from a different city / country‚Äî Full benefits package: paid vacation and sick leave‚Äî Continuous professional development (free internal and external professional trainings)‚Äî Free English classes in the company office‚Äî Free use of the services provided by Namecheap (for non-commercial purposes)‚Äî Quarterly teambuilding activities and company corporate events ‚Äî RDX gym membership ‚Äî Coffee, tea, fruits Responsibilities ‚Äî Provide Marketing department with deep insights regarding all marketing activities‚Äî Support in decision-making‚Äî Building forecasts and predictive customer behavioral models‚Äî Data analysis to detect the reasons for deviations in the main and secondary metrics‚Äî Help maximize revenue by bringing early attention issues or changes in trends related to our products, business practice, processes, finances, and accounting‚Äî Ensure that data is tracked in the proper manner‚Äî Looking for alternative data sources and tools‚Äî Initiate deep digging analysis‚Äî Create visualizations and presenting them to the stakeholders Project description Data Analyst will become a member of a professional and motivated team. Our main goal is to transform data into knowledge. Data Analyst will work closely with the Marketing department and will help with data analysis, finding patterns, looking for hidden correlations.",Data Analyst (Marketing)@ZONE3000,https://jobs.dou.ua/companies/zone3000/vacancies/126184/, Kharkiv,Data Analyst (Marketing),21 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/visiquate/,VisiQuate,"{""Required skills"": [""\u25cf 2+ years experience as a business analyst, data analyst or reporting analyst; \u25cf English"", ""upper intermediate level at least; \u25cf Proficiency in MS Excel (creating reports, formulas, macros, pivot tables); \u25cf 1+ year experience in SQL language; \u25cf 1+ year MS Access reporting and development experience""], ""As a plus"": [""\u25cf High-level understanding of major Healthcare data sets: Accounts, Transactions, Charges\u25cf 1+ years Healthcare data experience\u25cf Basic proficiency in desktop applications such as MS Word and MS PowerPoint\u25cf Knowledge/Experience of working with BI tools""], ""We offer"": [""\u25cf Work in a team of smart and talented people;\u25cf Be a part of a fast-growing product company;\u25cf Comfortable office in the city center (Rymarska Str 32);\u25cf Holidays per Ukrainian calendar;\u25cf Flexible work schedule within Ukrainian business hours and a friendly atmosphere in the office;\u25cf Free English classes.""], ""Responsibilities"": [""This person needs to interpret data, analyze results and provide ongoing reports, generally using MS Excel and MS SQL as database.""], ""Project description"": [""VisiQuate is a daily SaaS-based service for healthcare and enterprise clients that makes the volume, variety, and velocity of Big Data simple and actionable. VisiQuate employes data scientists, visual scientists, and subject matter experts who integrate terabytes of incompatible Big Data onto a single platform.We\u2019re searching for amazing people to join our team in relation to the expansion of one of the long term and important projects.""]}",,"Required skills ‚óè 2+ years experience as a business analyst, data analyst or reporting analyst; ‚óè English ‚Äî upper intermediate level at least; ‚óè Proficiency in MS Excel (creating reports, formulas, macros, pivot tables); ‚óè 1+ year experience in SQL language; ‚óè 1+ year MS Access reporting and development experience As a plus ‚óè High-level understanding of major Healthcare data sets: Accounts, Transactions, Charges‚óè 1+ years Healthcare data experience‚óè Basic proficiency in desktop applications such as MS Word and MS PowerPoint‚óè Knowledge/Experience of working with BI tools We offer ‚óè Work in a team of smart and talented people;‚óè Be a part of a fast-growing product company;‚óè Comfortable office in the city center (Rymarska Str 32);‚óè Holidays per Ukrainian calendar;‚óè Flexible work schedule within Ukrainian business hours and a friendly atmosphere in the office;‚óè Free English classes. Responsibilities This person needs to interpret data, analyze results and provide ongoing reports, generally using MS Excel and MS SQL as database. Project description VisiQuate is a daily SaaS-based service for healthcare and enterprise clients that makes the volume, variety, and velocity of Big Data simple and actionable. VisiQuate employes data scientists, visual scientists, and subject matter experts who integrate terabytes of incompatible Big Data onto a single platform.We‚Äôre searching for amazing people to join our team in relation to the expansion of one of the long term and important projects.",Data Reporting Analyst@VisiQuate,https://jobs.dou.ua/companies/visiquate/vacancies/38768/, Kharkiv,Data Reporting Analyst,20 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/exadel/,Exadel Inc.,"{""Required skills"": [""Good knowledge of Spark;"", ""Good knowledge of SQL;"", ""Hands on experience with Scala and Java, Python, Shell (as plus);"", ""Hands on experience with Hadoop ecosystem: HDFS, Yarn, Livy;"", ""Knowledge of AWS: S3, EC2, EMR;"", ""English level: Upper-Intermediate.""], ""We offer"": [""Vacation is 20 working days / till 20 working days per year for sick leaves;"", ""Full payment of taxes;"", ""English courses;"", ""Flexible work schedule;"", ""Friendly environment;"", ""Medical insurance;"", ""Opportunity for career growth.You can find more information about Exadel in Ukraine here: www.facebook.com/exadelukraine""], ""Responsibilities"": [""Contribute in all phases of the development lifecycle;"", ""Write well designed, testable, efficient code;"", ""Ensure designs are in compliance with specifications;"", ""Interact with team, arrange meetings to find out and clarify the project\u2019s requirements;"", ""Implementation of requirements based on scala\\spark.""], ""Project description"": [""Our customer is a leading global information and measurement company that enables organisations to understand consumers and consumer behavior. He measures and monitors what consumers watch (programming, advertising) and what consumers buy (categories, brands, products) on a global and local basis. The company has a presence in approximately 100 countries spread across Africa, Asia, Australia, Europe, Middle East, North America, South America and Russia.""]}",,"Required skills ‚Ä¢ Good knowledge of Spark;‚Ä¢ Good knowledge of SQL;‚Ä¢ Hands on experience with Scala and Java, Python, Shell (as plus);‚Ä¢ Hands on experience with Hadoop ecosystem: HDFS, Yarn, Livy;‚Ä¢ Knowledge of AWS: S3, EC2, EMR;‚Ä¢ English level: Upper-Intermediate. We offer ‚Ä¢ Vacation is 20 working days / till 20 working days per year for sick leaves;‚Ä¢ Full payment of taxes;‚Ä¢ English courses;‚Ä¢ Flexible work schedule;‚Ä¢ Friendly environment;‚Ä¢ Medical insurance;‚Ä¢ Opportunity for career growth.You can find more information about Exadel in Ukraine here: www.facebook.com/exadelukraine Responsibilities ‚Ä¢ Contribute in all phases of the development lifecycle;‚Ä¢ Write well designed, testable, efficient code;‚Ä¢ Ensure designs are in compliance with specifications;‚Ä¢ Interact with team, arrange meetings to find out and clarify the project‚Äôs requirements;‚Ä¢ Implementation of requirements based on scala\spark. Project description Our customer is a leading global information and measurement company that enables organisations to understand consumers and consumer behavior. He measures and monitors what consumers watch (programming, advertising) and what consumers buy (categories, brands, products) on a global and local basis. The company has a presence in approximately 100 countries spread across Africa, Asia, Australia, Europe, Middle East, North America, South America and Russia. About the Project:The aim of the Project is transformation of data on watching of television programming, partition of data according to television programming and duration of advertising.",Big Data Developer@Exadel Inc.,https://jobs.dou.ua/companies/exadel/vacancies/132423/," Kharkiv, Lviv, Odesa, Vinnytsia",Big Data Developer,18 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/augmented-pixels/,Augmented Pixels,"{""Required skills"": [""\u0421\u043e\u0437\u0434\u0430\u0435\u043c \u043f\u0440\u043e\u0434\u0443\u043a\u0442\u044b, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043b\u0438\u0446\u0435\u043d\u0437\u0438\u0440\u0443\u044e\u0442 \u043a\u043e\u043c\u043f\u0430\u043d\u0438\u0438 \u043c\u0438\u0440\u043e\u0432\u043e\u0433\u043e \u0443\u0440\u043e\u0432\u043d\u044f.""], ""As a plus"": [""\u0421\u0444\u0435\u0440\u0430 \u043d\u0430\u0432\u0438\u0433\u0430\u0446\u0438\u0438 (SLAM, Mapping etc) \u0434\u043b\u044f AR/VR Glasses and Robots.""], ""We offer"": [""\u0418\u0449\u0435\u043c \u0432 \u043f\u0435\u0440\u0432\u043e\u043a\u043b\u0430\u0441\u0441\u043d\u0443\u044e \u043a\u043e\u043c\u0430\u043d\u0434\u0443 \u0438\u043d\u0436\u0435\u043d\u0435\u0440\u043e\u0432-\u0440\u0435\u0441\u0435\u0440\u0447\u0435\u0440\u043e\u0432, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0441\u043f\u043e\u0441\u043e\u0431\u043d\u044b \u0441\u0430\u043c\u043e\u0441\u0442\u043e\u044f\u0442\u0435\u043b\u044c\u043d\u043e \u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c \u043d\u0430\u0434 \u0441\u043b\u043e\u0436\u043d\u044b\u043c\u0438 \u0437\u0430\u0434\u0430\u0447\u0430\u043c\u0438, \u0442\u0440\u0435\u0431\u0443\u044e\u0449\u0438\u0435 \u043d\u0435 \u0442\u043e\u043b\u044c\u043a\u043e \u0430\u043a\u0430\u0434\u0435\u043c\u0438\u0447\u0435\u0441\u043a\u043e\u0433\u043e \u043c\u0430\u0442. \u0431\u0435\u043a\u0433\u0440\u0430\u0443\u043d\u0434\u0430, \u043d\u043e \u0438 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0443\u044e \u043a\u0443\u043b\u044c\u0442\u0443\u0440\u0443 \u0440\u0430\u0431\u043e\u0442\u044b \u0434\u043b\u044f \u0432\u044b\u043f\u0443\u0441\u043a\u0430 \u0442\u0435\u0445\u043d\u043e\u043b\u043e\u0433\u0438\u0439 \u043d\u0430 \u0440\u044b\u043d\u043e\u043a(\u0430 \u043d\u0435 \u00ab\u043f\u0440\u043e\u0442\u043e\u0442\u0438\u043f\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u0432 \u0441\u0442\u043e\u043b\u00bb). Must have"", ""C++"", ""English (intermediate+)"", ""Algorithms, data structures"", ""Math: linear algebra, calculus, statistics, bayesian filtering, smoothing, nonlinear optimizations"", ""Machine learning: neural networks, regressions"", ""R&D: \u0443\u043c\u0435\u043d\u0438\u0435 \u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c \u0441 \u043d\u0430\u0443\u0447\u043d\u044b\u043c\u0438 \u0441\u0442\u0430\u0442\u044c\u044f\u043c\u0438, \u043e\u043f\u044b\u0442 \u0441\u0430\u043c\u043e\u0441\u0442\u043e\u044f\u0442\u0435\u043b\u044c\u043d\u044b\u0445 \u0438\u0441\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u043d\u0438\u0439"", ""\u0441\u0430\u043c\u043e\u0441\u0442\u043e\u044f\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u044c, \u0443\u043c\u0435\u043d\u0438\u0435 \u0447\u0435\u0442\u043a\u043e \u0438\u0437\u043b\u0430\u0433\u0430\u0442\u044c \u0441\u0432\u043e\u0438 \u043c\u044b\u0441\u043b\u0438 \u0438 \u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c \u0432 \u043a\u043e\u043c\u0430\u043d\u0434\u0435""], ""Responsibilities"": [""Python, ROS, Linux, OpenCV, TensorFlow, MATLAB"", ""academic background (math, physics, computer science)"", ""experience in Computer Vision: vSLAM, SLAM, object detection/recognition, segmentation, 3d reconstruction"", ""experience in photogrammetry""], ""Project description"": [""\u0417\u043f \u0443\u043a\u0430\u0437\u0430\u043d\u0430 \u0434\u043b\u044f \u0423\u043a\u0440\u0430\u0438\u043d\u044b(\u043e\u0444\u0438\u0441\u044b \u0432 \u041e\u0434\u0435\u0441\u0441\u0435 \u0438 \u041a\u0438\u0435\u0432\u0435). \u0415\u0441\u0442\u044c \u0441\u0442\u043e\u043a\u043e\u043f\u0448\u043d\u044b. \u0412\u043e\u0437\u043c\u043e\u0436\u0435\u043d \u043f\u0435\u0440\u0435\u0435\u0437\u0434 \u0432 \u0421\u0428\u0410 \u0432 \u043f\u0435\u0440\u0441\u043f\u0435\u043a\u0442\u0438\u0432\u0435.""]}",,"Required skills –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–¥—É–∫—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –ª–∏—Ü–µ–Ω–∑–∏—Ä—É—é—Ç –∫–æ–º–ø–∞–Ω–∏–∏ –º–∏—Ä–æ–≤–æ–≥–æ —É—Ä–æ–≤–Ω—è. –°—Ñ–µ—Ä–∞ –Ω–∞–≤–∏–≥–∞—Ü–∏–∏ (SLAM, Mapping etc) –¥–ª—è AR/VR Glasses and Robots. –ò—â–µ–º –≤ –ø–µ—Ä–≤–æ–∫–ª–∞—Å—Å–Ω—É—é –∫–æ–º–∞–Ω–¥—É –∏–Ω–∂–µ–Ω–µ—Ä–æ–≤-—Ä–µ—Å–µ—Ä—á–µ—Ä–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ —Å–ø–æ—Å–æ–±–Ω—ã —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ —Ä–∞–±–æ—Ç–∞—Ç—å –Ω–∞–¥ —Å–ª–æ–∂–Ω—ã–º–∏ –∑–∞–¥–∞—á–∞–º–∏, —Ç—Ä–µ–±—É—é—â–∏–µ –Ω–µ —Ç–æ–ª—å–∫–æ –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–æ–≥–æ –º–∞—Ç. –±–µ–∫–≥—Ä–∞—É–Ω–¥–∞, –Ω–æ –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é –∫—É–ª—å—Ç—É—Ä—É —Ä–∞–±–æ—Ç—ã –¥–ª—è –≤—ã–ø—É—Å–∫–∞ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π –Ω–∞ —Ä—ã–Ω–æ–∫(–∞ –Ω–µ ¬´–ø—Ä–æ—Ç–æ—Ç–∏–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ —Å—Ç–æ–ª¬ª). Must have‚Äî C++‚Äî English (intermediate+)‚Äî Algorithms, data structures‚Äî Math: linear algebra, calculus, statistics, bayesian filtering, smoothing, nonlinear optimizations‚Äî Machine learning: neural networks, regressions ‚Äî R&D: —É–º–µ–Ω–∏–µ —Ä–∞–±–æ—Ç–∞—Ç—å —Å –Ω–∞—É—á–Ω—ã–º–∏ —Å—Ç–∞—Ç—å—è–º–∏, –æ–ø—ã—Ç —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω—ã—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π‚Äî —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ—Å—Ç—å, —É–º–µ–Ω–∏–µ —á–µ—Ç–∫–æ –∏–∑–ª–∞–≥–∞—Ç—å —Å–≤–æ–∏ –º—ã—Å–ª–∏ –∏ —Ä–∞–±–æ—Ç–∞—Ç—å –≤ –∫–æ–º–∞–Ω–¥–µ As a plus ‚Äî Python, ROS, Linux, OpenCV, TensorFlow, MATLAB‚Äî academic background (math, physics, computer science)‚Äî experience in Computer Vision: vSLAM, SLAM, object detection/recognition, segmentation, 3d reconstruction‚Äî experience in photogrammetry We offer –ó–ø —É–∫–∞–∑–∞–Ω–∞ –¥–ª—è –£–∫—Ä–∞–∏–Ω—ã(–æ—Ñ–∏—Å—ã –≤ –û–¥–µ—Å—Å–µ –∏ –ö–∏–µ–≤–µ). –ï—Å—Ç—å —Å—Ç–æ–∫–æ–ø—à–Ω—ã. –í–æ–∑–º–æ–∂–µ–Ω –ø–µ—Ä–µ–µ–∑–¥ –≤ –°–®–ê –≤ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–µ. Responsibilities –ú—ã —Å–æ–∑–¥–∞–µ–º —Å–ª–æ–∂–Ω—É—é —Å–∏—Å—Ç–µ–º—É, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –∫–æ–º–ø—å—é—Ç–µ—Ä—É(—Ä–æ–±–æ—Ç ‚Äî —Ç–µ–ª–µ—Ñ–æ–Ω ‚Äî –æ—á–∫–∏) –æ—Ç–≤–µ—á–∞—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã:‚Äî –≥–¥–µ —è –Ω–∞—Ö–æ–∂—É—Å—å?‚Äî —ç—Ç–æ —Å—Ç–µ–Ω–∞ –∏–ª–∏ —Ç–∞–º –º–æ–∂–Ω–æ –µ—Ö–∞—Ç—å?‚Äî –≥–¥–µ –Ω–∞—Ö–æ–¥—è—Ç—Å—è –¥—Ä—É–≥–∏–µ —Ä–æ–±–æ—Ç—ã –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –º–µ–Ω—è?‚Äî –∫–∞–∫–∏–µ –æ–±—ä–µ–∫—Ç—ã —è –≤–∏–∂—É –∏ –∫–∞–∫ –æ–Ω–∏ –Ω–∞–∑—ã–≤–∞—é—Ç—Å—è?‚Äî –∫–∞—Ä—Ç–∞? –∏ —Ç.–¥. –í—ã –±—É–¥–µ—Ç–µ —Ä–∞–∑—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –º–æ–¥—É–ª–∏ —ç—Ç–æ–π —Å–∏—Å—Ç–µ–º—ã. –í –ø—Ä–æ—Ü–µ—Å—Å–µ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –Ω—É–∂–Ω–æ:‚Äî –ø—Ä–∏–Ω–∏–º–∞—Ç—å —Ä–µ—à–µ–Ω–∏—è –≤ —É—Å–ª–æ–≤–∏—è—Ö —Å –≤—ã—Å–æ–∫–æ–π —Å—Ç–µ–ø–µ–Ω—å—é –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏‚Äî —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –ø–ª–∞–Ω —Å–≤–æ–µ–π —Ä–∞–±–æ—Ç—ã (–∫–æ–º–∞–Ω–¥–∞ —Ä–µ—à–∞–µ—Ç —Ç–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä–Ω–æ –∫—É–¥–∞ –∫–æ–ø–∞—Ç—å, –∫–∞–∫ –∏ —á–µ–º –í—ã –¥–æ–ª–∂–Ω—ã —Ä–µ—à–∞—Ç—å —Å–∞–º–∏)‚Äî –ø—Ä–æ—Ç–æ—Ç–∏–ø–∏—Ä–æ–≤–∞—Ç—å, —Ä–∞–∑—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å, —Ä–µ–∞–ª–∏–∑–æ–≤—ã–≤–∞—Ç—å, –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã‚Äî —Ä–∞–∑—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –Ω–µ—Ç—Ä–∏–≤–∏–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∏–∑–º–µ—Ä–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–∞–±–æ—Ç—ã –ø–æ–¥—Å–∏—Å—Ç–µ–º‚Äî —á–∏—Ç–∞—Ç—å, –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å, –ø—Ä–∏–º–µ–Ω—è—Ç—å –Ω–∞—É—á–Ω—ã–µ —Å—Ç–∞—Ç—å–∏, –ø–æ—Å—Ç–æ—è–Ω–Ω–æ –±—ã—Ç—å –≤ –∫—É—Ä—Å–µ State of the Art –≤ —Å—Ñ–µ—Ä–µ–æ–±—â–∞—Ç—å—Å—è —Å –¥—Ä—É–≥–∏–º–∏ –∏–Ω–∂–µ–Ω–µ—Ä–∞–º–∏ –∫–æ–º–ø–∞–Ω–∏–∏, —Å—Ç–∞–≤–∏—Ç—å –∏–º –∑–∞–¥–∞—á–∏, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ —á—Ç–æ-—Ç–æ –¥–æ—Ä–∞–±–æ—Ç–∞—Ç—å, –ø–æ—á–∏–Ω–∏—Ç—å –∏ —Ç.–¥. Project description Autonomous Navigation for Robotics. Inside out tracking for AR/VR Glasses.Web: augmentedpixels.com",R&D Engineer in¬†Computer Vision¬†‚Äî 3D¬†Mapping for Robotics and AR/VR@Augmented Pixels,https://jobs.dou.ua/companies/augmented-pixels/vacancies/44607/," Kyiv, Odesa",R&D Engineer in¬†Computer Vision¬†‚Äî 3D¬†Mapping for Robotics and AR/VR,18 September 2020,–æ—Ç¬†$4000,2020-10-13,,dou
https://jobs.dou.ua/companies/zone3000/,ZONE3000,"{""Required skills"": [""Bachelor\u2019s degree in Computer Science, information management or similar field"", ""At least 2 years business intelligence experience"", ""Experience with SQL databases"", ""Experience gathering and analyzing system requirements"", ""Strong problem-solving skills and an analytical mind"", ""Highly accurate and thorough in all tasks"", ""Able to thrive in a fast-paced, deadline-driven environment"", ""Collaborative team player"", ""English level"", ""upper-intermediate""], ""As a plus"": [""Experience with MongoDB, Hadoop, Apache Spark or any other database modeling and management tools"", ""Experience with C4 model or any other visualizing tool""], ""We offer"": [""High & competitive salary"", ""Challenging work in an international professional environment"", ""Opportunity to influence software development process, to be the owner of the product in your field of expertise"", ""Opportunity to apply SAFe methodology"", ""Flexible management"", ""Flexible / Casual Leave"", ""Relocation Bonus when moving from a different city / country"", ""Full benefits package: paid vacation and sick leave"", ""Continuous professional development (free internal and external professional trainings)"", ""Free English classes in the company office"", ""Free use of the services provided by Namecheap (for non-commercial purposes)"", ""Quarterly teambuilding activities and company corporate events"", ""RDX gym membership"", ""Coffee, tea, fruits""], ""Responsibilities"": [""Creating and implementing tracking documents to meet stated requirements for metadata management, operational data stores and Extract Transform Load environments"", ""Design conceptual and logical data models and flowcharts"", ""Assisting with developing database solutions to store and retrieve company information"", ""Collaborate with internal customers and IT partners, including system architects, software developers, database administrators, design analysts and information modeling experts to determine project requirements and capabilities, and strategize development and implementation timelines"", ""Performing regular data integrity and quality audits, collating and verifying data from multiple sources"", ""Research new technologies, data modeling methods and information management systems todetermine which ones should be incorporated into company data architectures, and develop implementation timelines and milestones""], ""Project description"": [""Looking for a Business Intelligence Data Specialist.Zone3000 works with Namecheap project (www.namecheap.com).Namecheap was founded in 2000 on the idea that all people deserve value-priced domains delivered through stellar service. Today, Namecheap is a leading ICANN-accredited domain name registrar and web hosting company with over two million customers and ten million domains under management"", ""and we\u2019re just getting started.Our culture is built on the values that we live every day; the way we work, the way we collaborate with our global network of colleagues and the way we relentlessly innovate solutions that meet the emerging needs of our customers.""]}",,"Required skills ‚Äî Bachelor‚Äôs degree in Computer Science, information management or similar field‚Äî At least 2 years business intelligence experience‚Äî Experience with SQL databases‚Äî Experience gathering and analyzing system requirements ‚Äî Strong problem-solving skills and an analytical mind‚Äî Highly accurate and thorough in all tasks‚Äî Able to thrive in a fast-paced, deadline-driven environment‚Äî Collaborative team player‚Äî English level ‚Äî upper-intermediate As a plus ‚Äî Experience with MongoDB, Hadoop, Apache Spark or any other database modeling and management tools‚Äî Experience with C4 model or any other visualizing tool We offer ‚Äî High & competitive salary‚Äî Challenging work in an international professional environment‚Äî Opportunity to influence software development process, to be the owner of the product in your field of expertise‚Äî Opportunity to apply SAFe methodology‚Äî Flexible management‚Äî Flexible / Casual Leave‚Äî Relocation Bonus when moving from a different city / country‚Äî Full benefits package: paid vacation and sick leave‚Äî Continuous professional development (free internal and external professional trainings)‚Äî Free English classes in the company office‚Äî Free use of the services provided by Namecheap (for non-commercial purposes)‚Äî Quarterly teambuilding activities and company corporate events ‚Äî RDX gym membership ‚Äî Coffee, tea, fruits Responsibilities ‚Äî Creating and implementing tracking documents to meet stated requirements for metadata management, operational data stores and Extract Transform Load environments‚Äî Design conceptual and logical data models and flowcharts‚Äî Assisting with developing database solutions to store and retrieve company information‚Äî Collaborate with internal customers and IT partners, including system architects, software developers, database administrators, design analysts and information modeling experts to determine project requirements and capabilities, and strategize development and implementation timelines‚Äî Performing regular data integrity and quality audits, collating and verifying data from multiple sources ‚Äî Research new technologies, data modeling methods and information management systems todetermine which ones should be incorporated into company data architectures, and develop implementation timelines and milestones Project description Looking for a Business Intelligence Data Specialist.Zone3000 works with Namecheap project (www.namecheap.com).Namecheap was founded in 2000 on the idea that all people deserve value-priced domains delivered through stellar service. Today, Namecheap is a leading ICANN-accredited domain name registrar and web hosting company with over two million customers and ten million domains under management ‚Äî and we‚Äôre just getting started.Our culture is built on the values that we live every day; the way we work, the way we collaborate with our global network of colleagues and the way we relentlessly innovate solutions that meet the emerging needs of our customers.",Business Intelligence Data Specialist@ZONE3000,https://jobs.dou.ua/companies/zone3000/vacancies/132391/, Kharkiv,Business Intelligence Data Specialist,18 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/binariks/,Binariks,"{""Required skills"": [""1+ year of experience working with Python or another object-oriented languageSQL mastery, including techniques for writing efficient code over large datasetsExperience with data preparation for statistical or machine learning modelsAbility to leverage critical data-driven thinking and enthusiasm for translating data into actionable insight to generate consistently accurate and useful analysis and modelsWill be a plus experience running ETLs on a data warehouse"", ""AWS RedshiftWill be a plus experience using Apache SparkAttention to detail and time management delivering high quality work while meeting deadlinesEnglish Intermediate +Knowledge of Agile/SCRUM and iterative development lifecyclesGood problem-solving skillsGood communication skills""], ""We offer"": [""18 working days of paid vacationPaid sick leaves(20 days per year)Flexible work schedulePossibility to share and gain knowledge on regular tech talksCompetitive and rewarding salary based on performance appraisals/knowledge evaluationComfortable office facilitiesChallenging and interesting projectsCorporate events and birthday celebrations""], ""Responsibilities"": [""Gathering requirements, setting priorities and making precise decisions within deadlinesBuild and maintain master databases / data warehousesEffectively present and communicate complex analytical or technical concepts to internal andexternal stakeholders""], ""Project description"": [""Binariks is looking for a highly motivated and skilled Strong junior Data Engineer, who will be working on the project for a strategy consulting group that focuses exclusively on the telecommunications, media and technology (TMT) related sectors.""]}",,"Required skills 1+ year of experience working with Python or another object-oriented languageSQL mastery, including techniques for writing efficient code over large datasetsExperience with data preparation for statistical or machine learning modelsAbility to leverage critical data-driven thinking and enthusiasm for translating data into actionable insight to generate consistently accurate and useful analysis and modelsWill be a plus experience running ETLs on a data warehouse ‚Äî AWS RedshiftWill be a plus experience using Apache SparkAttention to detail and time management delivering high quality work while meeting deadlinesEnglish Intermediate +Knowledge of Agile/SCRUM and iterative development lifecyclesGood problem-solving skillsGood communication skills We offer 18 working days of paid vacationPaid sick leaves(20 days per year)Flexible work schedulePossibility to share and gain knowledge on regular tech talksCompetitive and rewarding salary based on performance appraisals/knowledge evaluationComfortable office facilitiesChallenging and interesting projectsCorporate events and birthday celebrations Responsibilities Gathering requirements, setting priorities and making precise decisions within deadlinesBuild and maintain master databases / data warehousesEffectively present and communicate complex analytical or technical concepts to internal andexternal stakeholders Project description Binariks is looking for a highly motivated and skilled Strong junior Data Engineer, who will be working on the project for a strategy consulting group that focuses exclusively on the telecommunications, media and technology (TMT) related sectors.",Strong junior¬†/ Middle Data Engineer@Binariks,https://jobs.dou.ua/companies/binariks/vacancies/132357/, Lviv,Strong junior¬†/ Middle Data Engineer,18 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/werush/,WeRush,"{""Required skills"": [""3-5 years of relevant experience in data analysis, data visualization and reporting using Tableau(or other viz tools), Excel and SQL;""], ""We offer"": [""Medium-Advanced SQL;""], ""Responsibilities"": [""Experience in Advanced Excel (including Pivot tables, Lookups, basic Macros);""], ""Project description"": [""Knowledge and in marketing function (campaigns, marketing funnel, customer segmentation, etc.) ideally in B2B area (enterprise products);""]}",,"Required skills ‚Äî 3-5 years of relevant experience in data analysis, data visualization and reporting using Tableau(or other viz tools), Excel and SQL; ‚Äî Medium-Advanced SQL; ‚Äî Experience in Advanced Excel (including Pivot tables, Lookups, basic Macros); ‚Äî Knowledge and in marketing function (campaigns, marketing funnel, customer segmentation, etc.) ideally in B2B area (enterprise products); ‚Äî Good project management and multi-tasking skills; ‚Äî Great communication skills and team player. We offer ‚Äî Competitive compensation; ‚Äî Full time job with flexible schedule; ‚Äî Paid sick leave, vacation and holidays; ‚Äî Opportunity of career growth; ‚Äî Free English language courses in our office; ‚Äî Positive corporate environment based on principles of respect, support and mentorship; ‚Äî Fascinating working atmosphere built up by real igaming fans; ‚Äî Team-buildings and corporate events. Do not miss your chance! We look forward to welcoming you to our team! Responsibilities ‚Äî Data manipulation (extracting, cleaning, organizing, processing) using Excel, SQL., Organize data from many sources and ensure data accuracy; ‚Äî Report business metrics, analyzing data and generate insights presenting data visualizations through Tableau effectively; ‚Äî Work on Advanced Excel (including Pivot tables, Lookups); ‚Äî Engage with internal clients, consulting them on all data related issues. Project description WeRush, international company founded in 2017, with offices in Europe, with products focusing on the following markets ‚Äî Sweden, Finland, Norway, Canada, New Zealand, UK showing extensive growth and is gaining on strong popularity among customers.We are looking for an experienced and passionate Business Analyst to join our team! Do not miss your chance! We look forward to welcoming you to our team!",BI¬†(Data) Analyst@WeRush,https://jobs.dou.ua/companies/werush/vacancies/132352/, Kyiv,BI¬†(Data) Analyst,18 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/lohika-systems/,Lohika,{},,"PURPOSE OF THE JOB:As Senior Engineer you will build an ETL solution in marketing domain, which involves data transformation, applying models and loading for further analysis. MAIN TASKS AND RESPONSIBILITIES:‚Ä¢ Design, develop, deliver and operate scalable, high-performance data processing software.‚Ä¢ Work proactively on the system architecture.‚Ä¢ Ensure high quality development standards (unit/integration tests, etc.)‚Ä¢ Collaboration with the product management team to incorporate the needs of our customers. EDUCATION, SKILLS AND EXPERIENCE:‚Ä¢ 5+ years with BigData technologies and tools.‚Ä¢ 3+ years of Python development experience (Python, Pyspark)‚Ä¢ Strong hands-on experience with Spark ‚Ä¢ Handling scalability issues, pipeline/cluster optimization‚Ä¢ Experience with AWS‚Ä¢ Good English (oral & written) and communication skills in general WOULD BE A PLUS:‚Ä¢ Pandas, databricks/koalas‚Ä¢ DevOps experience for Big Data solutions. LOHIKA BENEFITS:‚Ä¢ Friendly and highly professional teams‚Ä¢ Flexible working hours with no overtime‚Ä¢ Regular performance reviews‚Ä¢ Internal training‚Ä¢ Comfortable office facilities (kitchens, gym, sports activities, yoga, lounge rooms, coffee machines, etc.)‚Ä¢ Christmas holidays (31st December ‚Äî7th January) and state holidays‚Ä¢ Fully paid English classes (twice per week) with own English teachers and native speakers‚Ä¢ Premium Medical insurance (medication, massage, and doctor in the office, etc.)‚Ä¢ Paid sick-leaves‚Ä¢ Life insurance‚Ä¢ 20 working days of annual paid vacation‚Ä¢ Incentives (marriage, childbirth)‚Ä¢ Corporate events (corporate parties and sports competitions) And much more! Please send your CV or contact us with more questions!",Senior Big Data Engineer for Retail Management Project@Lohika,https://jobs.dou.ua/companies/lohika-systems/vacancies/132348/, Lviv,Senior Big Data Engineer for Retail Management Project,18 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/fastbet/,Fastbet,{},,"We are the fast-growing product company and now looking for a Sport Mathematician.Our main direction is producing content for betting companies, such as broadcasts, incidents, and derivative content. We conduct tournaments in fast sports disciplines such as table tennis, FIFA, CS: GO, organize video broadcasts, and develop community among sports participants.",Sport Mathematician@Fastbet,https://jobs.dou.ua/companies/fastbet/vacancies/128875/, Kyiv,Sport Mathematician,18 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/star/,Star,{},,"Star is a global consultancy that connects strategy, design, engineering and marketing services into a seamless workflow devised to support our clients every step of the way ‚Äî no matter how long or complex their journey. We anchor everything we do in clear and compelling endgames, which in turn enable our strategists, designers and engineers to create useful, scalable products and solutions. We are 750 strategists, designers, engineers and marketers in 12 locations around the world, and we are here to make every great idea, every great person and every great company shine. That is why we‚Äôre called Star. We‚Äôre looking for an eager learner who wants to constantly improve their data and machine learning skills. You will join our RnD team and will work on various projects from IoT to computer vision and text processing worlds. You will have an opportunity to contribute your knowledge to end-to-end data projects hosted in the cloud. It might be an ideal position for a data scientist who wants to apply and improve their deep learning skills. Qualification and experience‚Äî At least 2 years of commercial experience with Python (Jupyter, pandas, numpy)‚Äî Experience with machine learning (scikit, xgboost, keras, pytorch)‚Äî Experience with natural language processing (nltk, word embeddings)‚Äî Strong mathematical and statistical skills‚Äî Great algorithmic skills‚Äî SQL knowledge‚Äî A master‚Äôs or bachelor‚Äôs degree in computer science or similarWould be a plus‚Äî Participating in Kaggle competitions‚Äî Familiarity with other programming languages (Java, Javascript)‚Äî Experience with ElasticSearch or Solr‚Äî Experience with image processing using deep learning‚Äî Knowledge of BI tools (Tableau, Data Studio, QuickSight) and ETL tools‚Äî Acquaintance with cloud data service (AWS, GCP, Azure) Star offers a competitive and rewarding salary and benefits package as well as an intellectually and creatively stimulating work environment. You will find professional flexibility, new ways of working and unique international travel opportunities.",Data Science Engineer@Star,https://jobs.dou.ua/companies/star/vacancies/132310/, Kyiv,Data Science Engineer,18 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/youscan/,YouScan,"{""Tell me\u00a0about your company"": [""YouScan is looking for Product Manager to join our Data Squad.""], ""Tell me\u00a0about your team"": [""YouScan.io is a social media listening solution that gives businesses the ability to become better by listening to their customers.YouScan was recognized as #1 solution in Visual Analytics in the world in Martech Challenge conducted by independent experts.Our customers are Pepsico, Google, L\u2019Oreal, Nestle and many other well-known brands.We\u2019re a Ukrainian product company with a strong internal culture.Each year we double our social media coverage, so we\u2019re always looking for a professional to join our team in the Kyiv office.""], ""What will be\u00a0my\u00a0responsibilities?"": [""You may have heard the joke that data is the new oil, believe it or not, but we are fueling the entire company:)""], ""Sounds good, what are your requirements?"": [""You will be a part of the Data Squad, responsible for collecting, structuring, and storing huge amounts of data, ~400M new mentions in social media daily. That\u2019s one of our four product squads, and it consists of 3 back-end engineers and 1 tech support at this moment.""], ""Anything else?"": [""Gather insights within the company (customer success, sales, CEO, marketing, etc.) to understand priorities, problems, and opportunities that affect data coverage.""]}",,"YouScan is looking for Product Manager to join our Data Squad. Tell me about your company YouScan.io is a social media listening solution that gives businesses the ability to become better by listening to their customers.YouScan was recognized as #1 solution in Visual Analytics in the world in Martech Challenge conducted by independent experts.Our customers are Pepsico, Google, L‚ÄôOreal, Nestle and many other well-known brands.We‚Äôre a Ukrainian product company with a strong internal culture.Each year we double our social media coverage, so we‚Äôre always looking for a professional to join our team in the Kyiv office. Tell me about your team You may have heard the joke that data is the new oil, believe it or not, but we are fueling the entire company:) You will be a part of the Data Squad, responsible for collecting, structuring, and storing huge amounts of data, ~400M new mentions in social media daily. That‚Äôs one of our four product squads, and it consists of 3 back-end engineers and 1 tech support at this moment. What will be my responsibilities? ‚Äî Gather insights within the company (customer success, sales, CEO, marketing, etc.) to understand priorities, problems, and opportunities that affect data coverage. ‚Äî Sell the strong sides of our data coverage within the company and explain the weak ones. ‚Äî Use analytics to measure the impact of our data. ‚Äî Analyze customer feedback and tons of raw ideas to come up with structured projects. ‚Äî Talk with potential data providers, select perspective ones. ‚Äî Develop the long term expansion plan of data coverage. ‚Äî Evaluate & compare competitors‚Äô data coverage. Sounds good, what are your requirements? We expect you to have experience working as a Product Manager at Product Company for at least 2 years, preferably B2B SaaS. We‚Äôre looking for a real teammate ‚Äî you share your vision, communicate, bring in your own ideas. The person with analytical skills, the ability to manage tons of details easily, and think fast. You also listen and hear other points of view, can look with another person‚Äôs eyes. You have a positive mindset, passion for software development, and get the job done. Anything else?",Product Manager¬†| Data Team@YouScan,https://jobs.dou.ua/companies/youscan/vacancies/132236/, Kyiv,Product Manager¬†| Data Team,17 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/brightgrove/,Brightgrove,"{""Required skills"": [""Understanding of what it takes to design, build, and maintain successful data platforms, services, and products"", ""Hands-on experience in Software Engineering (e.g. programming languages especially Python, big data technologies)"", ""Practical skills Data modeling and mining, building Data pipelines"", ""Experience with cloud-based services, preferable AWS (Lambda, RDS, EC2, S3, Glue, Athena, SQS, CodePipeline, etc.)"", ""Excellent communication skills to collaborate with business stakeholders, fellow engineers, and management staff"", ""Upper-Intermediate English is a must""], ""As a plus"": [""Practical experience with Docker, Kubernetes, Airflow and CloudFormation/Terraform (or willingness to quickly learn and work with them)""], ""We offer"": [""Very warm and friendly working environment"", ""Professional and career growth"", ""No corporate BS"", ""we\u2019re moving too fast for that"", ""Competitive compensation depending on experience and skills"", ""Opportunities to travel international and between our offices"", ""Working with the latest technologies"", ""Excellent opportunities to work with remarkable teams from all over the world"", ""Flexible working hours"", ""as long as you get the work done"", ""Comfortable and cozy office in the city center"", ""Awesome corporate events""], ""Responsibilities"": [""At Brightgrove, developers are integrated, involved, and essential to each part of our company. As with wine, great software is created by great people.""], ""Project description"": [""You\u2019ll use state-of-art technologies to develop an innovative data landscape for driving the business with up-to-date and real-time analytical solutions coupled.""]}",,"Required skills ‚Ä¢ Understanding of what it takes to design, build, and maintain successful data platforms, services, and products‚Ä¢ Hands-on experience in Software Engineering (e.g. programming languages especially Python, big data technologies)‚Ä¢ Practical skills Data modeling and mining, building Data pipelines‚Ä¢ Experience with cloud-based services, preferable AWS (Lambda, RDS, EC2, S3, Glue, Athena, SQS, CodePipeline, etc.)‚Ä¢ Excellent communication skills to collaborate with business stakeholders, fellow engineers, and management staff‚Ä¢ Upper-Intermediate English is a must As a plus ‚Ä¢ Practical experience with Docker, Kubernetes, Airflow and CloudFormation/Terraform (or willingness to quickly learn and work with them) We offer ‚Ä¢ Very warm and friendly working environment‚Ä¢ Professional and career growth‚Ä¢ No corporate BS ‚Äî we‚Äôre moving too fast for that‚Ä¢ Competitive compensation depending on experience and skills‚Ä¢ Opportunities to travel international and between our offices‚Ä¢ Working with the latest technologies‚Ä¢ Excellent opportunities to work with remarkable teams from all over the world‚Ä¢ Flexible working hours ‚Äî as long as you get the work done‚Ä¢ Comfortable and cozy office in the city center‚Ä¢ Awesome corporate events At Brightgrove, developers are integrated, involved, and essential to each part of our company. As with wine, great software is created by great people. Responsibilities You‚Äôll use state-of-art technologies to develop an innovative data landscape for driving the business with up-to-date and real-time analytical solutions coupled. ‚Ä¢ Create new data services in collaboration with product owners, peers, and other stakeholders‚Ä¢ Be up to speed in our AWS cloud-based environment, get things done with the help of technologies like Glue, Athena, Spark, Kinesis, Lambda and other AWS services‚Ä¢ Take part in the building of data architecture for ingestion, processing, and transferring of data for large-scale applications‚Ä¢ Research and discover new methods to acquire data, and new applications for existing data‚Ä¢ Work with other data team members including data architects, data analysts, and data scientists, product managers, and business stakeholders Project description About the Client:Our partner is a well-established, leading hospitality solutions brand from Germany with a worldwide network. Being one of the global market giants in the business of the travel industry, the company provides various top-grade hotel management portals. Their solutions simplify the booking and management processes equally for corporations, travel managers, and travelers. About the Project:The project‚Äôs main goal is to refactor an existing middle-tier service layer towards a modern cloud-based solution. You‚Äôre going to work in a distributed team alongside world-class software architects, developers, and subject matter experts. The team is going to shape and implement the technological renovation of one of the top ecosystems in the domain of private and corporate travel and hospitality. About the Team:Be among the first who are going to join our modern development center in Kyiv!Our globally distributed team is responsible for turning data into insights, services, and products for one of the leaders in the hospitality and travel domain. Around 50 Data Specialists (Analysts, Scientists, and Engineers) work together to create solutions that ultimately serve millions of travelers worldwide.We leverage the rich data landscape (spend data, booking data, rate data, web analytics data, etc.) to enable business stakeholders and customers with advanced analytical solutions, and provide information on travelers‚Äô behavior, and lodging market. We also bring state-of-the-art algorithms in production to enhance Search & Booking experience (with personalized recommendations) and optimize the corporate hotel portfolio to meet both the requirements of the customers and the needs of the travelers.",Data Engineer for Global Hotel Solution Provider@Brightgrove,https://jobs.dou.ua/companies/brightgrove/vacancies/132233/, Kyiv,Data Engineer for Global Hotel Solution Provider,17 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/eleks/,ELEKS,"{""Required skills"": [""At least 5+ years of experience in Data EngineeringSolid Experience with MySQL, Gemfire, IBM DB2, CosmosDB,Good to know CI/CD, Jenkins, Concourse, JavaExperience in building ETLs""], ""As a plus"": [""Understanding of TDD and extreme programmingKnowledge of cloud-based technologiesPivotal tech stack knowledge""], ""We offer"": [""Close cooperation with a customerChallenging tasksCompetence developmentAbility to influence project technologiesTeam of professionalsDynamic environment with low level of bureaucracy""], ""Responsibilities"": [""Develop solutions and algorithms according to technical specifications or other requirements documentation; use standard algorithms in the applicable casesWrite program code according to the defined application architectureStructure and format the source code, comment and mark up the code, as well as name variables, functions, classes, data structures, and files according to the company conventions and industry best practicesModify existing code and verify its functioning. Analyze code compliance with readability and performance standardsUse version control systems to track code optimization progress and to merge or split program code entities. Commit changes according to version control rulesPerform analysis, verification, and debugging of the software code at the level of application unitsDetect defects, apply debugging methods and techniques, correctly interpret bug reports, as well as apply modern compilers, debuggers, and program code optimizersAble to develop procedures for testing code availability, collecting diagnostic data, generating test data sets with necessary characteristics, identifying required software characteristics etc.Reproduce defects logged in an issue tracking system, identify defect causes, and then modify code to eliminate defectsAble to develop and document program interfaces, software module and component assembling procedures, software deployment and update procedures as well as data migration and transformation (conversion) proceduresEstimate and set up task completion terms independentlyEvaluate and coordinate task deadlines with Technical Leader or Project ManagerMay have valid competence-related certificationsParticipate, both as a trainer or a trainee, in various learning programs outside the major project""], ""Project description"": [""We are working together with one of the biggest health insurance company in the US with HQ in Chicago. Project portfolio contains various projects connected to writing, combining and integration of APIs from partners, clients etc. Mostly all of them are built using a microservices architecture. Some of the projects developed using the XP methodology with all necessary ceremonies as TDD, pair programming, lot of communication and frequent releases.""]}",,"Required skills At least 5+ years of experience in Data EngineeringSolid Experience with MySQL, Gemfire, IBM DB2, CosmosDB,Good to know CI/CD, Jenkins, Concourse, JavaExperience in building ETLs As a plus Understanding of TDD and extreme programmingKnowledge of cloud-based technologiesPivotal tech stack knowledge We offer Close cooperation with a customerChallenging tasksCompetence developmentAbility to influence project technologiesTeam of professionalsDynamic environment with low level of bureaucracy Responsibilities Develop solutions and algorithms according to technical specifications or other requirements documentation; use standard algorithms in the applicable casesWrite program code according to the defined application architectureStructure and format the source code, comment and mark up the code, as well as name variables, functions, classes, data structures, and files according to the company conventions and industry best practicesModify existing code and verify its functioning. Analyze code compliance with readability and performance standardsUse version control systems to track code optimization progress and to merge or split program code entities. Commit changes according to version control rulesPerform analysis, verification, and debugging of the software code at the level of application unitsDetect defects, apply debugging methods and techniques, correctly interpret bug reports, as well as apply modern compilers, debuggers, and program code optimizersAble to develop procedures for testing code availability, collecting diagnostic data, generating test data sets with necessary characteristics, identifying required software characteristics etc.Reproduce defects logged in an issue tracking system, identify defect causes, and then modify code to eliminate defectsAble to develop and document program interfaces, software module and component assembling procedures, software deployment and update procedures as well as data migration and transformation (conversion) proceduresEstimate and set up task completion terms independentlyEvaluate and coordinate task deadlines with Technical Leader or Project ManagerMay have valid competence-related certificationsParticipate, both as a trainer or a trainee, in various learning programs outside the major project Project description We are working together with one of the biggest health insurance company in the US with HQ in Chicago. Project portfolio contains various projects connected to writing, combining and integration of APIs from partners, clients etc. Mostly all of them are built using a microservices architecture. Some of the projects developed using the XP methodology with all necessary ceremonies as TDD, pair programming, lot of communication and frequent releases.",Data Engineer@ELEKS,https://jobs.dou.ua/companies/eleks/vacancies/132167/," relocation, remote",Data Engineer,17 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/kyivstar/,Kyivstar,"{""Required skills"": [""7+ years of success in consultative/complex technical sales and deployment projects, architecture, design, implementation, and/or support of highly distributed applications required"", ""5+ years experience in Data Platforms (RDBMS, noSQL), Analytics (SQL including postgre, Azure SQL, Power BI), Artificial Intelligence/Machine Learning (Spark, Azure ML, Azure Databricks)"", ""Enterprise with cloud and hybrid infrastructures, preparing complex architecture designs, leading database migrations, and technology management required"", ""Expertise in data estate workloads like HDInsight, Hadoop, Cloudera, Spark"", ""Strong executive presence including communication and presentation skills with a high degree of comfort to large and small technical audiences"", ""Strong partner relationship management skills"", ""Data Security knowledge, data masking and data encryption""], ""We offer"": [""A unique experience of working the most customers beloved and largest mobile operator in Ukraine."", ""Real opportunity to ship digital products to millions of customers"", ""To contribute into building the biggest analytical cloud environment in Ukraine"", ""To create Big Data/AI products, changing the whole industry and influencing Ukraine"", ""To be involved in real Big Data projects with Petabytes of data and Billions of events daily processed in Real-time."", ""A competitive salary."", ""Great possibilities for professional development and career growth."", ""Medical insurance."", ""Life insurance"", ""Friendly & Collaborative Environment""], ""Responsibilities"": [""Understand customers\u2019 overall data estate, IT and business priorities and success measures to design implementation architectures and solutions."", ""Apply technical knowledge to architect solutions that meet business and IT needs, create Data Platform, and ensure long term technical viability of new deployments, infusing key analytics and AI technologies"", ""Ensure that solution exhibits high levels of performance, security, scalability, maintainability, appropriate reusability and reliability upon deployment"", ""Collaborate with other Cloud Solution Architects in developing complex end-to-end Enterprise solutions on Microsoft Azure platform, Maintain technical skills and knowledge, keeping up to date with market trends and competitive insights; collaborate and share with the technical community while educate customers on Azure platform""], ""Project description"": [""We are innovative Cloud Big Data & Artificial Intelligence Center of Excellence in Ukraine, and main target for us is to design, deliver and implement the cloud-based data management and BI platform across the Business using Big Data, IoT, Machine Learning, and AI Cloud technologies. We help our Clients to get deep understanding of Cloud benefits by providing the services like: client infrastructure discovery, preparing Future proof Architecture and Cloud transformation program, building cloud-based data Platform and High-grade data visualizations. We use the best worldwide practice to be a fully compliance with Data Security and Customer\u2019s legal requirements.""]}",,"Required skills ‚Äî 7+ years of success in consultative/complex technical sales and deployment projects, architecture, design, implementation, and/or support of highly distributed applications required‚Äî 5+ years experience in Data Platforms (RDBMS, noSQL), Analytics (SQL including postgre, Azure SQL, Power BI), Artificial Intelligence/Machine Learning (Spark, Azure ML, Azure Databricks)‚Äî Enterprise with cloud and hybrid infrastructures, preparing complex architecture designs, leading database migrations, and technology management required‚Äî Expertise in data estate workloads like HDInsight, Hadoop, Cloudera, Spark‚Äî Strong executive presence including communication and presentation skills with a high degree of comfort to large and small technical audiences‚Äî Strong partner relationship management skills‚Äî Data Security knowledge, data masking and data encryption We offer ‚Äî A unique experience of working the most customers beloved and largest mobile operator in Ukraine.‚Äî Real opportunity to ship digital products to millions of customers‚Äî To contribute into building the biggest analytical cloud environment in Ukraine‚Äî To create Big Data/AI products, changing the whole industry and influencing Ukraine‚Äî To be involved in real Big Data projects with Petabytes of data and Billions of events daily processed in Real-time. ‚Äî A competitive salary.‚Äî Great possibilities for professional development and career growth.‚Äî Medical insurance.‚Äî Life insurance‚Äî Friendly & Collaborative Environment Responsibilities ‚Äî Understand customers‚Äô overall data estate, IT and business priorities and success measures to design implementation architectures and solutions.‚Äî Apply technical knowledge to architect solutions that meet business and IT needs, create Data Platform, and ensure long term technical viability of new deployments, infusing key analytics and AI technologies‚Äî Ensure that solution exhibits high levels of performance, security, scalability, maintainability, appropriate reusability and reliability upon deployment‚Äî Collaborate with other Cloud Solution Architects in developing complex end-to-end Enterprise solutions on Microsoft Azure platform, Maintain technical skills and knowledge, keeping up to date with market trends and competitive insights; collaborate and share with the technical community while educate customers on Azure platform Project description We are innovative Cloud Big Data & Artificial Intelligence Center of Excellence in Ukraine, and main target for us is to design, deliver and implement the cloud-based data management and BI platform across the Business using Big Data, IoT, Machine Learning, and AI Cloud technologies. We help our Clients to get deep understanding of Cloud benefits by providing the services like: client infrastructure discovery, preparing Future proof Architecture and Cloud transformation program, building cloud-based data Platform and High-grade data visualizations. We use the best worldwide practice to be a fully compliance with Data Security and Customer‚Äôs legal requirements. Our clients are more than 100 biggest Ukrainian Companies, and our team successfully delivered over 50 analytical products every month. Joining our team you will get a unique experience to deep dive and drive development team within high priority customer‚Äôs BigData and digital transformation Projects using Microsoft Azure Services in collaboration with the best Microsoft team players",Cloud Technology Lead (BigData)@Kyivstar,https://jobs.dou.ua/companies/kyivstar/vacancies/132109/, Kyiv,Cloud Technology Lead (BigData),16 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/kyivstar/,Kyivstar,"{""Required skills"": [""3+ years of programming experience or relevant ETL experience"", ""Data formats knowledge and the differences between them"", ""Solid experience with Hadoop stack"", ""Experience with RDBMS and/or NoSQL"", ""Experience with Kafka"", ""Understanding of processes design and development for data modeling, processing and analysis"", ""Experience with Java and/or Scala and/or python"", ""Knowledge of version control system: git or bitbucket"", ""Background in test driven development, automated testing and other software engineering best practices (e.g., performance, security, BDD, etc.)"", ""Docker/Kubernetes paradigm understanding""], ""We offer"": [""- A unique experience of working the most customers beloved and largest mobile operator in Ukraine- Real opportunity to ship digital products to millions of customers- A competitive salary- Remote day per week- Great possibilities for professional development and career growth- Medical insurance- Life insurance- Friendly & Collaborative Environment""], ""Responsibilities"": [""Developing ETL flows based on Hadoop/BigData stack technology: Apache Nifi, Hive, Spark, Kafka"", ""Troubleshooting and performance optimization for data processing flows, data models""], ""Project description"": [""Kyivstar is looking for a BigData Engineer to develop a platform for collection, processing and storing petabytes data. Built with open source technologies, the systems handle billions of records daily, generating terabytes of data.""]}",,"Required skills ‚Äî 3+ years of programming experience or relevant ETL experience‚Äî Data formats knowledge and the differences between them‚Äî Solid experience with Hadoop stack‚Äî Experience with RDBMS and/or NoSQL ‚Äî Experience with Kafka‚Äî Understanding of processes design and development for data modeling, processing and analysis‚Äî Experience with Java and/or Scala and/or python‚Äî Knowledge of version control system: git or bitbucket‚Äî Background in test driven development, automated testing and other software engineering best practices (e.g., performance, security, BDD, etc.)‚Äî Docker/Kubernetes paradigm understanding We offer - A unique experience of working the most customers beloved and largest mobile operator in Ukraine- Real opportunity to ship digital products to millions of customers- A competitive salary- Remote day per week- Great possibilities for professional development and career growth- Medical insurance- Life insurance- Friendly & Collaborative Environment Responsibilities ‚Äî Developing ETL flows based on Hadoop/BigData stack technology: Apache Nifi, Hive, Spark, Kafka ‚Äî Troubleshooting and performance optimization for data processing flows, data models Project description Kyivstar is looking for a BigData Engineer to develop a platform for collection, processing and storing petabytes data. Built with open source technologies, the systems handle billions of records daily, generating terabytes of data.",BigData ETL Engineer@Kyivstar,https://jobs.dou.ua/companies/kyivstar/vacancies/116530/, Kyiv,BigData ETL Engineer,16 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/softserve/,SoftServe,{},,"WE ARE Our company provides purpose-built products for IT professionals, MSPs, and DevOps pros. We offer value-driven products and tools that solve a broad range of IT management challenges‚Äîwhether those challenges are related to networks, servers, applications, storage, virtualization, cloud, or development operations.About the project Report Management tool helps IT providers show their value to customers. It contains over 45 premade reports and gives the possibility to create their own custom reports for specific clients. YOU ARE ‚Ä¢ Possessing more than 2-year experience in designing and developing commercial software as a DB Engineer‚Ä¢ Knowledgeable about T-SQL and SQL Server 2012 (and above)‚Ä¢ Experienced with Microsoft Windows Server and IIS, Microsoft BI Suite ‚Äî SSRS and SSIS, Relational database design, development and Data Warehousing experience, InstallShield‚Ä¢ Proficient in Microsoft MVC and Microsoft WCF‚Ä¢ The person having programming experience with C# and ASP.NET Web Forms‚Ä¢ Eager for constant learning and improving‚Ä¢ A self-driven person helping the team meet project delivery milestones‚Ä¢ A team player with excellent communications skills, a good attitude, and a friendly personality‚Ä¢ Demonstrating fluent spoken/written English as it is used on a daily basis YOU WANT TO WORK WITH ‚Ä¢ Development and releases of service packs for the Report Manager product‚Ä¢ Addressing customer tickets and resolving product issues that are discovered‚Ä¢ Updating the product as new versions of Microsoft components/versions/operating systems are released‚Ä¢ Verifying compatibility with updates and new features of another product‚Ä¢ Increasing automation test coverage‚Ä¢ Customer issue analysis trends and creation of tools/utilities for support to avoid/re-mediate customer tickets TOGETHER WE WILL ‚Ä¢ Grow technically‚Ä¢ Provide our client with a high level of support‚Ä¢ Advance product by suggesting our improvements‚Ä¢ Add value to the business",Middle/Senior DW/BI Engineer (ID¬†56150)@SoftServe,https://jobs.dou.ua/companies/softserve/vacancies/132095/, Lviv,Middle/Senior DW/BI Engineer (ID¬†56150),16 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/data-science-ua/,Data Science UA,"{""Required skills"": [""2-3 years of Javascript experience"", ""1-2 years with React Native"", ""ES6"", ""React/Redux"", ""CSS"", ""HTTP API, REST"", ""Google Maps+"", ""Git""], ""We offer"": [""work remotely"", ""small team"", ""interesting project""], ""Responsibilities"": [""writing a mobile application using React Native,"", ""developemnt task on iOS + Android"", ""Using of Google Maps API""], ""Project description"": [""We are developing a new blockchain technology platform which is used by business and government for creating reusable blocks of solutions.""]}",,"Required skills ‚Äî 2-3 years of Javascript experience‚Äî 1-2 years with React Native‚Äî ES6‚Äî React/Redux‚Äî CSS‚Äî HTTP API, REST‚Äî Google Maps+‚Äî Git We offer ‚Äî work remotely‚Äî small team‚Äî interesting project Responsibilities ‚Äî writing a mobile application using React Native,‚Äî developemnt task on iOS + Android‚Äî Using of Google Maps API Project description We are developing a new blockchain technology platform which is used by business and government for creating reusable blocks of solutions.",React Native developer@Data Science UA,https://jobs.dou.ua/companies/data-science-ua/vacancies/132083/," Kyiv, remote",React Native developer,16 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/data-science-ua/,Data Science UA,"{""Required skills"": [""Solid mathematical background."", ""5+ years of software engineering experience."", ""Experience researching and developing 3d algorithms."", ""3+ years of experience with Python."", ""Practical experience with at least one deep learning frameworks (Tensorflow, PyTorch)."", ""Practical experience in developing classical optimization methods solutions.""], ""As a plus"": [""Practical experience with differentiable rendering or multiview problems is a big plus.""], ""We offer"": [""Stock option plan;"", ""Investment in your growth and self-development;"", ""Competitive compensation;"", ""20 working days of paid vacations and paid sick leaves;"", ""10 remote days each month and one remote month from another country per year;"", ""Foreign language classes inhouse and communication with native speakers;"", ""Online fitness with a corporate trainer;"", ""Modern and conveniently located offices with good working conditions;"", ""Corporate, social and cultural events.""], ""Responsibilities"": [""Entire pipeline improvement that corresponds to human 3d models, from raw scan to the parametric space(registration, rigging, etc.)"", ""Analyzing existing features, reveal the hidden problems, etc."", ""Proposing new ideas.""], ""Project description"": [""Our partner is on a mission to bring deeper levels of personalization to the consumer shopping experience by turning body data into business intelligence and remove the challenge of fit communication between consumers and the places they love to shop.""]}",,"Required skills ‚Äî Solid mathematical background.‚Äî 5+ years of software engineering experience.‚Äî Experience researching and developing 3d algorithms.‚Äî 3+ years of experience with Python.‚Äî Practical experience with at least one deep learning frameworks (Tensorflow, PyTorch).‚Äî Practical experience in developing classical optimization methods solutions. As a plus ‚Äî Practical experience with differentiable rendering or multiview problems is a big plus. We offer ‚Äî Stock option plan;‚Äî Investment in your growth and self-development;‚Äî Competitive compensation;‚Äî 20 working days of paid vacations and paid sick leaves;‚Äî 10 remote days each month and one remote month from another country per year;‚Äî Foreign language classes inhouse and communication with native speakers;‚Äî Online fitness with a corporate trainer;‚Äî Modern and conveniently located offices with good working conditions;‚Äî Corporate, social and cultural events. Responsibilities ‚Äî Entire pipeline improvement that corresponds to human 3d models, from raw scan to the parametric space(registration, rigging, etc.)‚Äî Analyzing existing features, reveal the hidden problems, etc.‚Äî Proposing new ideas. Project description Our partner is on a mission to bring deeper levels of personalization to the consumer shopping experience by turning body data into business intelligence and remove the challenge of fit communication between consumers and the places they love to shop. We are result-oriented enthusiasts who work tirelessly to build technology that has the potential to transform the fashion industry and how we shop online. We combine expertise and experience and work together to deliver outstanding results for our clients. This is a great opportunity for you to shape the growth, development and culture of an exciting and very fast-growing company in the retail market. As a 3D Engineer, you will work on our 3d models, from raw scan to the parametric space(registration, rigging, etc.). If you want to be a part of the company that will change the online shopping market, we‚Äôd love to meet you!",Senior ML/3D engineer@Data Science UA,https://jobs.dou.ua/companies/data-science-ua/vacancies/132078/, remote,Senior ML/3D engineer,16 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/casafari-real-estate-data/,CASAFARI - real estate data,"{""Required skills"": [""Hello!In these uncertain times, CASAFARI is on top and now we are looking for a Senior Data Science / Machine Learning Engineer to extend our team focused on Real Estate industry-leading realtime decision-making tools and automation.""], ""As a plus"": [""For this role, you need to be a guru in:""], ""We offer"": [""the mathematical statistics,"", ""think critically,"", ""be able to express thoughts, present ideas, have a constructive dialogue,"", ""be able to exist in a fast-paced environment,"", ""have a high level of collaboration and proactivity"", ""we are a startup,"", ""have conversational English (communication on ordinary and technical topics).""], ""Responsibilities"": [""Our technology stack and requirements:""], ""Project description"": [""Python 3,Pandas,Scikit-learn,XGBoost,PyTorch,MySQL \u0438 MongoDB.""]}",,"Required skills Hello!In these uncertain times, CASAFARI is on top and now we are looking for a Senior Data Science / Machine Learning Engineer to extend our team focused on Real Estate industry-leading realtime decision-making tools and automation. For this role, you need to be a guru in: ‚Äî the mathematical statistics,‚Äî think critically,‚Äî be able to express thoughts, present ideas, have a constructive dialogue,‚Äî be able to exist in a fast-paced environment,‚Äî have a high level of collaboration and proactivity ‚Äî we are a startup,‚Äî have conversational English (communication on ordinary and technical topics). Our technology stack and requirements: Python 3,Pandas,Scikit-learn,XGBoost,PyTorch,MySQL –∏ MongoDB. As a plus If you haven‚Äôt worked with something or worked with analogs (for example, TensorFlow and Keras instead of PyTorch), do not hesitate to apply for a job. We offer The tasks we solve relate to NLP, computer vision, clustering, classification, deep learning as well: .Daily collection and analysis of data (crawling and parsing) from several thousand sources,.Anomalies during the collection and primary processing of data, type determination (apartment or house).Identify the characteristics of the house from unstructured text,.Search for a real estate duplicates with different input data (prices, area, type, pictures, etc.),.Property condition (new, old),.Image classification (kitchen, bedroom, bathroom, pool),.Market analysis algorithm (market analytics),.Models for real estate valuation and selection of analogs,.Building and optimizing an investment portfolio. Responsibilities What will need to be done is to develop ongoing projects (to create new features, code refactoring) and improve data processes (ETL).We are open to suggestions ‚Äî to processes and approaches in development and the team as a whole, infrastructure, tools. Project description CASAFARI is young but growing fast, which means you‚Äôll get to help shape our future and revolutionize the real estate industry. We‚Äôre looking for a passionate, proactive and pragmatic Machine Learning Engineer to join our team of 14+ nationalities. You‚Äôll grow with us as we strive to fulfill our mission and vision: to bring transparency to the real estate market with the cleanest and most complete real estate database in the world. Our end products are a metasearch like Trivago or Skyscanner, a daily market data feed like Bloomberg Terminal, and we are currently building property market analytics with the most complete database updated in real time.",Senior Machine Learning Engineer@CASAFARI - real estate data,https://jobs.dou.ua/companies/casafari-real-estate-data/vacancies/132075/," Kharkiv, Lisbon (Portugal), remote",Senior Machine Learning Engineer,16 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/genesis-technology-partners/,Genesis,{},,"Genesis is one of the largest IT companies in Ukraine with more than 1500 people in 9 countries, who create products for 200 million users monthly. We are the most high-loaded company in the country and one of the largest partner of Facebook, Google, Snapchat and Apple in the CEE region. Our team is one of the best high-tech teams in Eastern Europe. Genesis was recognised by DOU.UA as the Best IT Employer in Ukraine in the category 800 ‚Äî 1500 employees over the last years. We received very high ratings across all major evaluation categories such as professional growth, compensation, working conditions, communication with management and colleagues, etc. Cpamatica ‚Äî is a global partner network, with operations in the USA, Europe, and Latam. Being a part of Genesis holding we create unique conditions for work. With proven expertise in online marketing, we know how to build a high technology company. Having a lot of freedom you are able to find out what you are good at and have opportunities to explore it. In other words, there‚Äôs a ton of room for your professional growth! What We‚Äôll Expect From You:‚Ä¢ 1-3 years of experience doing quantitative product/marketing analysis;‚Ä¢ 1-3 years of initiating and driving projects to completion with minimal guidance;‚Ä¢ Highly skilled in data visualization tools such as Tableau;‚Ä¢ Experience using product analytics tools like Grafana, Google Analytics;‚Ä¢ Proficiency in using SQL;‚Ä¢ Proficiency in using Python. Who You Are:‚Ä¢ You have a passion for creating and supporting new great products;‚Ä¢ You are highly motivated and hard-working as well as curious and creative at problem-solving;‚Ä¢ You have strong verbal and written communication skills;‚Ä¢ You thrive on collaboration, working side by side with people of all backgrounds and disciplines;‚Ä¢ Analytical mindset; ability to structure information and dive deeper when it is needed. What You‚Äôll Do:‚Ä¢ Passion for digital advertising;‚Ä¢ Be organised and self-motivated;‚Ä¢ Strong analytical skills and ability to make data-driven decisions;‚Ä¢ Entrepreneurial Mindset;‚Ä¢ English ‚Äî Upper Intermediate or higher. Genesis is a unique place for the development and growth with:‚Ä¢ Expertise in the development of high-loaded products in international markets;‚Ä¢ Unique opportunities for learning including various training and seminars within the company, access to a valuable and extensive book library, English courses and participation in key IT industry events worldwide;‚Ä¢ Perfect working conditions: an excellent office in a 5 minutes‚Äô walk from Taras Shevchenko metro station, free food and drinks in the office, medical insurance. The company has professional coaches in football, basketball and running and supports various sports initiatives (swim races, marathons, etc).",Middle/Senior Data Analyst@Genesis,https://jobs.dou.ua/companies/genesis-technology-partners/vacancies/126001/, Kyiv,Middle/Senior Data Analyst,16 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/zoral/,Zoral,"{""Required skills"": [""Technical skills & hands on experience:- Structured/semi-structured/unstructured Data Analysis, Data Mining and Machine Learning approaches and tools;- Python;- Prolog;- Cloud platforms: AWS, GCP;- Containers: Docker, Kubernetes;- OOP/OOD;- Distributed secure systems architecture & design patterns and best practices.""], ""We offer"": [""Software development skills:- Familiar with Agile software development methodologies (KANBAN, SCRUM);- Git;- JIRA.""], ""Responsibilities"": [""Other skills required:- Good spoken and written English;- Creative;- Proactive;- Self-sufficient: able to work and deliver with the minimal support required;- Leadership.""], ""Project description"": [""Attractive salary and good opportunities for career and personal development"", ""Compensation package"", ""Good opportunities for career and personal development"", ""Office in a picturesque place""]}",,"Required skills Technical skills & hands on experience:- Structured/semi-structured/unstructured Data Analysis, Data Mining and Machine Learning approaches and tools;- Python;- Prolog;- Cloud platforms: AWS, GCP;- Containers: Docker, Kubernetes;- OOP/OOD;- Distributed secure systems architecture & design patterns and best practices. Software development skills:- Familiar with Agile software development methodologies (KANBAN, SCRUM);- Git;- JIRA. Other skills required:- Good spoken and written English;- Creative;- Proactive;- Self-sufficient: able to work and deliver with the minimal support required;- Leadership. We offer ‚Ä¢ Attractive salary and good opportunities for career and personal development‚Ä¢ Compensation package‚Ä¢ Good opportunities for career and personal development‚Ä¢ Office in a picturesque place Responsibilities ‚Ä¢ Capture key business requirements in communication & cooperation with the key business stakeholders;‚Ä¢ Analyze of and research on detailed requirement, solution architecture & design options;‚Ä¢ Develop of PoC (proof of concept) and demo prototypes to validate the requirements and design;‚Ä¢ Develop production-ready solutions based on the requirements captured and design created, in close cooperation with or as part of the development project team, which may include (depending on the project):o Consultancy on or ownership of the solution design and architectureo Detailed design and development of the key solution componentso Oversee of development of the solution componentso Participation in the team effort on the solution‚Äôs CI/CD, environments, development & delivery process.‚Ä¢ Assist the development team with improving and enhancing the solution, based on the key stakeholders‚Äô requirements. Project description Zoral specialize in advanced software fields such as BI, Data Mining, Artificial Intelligence, Machine Learning (AI/ML), High Speed Computing, Cloud Computing, BIG Data Predictive Analytics, Unstructured Data processing, Finance, Risk Management and Security.We are searching for a professional and courteous Client Services Manager with great people skills to help our customers. The Client Services Manager will be tasked with maintaining current clients‚Äô needs, keeping accurate correspondence records, meeting regularly with management, updating client details. Your dedication to the needs of our clients will encourage client loyalty through positive client-to-business engagement.To ensure success in this role, the ideal candidate should demonstrate excellent active listening and communication skills, good personal presentation, politeness and tact, and be able to function in a high pressure environment. The Client Services Manager should provide timely solutions to client‚Äôs problems, build sustainable and continuous relationships with clients, show initiative and drive when dealing with client requests.",Intelligent Software Solutions Engineer/Lead@Zoral,https://jobs.dou.ua/companies/zoral/vacancies/132055/, Kyiv,Intelligent Software Solutions Engineer/Lead,16 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/requestum/,Requestum,"{""Required skills"": [""Requestum team is looking for a talented data scientist who is ready to accept challenges in solving interesting and nontrivial problems!We need a self-confident specialist for upcoming projects/outsourcing/support and expedites.""], ""As a plus"": [""Required experience with technologies"", ""Experience as Data Science (Computer Vision) developer"", ""at least 2 years"", ""Experience with Python and C++"", ""Experience with the next libraries: OpenCV, NumPy, SciPy, Scikit-learn, TensorFlow, Keras, or PyTorch.""], ""We offer"": [""Additional skills"", ""Skilled in analytical geometry"", ""Base knowledge in machine learning."", ""Skills in problem analysis and decomposition."", ""Ability to develop unusual algorithms to solve a problem."", ""Base knowledge in statistics, probability theory, numerical methods, discrete math."", ""Ability to work as a team member"", ""Technical English""]}",,"Required skills Requestum team is looking for a talented data scientist who is ready to accept challenges in solving interesting and nontrivial problems!We need a self-confident specialist for upcoming projects/outsourcing/support and expedites. Required experience with technologies‚Ä¢ Experience as Data Science (Computer Vision) developer ‚Äî at least 2 years‚Ä¢ Experience with Python and C++‚Ä¢ Experience with the next libraries: OpenCV, NumPy, SciPy, Scikit-learn, TensorFlow, Keras, or PyTorch. Additional skills‚Ä¢ Skilled in analytical geometry‚Ä¢ Base knowledge in machine learning.‚Ä¢ Skills in problem analysis and decomposition.‚Ä¢ Ability to develop unusual algorithms to solve a problem.‚Ä¢ Base knowledge in statistics, probability theory, numerical methods, discrete math.‚Ä¢ Ability to work as a team member‚Ä¢ Technical English As a plus Nice to have:‚Ä¢ Training and model integration for problem-solving experience (Object Detection, Image Segmentation, Instance Segmentation, Image Classification.)‚Ä¢ Experience in solving standard computer vision problems, such as finding shapes, contours, highlighting areas of interest, etc.‚Ä¢ Experience with GPU.‚Ä¢ Experience in scaling computing, both at the process / thread-level and at the cluster level.‚Ä¢ Basic experience in developing client-server applications. We offer We offer:‚Ä¢ Work in a friendly, ambitious team of professionals‚Ä¢ Opportunity for professional growth and career development‚Ä¢ Social package (paid vacation, sickness leaves)‚Ä¢ Flexible work schedule ‚Ä¢ Corporate activities & events ‚Ä¢ Comfortable working environment and office in the city center",Data Scientist (Computer Vision)@Requestum,https://jobs.dou.ua/companies/requestum/vacancies/126650/, Kharkiv,Data Scientist (Computer Vision),16 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/railsware/,Railsware,"{""Required skills"": [""\ud83d\udd39You have at least 2+ years of experience as a Data Analyst working with small-to-medium datasets on a wide range of contexts\ud83d\udd39Extensive knowledge of SQL + Python / R\ud83d\udd39Strong background in Statistics and Probability theory\ud83d\udd39Hands-on experience in data visualization via reports and dashboards (e.g., Data Studio, Tableau, Power BI, etc.)\ud83d\udd39Skills in tuning data flows and setting up automations between various data sources \ud83d\udd39Good skills in management of own tasks and projects independently, writing notes, project scope\ud83d\udd39You are always curious and ready to do deep research, interview the teams, detect and understand their needs for analysis\ud83d\udd39You can deliver own solutions that will bring value to projects you\u2019ll work on based on the business objectives\ud83d\udd39You value quality and simplicity and know when to simplify your solutions, or even make them self supportable\ud83d\udd39Proactiveness"", ""this skill is must-have to work at Railsware :) You are an easy-going, fast learner who is always ready to experiment with new concepts and tools\ud83d\udd39Fluent English verbal and written communication skills"", ""you must be able to explain data models, describe processes, and deliver data in an easy to understand manner for non-data savvy users""], ""As a plus"": [""\ud83d\udd39Software product analysis experience would be a great plus\ud83d\udd39Javascript knowledge\ud83d\udd39GitHub usage experience\ud83d\udd39Data infrastructure organization, monitoring and maintenance: managing ETL, writing tests for data analytics code, maintaining smart seed data and monitoring data consistency\ud83d\udd39Experience with Jupyter Notebook\ud83d\udd39Experience with Google AppsScript\ud83d\udd39Experience in GSheets or Excel, e.g. multiline array formulas, Query function\ud83d\udd39Experience in Machine Learning""], ""We offer"": [""\ud83d\udc68\u200d\ud83c\udf93Outstanding development culture: bit.ly/rwdouculture\ud83d\udea9Collaborate with us remotely from any location or in one of our offices: bit.ly/rwdouremote\u23f0 We offer flexible hours\ud83d\udcb8 Get competitive compensation, yearly bonus, access to savings program and microcredits\ud83d\udcbbThanks to our hardware policy, we use the best equipment and can regularly update it\ud83c\udf34 34 days a year as a paid time off (24 standard days + 10 more to cover public holidays)\ud83c\udfc3Health policy budget will cover your private sports and healthcare expenses\ud83e\udd1dParticipate in local and international conferences\u2708\ufe0fEvery year we go for a 7-day company trip with our families. We\u2019ve already visited Austrian Alps, Crete, Italy, and Croatia together\ud83c\udfe2Our offices are equipped with modern ergonomic chairs and standing desks\ud83c\udf52You can always find fresh food and drinks in our kitchen""], ""Responsibilities"": [""\ud83d\udd25See all Railsware benefits here: l.rw.rw/benefits""], ""Project description"": []}",,"Required skills üîπYou have at least 2+ years of experience as a Data Analyst working with small-to-medium datasets on a wide range of contextsüîπExtensive knowledge of SQL + Python / RüîπStrong background in Statistics and Probability theoryüîπHands-on experience in data visualization via reports and dashboards (e.g., Data Studio, Tableau, Power BI, etc.)üîπSkills in tuning data flows and setting up automations between various data sources üîπGood skills in management of own tasks and projects independently, writing notes, project scopeüîπYou are always curious and ready to do deep research, interview the teams, detect and understand their needs for analysisüîπYou can deliver own solutions that will bring value to projects you‚Äôll work on based on the business objectivesüîπYou value quality and simplicity and know when to simplify your solutions, or even make them self supportableüîπProactiveness ‚Äî this skill is must-have to work at Railsware :) You are an easy-going, fast learner who is always ready to experiment with new concepts and toolsüîπFluent English verbal and written communication skills ‚Äî you must be able to explain data models, describe processes, and deliver data in an easy to understand manner for non-data savvy users As a plus üîπSoftware product analysis experience would be a great plusüîπJavascript knowledgeüîπGitHub usage experienceüîπData infrastructure organization, monitoring and maintenance: managing ETL, writing tests for data analytics code, maintaining smart seed data and monitoring data consistencyüîπExperience with Jupyter NotebooküîπExperience with Google AppsScriptüîπExperience in GSheets or Excel, e.g. multiline array formulas, Query functionüîπExperience in Machine Learning We offer üë®‚ÄçüéìOutstanding development culture: bit.ly/rwdoucultureüö©Collaborate with us remotely from any location or in one of our offices: bit.ly/rwdouremote‚è∞ We offer flexible hoursüí∏ Get competitive compensation, yearly bonus, access to savings program and microcreditsüíªThanks to our hardware policy, we use the best equipment and can regularly update itüå¥ 34 days a year as a paid time off (24 standard days + 10 more to cover public holidays)üèÉHealth policy budget will cover your private sports and healthcare expensesü§ùParticipate in local and international conferences‚úàÔ∏èEvery year we go for a 7-day company trip with our families. We‚Äôve already visited Austrian Alps, Crete, Italy, and Croatia togetherüè¢Our offices are equipped with modern ergonomic chairs and standing desksüçíYou can always find fresh food and drinks in our kitchen üî•See all Railsware benefits here: l.rw.rw/benefits Responsibilities The major area of responsibility will be related to the data of our own products, company operations, and consultancy. The insights to be provided will cover various parts of the business including forecasting, team efficiency, user engagement, project progress, etc. Some examples of the results we would like to see:üî∏A system to keep the right balance between profits and losses (collect and process data from various sources, spot outliers in expenses, etc.)üî∏Railsware products metrics monitor (MRR, LTV, Stickiness, Churn, etc.)üî∏Research of sales and marketing: leads quality, geography, sources, conversion rates, etcüî∏Qualification of product early adopters to distinguish between one-person companies and huge businesses Project description We‚Äôre looking for a true Data Analysis geek to supercharge Railsware in making smart data-driven decisions for the business. The key aim of the position is to gather a lot of data from different sources, process it masterfully, and convert it to simple and useful insights for the team. We can promise that you won‚Äôt be bored here. The results of your work will be a valuable contribution to the development of our own products, clients‚Äô solutions, and improvement of the company‚Äôs internal operations (like recruitment). We would really like to combine your strong expertise and freedom in coming up with ideas and their implementation. That‚Äôs how we do it here, at Railsware. Railsware is a product studio. Since 2007, we have shaped our own ‚Äúknow-how‚Äù in product creation. Railsware helped many US and EU startups to turn into multi-million-dollar companies. We have built our own products, that now have 600K+ of excited users and generate $1M+ a year.We look for people with high potential ready to evolve in multiple directions. The right hires shape a team of A-players to learn from and evolve together. We support Railswarians with outstanding benefits and remote culture. Ready to become the next Railswarian? Apply today! üîπJoin our team: railsware.com/careersüîπProducts we‚Äôve built: railsware.com/case-studiesüîπBlog: railsware.com/blogüîπYouTube: youtube.com/c/railsware üîπClients about us: youtu.be/EIEFStPmmz8",Data Analyst@Railsware,https://jobs.dou.ua/companies/railsware/vacancies/88796/," Kyiv, Krakow (Poland), remote",Data Analyst,16 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/data-science-ua/,Data Science UA,"{""Required skills"": [""Requirements:"", ""Have at least 1 years of related work experience"", ""Advanced PC user (MS Office with emphasis on Excel, google-docs)"", ""Experience in making different presentations and reports"", ""Working knowledge about feedback forms"", ""Willing to help with various additional tasks on a regular basis"", ""Detail-oriented and be able to follow processes that will inevitably change over time as we grow"", ""Can keep thousand of things in their mind at the same time"", ""Fast in troubleshooting and escalation"", ""Upper-intermediate English.""], ""As a plus"": [""Experince with ATS/CRM is a plus.""], ""We offer"": [""Propose:"", ""Opportunity to drive the process and be creative"", ""Various opportunities to grow within our Recruitment team and build process from scratch"", ""Possibility to take part in recruitment activities with further growth"", ""Warm and friendly working environment"", ""Fully-equipped perfect office space located in the city center (\u201cCreative Quarter\u201d co-working)"", ""Flexible schedule and the ability to work remotely.""], ""Responsibilities"": [""Resposibilities:1) Aministrative tasks (90% regular tasks):"", ""Documents administration (drafting, signing, scanning etc.)"", ""Undertake clerical duties (e.g. answering emails, scheduling calls and meetings etc.)"", ""Analytics and reporting"", ""Execute other tasks assigned by line manager.""], ""Project description"": [""2) Recruitment team assistance (10% regular tasks):"", ""Maintain candidate database"", ""Prepare and post job descriptions"", ""Helping with initial applicant screening"", ""Taking part in mproving recruitment process.""]}",,"Required skills Requirements:‚Äî Have at least 1 years of related work experience‚Äî Advanced PC user (MS Office with emphasis on Excel, google-docs)‚Äî Experience in making different presentations and reports‚Äî Working knowledge about feedback forms ‚Äî Willing to help with various additional tasks on a regular basis‚Äî Detail-oriented and be able to follow processes that will inevitably change over time as we grow‚Äî Can keep thousand of things in their mind at the same time‚Äî Fast in troubleshooting and escalation‚Äî Upper-intermediate English. As a plus ‚Äî Experince with ATS/CRM is a plus. We offer Propose:‚Äî Opportunity to drive the process and be creative‚Äî Various opportunities to grow within our Recruitment team and build process from scratch‚Äî Possibility to take part in recruitment activities with further growth‚Äî Warm and friendly working environment‚Äî Fully-equipped perfect office space located in the city center (‚ÄúCreative Quarter‚Äù co-working)‚Äî Flexible schedule and the ability to work remotely. Responsibilities Resposibilities:1) Aministrative tasks (90% regular tasks):‚Äî Documents administration (drafting, signing, scanning etc.)‚Äî Undertake clerical duties (e.g. answering emails, scheduling calls and meetings etc.)‚Äî Analytics and reporting‚Äî Execute other tasks assigned by line manager. 2) Recruitment team assistance (10% regular tasks):‚Äî Maintain candidate database‚Äî Prepare and post job descriptions‚Äî Helping with initial applicant screening‚Äî Taking part in mproving recruitment process. Project description Our company is looking for Administration manager. You will be involved in different types of administrative tasks within Recruitment team. We hope you‚Äôll help us keeping our hiring process running.",Administration manager@Data Science UA,https://jobs.dou.ua/companies/data-science-ua/vacancies/131994/, Kyiv,Administration manager,15 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/ticketsell/,ticketsell,"{""Required skills"": [""At least 3 years of extensive development with the use of Python."", ""Practical experience in the development of predictive models is a must (time series, decision-making trees, neural networks)."", ""Experience in solving complex tasks related to user sensitivity (price sensitivity in particular) will be a great plus."", ""Models optimization and feature engineering skills."", ""Database Management Systems: PostgreSQL (triggers, views, stored procedures, domains, PL/pgSQL)."", ""Linux.""], ""As a plus"": [""Exceptional problem-solving and acceptable communication skills."", ""Knowledge of English is a plus.""], ""We offer"": [""An early-stage company experiencing, sustained growth across the CIS and European markets."", ""Promotion potential to data science lead at fast-growing environment."", ""Competitive salary."", ""Welcoming office located in the city center.""], ""Responsibilities"": [""Improve existing models (sales prediction, etc.)."", ""Develop new models, systems, architectures."", ""Perform code refactoring and optimization."", ""Document and test code.""], ""Project description"": [""We\u2019re a growth stage start-up with a unique SaaS product. We have been selected to Startup Wise Guys CEE leading acceleration program. Our product is predictive analytics that helps concert and sport event promoters to execute the demand-driven pricing.""]}",,"Required skills ‚Äî At least 3 years of extensive development with the use of Python.‚Äî Practical experience in the development of predictive models is a must (time series, decision-making trees, neural networks).‚Äî Experience in solving complex tasks related to user sensitivity (price sensitivity in particular) will be a great plus. ‚Äî Models optimization and feature engineering skills.‚Äî Database Management Systems: PostgreSQL (triggers, views, stored procedures, domains, PL/pgSQL).‚Äî Linux. As a plus ‚Äî Exceptional problem-solving and acceptable communication skills.‚Äî Knowledge of English is a plus. We offer ‚Äî An early-stage company experiencing, sustained growth across the CIS and European markets.‚Äî Promotion potential to data science lead at fast-growing environment.‚Äî Competitive salary.‚Äî Welcoming office located in the city center. Responsibilities ‚Äî Improve existing models (sales prediction, etc.).‚Äî Develop new models, systems, architectures.‚Äî Perform code refactoring and optimization.‚Äî Document and test code. Project description We‚Äôre a growth stage start-up with a unique SaaS product. We have been selected to Startup Wise Guys CEE leading acceleration program. Our product is predictive analytics that helps concert and sport event promoters to execute the demand-driven pricing. Our mission is to help promoters to sell and price tickets based on demand in the most effective way so that the last ticket is sold 1 minute before the show. We need a talented, disciplined and experienced professional with clear previous achievements and a strong commitment to building industry impactful products. (IMPORTANT) The candidate must have the ambition to take the responsibility of the ML side of the product and grow it as a baby.",Data scientist (predictive modeling)@ticketsell,https://jobs.dou.ua/companies/ticketsell/vacancies/113666/, Kyiv,Data scientist (predictive modeling),15 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/custom-solutions/,Custom Solutions,"{""Required skills"": [""\u25cf TSQL (SSMS) \u043e\u043f\u044b\u0442 \u0440\u0430\u0431\u043e\u0442\u044b 1+ \u0433\u043e\u0434 (\u0441\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u0442\u0430\u0431\u043b\u0438\u0446, \u0444\u0443\u043d\u043a\u0446\u0438\u0439, \u043f\u0440\u043e\u0446\u0435\u0434\u0443\u0440, \u0442\u0440\u0438\u0433\u0433\u0435\u0440\u043e\u0432 \u0438 \u0442.\u0434.)\u25cf \u0417\u043d\u0430\u043d\u0438\u0435 \u0430\u043d\u0433\u043b\u0438\u0439\u0441\u043a\u043e\u0433\u043e (\u0440\u0430\u0437\u0433\u043e\u0432\u043e\u0440\u043d\u044b\u0439 Intermediate, \u043f\u0438\u0441\u044c\u043c\u0435\u043d\u043d\u044b\u0439 Intermediate+/-)\u25cf \u0412\u044b\u0441\u0448\u0435\u0435 \u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u0435 \u043d\u0430 \u0442\u0435\u0445\u043d\u0438\u0447\u0435\u0441\u043a\u043e\u043c \u0444\u0430\u043a\u0443\u043b\u044c\u0442\u0435\u0442\u0435""], ""As a plus"": [""\u25cf \u0417\u043d\u0430\u043d\u0438\u0435 Excel (\u0440\u0430\u0431\u043e\u0442\u0430 \u0441 datasets)\u25cf \u0417\u043d\u0430\u043d\u0438\u0435 PowerBI\u25cf \u041e\u043f\u044b\u0442 \u0440\u0430\u0431\u043e\u0442\u044b \u0441 Invoices"", ""Collections \u0431\u0438\u0437\u043d\u0435\u0441 \u043c\u043e\u0434\u0435\u043b\u044c\u044e""], ""We offer"": [""\u25cf \u041f\u043e\u043b\u043d\u0430\u044f \u0437\u0430\u043d\u044f\u0442\u043e\u0441\u0442\u044c (40 \u0447\u0430\u0441\u043e\u0432 \u0432 \u043d\u0435\u0434\u0435\u043b\u044e)\u25cf \u0413\u0438\u0431\u043a\u0438\u0439 \u0433\u0440\u0430\u0444\u0438\u043a \u0440\u0430\u0431\u043e\u0442\u044b (\u043d\u0430\u0447\u0430\u043b\u043e \u0440\u0430\u0431\u043e\u0447\u0435\u0433\u043e \u0434\u043d\u044f \u0441 9-11)\u25cf \u041e\u0442\u043a\u0440\u044b\u0442\u043e\u0441\u0442\u044c \u0438 \u043f\u0440\u043e\u0441\u0442\u043e\u0442\u0443 \u0432 \u043e\u0431\u0449\u0435\u043d\u0438\u0438 \u0441\u043e \u0432\u0441\u0435\u043c\u0438 \u0441\u043e\u0442\u0440\u0443\u0434\u043d\u0438\u043a\u0430\u043c\u0438 \u043a\u043e\u043c\u043f\u0430\u043d\u0438\u0438\u25cf \u041e\u0444\u0438\u0441 \u0432 \u0440\u0430\u0439\u043e\u043d\u0435 \u043f\u0430\u0440\u043a\u0430 \u0428\u0435\u0432\u0447\u0435\u043d\u043a\u043e (\u0441 \u043b\u0435\u0442\u043d\u0438\u043c \u0434\u0432\u043e\u0440\u0438\u043a\u043e\u043c, \u0433\u0434\u0435 \u043a\u043e\u043c\u0430\u043d\u0434\u0430 \u0434\u0435\u043b\u0430\u0435\u0442 BBQ)\u25cf \u0414\u0432\u0430 \u0440\u0430\u0437\u0430 \u0432 \u043d\u0435\u0434\u0435\u043b\u044e \u0437\u0430\u043d\u044f\u0442\u0438\u044f \u0430\u043d\u0433\u043b\u0438\u0439\u0441\u043a\u0438\u043c \u0432 \u043e\u0444\u0438\u0441\u0435\u25cf 20 \u0434\u043d\u0435\u0439 \u043e\u0442\u043f\u0443\u0441\u043a\u0430, \u043e\u043f\u043b\u0430\u0447\u0438\u0432\u0430\u0435\u043c\u044b\u0435 \u0431\u043e\u043b\u044c\u043d\u0438\u0447\u043d\u044b\u0435\u25cf \u041e\u0444\u0438\u0446\u0438\u0430\u043b\u044c\u043d\u043e\u0435 \u0442\u0440\u0443\u0434\u043e\u0443\u0441\u0442\u0440\u043e\u0439\u0441\u0442\u0432\u043e (\u043f\u0440\u0438 \u0443\u0441\u043f\u0435\u0448\u043d\u043e\u043c \u043f\u0440\u043e\u0445\u043e\u0436\u0434\u0435\u043d\u0438\u0438 \u0438\u0441\u043f\u044b\u0442\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0433\u043e \u043f\u0435\u0440\u0438\u043e\u0434\u0430"", ""2 \u043c\u0435\u0441\u044f\u0446\u0430)""], ""Responsibilities"": [""1. \u0420\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0438 \u043f\u043e\u0434\u0434\u0435\u0440\u0436\u0430\u043d\u0438\u0435 PowerBI \u043e\u0442\u0447\u0435\u0442\u043e\u0432 \u0434\u043b\u044f \u043a\u043b\u0438\u0435\u043d\u0442\u043e\u04322. \u0418\u0437\u0443\u0447\u0435\u043d\u0438\u0435 DS \u043f\u043e \u0438\u043c\u0435\u044e\u0449\u0438\u043c\u0441\u044f \u0432 \u043a\u043e\u043c\u043f\u0430\u043d\u0438\u0438 \u043d\u0430\u0440\u0430\u0431\u043e\u0442\u043a\u0430\u043c3. \u041f\u0440\u043e\u0432\u0435\u0434\u0435\u043d\u0438\u0435 \u0441\u0430\u043c\u043e\u0441\u0442\u043e\u044f\u0442\u0435\u043b\u044c\u043d\u044b\u0445 \u0438\u0441\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u043d\u0438\u0439 \u0432 \u043e\u0431\u043b\u0430\u0441\u0442\u0438 DS, \u0432\u043a\u043b\u044e\u0447\u0435\u043d\u0438\u0435 \u043f\u0440\u043e\u0432\u0435\u0434\u0435\u043d\u043d\u044b\u0445 \u0438\u0441\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u043d\u0438\u0439 \u0432 \u0431\u0430\u0437\u0443 \u0437\u043d\u0430\u043d\u0438\u0439 \u043a\u043e\u043c\u043f\u0430\u043d\u0438\u0438""], ""Project description"": [""Custom Solutions"", ""\u0430\u0443\u0442\u0441\u043e\u0440\u0441\u0438\u043d\u0433\u043e\u0432\u0430\u044f \u043a\u043e\u043c\u043f\u0430\u043d\u0438\u044f, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u043f\u0440\u0435\u0438\u043c\u0443\u0449\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u043e \u0437\u0430\u043d\u0438\u043c\u0430\u0435\u0442\u0441\u044f \u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u043a\u043e\u0439 \u041f\u041e \u0434\u043b\u044f \u0444\u0438\u043d\u0430\u043d\u0441\u043e\u0432\u044b\u0445 \u0438 \u0442\u0440\u0435\u043a\u0438\u043d\u0433\u043e\u0432\u044b\u0445 \u0441\u0438\u0441\u0442\u0435\u043c. \u041a\u043e\u043c\u043f\u0430\u043d\u0438\u044f \u043d\u0430 \u0440\u044b\u043d\u043a\u0435 \u0414\u043d\u0435\u043f\u0440\u0430 \u0441 2015 \u0433\u043e\u0434\u0430. \u041e\u0441\u043d\u043e\u0432\u043d\u044b\u0435 \u043a\u043b\u0438\u0435\u043d\u0442\u044b \u0441\u043e\u0441\u0440\u0435\u0434\u043e\u0442\u043e\u0447\u0435\u043d\u044b \u0432 \u0410\u0432\u0441\u0442\u0440\u0430\u043b\u0438\u0438, \u0421\u0428\u0410, \u041a\u0430\u043d\u0430\u0434\u0435.\u0412 \u043a\u043e\u043c\u043f\u0430\u043d\u0438\u0438 \u043d\u0435\u0442 \u0441\u043b\u043e\u0436\u043d\u044b\u0445 \u0438\u0435\u0440\u0430\u0440\u0445\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440. \u041e\u0431\u0449\u0435\u043d\u0438\u0435 \u0441\u0432\u043e\u0431\u043e\u0434\u043d\u043e\u0435 \u0438 \u043e\u0442\u043a\u0440\u044b\u0442\u043e\u0435. \u041f\u043e\u044d\u0442\u043e\u043c\u0443 \u043e\u0431\u0449\u0435\u043d\u0438\u0435 \u0441 \u043a\u043e\u043b\u043b\u0435\u0433\u0430\u043c\u0438-\u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u0447\u0438\u043a\u0430\u043c\u0438 \u0434\u043b\u044f \u0432\u0430\u0441 \u043d\u0435 \u0431\u0443\u0434\u0435\u0442 \u043e\u0442\u043b\u0438\u0447\u0430\u0442\u044c\u0441\u044f \u043e\u0442 \u043e\u0431\u0449\u0435\u043d\u0438\u044f \u0441 \u0421\u0422\u041e \u043a\u043e\u043c\u043f\u0430\u043d\u0438\u0438.\u041a\u043e\u043c\u0430\u043d\u0434\u0430 \u0441\u043e\u0441\u0442\u043e\u0438\u0442 \u0438\u0437 15 \u0441\u043e\u0442\u0440\u0443\u0434\u043d\u0438\u043a\u043e\u0432.""]}",,"Required skills ‚óè TSQL (SSMS) –æ–ø—ã—Ç —Ä–∞–±–æ—Ç—ã 1+ –≥–æ–¥ (—Å–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü, —Ñ—É–Ω–∫—Ü–∏–π, –ø—Ä–æ—Ü–µ–¥—É—Ä, —Ç—Ä–∏–≥–≥–µ—Ä–æ–≤ –∏ —Ç.–¥.)‚óè –ó–Ω–∞–Ω–∏–µ –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ (—Ä–∞–∑–≥–æ–≤–æ—Ä–Ω—ã–π Intermediate, –ø–∏—Å—å–º–µ–Ω–Ω—ã–π Intermediate+/-)‚óè –í—ã—Å—à–µ–µ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –Ω–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–º —Ñ–∞–∫—É–ª—å—Ç–µ—Ç–µ As a plus ‚óè –ó–Ω–∞–Ω–∏–µ Excel (—Ä–∞–±–æ—Ç–∞ —Å datasets)‚óè –ó–Ω–∞–Ω–∏–µ PowerBI‚óè –û–ø—ã—Ç —Ä–∞–±–æ—Ç—ã —Å Invoices ‚Äî Collections –±–∏–∑–Ω–µ—Å –º–æ–¥–µ–ª—å—é We offer ‚óè –ü–æ–ª–Ω–∞—è –∑–∞–Ω—è—Ç–æ—Å—Ç—å (40 —á–∞—Å–æ–≤ –≤ –Ω–µ–¥–µ–ª—é)‚óè –ì–∏–±–∫–∏–π –≥—Ä–∞—Ñ–∏–∫ —Ä–∞–±–æ—Ç—ã (–Ω–∞—á–∞–ª–æ —Ä–∞–±–æ—á–µ–≥–æ –¥–Ω—è —Å 9-11)‚óè –û—Ç–∫—Ä—ã—Ç–æ—Å—Ç—å –∏ –ø—Ä–æ—Å—Ç–æ—Ç—É –≤ –æ–±—â–µ–Ω–∏–∏ —Å–æ –≤—Å–µ–º–∏ —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞–º–∏ –∫–æ–º–ø–∞–Ω–∏–∏‚óè –û—Ñ–∏—Å –≤ —Ä–∞–π–æ–Ω–µ –ø–∞—Ä–∫–∞ –®–µ–≤—á–µ–Ω–∫–æ (—Å –ª–µ—Ç–Ω–∏–º –¥–≤–æ—Ä–∏–∫–æ–º, –≥–¥–µ –∫–æ–º–∞–Ω–¥–∞ –¥–µ–ª–∞–µ—Ç BBQ)‚óè –î–≤–∞ —Ä–∞–∑–∞ –≤ –Ω–µ–¥–µ–ª—é –∑–∞–Ω—è—Ç–∏—è –∞–Ω–≥–ª–∏–π—Å–∫–∏–º –≤ –æ—Ñ–∏—Å–µ‚óè 20 –¥–Ω–µ–π –æ—Ç–ø—É—Å–∫–∞, –æ–ø–ª–∞—á–∏–≤–∞–µ–º—ã–µ –±–æ–ª—å–Ω–∏—á–Ω—ã–µ‚óè –û—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–µ —Ç—Ä—É–¥–æ—É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ (–ø—Ä–∏ —É—Å–ø–µ—à–Ω–æ–º –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏–∏ –∏—Å–ø—ã—Ç–∞—Ç–µ–ª—å–Ω–æ–≥–æ –ø–µ—Ä–∏–æ–¥–∞ ‚Äî 2 –º–µ—Å—è—Ü–∞) Responsibilities 1. –†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –∏ –ø–æ–¥–¥–µ—Ä–∂–∞–Ω–∏–µ PowerBI –æ—Ç—á–µ—Ç–æ–≤ –¥–ª—è –∫–ª–∏–µ–Ω—Ç–æ–≤2. –ò–∑—É—á–µ–Ω–∏–µ DS –ø–æ –∏–º–µ—é—â–∏–º—Å—è –≤ –∫–æ–º–ø–∞–Ω–∏–∏ –Ω–∞—Ä–∞–±–æ—Ç–∫–∞–º3. –ü—Ä–æ–≤–µ–¥–µ–Ω–∏–µ —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω—ã—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –≤ –æ–±–ª–∞—Å—Ç–∏ DS, –≤–∫–ª—é—á–µ–Ω–∏–µ –ø—Ä–æ–≤–µ–¥–µ–Ω–Ω—ã—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π –∫–æ–º–ø–∞–Ω–∏–∏ Project description Custom Solutions ‚Äî –∞—É—Ç—Å–æ—Ä—Å–∏–Ω–≥–æ–≤–∞—è –∫–æ–º–ø–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ –∑–∞–Ω–∏–º–∞–µ—Ç—Å—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–æ–π –ü–û –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –∏ —Ç—Ä–µ–∫–∏–Ω–≥–æ–≤—ã—Ö —Å–∏—Å—Ç–µ–º. –ö–æ–º–ø–∞–Ω–∏—è –Ω–∞ —Ä—ã–Ω–∫–µ –î–Ω–µ–ø—Ä–∞ —Å 2015 –≥–æ–¥–∞. –û—Å–Ω–æ–≤–Ω—ã–µ –∫–ª–∏–µ–Ω—Ç—ã —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á–µ–Ω—ã –≤ –ê–≤—Å—Ç—Ä–∞–ª–∏–∏, –°–®–ê, –ö–∞–Ω–∞–¥–µ.–í –∫–æ–º–ø–∞–Ω–∏–∏ –Ω–µ—Ç —Å–ª–æ–∂–Ω—ã—Ö –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä. –û–±—â–µ–Ω–∏–µ —Å–≤–æ–±–æ–¥–Ω–æ–µ –∏ –æ—Ç–∫—Ä—ã—Ç–æ–µ. –ü–æ—ç—Ç–æ–º—É –æ–±—â–µ–Ω–∏–µ —Å –∫–æ–ª–ª–µ–≥–∞–º–∏-—Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∞–º–∏ –¥–ª—è –≤–∞—Å –Ω–µ –±—É–¥–µ—Ç –æ—Ç–ª–∏—á–∞—Ç—å—Å—è –æ—Ç –æ–±—â–µ–Ω–∏—è —Å –°–¢–û –∫–æ–º–ø–∞–Ω–∏–∏.–ö–æ–º–∞–Ω–¥–∞ —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ 15 —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–æ–≤.",Data scientist (PowerBI report developer)@Custom Solutions,https://jobs.dou.ua/companies/custom-solutions/vacancies/131970/, Dnipro,Data scientist (PowerBI report developer),15 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/n-ix/,N-iX,{},,"Our client is a US company that specializes in delivering reliable Internet connectivity and entertainment multimedia to aircrafts worldwide, enhancing the experience for both passengers and crew. At this moment we started our cooperation with the Business Intelligence, Data Analysis and BigData solutions support direction and looking for talents who can contribute to the complex data management and analysis projects. Responsibilities and job specifics:‚Ä¢ Identify, analyze, and interpret trends and patterns in complex data sets‚Ä¢ Conduct analytical research to answer business questions‚Ä¢ Acquire data from primary or secondary data sources and maintain databases/data system‚Ä¢ Design and build Machine Learning solutions Qualification specifications:‚Ä¢ Strong experience with Python, SQL‚Ä¢ Good knowledge of statistics and probability theory‚Ä¢ Applied Machine Learning experience and a solid understanding of Machine Learning algorithms and concepts‚Ä¢ Strong mathematical background‚Ä¢ Good understanding of software development methodologies‚Ä¢ Strong analytically, sharp and able to think on their feet‚Ä¢ Analytical Skills: Data scientists work with large amounts of data: facts, figures, and number crunching. You will need to see through the data and analyze it to find conclusions‚Ä¢ Critical Thinking: Data scientists must look at the numbers, trends, and data and come to new conclusions based on the findings‚Ä¢ Attention to Detail: Data is precise. Data scientists have to make sure they are vigilant in their analysis to come to correct conclusions‚Ä¢ Communication Skills: Data scientists are often called to present their findings, or translate the data into an understandable document. You will need to write and speak clearly, easily communicating complex ideas Nice to have:‚Ä¢ Hands-on experience with Spark‚Ä¢ Experience using AWS‚Ä¢ Experience with Tableau We offer:‚Ä¢ Flexible working hours‚Ä¢ A competitive salary and good compensation package‚Ä¢ Possibility of partial remote work‚Ä¢ Best hardware‚Ä¢ A masseur and a corporate doctor‚Ä¢ Healthcare & sport benefits‚Ä¢ An inspiring and comfy office Professional growth:‚Ä¢ Challenging tasks and innovative projects‚Ä¢ Meetups and events for professional development‚Ä¢ An individual development plan‚Ä¢ Mentorship program Fun:‚Ä¢ Corporate events and outstanding parties‚Ä¢ Exciting team buildings‚Ä¢ Memorable anniversary presents‚Ä¢ A fun zone where you can play video games, foosball, ping pong, and more.",Data Science Engineer@N-iX,https://jobs.dou.ua/companies/n-ix/vacancies/131967/," Kyiv, Lviv, remote",Data Science Engineer,15 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/kopentech-llc/,KopenTech,"{""Required skills"": [""Analytical skills and attentiveness to details Proficient with Excel Intermediate English Interest in the financial industry""], ""We offer"": [""Professional and career growth opportunities Competitive salary Continuous intellectual challenge Ability to work remotely or in co-working space 20 vacation days Multinational team of professionals Deep dive into the finance industry""], ""Responsibilities"": [""Data entry and verification Preparing reports""], ""Project description"": [""Trading platform for structured products.""]}",,"Required skills Analytical skills and attentiveness to details Proficient with Excel Intermediate English Interest in the financial industry We offer Professional and career growth opportunities Competitive salary Continuous intellectual challenge Ability to work remotely or in co-working space 20 vacation days Multinational team of professionals Deep dive into the finance industry Responsibilities Data entry and verification Preparing reports Project description Trading platform for structured products. We transform chaotic communications, manual spreadsheets, and complex workflows into a streamlined process. Our technology allows market participants to save time, increase demand, and make more data-driven decisions",Data Entry Specialist whit QA/Dev position prospect@KopenTech,https://jobs.dou.ua/companies/kopentech-llc/vacancies/131934/, remote,Data Entry Specialist whit QA/Dev position prospect,15 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/ring-ukraine/,Ring Ukraine,{},,"We are looking for a passionate and experienced Data Analyst to join our Monitoring team. In this role, you will get the exciting opportunity to work with product managers to understand their business requirements and deliver solutions and actionable insight throughout the product development lifecycle. Our team is serious about developing great iOS/Android/Web user experiences and hardware products with a scalable data-focused approach. A successful candidate will be a person who enjoys diving deep into data analysis, discovering root causes, and implementing long-term solutions. You will help customize business policies and processes while developing new metrics and functionality. ‚Äî 3+ years of experience as BI/Data Analyst‚Äî Significant experience using SQL for statistical analysis involving large data sets, solid SQL optimization skills‚Äî Experience using Tableau‚Äî Significant experience creating and maintaining ETLs and databases in a business environment with large-scale, complex datasets‚Äî Experience creating and maintaining a data model.‚Äî Experienced AWS user with an in-depth understanding of AWS technology stack, experience spinning up and managing AWS services.‚Äî Experience in logs analysis (iOS/Android/Web)‚Äî English level ‚Äî B2 ‚Äî Electrical engineering PCB Component/firmware comprehension‚Äî Linux application log comprehension‚Äî IoT device setup/management comprehension ‚Äî Experienced Python/R developer‚Äî Docker and NoSQL knowledge, hands-on experience with DataDog, Kibana, MixPanel, Heap, Google Analytics, Firebase, Fabric, Quicksight ‚Äî Opportunity to influence the products‚Äô quality supporting company mission to make neighborhoods safer‚Äî Challenging tasks and professional growth‚Äî Competitive salary and perks‚Äî PE accounting and support‚Äî 18 paid vacation days per year, paid public holidays according to the Ukrainian legislation‚Äî Social package, including gym membership compensation, medical insurance‚Äî Free office meals, fruits, and cookies‚Äî Educational possibilities, knowledge hubs, and free corporate English classes‚Äî Career plan, professional growth, and semiannual performance review.",Data Analyst@Ring Ukraine,https://jobs.dou.ua/companies/ring-ukraine/vacancies/119506/, Kyiv,Data Analyst,15 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/datarobot/,DataRobot,"{""Required skills"": [""Java/Scala Programming (3+ years)Data manipulation using SparkAPI DesignMongoDB / NoSQL DatabasesSQL / Traditional DatabasesData manipulationFamiliarity with Linux operating systemFamiliarity with collaborative development, including code reviews. Either in a commercial or open-source environment. Strong desire to build efficient and scalable software""], ""As a plus"": [""Familiarity with the Kubernetes or Hadoop ecosystemsLinux shell scriptingJenkins or other Continuous Integration platformsAWS, Azure or GCP experienceExperience in a start-up environment""], ""Responsibilities"": [""As a Backend Engineer, you will develop new data engineering/transformation tools, build new application features, and construct new APIs. This position is primarily about implementing completely new capabilities in our Data Prep product, working up and down the backend stack to design database collections, services /middleware, and complex API interfaces that power the core of DataRobot\u2019s Data Prep technology. You will need to constantly design for scale, usability, and simplicity/maintainability while working in a fast-paced startup environment. Familiarity with design patterns for both SaaS applications and patterns for fast and memory-efficient data manipulation will be important. Also important"", ""the ideal candidate would have strong development skills in both Java and Python to bridge across multiple application components.""], ""Project description"": [""Along the way, you will develop comprehensive automated testing to ensure the product is reliable and scalable. You will help us handle issues promptly to minimize disruption to users as we roll out new state-of-the-art features. Pairing with and mentoring colleagues is a must. We are a very collaborative team.""]}",,"Required skills Java/Scala Programming (3+ years)Data manipulation using SparkAPI DesignMongoDB / NoSQL DatabasesSQL / Traditional DatabasesData manipulationFamiliarity with Linux operating systemFamiliarity with collaborative development, including code reviews. Either in a commercial or open-source environment. Strong desire to build efficient and scalable software As a plus Familiarity with the Kubernetes or Hadoop ecosystemsLinux shell scriptingJenkins or other Continuous Integration platformsAWS, Azure or GCP experienceExperience in a start-up environment Responsibilities As a Backend Engineer, you will develop new data engineering/transformation tools, build new application features, and construct new APIs. This position is primarily about implementing completely new capabilities in our Data Prep product, working up and down the backend stack to design database collections, services /middleware, and complex API interfaces that power the core of DataRobot‚Äôs Data Prep technology. You will need to constantly design for scale, usability, and simplicity/maintainability while working in a fast-paced startup environment. Familiarity with design patterns for both SaaS applications and patterns for fast and memory-efficient data manipulation will be important. Also important ‚Äî the ideal candidate would have strong development skills in both Java and Python to bridge across multiple application components. Along the way, you will develop comprehensive automated testing to ensure the product is reliable and scalable. You will help us handle issues promptly to minimize disruption to users as we roll out new state-of-the-art features. Pairing with and mentoring colleagues is a must. We are a very collaborative team. Project description The Global Data Domain at DataRobot helps make AI incredibly easy to build by surfacing and connecting users to all enterprise data, and enabling data prep at modeling and prediction time. In this role, you‚Äôll help enable our users to easily interact with the data required to train models and generate predictions. Additionally, you‚Äôll help design solutions that foster collaboration amongst distributed teams of engineers, data scientists, and beyond.","Backend Java Engineer, Data Management@DataRobot",https://jobs.dou.ua/companies/datarobot/vacancies/125369/," Kyiv, Kharkiv, Lviv, Odesa, Khmelnytskyi","Backend Java Engineer, Data Management",15 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/datarobot/,DataRobot,"{""Required skills"": [""Strong experience in Python and either Java or ScalaExperience with large applications and systems developmentStrong experience using JDBC for database accessStrong database and blob storage experienceExperience integrating applications with cloud data storage systems like S3, GCS, Azure Blob Storage, etc.Experience with authentication methods like Kerberos, SSO, Active Directory or similarFamiliarity with HTTP and experience using tools such as curl or Postman to test HTTP APIsAbility and willingness to quickly learn about new technologies""], ""As a plus"": [""Experience with Kubernetes ecosystem or Hadoop ecosystemExperience developing applications using DockerExperience with Gradle or other build systemsExperience with functional programming""], ""Responsibilities"": [""As an Ingest Backend Engineer, you will develop ingest systems to integrate DataRobot with databases, cloud storage systems, SaaS products, and other sources of data, and you will help enhance and maintain these systems as they evolve to meet future connectivity and performance needs. You will contribute to the design of new frameworks to support reading, writing, authentication, failure handling, and more with high performance and reliability. These ingest systems are a key component of the overall architecture of DataRobot.Ideally, a candidate can bring new ideas from concept to implementation, write quality, testable code, and participate in design/development discussions. They also have a can-do attitude and highly value the customer experience above all else, and embrace maintaining and firefighting systems when necessary. The ability and willingness to quickly learn new technologies are essential for this role.""], ""Project description"": [""The Global Data Domain at DataRobot helps make AI incredibly easy to build by surfacing and connecting users to all enterprise data, and enabling data prep at modeling and prediction time. In this role, you\u2019ll help enable our users to easily interact with the data required to train models and generate predictions. Additionally, you\u2019ll help design solutions that foster collaboration amongst distributed teams of engineers, data scientists, and beyond.""]}",,"Required skills Strong experience in Python and either Java or ScalaExperience with large applications and systems developmentStrong experience using JDBC for database accessStrong database and blob storage experienceExperience integrating applications with cloud data storage systems like S3, GCS, Azure Blob Storage, etc.Experience with authentication methods like Kerberos, SSO, Active Directory or similarFamiliarity with HTTP and experience using tools such as curl or Postman to test HTTP APIsAbility and willingness to quickly learn about new technologies As a plus Experience with Kubernetes ecosystem or Hadoop ecosystemExperience developing applications using DockerExperience with Gradle or other build systemsExperience with functional programming Responsibilities As an Ingest Backend Engineer, you will develop ingest systems to integrate DataRobot with databases, cloud storage systems, SaaS products, and other sources of data, and you will help enhance and maintain these systems as they evolve to meet future connectivity and performance needs. You will contribute to the design of new frameworks to support reading, writing, authentication, failure handling, and more with high performance and reliability. These ingest systems are a key component of the overall architecture of DataRobot.Ideally, a candidate can bring new ideas from concept to implementation, write quality, testable code, and participate in design/development discussions. They also have a can-do attitude and highly value the customer experience above all else, and embrace maintaining and firefighting systems when necessary. The ability and willingness to quickly learn new technologies are essential for this role. Project description The Global Data Domain at DataRobot helps make AI incredibly easy to build by surfacing and connecting users to all enterprise data, and enabling data prep at modeling and prediction time. In this role, you‚Äôll help enable our users to easily interact with the data required to train models and generate predictions. Additionally, you‚Äôll help design solutions that foster collaboration amongst distributed teams of engineers, data scientists, and beyond.","Ingest (Data) Backend Engineer, Data Management@DataRobot",https://jobs.dou.ua/companies/datarobot/vacancies/125355/," Kyiv, Lviv","Ingest (Data) Backend Engineer, Data Management",15 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/ciklum/,Ciklum,{},,"Ciklum Digital is looking for a Middle Data Quality Engineer to join the UA team on a full-time basis. You will join a highly motivated team and will be working on a modern solution for our existing client. We are looking for technology experts who want to make an impact on new business by applying best practices and taking ownership. About the Project: Client, a truly modern media company, gives anyone with digitally bound content the ability to upload and distribute their publications worldwide. In just minutes. Each day, more than 20,000 newly uploaded publications become instantly available to active readers around the world who use Client Company‚Äôs site and mobile apps to discover and engage with what they love, from magazines, newspapers and portfolios, to catalogs, DIY guides, community programs and more.",Middle Data Quality Engineer for Ciklum Digital (2000033F)@Ciklum,https://jobs.dou.ua/companies/ciklum/vacancies/127409/, Kyiv,Middle Data Quality Engineer for Ciklum Digital (2000033F),15 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/ciklum/,Ciklum,{},,"On behalf of Ciklum Digital, Ciklum is looking for a Senior Data Engineer to join the UA team on a full-time basis. You will join a highly motivated team and will be working on a modern solution for our existing client. We are looking for technology experts who want to make an impact on new business by applying best practices and taking ownership. Project description:This interesting and challenging initiative involves designing, implementing and maintaining a complex Intelligence platform capable of processing multipart Big Data from various established vendors, managing such data in its totality to produce meaningful real-life investment-grade, event-signals for a leading US based private equity investor with focus in high-tech industry with portfolio that stretches across the globe. Ciklum has been actively and successfully involved in building and maintaining this AWS based product for over three years and is the lead systems integrator for the intelligent, predictive decision-making process and technology of the entire end-to-end marketing and decision-science platform that includes data ingestion, cleansing, standardizing, codifying, co-relating and making predictive decisions based on heuristic analytical models.",Senior Data Engineer for Ciklum Digital (200002AI)@Ciklum,https://jobs.dou.ua/companies/ciklum/vacancies/123541/, Kyiv,Senior Data Engineer for Ciklum Digital (200002AI),15 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/ciklum/,Ciklum,{},,"On behalf of WalkMe, Ciklum is looking for a Senior Big Data Engineer to join Kyiv team on a full-time basis. WalkMe is looking to hire a brilliant BigData Engineer to join the Data Unit in our Engineering Department. This is your chance to join one of the fastest growing and most exciting startups in Israel. WalkMe‚Äôs Data Unit consists of end to end web specialists who meet thought provoking challenges on a daily basis. WalkMe collects data from millions concurrent users what produces a traffic of billions of daily events. WalkMe Data unit is responsible for the data infrastructure in WalkMe. As a BigData engineer at in the Data unit you will be facing two main challenges: We raise our own leaders and encourage employees in all ranks to take initiative and own their work, innovate and take an active role in the company‚Äôs success! Additionally, WalkMe has been recognized for its exceptional culture with awards from Glassdoor, Entrepreneur Magazine, Forbes and more. We strive to hire passionate individuals with a drive for excellence, who live and breathe innovation and are excited to be part of our incredibly talented community of employees. We‚Äôve found that the winning formula for excellence is enabling and encouraging our bright talent to take charge and contribute to the company‚Äôs success by executing their creative ideas. Through our commitment to transparency and digital oriented operations, we are able to play to the strengths of our employees, ensure our goals are aligned, and match the brilliance of our people with a community oriented environment they can thrive in. We work hard, we work together and this is what stands behind our incredible results.",Senior Big Data Engineer for WalkMe (200002KE)@Ciklum,https://jobs.dou.ua/companies/ciklum/vacancies/124829/, Kyiv,Senior Big Data Engineer for WalkMe (200002KE),15 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/sd-solutions/,SD Solutions,"{""Required skills"": [""Excellent understanding of machine learning techniques and algorithms, such as k-NN, Naive Bayes, SVM, Decision Forests, etc.4+ years of experience with common data science toolkits, such as TPOT, R, Weka, NumPy, MatLab, etcGreat communication skills3+ years of experience with data visualization tools, such as D3.js, GGplot, etc.Proficiency in using query languages such as SQL, Hive, Pig4+ years of experience with NoSQL databases, such as MongoDB, Cassandra, HBaseGood applied statistics skills, such as distributions, statistical testing, regression, etc.Good scripting and programming skillsData-oriented personality""], ""We offer"": [""Flexible schedule;Competitive compensation and social packages;Opportunity to participate in various internal/external events (educational programs, seminars, training sessions);Comfortable office with modern infrastructure;Food supply (e.g. lunches, cookies, fruits as well as tea, coffee)21 calendar days of paid vacation, paid sick leave.""], ""Responsibilities"": [""Selecting features, building and optimizing classifiers using machine learning techniquesData mining using state-of-the-art methodsExtending the company\u2019s data with third party sources of information when neededEnhancing data collection procedures to include information that is relevant for building analytic systemsProcessing, cleansing, and verifying the integrity of data used for analysisDoing ad-hoc analysis and presenting results in a clear mannerCreating automated anomaly detection systems and constant tracking of its performance""], ""Project description"": [""NOVOS (novos.gg) is a startup that builds a training platform for gamers who wish to pursue their dreams and take their gaming skills to the next level. NOVOS methodology is based on proven best practices from the skill-building world. We leverage cutting edge technology to provide gamers with a structured framework and an engaging training routine that helps them become the best gamers they can possibly be.""]}",,"Required skills Excellent understanding of machine learning techniques and algorithms, such as k-NN, Naive Bayes, SVM, Decision Forests, etc.4+ years of experience with common data science toolkits, such as TPOT, R, Weka, NumPy, MatLab, etcGreat communication skills3+ years of experience with data visualization tools, such as D3.js, GGplot, etc.Proficiency in using query languages such as SQL, Hive, Pig4+ years of experience with NoSQL databases, such as MongoDB, Cassandra, HBaseGood applied statistics skills, such as distributions, statistical testing, regression, etc.Good scripting and programming skillsData-oriented personality We offer Flexible schedule;Competitive compensation and social packages;Opportunity to participate in various internal/external events (educational programs, seminars, training sessions);Comfortable office with modern infrastructure;Food supply (e.g. lunches, cookies, fruits as well as tea, coffee)21 calendar days of paid vacation, paid sick leave. Responsibilities Selecting features, building and optimizing classifiers using machine learning techniquesData mining using state-of-the-art methodsExtending the company‚Äôs data with third party sources of information when neededEnhancing data collection procedures to include information that is relevant for building analytic systemsProcessing, cleansing, and verifying the integrity of data used for analysisDoing ad-hoc analysis and presenting results in a clear mannerCreating automated anomaly detection systems and constant tracking of its performance Project description NOVOS (novos.gg) is a startup that builds a training platform for gamers who wish to pursue their dreams and take their gaming skills to the next level. NOVOS methodology is based on proven best practices from the skill-building world. We leverage cutting edge technology to provide gamers with a structured framework and an engaging training routine that helps them become the best gamers they can possibly be.",Data Scientist (part time)@SD Solutions,https://jobs.dou.ua/companies/sd-solutions/vacancies/125791/," Kyiv, remote",Data Scientist (part time),15 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/softserve/,SoftServe,{},,"WE ARE SoftServe is the team of professionals who create high-quality software products by working with cutting-edge technologies.Our client is one of the largest Canadian retail companies which sells a wide range of automotive, hardware, sports and leisure, and home products. Our super friendly and collaborative team will help you to grow professionally in the retail domain on enterprise account. With us you will face challenging and interesting tasks. YOU ARE ‚Ä¢ Hands-on with Hadoop, Hive‚Ä¢ Skilled in SQL‚Ä¢ Experienced in PySpark‚Ä¢ Strong in understanding of database structures, theories, principles‚Ä¢ Able to perform tuning of the ETL process and SQL queries, and recommend and implement ETL and query tuning techniques‚Ä¢ Experienced in writing and tuning complex SQLs and use analytical functions‚Ä¢ Flexible and motivated to work in a fast-paced environment‚Ä¢ Showing excellent soft skills and experience in communication with end business users‚Ä¢ Demonstrating strong analytical skills, sense of responsibility, self-organization, self-improvement potential‚Ä¢ Passionate about learning new technologies‚Ä¢ Possessing basic knowledge in Scala (nice to have) ‚Ä¢ Demonstrating good technical English skills in all aspects on at least Intermediate level YOU WANT TO WORK WITH ‚Ä¢ Data analysis and technical requirements‚Ä¢ The customer in close cooperation‚Ä¢ Risk assessment, defining dependencies and assumptions‚Ä¢ Technical specifications and vision document creation‚Ä¢ Design of ETL solution TOGETHER WE WILL ‚Ä¢ Be a part of the fast-moving development team that use Agile methodologies best practices‚Ä¢ Work with Top Solutions architects in the company‚Ä¢ Work with a variety of data sources‚Ä¢ Develop ETL technical specifications, designs, tests, implements, and supports optimal data solutions for end-to-end data processing ‚Ä¢ Document ETL data mappings, data dictionaries, processes, programs, and solution",Middle/Senior Hadoop Software Engineer (ID¬†54941)@SoftServe,https://jobs.dou.ua/companies/softserve/vacancies/131887/, Kyiv,Middle/Senior Hadoop Software Engineer (ID¬†54941),15 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/cybervision/,"CyberVision, Inc.","{""Required skills"": [""* Strong Linux system administrator skills (3+ Yrs)* Working knowledge of Hadoop components *Knowledge of file system and Linux Os internals latency, throughput, availability, consistency, security, etc.*Ability to communicate technical concepts clearly and effectively.* Strong troubleshooting and debugging skills, with a passion for problem-solving and investigation* Ability and willingness to learn new technologies*Good is written and verbal communication skills in English""], ""As a plus"": [""*Shell scripting (Shell, Awk) ability to trace (read/understand) Java/ C++*SQL/ NoSQL Databases knowledge*familiar with different security technologies ( Kerberos, SSL, etc.)*understanding of Java/ JVM concepts is a plus*prior technical support experience is a plus""], ""Project description"": [""One of the biggest companies in the Hadoop world provides industry\u2019s only converged data platform that integrates the power of Hadoop and Spark with global event streaming, real-time database capabilities, and enterprise storage. We are looking for talented engineers to help support Platform functionality on customers\u2019 side and make it the platform of choice for operational and analytic big data use-cases.""]}",,"Required skills * Strong Linux system administrator skills (3+ Yrs)* Working knowledge of Hadoop components *Knowledge of file system and Linux Os internals latency, throughput, availability, consistency, security, etc.*Ability to communicate technical concepts clearly and effectively.* Strong troubleshooting and debugging skills, with a passion for problem-solving and investigation* Ability and willingness to learn new technologies*Good is written and verbal communication skills in English As a plus *Shell scripting (Shell, Awk) ability to trace (read/understand) Java/ C++*SQL/ NoSQL Databases knowledge*familiar with different security technologies ( Kerberos, SSL, etc.)*understanding of Java/ JVM concepts is a plus*prior technical support experience is a plus Project description One of the biggest companies in the Hadoop world provides industry‚Äôs only converged data platform that integrates the power of Hadoop and Spark with global event streaming, real-time database capabilities, and enterprise storage. We are looking for talented engineers to help support Platform functionality on customers‚Äô side and make it the platform of choice for operational and analytic big data use-cases.","Java/Hadoop Application L3¬†Support Engineer@CyberVision, Inc.",https://jobs.dou.ua/companies/cybervision/vacancies/131884/, Kyiv,Java/Hadoop Application L3¬†Support Engineer,15 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/ciklum/,Ciklum,{},,"On behalf of Ciklum Digital, we are looking for a Data Architect to join our Ukrainian team (Kyiv, Dnipro, Kharkiv, Lviv, Vinnytsia, Odessa ) on a full-time basis to create innovative solutions that exceed the needs of our customers.",Data Architect for Ciklum Digital@Ciklum,https://jobs.dou.ua/companies/ciklum/vacancies/131882/," Kyiv, Kharkiv, Lviv, Dnipro, Odesa, Vinnytsia",Data Architect for Ciklum Digital,15 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/parimatch-tech/,Parimatch Tech,{},,"‚Äî Responsible for products and projects delivery;‚Äî Responsible for change and improvements in day-to-day activities;‚Äî Responsible for definition and specification of the quality for product requirements;‚Äî Collaboration with Dev, QA and other PM‚Äôs to establish processes, procedures and standards;‚Äî Constant achievement of product delivery operational objectives. ‚Äî Responsible for motivation of Dev Leads team;‚Äî Responsible for Evaluation (marks, goals);‚Äî Responsible for Leads, Dev team members career development;‚Äî Determine staff training needs. ‚Äî Investigation and communication of customers complaints;‚Äî Dev resource management;‚Äî Enforcement of policies and procedures across stream;‚Äî Dev team representation and advocating on different stakeholders meetings. ‚Äî 3+ years of experience in managing software development projects;‚Äî Proven project management and planning skills and a history of delivering projects on time;‚Äî High level of responsibility, adherence to deadlines;‚Äî Deep understanding of Change and RISK management;‚Äî Minimum 2 years of hands-on experience in Agile/Scrum;‚Äî Hands-on experience using task tracking systems, ideally Jira.",Middle Project Manager for Data and Analytics Stream@Parimatch Tech,https://jobs.dou.ua/companies/parimatch-tech/vacancies/131867/, Kyiv,Middle Project Manager for Data and Analytics Stream,15 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/data-science-ua/,Data Science UA,"{""Required skills"": [""Experience with the web (a large number of events);"", ""Successful cases of working with an impersonal audience (user profiling, clustering, look-a-like);"", ""Deep knowledge of the principles and basic algorithms of machine learning, model quality assessments;"", ""Experience in building and outputting models in production;"", ""Working with Spark (PySpark).""], ""As a plus"": [""Experience with online advertising""], ""We offer"": [""New office;"", ""English language courses with a native speaker;"", ""Flexible approach to the schedule. The main thing for us is the result and the involvement;"", ""Corporate participation in sports, eco- and social projects;"", ""Premium level health insurance package.""], ""Responsibilities"": [""Determination of user data to be used;"", ""Creation of a model for building user profiles;"", ""Audience clustering for ad personalization;"", ""Optimization of the conversion of displayed ads.""], ""Project description"": [""An international leader in the native advertising market, founded in 2008. The platform helps the media monetize the audience, and the brands"", ""to convey the correct advertising message to its consumers.""]}",,"Required skills ‚Äî Experience with the web (a large number of events);‚Äî Successful cases of working with an impersonal audience (user profiling, clustering, look-a-like);‚Äî Deep knowledge of the principles and basic algorithms of machine learning, model quality assessments;‚Äî Experience in building and outputting models in production;‚Äî Working with Spark (PySpark). As a plus ‚Äî Experience with online advertising We offer ‚Äî New office;‚Äî English language courses with a native speaker;‚Äî Flexible approach to the schedule. The main thing for us is the result and the involvement;‚Äî Corporate participation in sports, eco- and social projects;‚Äî Premium level health insurance package. Responsibilities ‚Äî Determination of user data to be used;‚Äî Creation of a model for building user profiles;‚Äî Audience clustering for ad personalization;‚Äî Optimization of the conversion of displayed ads. Project description An international leader in the native advertising market, founded in 2008. The platform helps the media monetize the audience, and the brands ‚Äî to convey the correct advertising message to its consumers. The company offers comprehensive solutions: from planning and developing an advertising strategy to its implementation and optimization. This is the company:‚Äî One of the largest martech-companies in the Ukrainian market;‚Äî Own Highload platform that provides 185 billion content recommendations for 850 million unique visitors in more than 60 languages;‚Äî Winner of numerous awards for innovation and product quality in the AdTech field.",Senior/Lead Data Scientist@Data Science UA,https://jobs.dou.ua/companies/data-science-ua/vacancies/131857/, Kyiv,Senior/Lead Data Scientist,15 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/wix/,WIX.com,{},,"We are: The Wix Business Analysis (BA) group. We are 130+ business analysts who strongly believe in using analytics to understand how our users interact with our products. With over 200 million users, we collect over a billion events a day‚Äîthat‚Äôs 3TB of data added daily! We do it all using cutting-edge technology to process, store, and query our data, including tools we develop ourselves as well as tools like Presto and Amazon S3. We‚Äôre looking for a Junior Data Analyst to join our team and learn how to work with big data, conduct and analyze A/B Tests and see beyond the numbers. You are: A graduate with BSc/BA in a highly quantitative field. You have high-level analytical skills and strong desire to learn new technologies and bring valuable contribution to a product you work with. You have theoretical knowledge of SQL and are familiar with A/B Testing theory. Bonus points if you have understanding of online business in general, and customer funnel, behavior and retention in particular. As a Junior Data Analyst, you will: ‚Ä¢ Be given a dedicated mentor, practice with cutting-edge data & visualization technologies.‚Ä¢ Explore, analyze and process massive amounts of data, and learn how to turn it into actionable business insights.‚Ä¢ Learn how to design, develop and monitor analytical reports and dashboards.",Junior Data Analyst@WIX.com,https://jobs.dou.ua/companies/wix/vacancies/128739/, Kyiv,Junior Data Analyst,15 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/data-science-ua/,Data Science UA,"{""Required skills"": [""confirmed track record in product development / analytics / finance"", ""hands-on experience with analytics:"", ""solid knowledge of A/B, A/B/n, A/A tests"", ""R / Python"", ""SQL"", ""Excel"", ""Tableau / Power BI / etc.""], ""As a plus"": [""Nice to Have:"", ""marketing / sales analytics (funnels, AO, volume forecasts, etc.)"", ""hands-on experience in conducting marketing researches / customer development"", ""understanding of premium niche: freemium games, gambling, dating"", ""understanding market-place."", ""experience in UI / UX:"", ""basic principles"", ""audience best practices"", ""focus control"", ""\u201cHooked\u201d or any other framework""], ""We offer"": [""Working conditions and social package:"", ""flexible schedule."", ""free breakfasts, lunches and snacks."", ""professional development."", ""compensation for sports activities (there is a corporate football team)."", ""medical insurance policy for all Elite level employees."", ""corporate doctor."", ""access to useful literature, trainings, seminars."", ""participation in key events of the IT industry (both in Ukraine and abroad)."", ""we work on FOP, group 3. The accountant opens, leads and advises FOP."", ""working in a cool team. We pay special attention to the search for talents and their development, so all employees are active and motivated to solve problems outside the box."", ""new large and spacious office in Podil (Kirillovskaya st., 40A. 5 minutes from metro Taras Shevchenko), PlayStation 4 Pro and other \u201cgoodies\u201d.""], ""Responsibilities"": [""Analytics Duties:"", ""monitor product funnel"", ""data analytics researches: retention, communication"", ""tight integration with marketing: reports, custom analytics"", ""increase speed of RnD team"", ""ownership over product development"", ""consistent approach to feature release cycle"", ""You manage all project\u2019s analytics"", ""You know how Facebook Ads works: auction, bidding, analytics"", ""You actively participate in product development:"", ""feature planning"", ""dev processes"", ""design review""], ""Project description"": [""Product Duties (depends on profile):"", ""generate product hypothesis"", ""design MVP, feature, split test"", ""review UI / UX"", ""competitors research""]}",,"Required skills ‚Äî confirmed track record in product development / analytics / finance‚Äî hands-on experience with analytics:‚Äî solid knowledge of A/B, A/B/n, A/A tests‚Äî R / Python‚Äî SQL‚Äî Excel‚Äî Tableau / Power BI / etc. As a plus Nice to Have:‚Äî marketing / sales analytics (funnels, AO, volume forecasts, etc.)‚Äî hands-on experience in conducting marketing researches / customer development‚Äî understanding of premium niche: freemium games, gambling, dating‚Äî understanding market-place.‚Äî experience in UI / UX:‚Äî basic principles‚Äî audience best practices‚Äî focus control‚Äî ‚ÄúHooked‚Äù or any other framework We offer Working conditions and social package:‚Äî flexible schedule.‚Äî free breakfasts, lunches and snacks.‚Äî professional development.‚Äî compensation for sports activities (there is a corporate football team).‚Äî medical insurance policy for all Elite level employees.‚Äî corporate doctor.‚Äî access to useful literature, trainings, seminars.‚Äî participation in key events of the IT industry (both in Ukraine and abroad).‚Äî we work on FOP, group 3. The accountant opens, leads and advises FOP.‚Äî working in a cool team. We pay special attention to the search for talents and their development, so all employees are active and motivated to solve problems outside the box.‚Äî new large and spacious office in Podil (Kirillovskaya st., 40A. 5 minutes from metro Taras Shevchenko), PlayStation 4 Pro and other ‚Äúgoodies‚Äù. Responsibilities Analytics Duties:‚Äî monitor product funnel‚Äî data analytics researches: retention, communication‚Äî tight integration with marketing: reports, custom analytics‚Äî increase speed of RnD team‚Äî ownership over product development‚Äî consistent approach to feature release cycle‚Äî You manage all project‚Äôs analytics‚Äî You know how Facebook Ads works: auction, bidding, analytics‚Äî You actively participate in product development:‚Äî feature planning‚Äî dev processes‚Äî design review Product Duties (depends on profile):‚Äî generate product hypothesis‚Äî design MVP, feature, split test‚Äî review UI / UX‚Äî competitors research Project Duties (depends on profile):‚Äî feature cycle planning‚Äî dev flow control Project description Our partenr is a global IT company in the industry of Social Discovery. We developing our own products used by millions of users worldwide. Also our company is a leaders in the market, enjoying triple-digit growth for all major KPIs.",Product data analyst@Data Science UA,https://jobs.dou.ua/companies/data-science-ua/vacancies/128481/, Kyiv,Product data analyst,14 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/ciklum/,Ciklum,{},,"On behalf of Ciklum Digital, Ciklum is looking for a Technical Lead Data Scientist to join the UA team on a full-time basis. You will join a highly motivated team and will be working on modern solutions for our clients. We are looking for technology experts who want to make an impact on new business by applying best practices and taking ownership.",Technical Lead Data Scientist for Ciklum Digital (200003VL)@Ciklum,https://jobs.dou.ua/companies/ciklum/vacancies/131782/," Kyiv, Lviv",Technical Lead Data Scientist for Ciklum Digital (200003VL),14 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/evoplay/,EVOPLAY,"{""Required skills"": [""\u0421\u0438\u043b\u044c\u043d\u044b\u0435 \u0430\u043d\u0430\u043b\u0438\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u043d\u0430\u0432\u044b\u043a\u0438 \u0438 \u043d\u0430\u0432\u044b\u043a\u0438 \u0438\u043d\u0442\u0435\u0440\u043f\u0440\u0435\u0442\u0430\u0446\u0438\u0438 \u0434\u0430\u043d\u043d\u044b\u0445.\u041e\u043f\u044b\u0442 \u0440\u0430\u0431\u043e\u0442\u044b \u0441 Python for DS (Pandas, NumPy, bs4, Selenium, Sklearn, SciPy, Keras).\u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0434\u0430\u043d\u043d\u044b\u0445 (plotly, matplotlib etc...).\u041e\u043f\u044b\u0442 \u0440\u0430\u0431\u043e\u0442\u044b \u0441 \u0441\u044b\u0440\u044b\u043c\u0438 \u0434\u0430\u043d\u043d\u044b\u043c\u0438, \u043f\u043e\u043d\u0438\u043c\u0430\u043d\u0438\u0435 \u043a\u0440\u0438\u0442\u0435\u0440\u0438\u0435\u0432 \u043e\u0447\u0438\u0441\u0442\u043a\u0438, data mining.\u041f\u0440\u0430\u043a\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u043e\u043f\u044b\u0442 \u0438 \u0445\u043e\u0440\u043e\u0448\u0435\u0435 \u043f\u043e\u043d\u0438\u043c\u0430\u043d\u0438\u0435 \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u0445 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u043e\u0432 \u043c\u0430\u0448\u0438\u043d\u043d\u043e\u0433\u043e \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f (log reg, linear regression, neural net, time series analysis).\u0425\u043e\u0440\u043e\u0448\u0435\u0435 \u043f\u043e\u043d\u0438\u043c\u0430\u043d\u0438\u0435 \u0433\u043b\u0443\u0431\u0438\u043d\u043d\u044b\u0445 \u043e\u0441\u043d\u043e\u0432 \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u043c\u0435\u0442\u043e\u0434\u043e\u0432.\u0417\u043d\u0430\u043d\u0438\u044f \u0442\u0435\u043e\u0440\u0438\u0438 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0438 \u0438 \u0432\u044b\u0441\u0448\u0435\u0439 \u043c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u043a\u0438 \u043d\u0430 \u0432\u044b\u0441\u043e\u043a\u043e\u043c \u0443\u0440\u043e\u0432\u043d\u0435.\u041f\u043e\u043d\u0438\u043c\u0430\u043d\u0438\u0435 \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u0438, \u0438\u043d\u0442\u0435\u0440\u043f\u0440\u0435\u0442\u0438\u0440\u0443\u0435\u043c\u043e\u0441\u0442\u0438, \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432,\u0438 \u0434\u0430\u043b\u044c\u043d\u0435\u0439\u0448\u0435\u0433\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u044f \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432 \u043c\u043e\u0434\u0435\u043b\u0438.""], ""As a plus"": [""\u041e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u0435 \u0432 \u043e\u0431\u043b\u0430\u0441\u0442\u0438 \u043c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u043a\u0438, \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u043a\u0438, computer science \u0438\u043b\u0438 \u0441\u043c\u0435\u0436\u043d\u044b\u0445 \u0442\u0435\u0445\u043d\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u0434\u0438\u0441\u0446\u0438\u043f\u043b\u0438\u043d.\u041e\u043f\u044b\u0442 \u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u0440\u0430\u0431\u043e\u0447\u0438\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u0434\u043b\u044f \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432 \u0441\u043f\u043e\u0440\u0442\u0438\u0432\u043d\u044b\u0445 \u0434\u0438\u0441\u0446\u0438\u043f\u043b\u0438\u043d (football, basketball, csgo, dota2, fortnite etc).\u0420\u0430\u0431\u043e\u0442\u0430 \u0441 \u0431\u0430\u0437\u0430\u043c\u0438 \u0434\u0430\u043d\u043d\u044b\u0445: MongoDB, PostgreSQL.\u0417\u043d\u0430\u043d\u0438\u0435 \u044f\u0437\u044b\u043a\u0430 Golang.""], ""We offer"": [""\u041e\u0444\u0438\u0441 \u0432 \u043f\u0430\u0440\u043a\u0435 \u043c. \u0428\u0443\u043b\u044f\u0432\u0441\u043a\u0430\u044f\u041c\u043e\u043b\u043e\u0434\u0430\u044f \u0434\u0440\u0443\u0436\u043d\u0430\u044f \u043a\u043e\u043c\u043f\u0430\u043d\u0438\u044f, \u043c\u0438\u043d\u0438\u043c\u0443\u043c \u0431\u044e\u0440\u043e\u043a\u0440\u0430\u0442\u0438\u0438.\u0413\u0438\u0431\u043a\u0438\u0439 \u0433\u0440\u0430\u0444\u0438\u043a (\u043d\u0430\u0447\u0430\u043b\u043e \u0440\u0430\u0431\u043e\u0442\u044b \u0441 8 \u0434\u043e 11, 8 \u0447/\u0434\u0435\u043d\u044c).\u0421\u043e\u0446.\u043f\u0430\u043a\u0435\u0442, 100% \u043e\u043f\u043b\u0430\u0442\u0430 \u043e\u0442\u043f\u0443\u0441\u043a\u043e\u0432 \u0438 \u0431\u043e\u043b\u044c\u043d\u0438\u0447\u043d\u044b\u0445.\u0411\u0435\u0441\u043f\u043b\u0430\u0442\u043d\u0430\u044f \u043f\u0430\u0440\u043a\u043e\u0432\u043a\u0430.\u0411\u0435\u0441\u043f\u043b\u0430\u0442\u043d\u044b\u0435 \u0443\u0440\u043e\u043a\u0438 \u0430\u043d\u0433\u043b\u0438\u0439\u0441\u043a\u043e\u0433\u043e \u044f\u0437\u044b\u043a\u0430 \u0441\u043e \u0441\u0432\u043e\u0438\u043c \u0443\u0447\u0438\u0442\u0435\u043b\u0435\u043c (3 \u0447\u0430\u0441\u0430 \u0432 \u043d\u0435\u0434\u0435\u043b\u044e).\u041a\u0443\u0445\u043d\u044f \u0432 \u043e\u0444\u0438\u0441\u0435, \u0441\u043e \u0432\u0441\u0435\u043c\u0438 \u0441\u043e\u043f\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u043c\u0438 \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u0430\u043c\u0438.""], ""Responsibilities"": [""\u0421\u0431\u043e\u0440, \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0438 \u043f\u0440\u0435\u0434\u0432\u0430\u0440\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0439 \u0430\u043d\u0430\u043b\u0438\u0437 \u0434\u0430\u043d\u043d\u044b\u0445.\u0424\u043e\u0440\u043c\u0443\u043b\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u0438 \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0430 \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u0433\u0438\u043f\u043e\u0442\u0435\u0437, \u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0435\u0439, \u043a\u0430\u043a \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u043a\u043b\u0430\u0441\u0441\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u043c\u0435\u0442\u043e\u0434\u043e\u0432 \u0442\u0430\u043a \u0438 ML \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u043e\u0432.\u0423\u043b\u0443\u0447\u0448\u0435\u043d\u0438\u0435 \u0443\u0436\u0435 \u0433\u043e\u0442\u043e\u0432\u044b\u0445 \u043c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439.\u0422\u044e\u043d\u0438\u043d\u0433 \u0438 \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u044f, \u0432\u043d\u0435\u0434\u0440\u0435\u043d\u0438\u0435 \u0438 \u0434\u0430\u043b\u044c\u043d\u0435\u0439\u0448\u0430\u044f \u043f\u043e\u0434\u0434\u0435\u0440\u0436\u043a\u0430 \u0432 \u043f\u0440\u043e\u0434\u0430\u043a\u0448\u0435\u043d\u0435 \u043c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439.""], ""Project description"": [""\u041f\u043b\u0430\u0442\u0444\u043e\u0440\u043c\u0430 \u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0430 \u043d\u0430 \u0431\u0430\u0437\u0435 \u043c\u0438\u043a\u0440\u043e\u0441\u0435\u0440\u0432\u0438\u0441\u043e\u0432, \u0441\u0435\u0439\u0447\u0430\u0441 \u044d\u0442\u043e \u043e\u043a\u043e\u043b\u043e \u0441\u043e\u0442\u043d\u0438 \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u0445 \u0441\u0435\u0440\u0432\u0438\u0441\u043e\u0432. \u041e\u0431\u0441\u043b\u0443\u0436\u0438\u0432\u0430\u0435\u0442 \u043f\u043b\u0430\u0442\u0444\u043e\u0440\u043c\u0430 \u043e\u043a\u043e\u043b\u043e \u0434\u0432\u0443\u0445 \u0434\u0435\u0441\u044f\u0442\u043a\u043e\u0432 \u0432\u044b\u0441\u043e\u043a\u043e\u043d\u0430\u0433\u0440\u0443\u0436\u0435\u043d\u043d\u044b\u0445 \u043c\u0438\u0440\u043e\u0432\u044b\u0445 \u0438\u0437\u0432\u0435\u0441\u0442\u043d\u044b\u0445 \u043f\u043e\u0440\u0442\u0430\u043b\u043e\u0432. \u0418\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u0441\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435 \u043e\u0431\u043b\u0430\u0447\u043d\u044b\u0435 \u0442\u0435\u0445\u043d\u043e\u043b\u043e\u0433\u0438\u0438 \u0432 \u043f\u0440\u043e\u0434\u0430\u043a\u0448\u0435\u043d, \u043a\u043e\u043d\u0442\u0435\u0439\u043d\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u044f.""]}",,"Required skills –°–∏–ª—å–Ω—ã–µ –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ –Ω–∞–≤—ã–∫–∏ –∏ –Ω–∞–≤—ã–∫–∏ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö.–û–ø—ã—Ç —Ä–∞–±–æ—Ç—ã —Å Python for DS (Pandas, NumPy, bs4, Selenium, Sklearn, SciPy, Keras).–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö (plotly, matplotlib etc...).–û–ø—ã—Ç —Ä–∞–±–æ—Ç—ã —Å —Å—ã—Ä—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏, –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤ –æ—á–∏—Å—Ç–∫–∏, data mining.–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –æ–ø—ã—Ç –∏ —Ö–æ—Ä–æ—à–µ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è (log reg, linear regression, neural net, time series analysis).–•–æ—Ä–æ—à–µ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –≥–ª—É–±–∏–Ω–Ω—ã—Ö –æ—Å–Ω–æ–≤ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –º–µ—Ç–æ–¥–æ–≤.–ó–Ω–∞–Ω–∏—è —Ç–µ–æ—Ä–∏–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –∏ –≤—ã—Å—à–µ–π –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏ –Ω–∞ –≤—ã—Å–æ–∫–æ–º —É—Ä–æ–≤–Ω–µ.–ü–æ–Ω–∏–º–∞–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏, –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç–∏, —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–π –≤–∞–∂–Ω–æ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤,–∏ –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –º–æ–¥–µ–ª–∏. As a plus –û–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ –æ–±–ª–∞—Å—Ç–∏ –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏, —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏, computer science –∏–ª–∏ —Å–º–µ–∂–Ω—ã—Ö —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –¥–∏—Å—Ü–∏–ø–ª–∏–Ω.–û–ø—ã—Ç —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ —Ä–∞–±–æ—á–∏—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å–ø–æ—Ä—Ç–∏–≤–Ω—ã—Ö –¥–∏—Å—Ü–∏–ø–ª–∏–Ω (football, basketball, csgo, dota2, fortnite etc).–†–∞–±–æ—Ç–∞ —Å –±–∞–∑–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö: MongoDB, PostgreSQL.–ó–Ω–∞–Ω–∏–µ —è–∑—ã–∫–∞ Golang. We offer –û—Ñ–∏—Å –≤ –ø–∞—Ä–∫–µ –º. –®—É–ª—è–≤—Å–∫–∞—è–ú–æ–ª–æ–¥–∞—è –¥—Ä—É–∂–Ω–∞—è –∫–æ–º–ø–∞–Ω–∏—è, –º–∏–Ω–∏–º—É–º –±—é—Ä–æ–∫—Ä–∞—Ç–∏–∏.–ì–∏–±–∫–∏–π –≥—Ä–∞—Ñ–∏–∫ (–Ω–∞—á–∞–ª–æ —Ä–∞–±–æ—Ç—ã —Å 8 –¥–æ 11, 8 —á/–¥–µ–Ω—å).–°–æ—Ü.–ø–∞–∫–µ—Ç, 100% –æ–ø–ª–∞—Ç–∞ –æ—Ç–ø—É—Å–∫–æ–≤ –∏ –±–æ–ª—å–Ω–∏—á–Ω—ã—Ö.–ë–µ—Å–ø–ª–∞—Ç–Ω–∞—è –ø–∞—Ä–∫–æ–≤–∫–∞.–ë–µ—Å–ø–ª–∞—Ç–Ω—ã–µ —É—Ä–æ–∫–∏ –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ —Å–æ —Å–≤–æ–∏–º —É—á–∏—Ç–µ–ª–µ–º (3 —á–∞—Å–∞ –≤ –Ω–µ–¥–µ–ª—é).–ö—É—Ö–Ω—è –≤ –æ—Ñ–∏—Å–µ, —Å–æ –≤—Å–µ–º–∏ —Å–æ–ø—É—Ç—Å—Ç–≤—É—é—â–∏–º–∏ —ç–ª–µ–º–µ–Ω—Ç–∞–º–∏. Responsibilities –°–±–æ—Ä, –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö.–§–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –≥–∏–ø–æ—Ç–µ–∑, –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π, –∫–∞–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏—Ö —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –º–µ—Ç–æ–¥–æ–≤ —Ç–∞–∫ –∏ ML –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤.–£–ª—É—á—à–µ–Ω–∏–µ —É–∂–µ –≥–æ—Ç–æ–≤—ã—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π.–¢—é–Ω–∏–Ω–≥ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è, –≤–Ω–µ–¥—Ä–µ–Ω–∏–µ –∏ –¥–∞–ª—å–Ω–µ–π—à–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞ –≤ –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π. Project description –ü–ª–∞—Ç—Ñ–æ—Ä–º–∞ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∞ –Ω–∞ –±–∞–∑–µ –º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å–æ–≤, —Å–µ–π—á–∞—Å —ç—Ç–æ –æ–∫–æ–ª–æ —Å–æ—Ç–Ω–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å–µ—Ä–≤–∏—Å–æ–≤. –û–±—Å–ª—É–∂–∏–≤–∞–µ—Ç –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –æ–∫–æ–ª–æ –¥–≤—É—Ö –¥–µ—Å—è—Ç–∫–æ–≤ –≤—ã—Å–æ–∫–æ–Ω–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –º–∏—Ä–æ–≤—ã—Ö –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –ø–æ—Ä—Ç–∞–ª–æ–≤. –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–±–ª–∞—á–Ω—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –≤ –ø—Ä–æ–¥–∞–∫—à–µ–Ω, –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∏–∑–∞—Ü–∏—è.",Data scientist@EVOPLAY,https://jobs.dou.ua/companies/evoplay/vacancies/128293/, Kyiv,Data scientist,14 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/newxel/,Newxel,"{""Required skills"": [""3+ years of experience with DB`s (SQL and/or NoSql)"", ""Hands-on experience with Python (Pandas or similar) and ETL stuff"", ""At least an Intermediate level of English""], ""As a plus"": [""Experience with RedShift, BigQuery, Airflow, Streaming process""], ""We offer"": [""Adequate management;"", ""Competitive salary;"", ""Flexible working schedule;"", ""Modern and comfortable office near the Vystavkovyi center.""], ""Responsibilities"": [""Own projects from initial discussions to release, including data exploration, architecture design, the benchmark of new technologies and product feedback;"", ""Leverage our data pipelines and create new ones, making sure the architecture will support the business requirements;"", ""Manage the development of distributed data pipelines and complex software designs to support high data rates using cloud-based tools & Python;"", ""Make data exploration in order to gather insights as part of the data processing development;"", ""Work closely with game analysts, data scientists and other key roles on the various data processes;"", ""Develop unit and integration test procedures.""], ""Project description"": [""Crazy Labs is a top 10 mobile games publisher and with over 3 billion downloads to date, we are now a worldwide leader in casual games development, distribution and innovation, looking for an excellent Mobile Developer, to join our best of breed framework Development Team.""]}",,"Required skills ‚Äî 3+ years of experience with DB`s (SQL and/or NoSql)‚Äî Hands-on experience with Python (Pandas or similar) and ETL stuff‚Äî At least an Intermediate level of English As a plus ‚Äî Experience with RedShift, BigQuery, Airflow, Streaming process We offer ‚Äî Adequate management;‚Äî Competitive salary;‚Äî Flexible working schedule;‚Äî Modern and comfortable office near the Vystavkovyi center. Responsibilities ‚Äî Own projects from initial discussions to release, including data exploration, architecture design, the benchmark of new technologies and product feedback;‚Äî Leverage our data pipelines and create new ones, making sure the architecture will support the business requirements;‚Äî Manage the development of distributed data pipelines and complex software designs to support high data rates using cloud-based tools & Python;‚Äî Make data exploration in order to gather insights as part of the data processing development;‚Äî Work closely with game analysts, data scientists and other key roles on the various data processes;‚Äî Develop unit and integration test procedures. Project description Crazy Labs is a top 10 mobile games publisher and with over 3 billion downloads to date, we are now a worldwide leader in casual games development, distribution and innovation, looking for an excellent Mobile Developer, to join our best of breed framework Development Team.",SQL/DB developer for Top-10 Mobile Games Publisher@Newxel,https://jobs.dou.ua/companies/newxel/vacancies/131731/, Kyiv,SQL/DB developer for Top-10 Mobile Games Publisher,14 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/newxel/,Newxel,"{""Required skills"": [""3+ years of experience with Python or Scala or Java"", ""Experience in AWS and/or GCP"", ""Experience with BigQuery, Airflow, Docker, and streaming process"", ""At least an Intermediate level of English""], ""As a plus"": [""Experience with Java, ETL, Pandas""], ""We offer"": [""Adequate management;"", ""Competitive salary;"", ""Flexible working schedule;"", ""Modern and comfortable office near the Vystavkovyi center.""], ""Responsibilities"": [""Own projects from initial discussions to release, including data exploration, architecture design, the benchmark of new technologies and product feedback;"", ""Leverage our data pipelines and create new ones, making sure the architecture will support the business requirements;"", ""Manage the development of distributed data pipelines and complex software designs to support high data rates using cloud-based tools & Python;"", ""Make data exploration in order to gather insights as part of the data processing development;"", ""Work closely with game analysts, data scientists and other key roles on the various data processes;"", ""Develop unit and integration test procedures.""], ""Project description"": [""Crazy Labs is a top 10 mobile games publisher and with over 3 billion downloads to date, we are now a worldwide leader in casual games development, distribution and innovation, looking for an excellent Mobile Developer, to join our best of breed framework Development Team.""]}",,"Required skills ‚Äî 3+ years of experience with Python or Scala or Java‚Äî Experience in AWS and/or GCP‚Äî Experience with BigQuery, Airflow, Docker, and streaming process‚Äî At least an Intermediate level of English As a plus ‚Äî Experience with Java, ETL, Pandas We offer ‚Äî Adequate management;‚Äî Competitive salary;‚Äî Flexible working schedule;‚Äî Modern and comfortable office near the Vystavkovyi center. Responsibilities ‚Äî Own projects from initial discussions to release, including data exploration, architecture design, the benchmark of new technologies and product feedback;‚Äî Leverage our data pipelines and create new ones, making sure the architecture will support the business requirements;‚Äî Manage the development of distributed data pipelines and complex software designs to support high data rates using cloud-based tools & Python;‚Äî Make data exploration in order to gather insights as part of the data processing development;‚Äî Work closely with game analysts, data scientists and other key roles on the various data processes;‚Äî Develop unit and integration test procedures. Project description Crazy Labs is a top 10 mobile games publisher and with over 3 billion downloads to date, we are now a worldwide leader in casual games development, distribution and innovation, looking for an excellent Mobile Developer, to join our best of breed framework Development Team.",Data Engineer for International gaming company@Newxel,https://jobs.dou.ua/companies/newxel/vacancies/122126/, Kyiv,Data Engineer for International gaming company,14 September 2020,,2020-10-13,,dou
https://jobs.dou.ua/companies/visiquate/,VisiQuate,"{""Required skills"": [""At least Upper-intermediate level of English;"", ""Able to create high value and high readability requirements specification;"", ""Able to support development team with domain knowledge;"", ""Demonstrate the ability to compile functional and non-functional requirements;"", ""Experienced in working on data warehouse/BI projects."", ""Knowledge in SQL/Excel/Access to be able to analyze data;"", ""UX/Usability principles understanding;"", ""Knowledge of Agile/Scrum;"", ""EDI format knowledge;""], ""As a plus"": [""US Healthcare finance knowledge.""], ""We offer"": [""Comfortable office in the city center;"", ""24 calendar days of paid vacation, paid sick leaves;"", ""Holidays per Ukrainian calendar;"", ""Flexible work schedule and a friendly atmosphere in the office;"", ""Free English classes.""], ""Responsibilities"": [""Drive the process from idea to implementation;"", ""Requirements elicitation/clarification;"", ""Write specifications;"", ""Update system documentation;"", ""Clarifying priorities;"", ""Communicating status and blockers;"", ""Tight collaboration with the development and design teams;"", ""Organizing and performing grooming sessions, supporting planning meetings;"", ""Performing acceptance testing"", ""ensuring the provided functionality satisfies customer needs/requirements;"", ""Making recommended go/no go decision.""], ""Project description"": [""We\u2019re a fast growing California-based Big Data company that is focused on disrupting business intelligence and analytics as currently defined. Our solutions make a difference because we\u2019re different.""]}",,"Required skills ‚Ä¢ At least Upper-intermediate level of English;‚Ä¢ Able to create high value and high readability requirements specification;‚Ä¢ Able to support development team with domain knowledge;‚Ä¢ Demonstrate the ability to compile functional and non-functional requirements;‚Ä¢ Experienced in working on data warehouse/BI projects.‚Ä¢ Knowledge in SQL/Excel/Access to be able to analyze data;‚Ä¢ UX/Usability principles understanding;‚Ä¢ Knowledge of Agile/Scrum;‚Ä¢ EDI format knowledge; As a plus ‚Ä¢ US Healthcare finance knowledge. We offer ‚Ä¢ Comfortable office in the city center;‚Ä¢ 24 calendar days of paid vacation, paid sick leaves;‚Ä¢ Holidays per Ukrainian calendar;‚Ä¢ Flexible work schedule and a friendly atmosphere in the office;‚Ä¢ Free English classes. Responsibilities ‚Ä¢ Drive the process from idea to implementation;‚Ä¢ Requirements elicitation/clarification;‚Ä¢ Write specifications;‚Ä¢ Update system documentation;‚Ä¢ Clarifying priorities;‚Ä¢ Communicating status and blockers;‚Ä¢ Tight collaboration with the development and design teams;‚Ä¢ Organizing and performing grooming sessions, supporting planning meetings;‚Ä¢ Performing acceptance testing ‚Äî ensuring the provided functionality satisfies customer needs/requirements;‚Ä¢ Making recommended go/no go decision. Project description We‚Äôre a fast growing California-based Big Data company that is focused on disrupting business intelligence and analytics as currently defined. Our solutions make a difference because we‚Äôre different. Instead of typical BI that delivers static dashboards based on lagging indicators, we give our clients streaming intelligence. Your job will be to help design our vision of delivering Insights as a Service, Prediction as a Service, even Prevention as a Service: All the things that let us describe ourselves as the first SaaS2 company.",Business Data Analyst@VisiQuate,https://jobs.dou.ua/companies/visiquate/vacancies/122236/, Kharkiv,Business Data Analyst,13 September 2020,,2020-10-13,,dou
