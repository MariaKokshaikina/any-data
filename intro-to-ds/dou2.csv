company_name,company_link,position,link,location,publish_date,salary,full_text,source,scraping_date,key,employment_type,seniority_level,description
JatApp,https://jobs.dou.ua/companies/jatapp/,Senior Data Analyst,https://jobs.dou.ua/companies/jatapp/vacancies/138227/?from=list_hot, Kyiv,10 November 2020,,"Required skills — 2+ года опыта в проуктовой/маркетинговой аналитеке веб-продуктов и мобильных приложений;— Опыт работы с Excel, BI и визуализации данних (Tableau);— Сильные знания SQL, достаточные для построения воронок, когорт и удержания сырых данных;— Опыт в ETL процессе;— Опыт построения аналитической инфраструктуры, а так же инфраструктуры баз данных с нуля;— Понимание методов отслеживания пользователей — cookies, redirects, utm_ tags, и т. д;— Владение английским на уровне intermediate и выше. As a plus — Знание R/Python желательно, но не обязательно. We offer — Офис в пешей доступности от метро;— Гибкий график (с 9 до 11 часов, рабочий день 8 часов);— Оплачиваемые больничные листы и 20 оплачиваемых отпускных дней;— Корпоративные курсы английского языка, курсы повышения квалификации;— Корпоративная библиотека;— Бесплатные ежедневные обеды и неограниченное количество кофе / чая / молока;— 100% компенсация тренажерного зала;— Медицинская страховка. Порекомендуйте кандидата и получите бонус до 1000 $! Просто заполните формуbit.ly/3keuT0J Responsibilities — Создание внутренней системы аналитики для веб-приложений и мобильных приложений с нуля;— Обеспечение аналитической поддержки продуктовых и маркетинговых команд;— Генерация гипотез продукта с целью увеличения показателей активности и монетизации;— A / B-тестирование, настраивание, контроль и отслеживание событий, анализирование данных, работа с выводами;— Разработка модели прогноза LTV пользователей;— Контроль правильной работы всей системы слежения;— Настройка автоматических отчетов в Tableau для маркетинговых и продуктовых команд;— Анализ показателей качества маркетинга и трафика для снижения стоимости трафика и повышения рентабельности инвестиций в маркетинг. Project description JatApp — это группа успешных компаний, разрабатывающих собственные продукты и занимающихся аутсорсингом. Наш собственный портфель продуктов включает в себя множество веб-продуктов и мобильных приложений в сферах коммунальных услуг, социальных сетей, образования, бизнеса и безопасности. Мы гордимся тем, что более 20 миллионов пользователей ежедневно взаимодействуют с нашими творениями. Основная миссия продуктов JatApp — делать жизнь людей ярче и проще, предлагая передовые решения и лучший пользовательский интерфейс. Сейчас мы ищем в нашу команду опытного и увлеченного Senior Data Analyst!",dou,2020-11-12,Senior Data Analyst@JatApp,,,"{""Required skills"": [""2+ \u0433\u043e\u0434\u0430 \u043e\u043f\u044b\u0442\u0430 \u0432 \u043f\u0440\u043e\u0443\u043a\u0442\u043e\u0432\u043e\u0439/\u043c\u0430\u0440\u043a\u0435\u0442\u0438\u043d\u0433\u043e\u0432\u043e\u0439 \u0430\u043d\u0430\u043b\u0438\u0442\u0435\u043a\u0435 \u0432\u0435\u0431-\u043f\u0440\u043e\u0434\u0443\u043a\u0442\u043e\u0432 \u0438 \u043c\u043e\u0431\u0438\u043b\u044c\u043d\u044b\u0445 \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u0439;"", ""\u041e\u043f\u044b\u0442 \u0440\u0430\u0431\u043e\u0442\u044b \u0441 Excel, BI \u0438 \u0432\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 \u0434\u0430\u043d\u043d\u0438\u0445 (Tableau);"", ""\u0421\u0438\u043b\u044c\u043d\u044b\u0435 \u0437\u043d\u0430\u043d\u0438\u044f SQL, \u0434\u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u044b\u0435 \u0434\u043b\u044f \u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u044f \u0432\u043e\u0440\u043e\u043d\u043e\u043a, \u043a\u043e\u0433\u043e\u0440\u0442 \u0438 \u0443\u0434\u0435\u0440\u0436\u0430\u043d\u0438\u044f \u0441\u044b\u0440\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445;"", ""\u041e\u043f\u044b\u0442 \u0432 ETL \u043f\u0440\u043e\u0446\u0435\u0441\u0441\u0435;"", ""\u041e\u043f\u044b\u0442 \u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u044f \u0430\u043d\u0430\u043b\u0438\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u0438\u043d\u0444\u0440\u0430\u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u044b, \u0430 \u0442\u0430\u043a \u0436\u0435 \u0438\u043d\u0444\u0440\u0430\u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u044b \u0431\u0430\u0437 \u0434\u0430\u043d\u043d\u044b\u0445 \u0441 \u043d\u0443\u043b\u044f;"", ""\u041f\u043e\u043d\u0438\u043c\u0430\u043d\u0438\u0435 \u043c\u0435\u0442\u043e\u0434\u043e\u0432 \u043e\u0442\u0441\u043b\u0435\u0436\u0438\u0432\u0430\u043d\u0438\u044f \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439"", ""cookies, redirects, utm_ tags, \u0438 \u0442. \u0434;"", ""\u0412\u043b\u0430\u0434\u0435\u043d\u0438\u0435 \u0430\u043d\u0433\u043b\u0438\u0439\u0441\u043a\u0438\u043c \u043d\u0430 \u0443\u0440\u043e\u0432\u043d\u0435 intermediate \u0438 \u0432\u044b\u0448\u0435.""], ""As a plus"": [""\u0417\u043d\u0430\u043d\u0438\u0435 R/Python \u0436\u0435\u043b\u0430\u0442\u0435\u043b\u044c\u043d\u043e, \u043d\u043e \u043d\u0435 \u043e\u0431\u044f\u0437\u0430\u0442\u0435\u043b\u044c\u043d\u043e.""], ""We offer"": [""\u041e\u0444\u0438\u0441 \u0432 \u043f\u0435\u0448\u0435\u0439 \u0434\u043e\u0441\u0442\u0443\u043f\u043d\u043e\u0441\u0442\u0438 \u043e\u0442 \u043c\u0435\u0442\u0440\u043e;"", ""\u0413\u0438\u0431\u043a\u0438\u0439 \u0433\u0440\u0430\u0444\u0438\u043a (\u0441 9 \u0434\u043e 11 \u0447\u0430\u0441\u043e\u0432, \u0440\u0430\u0431\u043e\u0447\u0438\u0439 \u0434\u0435\u043d\u044c 8 \u0447\u0430\u0441\u043e\u0432);"", ""\u041e\u043f\u043b\u0430\u0447\u0438\u0432\u0430\u0435\u043c\u044b\u0435 \u0431\u043e\u043b\u044c\u043d\u0438\u0447\u043d\u044b\u0435 \u043b\u0438\u0441\u0442\u044b \u0438 20 \u043e\u043f\u043b\u0430\u0447\u0438\u0432\u0430\u0435\u043c\u044b\u0445 \u043e\u0442\u043f\u0443\u0441\u043a\u043d\u044b\u0445 \u0434\u043d\u0435\u0439;"", ""\u041a\u043e\u0440\u043f\u043e\u0440\u0430\u0442\u0438\u0432\u043d\u044b\u0435 \u043a\u0443\u0440\u0441\u044b \u0430\u043d\u0433\u043b\u0438\u0439\u0441\u043a\u043e\u0433\u043e \u044f\u0437\u044b\u043a\u0430, \u043a\u0443\u0440\u0441\u044b \u043f\u043e\u0432\u044b\u0448\u0435\u043d\u0438\u044f \u043a\u0432\u0430\u043b\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438;"", ""\u041a\u043e\u0440\u043f\u043e\u0440\u0430\u0442\u0438\u0432\u043d\u0430\u044f \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0430;"", ""\u0411\u0435\u0441\u043f\u043b\u0430\u0442\u043d\u044b\u0435 \u0435\u0436\u0435\u0434\u043d\u0435\u0432\u043d\u044b\u0435 \u043e\u0431\u0435\u0434\u044b \u0438 \u043d\u0435\u043e\u0433\u0440\u0430\u043d\u0438\u0447\u0435\u043d\u043d\u043e\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043a\u043e\u0444\u0435 / \u0447\u0430\u044f / \u043c\u043e\u043b\u043e\u043a\u0430;"", ""100% \u043a\u043e\u043c\u043f\u0435\u043d\u0441\u0430\u0446\u0438\u044f \u0442\u0440\u0435\u043d\u0430\u0436\u0435\u0440\u043d\u043e\u0433\u043e \u0437\u0430\u043b\u0430;"", ""\u041c\u0435\u0434\u0438\u0446\u0438\u043d\u0441\u043a\u0430\u044f \u0441\u0442\u0440\u0430\u0445\u043e\u0432\u043a\u0430.""], ""Responsibilities"": [""\u041f\u043e\u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0443\u0439\u0442\u0435 \u043a\u0430\u043d\u0434\u0438\u0434\u0430\u0442\u0430 \u0438 \u043f\u043e\u043b\u0443\u0447\u0438\u0442\u0435 \u0431\u043e\u043d\u0443\u0441 \u0434\u043e 1000 $! \u041f\u0440\u043e\u0441\u0442\u043e \u0437\u0430\u043f\u043e\u043b\u043d\u0438\u0442\u0435 \u0444\u043e\u0440\u043c\u0443bit.ly/3keuT0J""], ""Project description"": [""\u0421\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u0432\u043d\u0443\u0442\u0440\u0435\u043d\u043d\u0435\u0439 \u0441\u0438\u0441\u0442\u0435\u043c\u044b \u0430\u043d\u0430\u043b\u0438\u0442\u0438\u043a\u0438 \u0434\u043b\u044f \u0432\u0435\u0431-\u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u0439 \u0438 \u043c\u043e\u0431\u0438\u043b\u044c\u043d\u044b\u0445 \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u0439 \u0441 \u043d\u0443\u043b\u044f;"", ""\u041e\u0431\u0435\u0441\u043f\u0435\u0447\u0435\u043d\u0438\u0435 \u0430\u043d\u0430\u043b\u0438\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u043f\u043e\u0434\u0434\u0435\u0440\u0436\u043a\u0438 \u043f\u0440\u043e\u0434\u0443\u043a\u0442\u043e\u0432\u044b\u0445 \u0438 \u043c\u0430\u0440\u043a\u0435\u0442\u0438\u043d\u0433\u043e\u0432\u044b\u0445 \u043a\u043e\u043c\u0430\u043d\u0434;"", ""\u0413\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u044f \u0433\u0438\u043f\u043e\u0442\u0435\u0437 \u043f\u0440\u043e\u0434\u0443\u043a\u0442\u0430 \u0441 \u0446\u0435\u043b\u044c\u044e \u0443\u0432\u0435\u043b\u0438\u0447\u0435\u043d\u0438\u044f \u043f\u043e\u043a\u0430\u0437\u0430\u0442\u0435\u043b\u0435\u0439 \u0430\u043a\u0442\u0438\u0432\u043d\u043e\u0441\u0442\u0438 \u0438 \u043c\u043e\u043d\u0435\u0442\u0438\u0437\u0430\u0446\u0438\u0438;"", ""A / B-\u0442\u0435\u0441\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435, \u043d\u0430\u0441\u0442\u0440\u0430\u0438\u0432\u0430\u043d\u0438\u0435, \u043a\u043e\u043d\u0442\u0440\u043e\u043b\u044c \u0438 \u043e\u0442\u0441\u043b\u0435\u0436\u0438\u0432\u0430\u043d\u0438\u0435 \u0441\u043e\u0431\u044b\u0442\u0438\u0439, \u0430\u043d\u0430\u043b\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0445, \u0440\u0430\u0431\u043e\u0442\u0430 \u0441 \u0432\u044b\u0432\u043e\u0434\u0430\u043c\u0438;"", ""\u0420\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u043c\u043e\u0434\u0435\u043b\u0438 \u043f\u0440\u043e\u0433\u043d\u043e\u0437\u0430 LTV \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439;"", ""\u041a\u043e\u043d\u0442\u0440\u043e\u043b\u044c \u043f\u0440\u0430\u0432\u0438\u043b\u044c\u043d\u043e\u0439 \u0440\u0430\u0431\u043e\u0442\u044b \u0432\u0441\u0435\u0439 \u0441\u0438\u0441\u0442\u0435\u043c\u044b \u0441\u043b\u0435\u0436\u0435\u043d\u0438\u044f;"", ""\u041d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0430 \u0430\u0432\u0442\u043e\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u043e\u0442\u0447\u0435\u0442\u043e\u0432 \u0432 Tableau \u0434\u043b\u044f \u043c\u0430\u0440\u043a\u0435\u0442\u0438\u043d\u0433\u043e\u0432\u044b\u0445 \u0438 \u043f\u0440\u043e\u0434\u0443\u043a\u0442\u043e\u0432\u044b\u0445 \u043a\u043e\u043c\u0430\u043d\u0434;"", ""\u0410\u043d\u0430\u043b\u0438\u0437 \u043f\u043e\u043a\u0430\u0437\u0430\u0442\u0435\u043b\u0435\u0439 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430 \u043c\u0430\u0440\u043a\u0435\u0442\u0438\u043d\u0433\u0430 \u0438 \u0442\u0440\u0430\u0444\u0438\u043a\u0430 \u0434\u043b\u044f \u0441\u043d\u0438\u0436\u0435\u043d\u0438\u044f \u0441\u0442\u043e\u0438\u043c\u043e\u0441\u0442\u0438 \u0442\u0440\u0430\u0444\u0438\u043a\u0430 \u0438 \u043f\u043e\u0432\u044b\u0448\u0435\u043d\u0438\u044f \u0440\u0435\u043d\u0442\u0430\u0431\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438 \u0438\u043d\u0432\u0435\u0441\u0442\u0438\u0446\u0438\u0439 \u0432 \u043c\u0430\u0440\u043a\u0435\u0442\u0438\u043d\u0433.""]}"
Amazing Apps,https://jobs.dou.ua/companies/amazingappstech/,Data Engineer,https://jobs.dou.ua/companies/amazingappstech/vacancies/133087/?from=list_hot, Kyiv,02 November 2020,,"Мы в поиске опытного Data Engineer, для совместного построения системы оперативного получения данных, системы мониторинга и уведомлений для контроля здоровья системы. Менеджмент DWH + data lake также является одной из ключевых задач роли. О нас:• Data инфраструктура для десятка приложений с миллионами загрузок• Модели прогноза LTV и ROI• Собственные ETL из всех систем и медиа источников• Поступление данных в режиме онлайн• Постоянный контроль данных• Централизованный DWH• Текущая команда: Data Engineer, WEB Analyst, Marketing Analyst + открыты позиции Data Analyst, Data Quality Analyst, Product Analyst Ключевые задачи:• Поставка данных, python — для работы с API FB, GA, рекламных сетей, AF и других• Доставка в DWH ивент стримов (AWS sdk-pinpoint-firehose-ec2-redshift)• Мониторинг ресурсов EC2 инстансов (win, ubuntu) для запуска cron задач• Ведение кодовой базы, рефакторинг существующих скриптов• Поддержка и доработка (SQS, lambda) инфраструктуры для работы с входящими потоками• Мониторинг инфраструктуры и ETL (grafana, graylog, sqs, DB resources, ec2 resources) • Account management — создание и ведение apps/tokens, доступов, бот аккаунтов, сервис аккаунтов и т.д. Навыки и опыт:• Опыт работы c облачной инфраструктурой, предпочтительно AWS (lambda, sqs, redshift, glue)• Знание Python, опыт импорта данных из API (FB, GA, рекламные сети), web scraping и т.д.• Опыт работы с VCS• Сильный SQL (оконные функции), оптимизация производительности БД• Английский язык на уровне чтения и понимания документации Будет плюсом:• Опыт администрирования БД• Опыт работы с Jenkins, Graylog, Vault• Опыт работы с потоковыми данными• Опыт работы с data lakes В нашей команде мы ценим:• Проактивность и ориентацию на результат. Ты задаешь вопросы и предлагаешь эксперименты, стремишься к улучшению существующих процессов и подходов• Эффективную коммуникацию в команде. Выполняешь договоренности, коммуницируешь своевременно, понятно, четко и лаконично, даешь больше деталей, если необходимо, готов помочь в достижении общей цели Мы предлагаем:Комфортные условия для работы•Офис со всеми удобствами, включая игровую комнату, тренажеры, массажное кресло и много других приятных мелочей•Гибкий график работы, один раз в неделю можно работать удаленно•20 рабочих дней оплачиваемого отпуска•Обеды за счет компании, фрукты, мороженое, полезные снэки и напиткиЗаботу о здоровье•Медицинская страховка с первого месяца работы•Оплачиваемые больничные•Ежедневные занятия йогой в офисеПрофессиональное развитие•Карьерные перспективы — у тебя будет возможность расти вместе с компанией. Для этого мы проводим Performance review 2 раза в год•Индивидуальный бюджет на внешние тренинги и курсы, митапы и семинары•Корпоративная онлайн и оффлайн библиотека•Индивидуальные онлайн уроки по изучению английского языка•Шеринг опытом и знаниями между командами Вот некоторые из наших приложений:Yoga Go — apple.co/2OXPszb / bit.ly/androidYogaGoMuscle Booster — apple.co/2LyMBA1 / bit.ly/androidMBapp • Наши продукты — лидеры в категории Health and Fitness по всему миру. Мы гордимся тем, что за 8 месяцев создали и вывели приложение Muscle Booster в ТОП-1 в мире в своей категории. Наша цель — вывести все приложения в лидеры• У нас можно реализовывать идеи без микроменеджмента и бюрократии, и почувствовать свободу творчества. Исследуй варианты глубже, слушай коллег, аргументируй и принимай решения Рекрутмент процесс:⭕️ Знакомство с рекрутером ----> ⭕️ Тестовое задание ----> ⭕️ Финальное интервью",dou,2020-11-12,Data Engineer@Amazing Apps,,,{}
Xenoss,https://jobs.dou.ua/companies/xenoss/,Back-End Eng (Java) for highload/bigdata project // QS,https://jobs.dou.ua/companies/xenoss/vacancies/137303/?from=list_hot," Kyiv, Kharkiv, Lviv, Dnipro, Odesa, Vinnytsia, Zhytomyr, Rivne, Uzhhorod, Chernihiv, Chernivtsi, New York (USA), remote",03 November 2020,,"Required skills — 4+ years of software development— Strong knowledge of Java— Experience working with NoSQL and SQL DBMS.— Strong algorithmic background and OOP — Docker— Good verbal and written English communication skills (Intermediate level or higher) Do not hesitate to apply if you are missing some specific experience. We will be happy to help you with gaining one. As a plus — AWS— Kubernetes experience is a plus— Experience working with Clickhouse We offer — Unlimited work-from-home option— Flexible working hours— 3 weeks of paid vacation and 2 weeks of paid sick-period— Senior team and challenging projects— Long-term employment— Zero bureaucracy— Covering costs of professional events and English lessons— Opportunity to choose the equipment you like— Various snacks, tea, coffee, and fresh fruits in the office Responsibilities You’ll be responsible for developing the scalable system handling tens of thousands of QPS measuring the performance of the advertising campaigns. You’ll be working up the system of integrating with different advertising networks including Google, Facebook, Twitter, etc. Project description The product is an advertising technology platform that allows integrations with multiple advertising sources (Google, Facebook, Twitter, etc) to provide an effective monetization mechanism for mobile app developers. The complexity of designing and developing an interesting app or game is challenging itself. The platform allows developers to integrate the most effective advertising platforms as a one-stop-shop that takes care of multiple integrations, effective optimization for maximum monetization of the mobile apps.",dou,2020-11-12,Back-End Eng (Java) for highload/bigdata project // QS@Xenoss,,,"{""Required skills"": [""4+ years of software development"", ""Strong knowledge of Java"", ""Experience working with NoSQL and SQL DBMS."", ""Strong algorithmic background and OOP"", ""Docker"", ""Good verbal and written English communication skills (Intermediate level or higher)""], ""As a plus"": [""Do not hesitate to apply if you are missing some specific experience. We will be happy to help you with gaining one.""], ""We offer"": [""AWS"", ""Kubernetes experience is a plus"", ""Experience working with Clickhouse""], ""Responsibilities"": [""Unlimited work-from-home option"", ""Flexible working hours"", ""3 weeks of paid vacation and 2 weeks of paid sick-period"", ""Senior team and challenging projects"", ""Long-term employment"", ""Zero bureaucracy"", ""Covering costs of professional events and English lessons"", ""Opportunity to choose the equipment you like"", ""Various snacks, tea, coffee, and fresh fruits in the office""], ""Project description"": [""You\u2019ll be responsible for developing the scalable system handling tens of thousands of QPS measuring the performance of the advertising campaigns. You\u2019ll be working up the system of integrating with different advertising networks including Google, Facebook, Twitter, etc.""]}"
Xenoss,https://jobs.dou.ua/companies/xenoss/,Senior Full Stack Engineer (React/Java) for bigdata project // VNT,https://jobs.dou.ua/companies/xenoss/vacancies/132425/?from=list_hot," Kyiv, Kharkiv, Lviv, Dnipro, Odesa, Vinnytsia, Rivne, Uzhhorod, Chernivtsi, remote",30 October 2020,,"Required skills — 5+ years of software development— Experienced Java developer— Experience working with ReactJS — Software design skills, design patterns— RDBMS and NoSQL databases— Advanced Linux user— Experience with Docker— Fluent English Do not hesitate to apply if you are missing some specific experience. We will be happy to help you with gaining one. As a plus — Nice-to-have skills— Experience working with Ad-tech projects — AWS, Google Cloud— TypeScript, Javascript We offer — Great experience and professional growth by working on challenging projects— Flexible working hours— Unlimited work-from-home option— Various snacks, tea, coffee, and fresh fruits in the office— Paid vacation and sick-period Responsibilities You’ll be working with the project team confront-end and back-end implementation of web services, high-performance scalable solutions, big data processing pipelines. Project description The solution we are working on is a complex distributed software mechanism controlling advertising campaigns running on leading entertainment media products like Rovio, EA, Spil Games, and many other gaming, entertainment, and lifestyle sites. The system enables the biggest advertisers across the world to run highly targeted ads, personalize ad communication, predict user consumption behavior, make deep multi-dimensional analytics. We serve over 2 billion ads per month across 400 publisher sites. Tech stack includes Java, Scala, Node, Typescript, AWS, Docker, MongoDB, Google BigQuery, Clickhouse.",dou,2020-11-12,Senior Full Stack Engineer (React/Java) for bigdata project // VNT@Xenoss,,,"{""Required skills"": [""5+ years of software development"", ""Experienced Java developer"", ""Experience working with ReactJS"", ""Software design skills, design patterns"", ""RDBMS and NoSQL databases"", ""Advanced Linux user"", ""Experience with Docker"", ""Fluent English""], ""As a plus"": [""Do not hesitate to apply if you are missing some specific experience. We will be happy to help you with gaining one.""], ""We offer"": [""Nice-to-have skills"", ""Experience working with Ad-tech projects"", ""AWS, Google Cloud"", ""TypeScript, Javascript""], ""Responsibilities"": [""Great experience and professional growth by working on challenging projects"", ""Flexible working hours"", ""Unlimited work-from-home option"", ""Various snacks, tea, coffee, and fresh fruits in the office"", ""Paid vacation and sick-period""], ""Project description"": [""You\u2019ll be working with the project team confront-end and back-end implementation of web services, high-performance scalable solutions, big data processing pipelines.""]}"
Xenoss,https://jobs.dou.ua/companies/xenoss/,Middle to Senior Back-End Engineer (Java) for bigdata project// VNT,https://jobs.dou.ua/companies/xenoss/vacancies/137321/?from=list_hot," Kyiv, Kharkiv, Lviv, Dnipro, Odesa, Vinnytsia, Zhytomyr, Rivne, Uzhhorod, Chernihiv, Chernivtsi, remote",03 November 2020,,"Required skills — 4+ years of software development— Strong knowledge of Java— Experience working with NoSQL and SQL DBMS.— Strong algorithmic background and OOP — Docker— Good verbal and written English Do not hesitate to apply if you are missing some specific experience. We will be happy to help you with gaining one. As a plus — AWS/GCP— Kubernetes experience is a plus— Experience working with Clickhouse We offer — Unlimited work-from-home option— Flexible working hours— 3 weeks of paid vacation and 2 weeks of paid sick-period— Senior team and challenging projects— Long-term employment— Zero bureaucracy— Covering costs of professional events and English lessons— Opportunity to choose the equipment you like— Various snacks, tea, coffee, and fresh fruits in the office Responsibilities You’ll be responsible for developing the scalable system handling tens of thousands of QPS measuring the advertising performance of the sites. You’ll be working up the system of integrating with different advertising networks including Google, Facebook, Twitter, etc. Project description The solution we are working on is a complex distributed software mechanism controlling advertising campaigns running on leading entertainment media products like Rovio, EA, Spil Games, and many other gaming, entertainment, and lifestyle sites. The system enables the biggest advertisers across the world to run highly targeted ads, personalize ad communication, predict user consumption behavior, make deep multi-dimensional analytics. We serve over 2 billion ads per month across 400 publisher sites. Tech stack includes Java, Scala, Node, Typescript, AWS, Docker, MongoDB, Google BigQuery, Clickhouse.",dou,2020-11-12,Middle to Senior Back-End Engineer (Java) for bigdata project// VNT@Xenoss,,,"{""Required skills"": [""4+ years of software development"", ""Strong knowledge of Java"", ""Experience working with NoSQL and SQL DBMS."", ""Strong algorithmic background and OOP"", ""Docker"", ""Good verbal and written English""], ""As a plus"": [""Do not hesitate to apply if you are missing some specific experience. We will be happy to help you with gaining one.""], ""We offer"": [""AWS/GCP"", ""Kubernetes experience is a plus"", ""Experience working with Clickhouse""], ""Responsibilities"": [""Unlimited work-from-home option"", ""Flexible working hours"", ""3 weeks of paid vacation and 2 weeks of paid sick-period"", ""Senior team and challenging projects"", ""Long-term employment"", ""Zero bureaucracy"", ""Covering costs of professional events and English lessons"", ""Opportunity to choose the equipment you like"", ""Various snacks, tea, coffee, and fresh fruits in the office""], ""Project description"": [""You\u2019ll be responsible for developing the scalable system handling tens of thousands of QPS measuring the advertising performance of the sites. You\u2019ll be working up the system of integrating with different advertising networks including Google, Facebook, Twitter, etc.""]}"
Uklon,https://jobs.dou.ua/companies/uklon/,Lead Data Engineer,https://jobs.dou.ua/companies/uklon/vacancies/137383/?from=list_hot, Kyiv,03 November 2020,,"Required skills — 5+ років досвіду як Data Engineer.— 3+ роки практичного досвіду роботи з даними в AWS.— Знання принципів Data Lake та Distributed Data Mesh.— Відмінне знання Python.— Відмінне знання SQL (PostgreSQL).— Досвід роботи з Kafka.— Досвід роботи з Amazon S3, Amazon Redshift, Amazon Athena.— Знання формату Apache Parquet.— Знання Git та Gitflow Workflow. As a plus — Знання Flask.— Досвід написання функцій AWS Lambda.— Досвід роботи з Amazon Glue (Crawler, Data Catalog).— Досвід роботи з Apache Hive та Apache Presto.— Досвід у Machine Learning (класифікація, регресія).— Досвід роботи з Kubeflow.— Досвід роботи на проекті, де робота з данними велась в AWS.— Досвід лідерства у команді з декількох Data інженерів. We offer — Квартальні бонуси. — інші соціальні бонуси та компенсації (народження дитини, відшкодування тестування на COVID-19 та ін.).— Можливість відвідувати оплачувані семінари, майстер-класи, тренінги та конференції; участь у хакатонах, форумах, team building sessions.— Обмін знаннями серед команд (лекції, курси, програма менторства); корпоративна бібліотека та підписка на Pluralsight.— Корпоративне навчання з англійської мови різних рівней.— Лікарняні без обмежень.— Основна відпустка 20 робочих днів на рік, додаткова — у день народження, за донорство. — Можливість вибрати зручний формат роботи (офіс/віддалений/міксований формат), тому ти маєш можливість працювати з будь-якого куточка України та світу.— Гнучкий графік.— Корпоративне дозвілля, спортивні секції (футбол, йога). Бути частиною Uklon Team це: — Працювати в команді експертів.— Широкі можливості навчання та зростання в команді, підтримка ініціатив.— Можливість постійно підвищувати свій рівень, виконуючи тільки цікаві задачі з викликом. Responsibilities — Написання proof of concept.— Запровадження Big Data платформи у cloud.— Оптимізація Big Data платформи та зменшення витрат на cloud.— Тісна взаємодія з Solutions Architect та CTO по розвитку Big Data платформи.— Розробка нового функціоналу.— Розробка плану професійного розвитку колег. Project description Uklon — найпопулярніший український райдшерінг-сервіс. Ми продуктова IT-компанія, що розробляє та підтримує одну з найбільших інфраструктур на українському ринку, забезпечуючи взаємодію водія та пасажира в межах здійснюваних поїздок. Uklon розробляє власні додатки та веб-сайти, через які користувачі мають можливість робити замовлення автомобіля, а водії — їх отримувати; створює власну систему управління внутрішніми процесами (CRM), власний картографічний сервіс та власні рішення, що відповідає за інтеграцію з платіжними провайдерами та інше. Продукт побудовано на базі мікросервісної архітектури з використанням хмарних обчислень. Наша команда завжди використовує найновіші технологічні рішення та методології, має мікросервісну архітектуру та хмарні обчислення, впроваджує інтелектуальні інструменти та намагається завжди бути найтехнологічнішим лідером у своєму напрямку. Більше про компанію та нашу команду дивись на сайті: careers.uklon.ua",dou,2020-11-12,Lead Data Engineer@Uklon,,,"{""Required skills"": [""5+ \u0440\u043e\u043a\u0456\u0432 \u0434\u043e\u0441\u0432\u0456\u0434\u0443 \u044f\u043a Data Engineer."", ""3+ \u0440\u043e\u043a\u0438 \u043f\u0440\u0430\u043a\u0442\u0438\u0447\u043d\u043e\u0433\u043e \u0434\u043e\u0441\u0432\u0456\u0434\u0443 \u0440\u043e\u0431\u043e\u0442\u0438 \u0437 \u0434\u0430\u043d\u0438\u043c\u0438 \u0432 AWS."", ""\u0417\u043d\u0430\u043d\u043d\u044f \u043f\u0440\u0438\u043d\u0446\u0438\u043f\u0456\u0432 Data Lake \u0442\u0430 Distributed Data Mesh."", ""\u0412\u0456\u0434\u043c\u0456\u043d\u043d\u0435 \u0437\u043d\u0430\u043d\u043d\u044f Python."", ""\u0412\u0456\u0434\u043c\u0456\u043d\u043d\u0435 \u0437\u043d\u0430\u043d\u043d\u044f SQL (PostgreSQL)."", ""\u0414\u043e\u0441\u0432\u0456\u0434 \u0440\u043e\u0431\u043e\u0442\u0438 \u0437 Kafka."", ""\u0414\u043e\u0441\u0432\u0456\u0434 \u0440\u043e\u0431\u043e\u0442\u0438 \u0437 Amazon S3, Amazon Redshift, Amazon Athena."", ""\u0417\u043d\u0430\u043d\u043d\u044f \u0444\u043e\u0440\u043c\u0430\u0442\u0443 Apache Parquet."", ""\u0417\u043d\u0430\u043d\u043d\u044f Git \u0442\u0430 Gitflow Workflow.""], ""As a plus"": [""\u0417\u043d\u0430\u043d\u043d\u044f Flask."", ""\u0414\u043e\u0441\u0432\u0456\u0434 \u043d\u0430\u043f\u0438\u0441\u0430\u043d\u043d\u044f \u0444\u0443\u043d\u043a\u0446\u0456\u0439 AWS Lambda."", ""\u0414\u043e\u0441\u0432\u0456\u0434 \u0440\u043e\u0431\u043e\u0442\u0438 \u0437 Amazon Glue (Crawler, Data Catalog)."", ""\u0414\u043e\u0441\u0432\u0456\u0434 \u0440\u043e\u0431\u043e\u0442\u0438 \u0437 Apache Hive \u0442\u0430 Apache Presto."", ""\u0414\u043e\u0441\u0432\u0456\u0434 \u0443 Machine Learning (\u043a\u043b\u0430\u0441\u0438\u0444\u0456\u043a\u0430\u0446\u0456\u044f, \u0440\u0435\u0433\u0440\u0435\u0441\u0456\u044f)."", ""\u0414\u043e\u0441\u0432\u0456\u0434 \u0440\u043e\u0431\u043e\u0442\u0438 \u0437 Kubeflow."", ""\u0414\u043e\u0441\u0432\u0456\u0434 \u0440\u043e\u0431\u043e\u0442\u0438 \u043d\u0430 \u043f\u0440\u043e\u0435\u043a\u0442\u0456, \u0434\u0435 \u0440\u043e\u0431\u043e\u0442\u0430 \u0437 \u0434\u0430\u043d\u043d\u0438\u043c\u0438 \u0432\u0435\u043b\u0430\u0441\u044c \u0432 AWS."", ""\u0414\u043e\u0441\u0432\u0456\u0434 \u043b\u0456\u0434\u0435\u0440\u0441\u0442\u0432\u0430 \u0443 \u043a\u043e\u043c\u0430\u043d\u0434\u0456 \u0437 \u0434\u0435\u043a\u0456\u043b\u044c\u043a\u043e\u0445 Data \u0456\u043d\u0436\u0435\u043d\u0435\u0440\u0456\u0432.""], ""We offer"": [""\u041a\u0432\u0430\u0440\u0442\u0430\u043b\u044c\u043d\u0456 \u0431\u043e\u043d\u0443\u0441\u0438."", ""\u0456\u043d\u0448\u0456 \u0441\u043e\u0446\u0456\u0430\u043b\u044c\u043d\u0456 \u0431\u043e\u043d\u0443\u0441\u0438 \u0442\u0430 \u043a\u043e\u043c\u043f\u0435\u043d\u0441\u0430\u0446\u0456\u0457 (\u043d\u0430\u0440\u043e\u0434\u0436\u0435\u043d\u043d\u044f \u0434\u0438\u0442\u0438\u043d\u0438, \u0432\u0456\u0434\u0448\u043a\u043e\u0434\u0443\u0432\u0430\u043d\u043d\u044f \u0442\u0435\u0441\u0442\u0443\u0432\u0430\u043d\u043d\u044f \u043d\u0430 COVID-19 \u0442\u0430 \u0456\u043d.)."", ""\u041c\u043e\u0436\u043b\u0438\u0432\u0456\u0441\u0442\u044c \u0432\u0456\u0434\u0432\u0456\u0434\u0443\u0432\u0430\u0442\u0438 \u043e\u043f\u043b\u0430\u0447\u0443\u0432\u0430\u043d\u0456 \u0441\u0435\u043c\u0456\u043d\u0430\u0440\u0438, \u043c\u0430\u0439\u0441\u0442\u0435\u0440-\u043a\u043b\u0430\u0441\u0438, \u0442\u0440\u0435\u043d\u0456\u043d\u0433\u0438 \u0442\u0430 \u043a\u043e\u043d\u0444\u0435\u0440\u0435\u043d\u0446\u0456\u0457; \u0443\u0447\u0430\u0441\u0442\u044c \u0443 \u0445\u0430\u043a\u0430\u0442\u043e\u043d\u0430\u0445, \u0444\u043e\u0440\u0443\u043c\u0430\u0445, team building sessions."", ""\u041e\u0431\u043c\u0456\u043d \u0437\u043d\u0430\u043d\u043d\u044f\u043c\u0438 \u0441\u0435\u0440\u0435\u0434 \u043a\u043e\u043c\u0430\u043d\u0434 (\u043b\u0435\u043a\u0446\u0456\u0457, \u043a\u0443\u0440\u0441\u0438, \u043f\u0440\u043e\u0433\u0440\u0430\u043c\u0430 \u043c\u0435\u043d\u0442\u043e\u0440\u0441\u0442\u0432\u0430); \u043a\u043e\u0440\u043f\u043e\u0440\u0430\u0442\u0438\u0432\u043d\u0430 \u0431\u0456\u0431\u043b\u0456\u043e\u0442\u0435\u043a\u0430 \u0442\u0430 \u043f\u0456\u0434\u043f\u0438\u0441\u043a\u0430 \u043d\u0430 Pluralsight."", ""\u041a\u043e\u0440\u043f\u043e\u0440\u0430\u0442\u0438\u0432\u043d\u0435 \u043d\u0430\u0432\u0447\u0430\u043d\u043d\u044f \u0437 \u0430\u043d\u0433\u043b\u0456\u0439\u0441\u044c\u043a\u043e\u0457 \u043c\u043e\u0432\u0438 \u0440\u0456\u0437\u043d\u0438\u0445 \u0440\u0456\u0432\u043d\u0435\u0439."", ""\u041b\u0456\u043a\u0430\u0440\u043d\u044f\u043d\u0456 \u0431\u0435\u0437 \u043e\u0431\u043c\u0435\u0436\u0435\u043d\u044c."", ""\u041e\u0441\u043d\u043e\u0432\u043d\u0430 \u0432\u0456\u0434\u043f\u0443\u0441\u0442\u043a\u0430 20 \u0440\u043e\u0431\u043e\u0447\u0438\u0445 \u0434\u043d\u0456\u0432 \u043d\u0430 \u0440\u0456\u043a, \u0434\u043e\u0434\u0430\u0442\u043a\u043e\u0432\u0430"", ""\u0443 \u0434\u0435\u043d\u044c \u043d\u0430\u0440\u043e\u0434\u0436\u0435\u043d\u043d\u044f, \u0437\u0430 \u0434\u043e\u043d\u043e\u0440\u0441\u0442\u0432\u043e."", ""\u041c\u043e\u0436\u043b\u0438\u0432\u0456\u0441\u0442\u044c \u0432\u0438\u0431\u0440\u0430\u0442\u0438 \u0437\u0440\u0443\u0447\u043d\u0438\u0439 \u0444\u043e\u0440\u043c\u0430\u0442 \u0440\u043e\u0431\u043e\u0442\u0438 (\u043e\u0444\u0456\u0441/\u0432\u0456\u0434\u0434\u0430\u043b\u0435\u043d\u0438\u0439/\u043c\u0456\u043a\u0441\u043e\u0432\u0430\u043d\u0438\u0439 \u0444\u043e\u0440\u043c\u0430\u0442), \u0442\u043e\u043c\u0443 \u0442\u0438 \u043c\u0430\u0454\u0448 \u043c\u043e\u0436\u043b\u0438\u0432\u0456\u0441\u0442\u044c \u043f\u0440\u0430\u0446\u044e\u0432\u0430\u0442\u0438 \u0437 \u0431\u0443\u0434\u044c-\u044f\u043a\u043e\u0433\u043e \u043a\u0443\u0442\u043e\u0447\u043a\u0430 \u0423\u043a\u0440\u0430\u0457\u043d\u0438 \u0442\u0430 \u0441\u0432\u0456\u0442\u0443."", ""\u0413\u043d\u0443\u0447\u043a\u0438\u0439 \u0433\u0440\u0430\u0444\u0456\u043a."", ""\u041a\u043e\u0440\u043f\u043e\u0440\u0430\u0442\u0438\u0432\u043d\u0435 \u0434\u043e\u0437\u0432\u0456\u043b\u043b\u044f, \u0441\u043f\u043e\u0440\u0442\u0438\u0432\u043d\u0456 \u0441\u0435\u043a\u0446\u0456\u0457 (\u0444\u0443\u0442\u0431\u043e\u043b, \u0439\u043e\u0433\u0430).""], ""Responsibilities"": [""\u0411\u0443\u0442\u0438 \u0447\u0430\u0441\u0442\u0438\u043d\u043e\u044e Uklon Team \u0446\u0435:""], ""Project description"": [""\u041f\u0440\u0430\u0446\u044e\u0432\u0430\u0442\u0438 \u0432 \u043a\u043e\u043c\u0430\u043d\u0434\u0456 \u0435\u043a\u0441\u043f\u0435\u0440\u0442\u0456\u0432."", ""\u0428\u0438\u0440\u043e\u043a\u0456 \u043c\u043e\u0436\u043b\u0438\u0432\u043e\u0441\u0442\u0456 \u043d\u0430\u0432\u0447\u0430\u043d\u043d\u044f \u0442\u0430 \u0437\u0440\u043e\u0441\u0442\u0430\u043d\u043d\u044f \u0432 \u043a\u043e\u043c\u0430\u043d\u0434\u0456, \u043f\u0456\u0434\u0442\u0440\u0438\u043c\u043a\u0430 \u0456\u043d\u0456\u0446\u0456\u0430\u0442\u0438\u0432."", ""\u041c\u043e\u0436\u043b\u0438\u0432\u0456\u0441\u0442\u044c \u043f\u043e\u0441\u0442\u0456\u0439\u043d\u043e \u043f\u0456\u0434\u0432\u0438\u0449\u0443\u0432\u0430\u0442\u0438 \u0441\u0432\u0456\u0439 \u0440\u0456\u0432\u0435\u043d\u044c, \u0432\u0438\u043a\u043e\u043d\u0443\u044e\u0447\u0438 \u0442\u0456\u043b\u044c\u043a\u0438 \u0446\u0456\u043a\u0430\u0432\u0456 \u0437\u0430\u0434\u0430\u0447\u0456 \u0437 \u0432\u0438\u043a\u043b\u0438\u043a\u043e\u043c.""]}"
Star,https://jobs.dou.ua/companies/star/,Head of Data Science,https://jobs.dou.ua/companies/star/vacancies/135340/?from=list_hot, Kyiv,16 October 2020,,"THE COMPANYStar is a global consultancy that connects strategy, design, engineering and marketing services into a seamless workflow devised to support our clients every step of the way — no matter how long or complex the technology-based business journey. Our strategists, designers and engineers create useful, scalable technology products and solutions. We are 750 strategists, designers, engineers and marketers in 12 locations around the world, and we are here to make every great idea, every great person and every great company shine. That is why we’re called Star. THE OPPORTUNITYYou will be an essential part of the Star Data Science practice and will work closely with many different clients across the business. With the use of machine learning and advanced analytics, you will contribute to the success of the business and help translate vision into actionable output. Your responsibilities will include contributing to strategy and roadmap planning for Data Science practice. Taking part in client and prospective client meetings and workshops. Investigating client requests and problems, analyzing options and proposing solutions. You will be leading projects, and managing and supporting other members of the team. THE PERSONMust have:— The individual will be an independent-thinking technical generalist with 8+ years of professional experience in data driven projects— 2+ years of strong client-facing experience and the ability to understand client challenges and articulate solutions.— 5+ years of commercial development experience and be proficient in at least one programming language (Python, Java).— Hands-on experience with classic machine learning (linear models and clustering) and deep learning (keras, pytorch) toolsets, and running ML models in production environments.— Experience in a leadership or mentoring role— Familiarity with at least one cloud provider (AWS, GCP, Azure)— Great presentation skills and have the ability to tell a story through data and explain technical concepts to senior management.— Excellent SQL skills— Strong mathematical and statistical skills Nice to have experience with:— BI tools (Tableau, QuickSight)— BigData stack (EMR, Hadoop, Spark)— ElasticSearch— ETL pipelines (Glue, NiFi, Airflow)— Additional programming languages (Java, Scala, Javascript, R)— Data Security Compliance (GDPR, HIPAA, PCI DSS) COMPENSATION AND BENEFITSStar offers a competitive salary and benefits package, as well as an intellectually and creatively stimulating work environment, flexible working hours, and unique international travel opportunities.",dou,2020-11-12,Head of Data Science@Star,,,{}
"TEAM International Services, Inc.",https://jobs.dou.ua/companies/team-international/,Lead/Senior Data Engineer (Scala),https://jobs.dou.ua/companies/team-international/vacancies/136984/?from=list_hot," Kyiv, remote",30 October 2020,,"Required skills • 4+ years of relevant work experience• Proven experience with Spark building data products at scale and speed• Strong software design and development experience in Scala, Spark• Strong understanding of application architecture fundamentals• Strong experience with cloud computing (we use AWS and GCP)• Ability to help define standards and best practices for the team• BS or MS in Computer Science would be a plus. We offer • Remote or office work according to your preferences• Competitive salary based on your experience level• Free English classes• 23+ working days of 100% paid vacation and sick days• Sports reimbursement program• Medical insurance health plan• Modern and comfortable office facilities near the city-center. Responsibilities As a Senior Data Engineer, you will be building out all aspects relating to our Data ecosystem and moving products from R&D into production scale. A successful Senior Data Engineer will possess a natural curiosity about data and clear technical ability. You will be both hands-on and strategic—with both a broad ecosystem-level understanding of our market space and the ability to work closely with engineering, data science and product teams to deliver software in an iterative, continual-release environment. This position involves close collaboration across several functional groups as well as interaction with executive stakeholders. What you’ll be doing: • Define: Work with internal stakeholders to design and develop components in the next generation of Data Products• Code: Build out new services and integrations to connect machine learning intelligence with the SaaS platform• Monitor & Deploy: Ensure high reliability of all maintained product offerings by building reporting and monitoring mechanisms into our infrastructure. Project description Flagship product — Account-Based marketing platform / solution driven by AI. They work with one of the largest B2B data sets in the world and leverages Artificial Intelligence and Machine Learning to solve the most complex challenges in B2B marketing. The proprietary database is built on a comprehensive foundation of 1st and 3rd party data and B2B signals. The growing database is optimized for B2B intelligence. Our team of Big Data Engineers and Data Scientists work together to develop and implement the sophisticated AI and machine learning that’s required to turn that data into insights and deliver it in ways marketers can actually see.",dou,2020-11-12,"Lead/Senior Data Engineer (Scala)@TEAM International Services, Inc.",,,"{""Required skills"": [""4+ years of relevant work experience"", ""Proven experience with Spark building data products at scale and speed"", ""Strong software design and development experience in Scala, Spark"", ""Strong understanding of application architecture fundamentals"", ""Strong experience with cloud computing (we use AWS and GCP)"", ""Ability to help define standards and best practices for the team"", ""BS or MS in Computer Science would be a plus.""], ""We offer"": [""Remote or office work according to your preferences"", ""Competitive salary based on your experience level"", ""Free English classes"", ""23+ working days of 100% paid vacation and sick days"", ""Sports reimbursement program"", ""Medical insurance health plan"", ""Modern and comfortable office facilities near the city-center.""], ""Responsibilities"": [""As a Senior Data Engineer, you will be building out all aspects relating to our Data ecosystem and moving products from R&D into production scale. A successful Senior Data Engineer will possess a natural curiosity about data and clear technical ability.""], ""Project description"": [""You will be both hands-on and strategic"", ""with both a broad ecosystem-level understanding of our market space and the ability to work closely with engineering, data science and product teams to deliver software in an iterative, continual-release environment. This position involves close collaboration across several functional groups as well as interaction with executive stakeholders.""]}"
DataArt,https://jobs.dou.ua/companies/dataart/,"Big Data Software Engineer (AWS, Python), Digital Insurance Services",https://jobs.dou.ua/companies/dataart/vacancies/136552/?from=list_hot," Kyiv, Kharkiv, Lviv, Dnipro, Odesa, Kherson, remote",28 October 2020,,"Required skills Experience building and maintaining backend Python servicesSpoken English As a plus Experience building and deploying Docker-ised applicationsExperience with AWS services (CLI, S3, Redshift, Lambda) We offer DataArt offers:• Professional Development:— Experienced colleagues who are ready to share knowledge;— The ability to switch projects, technology stacks, try yourself in different roles;— Over 150 courses for workplace-based training— Study and practice of English: courses and communication with colleagues and clients from different countries;— Support of speakers who make presentations at conferences and meetings of technology communities.• The ability to focus on your work: a lack of bureaucracy and micromanagement, and convenient corporate services;• Lack of dress code, friendly atmosphere, concern for the comfort of specialists;• Flexible schedule and the ability to work remotely;• The ability to work in any of our development centers. Responsibilities Integrate third-party systems and APIs (inbound and outbound) into our internal systems and data lake using Python scriptsBuild reports based on S3 and Redshift dataBuild data quality monitoring and alerting for incoming integrations to ensure validity within our systemRefactor an existing project into Python (validation and processing of incoming data with alerting, Lambda functions to integrate with outside APIs) Project description The client is bringing digital insurance service to its clients, and building a product that is rebooting the insurance industry. The client hires DataArt to help them with the data transformation project and to reinforce the existing team. The client is located in Germany, but the team is multinational, so having verbal & written English communication is a key requirement. The candidate should have a proven background in implementing data projects. The technology stack for this position is Python, Docker, AWS, Redshift, Lambda.",dou,2020-11-12,"Big Data Software Engineer (AWS, Python), Digital Insurance Services@DataArt",,,"{""Required skills"": [""Experience building and maintaining backend Python servicesSpoken English""], ""As a plus"": [""Experience building and deploying Docker-ised applicationsExperience with AWS services (CLI, S3, Redshift, Lambda)""], ""We offer"": [""DataArt offers:"", ""Professional Development:"", ""Experienced colleagues who are ready to share knowledge;"", ""The ability to switch projects, technology stacks, try yourself in different roles;"", ""Over 150 courses for workplace-based training"", ""Study and practice of English: courses and communication with colleagues and clients from different countries;"", ""Support of speakers who make presentations at conferences and meetings of technology communities."", ""The ability to focus on your work: a lack of bureaucracy and micromanagement, and convenient corporate services;"", ""Lack of dress code, friendly atmosphere, concern for the comfort of specialists;"", ""Flexible schedule and the ability to work remotely;"", ""The ability to work in any of our development centers.""], ""Responsibilities"": [""Integrate third-party systems and APIs (inbound and outbound) into our internal systems and data lake using Python scriptsBuild reports based on S3 and Redshift dataBuild data quality monitoring and alerting for incoming integrations to ensure validity within our systemRefactor an existing project into Python (validation and processing of incoming data with alerting, Lambda functions to integrate with outside APIs)""], ""Project description"": [""The client is bringing digital insurance service to its clients, and building a product that is rebooting the insurance industry. The client hires DataArt to help them with the data transformation project and to reinforce the existing team.""]}"
Clario,https://jobs.dou.ua/companies/clario/,Billing/Data Analyst,https://jobs.dou.ua/companies/clario/vacancies/132961/?from=list_hot, Kyiv,26 October 2020,,"Hi, We are Clario, a consumer-focused cybersecurity company on a mission to change an industry. Over 800 professionals, including 600 digital security experts, with one common goal — supporting everyone’s right to a digital life, secured. We’re here to create a next-generation digital security solution with a human touch. Join us and help people take back control of their digital privacy and security. Clario Team is looking for a Billing/Data Analyst for full-time work to help build the Clario brand that aims to become the disruptive global leader in digital privacy and security. • Analyzing performance of payment processors.• Research and find insights based on user behavior patterns on payment page.• Build automated dashboards for visualizing key metrics and trends to support Marketing functions (MySQL, HIVE, Tableau)• Analyze A/B tests regarding payment page.• Communicate with stakeholders highlighting problems and opportunities, suggest actions to be taken based on analysis. • Superior analytical and problem-solving abilities with keen attention to details• Self-starter with ability to work in a fast-paced environment and manage tight deadlines• Strong communication and presentation skills• Strong SQL knowledge• Experience of building reports via Tableau• 2+ years of experience working as a billing or data analyst with IT technical background.• Understanding payment flow and supporting metrics (chargeback rate, refund rate etc.)• Ability to work with a large amount of data and figures, IT platforms, systems, tools, and databases• Knowledge of Hadoop, Hive, and MapReduce would be an advantage• English level — Upper Intermediate +• University degree in Math, Applied Mathematics, Computer Science or similar beneficial but not mandatory We are not just a company, we are Clario! We put the customer at the heart of all that we do, we achieve our best together, take responsibility, and challenge our limits to create a difference! To apply for this position, please send your CV (in English only) with a detailed description of your career, experience, skills, and projects. We guarantee the privacy of any information received",dou,2020-11-12,Billing/Data Analyst@Clario,,,{}
Grammarly,https://jobs.dou.ua/companies/grammarly/,"Software Engineer, Data Platform",https://jobs.dou.ua/companies/grammarly/vacancies/138703/, Kyiv,12 November 2020,,"Grammarly empowers people to thrive and connect, whenever and wherever they communicate. Every day, 30 million people around the world use our AI-powered writing assistant. All of this begins with our team collaborating in a values-driven and learning-oriented environment. Grammarly’s success depends on ability to keep user text secure while also using data to improve our product. To achieve our ambitious goals, we’re looking for a Software Engineer to help us create a world-class management platform for user text data. This is a unique opportunity to experience all aspects of building complex software systems: contributing to the strategy, defining the architecture, and building and shipping to production. Grammarly’s engineers and researchers have the freedom to innovate and uncover breakthroughs—and, in turn, influence our product roadmap. The complexity of our technical challenges is growing rapidly as we scale our interfaces, algorithms, and infrastructure. Read more about our stack or hear from our team on our technical blog. As an iOS Engineer at Grammarly, you will: • Use a lot of AWS, write some code, build data pipelines, create brand new systems, and contribute to the existing ones.• Build a platform for managing all aspects of data lifecycle.• Work with partners (researchers, ML engineers, and others) across many other engineering teams, building tools for effortless work with the data.• Influence the big picture: make architectural decisions, research relevant technology, and plan for the future. • Embodies our EAGER values—is ethical, adaptable, gritty, empathetic, and remarkable.• Has experience with system design and building internal tools; can make a good choice of a third-party service to do what you need or, alternatively, can put together a quick and simple solution on your own.• Has good knowledge and at least some experience with AWS. (Alternatively, someone who has deep expertise in Azure or GCE and is willing to learn AWS quickly.)• Has at least some experience with some of the following: Apache Zeppelin, Spark, Python, PySpark, Java, and Scala. (This is not a strict requirement, but we would be impressed!)• Understands data structures and algorithms at a level sufficient to write performant code when working with large datasets or large incoming data streams.• Has at least three years of experience managing a live production environment, preferably a high-load system.• Is a good communicator. We are a very tightly integrated team, and we work together all the time.• There is no mine-yours attitude. We expect colleagues to communicate proactively: talk, email, chat, commit messages—you name it. • Professional growth: We hire people we trust, and we give team members autonomy to do their best work. We also support professional development with training, coaching, and regular feedback.• A connected team: Grammarly builds a product that helps people connect, and we apply this mindset to our own team. We have a highly collaborative culture supported by our EAGER values. We also take time to celebrate our colleagues and accomplishments with global, local, and team-specific events and programs.• Comprehensive benefits: Grammarly offers all team members competitive pay along with a benefits package that includes superior health care. We also offer ample and defined time off, catered lunches, gym and recreation stipends, admission discounts, and more. At Grammarly, we value our differences, and we encourage all—especially those whose identities are traditionally underrepresented in tech organizations—to apply. Grammarly is an equal opportunity company. We do not discriminate on the basis of race, religion, color, gender expression or identity, sexual orientation, national origin, citizenship, age, marital status, veteran status, disability status, criminal prosecution, judgment in a criminal case, or any other characteristic protected by law.",dou,2020-11-12,"Software Engineer, Data Platform@Grammarly",,,{}
MacPaw,https://jobs.dou.ua/companies/macpaw/,Data Engineer,https://jobs.dou.ua/companies/macpaw/vacancies/133086/, Kyiv,12 November 2020,,"Required skills We are looking for an experienced Data Engineer to scale the team. Together with 2 Data Engineers, you will partner closely with all MacPaw products and teams to solve ongoing business problems. We aim to develop smart data direction in the company, and now we start getting more requests for data engineering solutions from our teams (products and services). In this role, you will work closely with Product Analysts and Data Scientists to understand the data needs of different stakeholders across MacPaw, provide proactive solutions, and enable our teams to extract insights and value from data. You will also help us integrate different data sources, improve our data system’s efficiency, reliability, and latency, help automate data pipelines, and improve our data model and overall architecture. Our position is a good match for someone keenly aware of and motivated by driving business value. As part of this team, you will pitch ideas and quickly see the impact your work makes. — Experience with message brokers— Practical skills with Apache BigData ecosystem— Knowledge of building cross-team solutions (Data Scientist, DevOps, Analyst) — Outstanding team— Opportunity to improve process and implement your ideas— Great conditions for education and development within the company (MacPaw Labs days, conferences, workshops, trainings, etc.)— Сare about your health (insurance, office gym, paid sick leaves, etc.) — Work-rest balance support (meditation/sleeping room, 20 vacation days, etc.)— UX driven office, equipment of your choice— English courses— 2 anti-stress cats Project description We are a growing team of developers, designers, and IT-professionals who love what they do. If you are as passionate about making good products, we’ll be thrilled to have you on board.We value happiness and satisfaction above all, so we try our best to enjoy full comfort at work. You’ll have no fixed hours or task overloads here. Creativity, inspiration, solidarity — that’s what MacPaw is about.",dou,2020-11-12,Data Engineer@MacPaw,,,"{""Required skills"": [""We are looking for an experienced Data Engineer to scale the team. Together with 2 Data Engineers, you will partner closely with all MacPaw products and teams to solve ongoing business problems.""], ""Project description"": [""We aim to develop smart data direction in the company, and now we start getting more requests for data engineering solutions from our teams (products and services).""]}"
SoftServe,https://jobs.dou.ua/companies/softserve/,Senior Big Data Engineer (ID 58375),https://jobs.dou.ua/companies/softserve/vacancies/138678/, Dnipro,12 November 2020,,"WE ARE The largest Big Data & AI Center of Excellence in Eastern Europe, our expertise in Machine Learning, AI, Big Data, IoT, Cloud technologies recognized by Google and Amazon.We are transforming the way thousands of global organizations do business by creating the most innovative solutions using proven architecture design methodologies, innovation frameworks, and experience design approaches. Our group successfully delivered 50+ discovery & consulting projects and 75+ implementation projects in 2019.Together with the SEI, Carnegie Mellon University, we invented Smart Decisions game, in order to promote architecture design methodologies, intended to facilitate the design of complex architectures. YOU ARE • Showing an excellent understanding of distributed computing technologies, approaches, and patterns• Really proficient in one of the following programming languages: Java, Scala, or Python• Familiar with Hadoop, NoSQL, MPP technologies, processing frameworks, or other Big Data technology areas: data ingestion, consolidation, streaming or batching• Experienced in at least one of the processing and computation frameworks: Kafka Streams, Storm, Spark, Flink, Beam/DataFlow, Akka, etc.• Experienced in at least one of the RDBMS or NoSQL engines: PostgreSQL, MySQL, Cassandra, HBase, Elasticsearch, Redis, MongoDB, Impala, Kudu, etc.• Having cloud experience (AWS, GCP, or Azure), which is a big plus point• The candidate who has experience implementing Data Lakes, Data Warehousing, or analytics systems (your advantage) YOU WANT TO WORK WITH • Creating sustainable Big Data and AI solutions• Evaluation cutting-edge Big Data technologies, implement PoCs and MVPs• Learning new technologies and obtain certifies• Learning how to design architectures using proven methodologies by SEI, Carnegie Mellon• Becoming a top expert, or a certified Architect• Gaining deep expertise in one of the clouds or multiple clouds TOGETHER WE WILL • Deliver discovery and consulting projects, such as solution design, technology assessment, and architecture evaluation together with CoE Lead Architects• Implement full-scale high-performance Data Platforms and decision-support systems• Utilize rapid prototyping techniques to accelerate the implementation of new technical solutions• Adopt cutting-edge technologies on a challenging project• Provide high-value services to a different range of companies: from startups to Fortune 100• Engage new clients and work closely with Sales team Our benefits • Assimilate best practices from experts, working in the team of top-notch Architects• Work closely or be a part of Google or Amazon professional services• Being a part of Center of Excellence, you will have a chance to address different business and technology challenges, drive multiple projects and initiatives• Boost your communication and leadership skills by obtaining experience on different projects• Attend and speak at international events",dou,2020-11-12,Senior Big Data Engineer (ID 58375)@SoftServe,,,{}
Raiffeisen Bank Aval,https://jobs.dou.ua/companies/aval/,Chief Data Architect,https://jobs.dou.ua/companies/aval/vacancies/138675/," Kyiv, remote",12 November 2020,,"Required skills The expertise we expect for this role:• Communication skills • Must be proficient with DATAHUB solutions/patterns which include ODL(Operational Data Lake), ODS (Operational Data Store) and ADL(Analytic Data Lake)• Must be proficient with Clouds generally and proficiently build the Cloud-Native solutions and tools to design BigData PaaS/IaaS• Must be proficient with BigData Processing/Transformation technologies, BigData Storage technologies, BigData Access technologies generally and associated with Cloud technologies specifically• Must be proficient with Data analysis and synthesis, Data communication, Data governance, Data modeling, Data standards, Data innovation, Metadata management, Problem resolution (data), Strategic thinking (data architecture)• Turning business problems into data design. You can design data architecture by dealing with problems that span different business areas. You can draw links between problems in order to reach common solutions. You can work across multiple subject areas, a single large or complicated subject area• Have experience to transform the data as corporate resources into Enterprise with Data Driven Architecture We offer • Join a large international company that provides possibilities for professional and personal growth• Involve into challenging, large-scale projects which have an impact for our customers• Knowledge sharing in our Group wide IT community including 14 Raiffeisen Banks• Flexible working schedule, 28 days of paid vacation, official employment, attractive social package, distant work possibilities• Competitive salary Responsibilities A chief data architect sets the vision for the organization’s use of data as directed by the business goals. At this level, you will:• Define the technological vision and lead the design of new services or new features and tools • Determine any necessary services and tool enhancements to meet project needs and ensure the feasibility of these upgrades• Ensure the coherence, efficiency, scalability, modularity, and compatibility of the features developed by the team• Analyze and resolve engineering issues pertaining to the services, tools, and/or middleware• Define the measures required to ensure the data engines optimal performance• Act as a point of contact for all technical issues on the data services and tools• Evaluate existing technologies and tools to determine their strengths and weaknesses and recommend those that best meet project objectives and expectations• Be accountable for the definition of the organization’s data strategy• Champion data architecture across the organization and set the standards and ways of working for the data architecture community• Provide advice to project teams and oversee the management of the full data product life cycle and lead the implementation of required solutions• Have responsibility for making sure that organizations systems are designed in accordance with the data architecture Project description Here is the Draft Vision of our Future Data Platform:The Data Platform is divided into the following distinct parts:• Core Data Platform• Onboarded Data Producer/Consumer Applications, Configuration & CustomisationsOnboarding & Management of Data Producer/Consumer ApplicationsApplication, Data Source, Job, Job Data, Job Steps, and Data Access TaxonomiesStandardized Push/Pull Streaming Ingestion InterfacesStandardized Push/Pull Batch Ingestion InterfacesPolyglot Data-lake Storage (e.g., Bucket, KV, NoSQL, Search, Warehouse, SQL, etc.)Customizable Ingestion Conformance-tier ProcessorsCustomizable Ingestion Stage ProcessorsCustomizable Ingestion Archive ProcessorsCustomizable Ingestion Optimised Format (e.g., Parquet/ORC) ProcessorsCustomizable Transformation/Enrichment ProcessorsCustomizable Microservices & API ProcessorsStandardized Query API (e.g., J/ODBC) Data Access InterfacesStandardized Export API, Bucket Endpoint & Notify Data Access InterfacesStandardized Streaming Egress Data Access InterfaceRBAC (Role-Based Access Controls)Secrets/Vault capabilitiesTLS (in-flight) & TDE (at-rest) Encryption capabilitiesOperational Metrics & Monitoring capabilitiesData Platform Resource Utilisation Tracking & Cost Allocation capabilitiesDLM, Archive, Retention capabilitiesSchema Registry capabilitiesLogin & Data Platform Portal Home UIAnalytics Notebook UI (e.g., Jupiter)Data Catalog UIVDI Workbench Portal UIJob Management UIAdministration UI",dou,2020-11-12,Chief Data Architect@Raiffeisen Bank Aval,,,"{""Required skills"": [""The expertise we expect for this role:"", ""Communication skills"", ""Must be proficient with DATAHUB solutions/patterns which include ODL(Operational Data Lake), ODS (Operational Data Store) and ADL(Analytic Data Lake)"", ""Must be proficient with Clouds generally and proficiently build the Cloud-Native solutions and tools to design BigData PaaS/IaaS"", ""Must be proficient with BigData Processing/Transformation technologies, BigData Storage technologies, BigData Access technologies generally and associated with Cloud technologies specifically"", ""Must be proficient with Data analysis and synthesis, Data communication, Data governance, Data modeling, Data standards, Data innovation, Metadata management, Problem resolution (data), Strategic thinking (data architecture)"", ""Turning business problems into data design. You can design data architecture by dealing with problems that span different business areas. You can draw links between problems in order to reach common solutions. You can work across multiple subject areas, a single large or complicated subject area"", ""Have experience to transform the data as corporate resources into Enterprise with Data Driven Architecture""], ""We offer"": [""Join a large international company that provides possibilities for professional and personal growth"", ""Involve into challenging, large-scale projects which have an impact for our customers"", ""Knowledge sharing in our Group wide IT community including 14 Raiffeisen Banks"", ""Flexible working schedule, 28 days of paid vacation, official employment, attractive social package, distant work possibilities"", ""Competitive salary""], ""Responsibilities"": [""A chief data architect sets the vision for the organization\u2019s use of data as directed by the business goals. At this level, you will:"", ""Define the technological vision and lead the design of new services or new features and tools"", ""Determine any necessary services and tool enhancements to meet project needs and ensure the feasibility of these upgrades"", ""Ensure the coherence, efficiency, scalability, modularity, and compatibility of the features developed by the team"", ""Analyze and resolve engineering issues pertaining to the services, tools, and/or middleware"", ""Define the measures required to ensure the data engines optimal performance"", ""Act as a point of contact for all technical issues on the data services and tools"", ""Evaluate existing technologies and tools to determine their strengths and weaknesses and recommend those that best meet project objectives and expectations"", ""Be accountable for the definition of the organization\u2019s data strategy"", ""Champion data architecture across the organization and set the standards and ways of working for the data architecture community"", ""Provide advice to project teams and oversee the management of the full data product life cycle and lead the implementation of required solutions"", ""Have responsibility for making sure that organizations systems are designed in accordance with the data architecture""], ""Project description"": [""Here is the Draft Vision of our Future Data Platform:The Data Platform is divided into the following distinct parts:"", ""Core Data Platform"", ""Onboarded Data Producer/Consumer Applications, Configuration & CustomisationsOnboarding & Management of Data Producer/Consumer ApplicationsApplication, Data Source, Job, Job Data, Job Steps, and Data Access TaxonomiesStandardized Push/Pull Streaming Ingestion InterfacesStandardized Push/Pull Batch Ingestion InterfacesPolyglot Data-lake Storage (e.g., Bucket, KV, NoSQL, Search, Warehouse, SQL, etc.)Customizable Ingestion Conformance-tier ProcessorsCustomizable Ingestion Stage ProcessorsCustomizable Ingestion Archive ProcessorsCustomizable Ingestion Optimised Format (e.g., Parquet/ORC) ProcessorsCustomizable Transformation/Enrichment ProcessorsCustomizable Microservices & API ProcessorsStandardized Query API (e.g., J/ODBC) Data Access InterfacesStandardized Export API, Bucket Endpoint & Notify Data Access InterfacesStandardized Streaming Egress Data Access InterfaceRBAC (Role-Based Access Controls)Secrets/Vault capabilitiesTLS (in-flight) & TDE (at-rest) Encryption capabilitiesOperational Metrics & Monitoring capabilitiesData Platform Resource Utilisation Tracking & Cost Allocation capabilitiesDLM, Archive, Retention capabilitiesSchema Registry capabilitiesLogin & Data Platform Portal Home UIAnalytics Notebook UI (e.g., Jupiter)Data Catalog UIVDI Workbench Portal UIJob Management UIAdministration UI""]}"
IBox,https://jobs.dou.ua/companies/ibox/,PL/SQL Developer,https://jobs.dou.ua/companies/ibox/vacancies/124825/, Kyiv,12 November 2020,$700–1500,"Required skills • Знание теории реляционных баз данных и концепции хранилища данных (DWH);• Знание Oracle SQL и PL/SQL (запросы, представления, функции, и процедуры);• Опыт разработки на Oracle DB на позициях DB Developer, DWH Developer, DB Engineer, SQL/Oracle Developer, DBA, Data/SQL Analyst, или аналогичных (готовы рассматривать кандидатов с эквивалентным опытом на других СУБД);• Опыт оптимизации производительности запросов. As a plus • Опыт визуализации/работы с какой-либо BI-системой (Oracle BI , Power BI, Tableau, Qlik Sense, DOMO, или других);• Знание R или Python. We offer • Зарплата в соответствии с уровнем технических навыков + проектные премии;• Пересмотр ЗП два раза в год;• 24 дня оплачиваемого отпуска;• Обучение с возможностью посещения платных конференций, тренингов, семинаров и т.д• Комфортный офис возле М «Лукьяновская» с удобной транспортной развязкой;• Другие плюшки в офисе (чай / кофе, теннис, PlayStation, йога, английский язык, корпоративные мероприятия и т.д.)• Гибкий рабочий график пн-пт с 8:00–10:00 до 17:00–19:00 (на период карантина удаленка). Responsibilities • Создание моделей данных в хранилище (DWH) на Oracle Database;• Поддержка и оптимизация существующей базы SQL и PL/SQL кода;• Создание и редактирование дашбордов в BI системе (Oracle BI);• Выгрузки и анализ данных по запросам других отделов (тикетам в JIRA). Project description IBox is the Ukrainian instant payments market pioneer.This year we turned 14 years old! Today, our pink and white terminals can be found in the most popular chain stores and malls, banks and companies’ offices, and even in restaurants.We help 4 thousand of our business partners become closer to 15 million customers in 600 cities and towns of the country.300 of our employees work, develop and have fun in our modern offices in 24 regions of Ukraine.",dou,2020-11-12,PL/SQL Developer@IBox,,,"{""Required skills"": [""\u0417\u043d\u0430\u043d\u0438\u0435 \u0442\u0435\u043e\u0440\u0438\u0438 \u0440\u0435\u043b\u044f\u0446\u0438\u043e\u043d\u043d\u044b\u0445 \u0431\u0430\u0437 \u0434\u0430\u043d\u043d\u044b\u0445 \u0438 \u043a\u043e\u043d\u0446\u0435\u043f\u0446\u0438\u0438 \u0445\u0440\u0430\u043d\u0438\u043b\u0438\u0449\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 (DWH);"", ""\u0417\u043d\u0430\u043d\u0438\u0435 Oracle SQL \u0438 PL/SQL (\u0437\u0430\u043f\u0440\u043e\u0441\u044b, \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u044f, \u0444\u0443\u043d\u043a\u0446\u0438\u0438, \u0438 \u043f\u0440\u043e\u0446\u0435\u0434\u0443\u0440\u044b);"", ""\u041e\u043f\u044b\u0442 \u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u043d\u0430 Oracle DB \u043d\u0430 \u043f\u043e\u0437\u0438\u0446\u0438\u044f\u0445 DB Developer, DWH Developer, DB Engineer, SQL/Oracle Developer, DBA, Data/SQL Analyst, \u0438\u043b\u0438 \u0430\u043d\u0430\u043b\u043e\u0433\u0438\u0447\u043d\u044b\u0445 (\u0433\u043e\u0442\u043e\u0432\u044b \u0440\u0430\u0441\u0441\u043c\u0430\u0442\u0440\u0438\u0432\u0430\u0442\u044c \u043a\u0430\u043d\u0434\u0438\u0434\u0430\u0442\u043e\u0432 \u0441 \u044d\u043a\u0432\u0438\u0432\u0430\u043b\u0435\u043d\u0442\u043d\u044b\u043c \u043e\u043f\u044b\u0442\u043e\u043c \u043d\u0430 \u0434\u0440\u0443\u0433\u0438\u0445 \u0421\u0423\u0411\u0414);"", ""\u041e\u043f\u044b\u0442 \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u0438 \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438 \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u0432.""], ""As a plus"": [""\u041e\u043f\u044b\u0442 \u0432\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438/\u0440\u0430\u0431\u043e\u0442\u044b \u0441 \u043a\u0430\u043a\u043e\u0439-\u043b\u0438\u0431\u043e BI-\u0441\u0438\u0441\u0442\u0435\u043c\u043e\u0439 (Oracle BI , Power BI, Tableau, Qlik Sense, DOMO, \u0438\u043b\u0438 \u0434\u0440\u0443\u0433\u0438\u0445);"", ""\u0417\u043d\u0430\u043d\u0438\u0435 R \u0438\u043b\u0438 Python.""], ""We offer"": [""\u0417\u0430\u0440\u043f\u043b\u0430\u0442\u0430 \u0432 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0438\u0438 \u0441 \u0443\u0440\u043e\u0432\u043d\u0435\u043c \u0442\u0435\u0445\u043d\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u043d\u0430\u0432\u044b\u043a\u043e\u0432 + \u043f\u0440\u043e\u0435\u043a\u0442\u043d\u044b\u0435 \u043f\u0440\u0435\u043c\u0438\u0438;"", ""\u041f\u0435\u0440\u0435\u0441\u043c\u043e\u0442\u0440 \u0417\u041f \u0434\u0432\u0430 \u0440\u0430\u0437\u0430 \u0432 \u0433\u043e\u0434;"", ""24 \u0434\u043d\u044f \u043e\u043f\u043b\u0430\u0447\u0438\u0432\u0430\u0435\u043c\u043e\u0433\u043e \u043e\u0442\u043f\u0443\u0441\u043a\u0430;"", ""\u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u0441 \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e\u0441\u0442\u044c\u044e \u043f\u043e\u0441\u0435\u0449\u0435\u043d\u0438\u044f \u043f\u043b\u0430\u0442\u043d\u044b\u0445 \u043a\u043e\u043d\u0444\u0435\u0440\u0435\u043d\u0446\u0438\u0439, \u0442\u0440\u0435\u043d\u0438\u043d\u0433\u043e\u0432, \u0441\u0435\u043c\u0438\u043d\u0430\u0440\u043e\u0432 \u0438 \u0442.\u0434"", ""\u041a\u043e\u043c\u0444\u043e\u0440\u0442\u043d\u044b\u0439 \u043e\u0444\u0438\u0441 \u0432\u043e\u0437\u043b\u0435 \u041c \u00ab\u041b\u0443\u043a\u044c\u044f\u043d\u043e\u0432\u0441\u043a\u0430\u044f\u00bb \u0441 \u0443\u0434\u043e\u0431\u043d\u043e\u0439 \u0442\u0440\u0430\u043d\u0441\u043f\u043e\u0440\u0442\u043d\u043e\u0439 \u0440\u0430\u0437\u0432\u044f\u0437\u043a\u043e\u0439;"", ""\u0414\u0440\u0443\u0433\u0438\u0435 \u043f\u043b\u044e\u0448\u043a\u0438 \u0432 \u043e\u0444\u0438\u0441\u0435 (\u0447\u0430\u0439 / \u043a\u043e\u0444\u0435, \u0442\u0435\u043d\u043d\u0438\u0441, PlayStation, \u0439\u043e\u0433\u0430, \u0430\u043d\u0433\u043b\u0438\u0439\u0441\u043a\u0438\u0439 \u044f\u0437\u044b\u043a, \u043a\u043e\u0440\u043f\u043e\u0440\u0430\u0442\u0438\u0432\u043d\u044b\u0435 \u043c\u0435\u0440\u043e\u043f\u0440\u0438\u044f\u0442\u0438\u044f \u0438 \u0442.\u0434.)"", ""\u0413\u0438\u0431\u043a\u0438\u0439 \u0440\u0430\u0431\u043e\u0447\u0438\u0439 \u0433\u0440\u0430\u0444\u0438\u043a \u043f\u043d-\u043f\u0442 \u0441 8:00\u201310:00 \u0434\u043e 17:00\u201319:00 (\u043d\u0430 \u043f\u0435\u0440\u0438\u043e\u0434 \u043a\u0430\u0440\u0430\u043d\u0442\u0438\u043d\u0430 \u0443\u0434\u0430\u043b\u0435\u043d\u043a\u0430).""], ""Responsibilities"": [""\u0421\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u0434\u0430\u043d\u043d\u044b\u0445 \u0432 \u0445\u0440\u0430\u043d\u0438\u043b\u0438\u0449\u0435 (DWH) \u043d\u0430 Oracle Database;"", ""\u041f\u043e\u0434\u0434\u0435\u0440\u0436\u043a\u0430 \u0438 \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u044f \u0441\u0443\u0449\u0435\u0441\u0442\u0432\u0443\u044e\u0449\u0435\u0439 \u0431\u0430\u0437\u044b SQL \u0438 PL/SQL \u043a\u043e\u0434\u0430;"", ""\u0421\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u0438 \u0440\u0435\u0434\u0430\u043a\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u0434\u0430\u0448\u0431\u043e\u0440\u0434\u043e\u0432 \u0432 BI \u0441\u0438\u0441\u0442\u0435\u043c\u0435 (Oracle BI);"", ""\u0412\u044b\u0433\u0440\u0443\u0437\u043a\u0438 \u0438 \u0430\u043d\u0430\u043b\u0438\u0437 \u0434\u0430\u043d\u043d\u044b\u0445 \u043f\u043e \u0437\u0430\u043f\u0440\u043e\u0441\u0430\u043c \u0434\u0440\u0443\u0433\u0438\u0445 \u043e\u0442\u0434\u0435\u043b\u043e\u0432 (\u0442\u0438\u043a\u0435\u0442\u0430\u043c \u0432 JIRA).""], ""Project description"": [""IBox is the Ukrainian instant payments market pioneer.This year we turned 14 years old! Today, our pink and white terminals can be found in the most popular chain stores and malls, banks and companies\u2019 offices, and even in restaurants.We help 4 thousand of our business partners become closer to 15 million customers in 600 cities and towns of the country.300 of our employees work, develop and have fun in our modern offices in 24 regions of Ukraine.""]}"
Data Science UA,https://jobs.dou.ua/companies/data-science-ua/,Machine Learning Engineer,https://jobs.dou.ua/companies/data-science-ua/vacancies/67871/, Kyiv,11 November 2020,,"Required skills Machine learning engineer develops deep learning / machine learning models, improves existing solutions,and provides results in a form that can be easily deployed, used and maintained.Machine learning engineer works with a full cycle of ML project development, including searching andpreparing datasets, creating deep learning models, testing and comparing, and preparing packages fordeploying. Specialized knowledge:● Understanding theoretical concepts of machine learning and neural networks.● Understanding theoretical concepts of deep learning architectures.● Understanding how these theoretical concepts could be applied to real-world problems.● Knowledge and hands-on experience with Python or other relevant programming languages.● Knowledge and hands-on experience with ML/DL frameworks (PyTorch, TensorFlow etc.).● Knowledge and hands-on experience with tools for data preprocessing and scraping. Skills and abilities:● Strong English verbal and written communication skills.● Ability to work independently with limited supervision. Experience:● Track record in deep learning, data science, machine learning.● Relevant levels of theoretical knowledge in data science and machine learning. Responsibilities ● Collecting, transforming, and preprocessing raw data to prepare it for modeling.● Building machine learning and deep learning models.● Designing, developing, training, and testing models and algorithms.● Providing comparative research on different algorithms and models.● Implementing the model in a form that can be easily used by engineers, documenting its interfaces.● Delivering the model in a form that can be easily deployable and maintained. Project description We are looking for a Machine Learning Engineer for our partner company.Based on different types of data, candidates can create machine learning / deep learning models. Ability to work in short iteration cycles (up to 2 weeks) from initial research to PoC prototype. Ability to work simultaneously on multiple tasks. Ability to work in a solo or a small team and be responsible for the final results. Lots of educational and self-educational activities.",dou,2020-11-12,Machine Learning Engineer@Data Science UA,,,"{""Required skills"": [""Machine learning engineer develops deep learning / machine learning models, improves existing solutions,and provides results in a form that can be easily deployed, used and maintained.Machine learning engineer works with a full cycle of ML project development, including searching andpreparing datasets, creating deep learning models, testing and comparing, and preparing packages fordeploying.""], ""Responsibilities"": [""Specialized knowledge:\u25cf Understanding theoretical concepts of machine learning and neural networks.\u25cf Understanding theoretical concepts of deep learning architectures.\u25cf Understanding how these theoretical concepts could be applied to real-world problems.\u25cf Knowledge and hands-on experience with Python or other relevant programming languages.\u25cf Knowledge and hands-on experience with ML/DL frameworks (PyTorch, TensorFlow etc.).\u25cf Knowledge and hands-on experience with tools for data preprocessing and scraping.""], ""Project description"": [""Skills and abilities:\u25cf Strong English verbal and written communication skills.\u25cf Ability to work independently with limited supervision.""]}"
Datagrok,https://jobs.dou.ua/companies/datagrok/,"Scientific Application Developer (JavaScript, Python)",https://jobs.dou.ua/companies/datagrok/vacancies/138558/, remote,11 November 2020,$2000–3500,"Required skills — At least three years of professional experience— JavaScript or TypeScript— At least one of: Python, R, Java, C#, C++, Matlab/Octave— Knowledge and experience in data science and/or life sciences— Good written English, ability to communicate directly with our clients— Strong verbal and written communication skills— Ability to learn and apply new technologies As a plus — Prior exposure to life sciences— Interest or experience in high performance— Experience with one of WebGL, WebAssembly, Dart Responsibilities — Front-end web and application logic development— Back-end development for scientific as well as data integration purposes— Help our clients write JS applications on top of the Datagrok platform— Design, implement, and operate data management systems — Use and improve developer-facing platform’s JS API— Integrate third-party JS libraries into the platform Project description datagrok.ai is looking for a full-stack application developer with an interest in life sciences to join our strong team of engineers and data scientists. We are a startup developing a next-generation data platform that revolutionizes the industry. Our proprietary technology efficiently processes datasets with tens of millions of rows in the browser and interactively visualizes them. We take pride in the platform’s UX and performance through technologies such as WebAssembly, WebGL, and browser-based multithreaded computations. Our enterprise-ready server supports running scripts in various data-science languages, such as Python and R. Using the full variety of platform facilities, we build real-world applications together with our customers. We are already running commercial projects with the world’s largest pharmaceutical companies. Not only some of these projects are ambitious, but are also positively impactful to millions of lives. Ask us how we are connected to the recent research and development of the COVID-19 vaccines. As the number of projects is rapidly growing, we are in demand for ambitious application developers with an edge in life sciences and/or data science eager to leverage the power of our product. While our core technology is primarily based on Dart and Postgres, it has multiple extension points with components developed in JavaScript, TypeScript, Python, R, Octave, Java, and C++, for building solutions on top of the platform. Primarily you will be involved in developing new applications in JavaScript, TypeScript, Python, and R, using Datagrok API and facilities, expanding the platform with new plugins where needed. Most of the applications to be developed are strongly connected to natural sciences and data science. There is a lot of room to grow, and there will be many opportunities to assume more responsibilities and learn new domains, such as cheminformatics, bioinformatics, deep learning, or high-performance computing. This is a remote position. It would be great if you could send us a project you have developed on your own (github repository is ok), or point to another project you have participated in.",dou,2020-11-12,"Scientific Application Developer (JavaScript, Python)@Datagrok",,,"{""Required skills"": [""At least three years of professional experience"", ""JavaScript or TypeScript"", ""At least one of: Python, R, Java, C#, C++, Matlab/Octave"", ""Knowledge and experience in data science and/or life sciences"", ""Good written English, ability to communicate directly with our clients"", ""Strong verbal and written communication skills"", ""Ability to learn and apply new technologies""], ""As a plus"": [""Prior exposure to life sciences"", ""Interest or experience in high performance"", ""Experience with one of WebGL, WebAssembly, Dart""], ""Responsibilities"": [""Front-end web and application logic development"", ""Back-end development for scientific as well as data integration purposes"", ""Help our clients write JS applications on top of the Datagrok platform"", ""Design, implement, and operate data management systems"", ""Use and improve developer-facing platform\u2019s JS API"", ""Integrate third-party JS libraries into the platform""], ""Project description"": [""datagrok.ai is looking for a full-stack application developer with an interest in life sciences to join our strong team of engineers and data scientists. We are a startup developing a next-generation data platform that revolutionizes the industry. Our proprietary technology efficiently processes datasets with tens of millions of rows in the browser and interactively visualizes them. We take pride in the platform\u2019s UX and performance through technologies such as WebAssembly, WebGL, and browser-based multithreaded computations. Our enterprise-ready server supports running scripts in various data-science languages, such as Python and R.""]}"
Parimatch Tech,https://jobs.dou.ua/companies/parimatch-tech/,Middle Data Analyst,https://jobs.dou.ua/companies/parimatch-tech/vacancies/138549/, Kyiv,11 November 2020,,"We are a highly successful Company with great ambitions. We operate on a very competitive market so every day we are looking for opportunities to be better. To be faster. Even faster. Never stand aside and never afraid to try. Having a lot of ideas we are very open to fresh ones. Equally important, we have resources to bring these into motion. — Develop business reports and dashboards according to business needs;— Ad-hoc analysis and reports;— Present findings and recommendations by creating visualizations of quantitative information;— Conduct data-discovery for data-driven insights;— Work with different teams to develop new data measurement;— Build forecasts and models to discover trends, outliers and dependencies. — 2+ years of demonstrated data analysis experience;— Strong knowledge of SQL (PostgreSQL, PL/SQL,MySQL);— Visualization data analysis/reporting using Tableau/Power BI or similar;— Knowledge of ML (supervised, unsupervised, semi-supervised learning);— Knowledge of Python/R (pandas, scikit-learn, numpy and other). — Knowledge sharing ability;— Experience with big data;— Degree in applied math.",dou,2020-11-12,Middle Data Analyst@Parimatch Tech,,,{}
GlobalLogic,https://jobs.dou.ua/companies/globallogic/,Senior Data Engineer (IRC102529),https://jobs.dou.ua/companies/globallogic/vacancies/138545/, Kyiv,11 November 2020,,"Required skills 3+ years of experience as a Data EngineerHands-on experience with AWS and good knowledge of its sub-components and servicesExperience with SQL, Redshift, Athena, Data modelingElasticsearch and ETL tools (e.g. Matillion, Glue)Knowledge of Lambdas, Containers, FargateCloudformation and/or TerraformExperience with Agile projectsGood understanding of Software Development LifecyclePerfect communication and cooperation skillsUpper-intermediate EnglishAbility to multi-task, prioritize and execute tasks in a fast-paced environment As a plus CloudFormationTerraform We offer Interesting and challenging work in a large and dynamically developing companyExciting projects involving the newest technologiesProfessional development opportunitiesExcellent compensation and benefits package, performance bonus programModern and comfortable office facilities Responsibilities Establish ETL PlatformCapture existing data flows, develop clear pipelinesTransition flows to ETL jobsPackage up complex tasks in containersEstablish a data platform (Redshift)Replicate master data Transform into a new schemaIndex unto Elasticsearch Project description The Client is UK company, providing to market business intelligence for mainly the pharma industry. The core of the product is data platform containing 10+ years of client experience on the market and giving the decision making support over it using ML and other intelligent approaches. The client is in the era of technological transformation and wants to migrate the Data platform to Amazon cloud and build redesigned and refreshed product apps over it.We are welcoming high professionals for this interesting journey, for making great solutions together!",dou,2020-11-12,Senior Data Engineer (IRC102529)@GlobalLogic,,,"{""Required skills"": [""3+ years of experience as a Data EngineerHands-on experience with AWS and good knowledge of its sub-components and servicesExperience with SQL, Redshift, Athena, Data modelingElasticsearch and ETL tools (e.g. Matillion, Glue)Knowledge of Lambdas, Containers, FargateCloudformation and/or TerraformExperience with Agile projectsGood understanding of Software Development LifecyclePerfect communication and cooperation skillsUpper-intermediate EnglishAbility to multi-task, prioritize and execute tasks in a fast-paced environment""], ""As a plus"": [""CloudFormationTerraform""], ""We offer"": [""Interesting and challenging work in a large and dynamically developing companyExciting projects involving the newest technologiesProfessional development opportunitiesExcellent compensation and benefits package, performance bonus programModern and comfortable office facilities""], ""Responsibilities"": [""Establish ETL PlatformCapture existing data flows, develop clear pipelinesTransition flows to ETL jobsPackage up complex tasks in containersEstablish a data platform (Redshift)Replicate master data Transform into a new schemaIndex unto Elasticsearch""], ""Project description"": [""The Client is UK company, providing to market business intelligence for mainly the pharma industry. The core of the product is data platform containing 10+ years of client experience on the market and giving the decision making support over it using ML and other intelligent approaches. The client is in the era of technological transformation and wants to migrate the Data platform to Amazon cloud and build redesigned and refreshed product apps over it.We are welcoming high professionals for this interesting journey, for making great solutions together!""]}"
Jabil Software Services,https://jobs.dou.ua/companies/kuatro-technologies/,ETL & Data Warehousing Architect,https://jobs.dou.ua/companies/kuatro-technologies/vacancies/138518/," Kharkiv, remote",11 November 2020,,"Required skills Responsibilities: • Maintain a deep and broad understanding of the technology stack being used by your organization as the lead technical resource for the design development, and maintenance of the program’s software systems• Provide technology direction, technical leadership, and mentoring to tech lead and development resources• Ensure development work product compliance to approved architecture, design patterns, security practices, and other relevant standards• Manage and support the operation of the SDLC spanning research, requirements analysis, prototyping, design, programming, reviews, documentation, and testing• Lead the technical delivery of effective, high standard software development and support services• Assist in coordinating the activities of development resources across various initiatives to meet business objectives and delivery timeline and cost expectations• Implement proactive processes or monitoring to identify/resolve incidents and defects• Perform research, feasibility analysis, and purpose changes to improve technology assets and/or mature processes in alignment with industry standards or best practices• Partner with Program Leadership to plan and execute project(s), ensure the team has appropriate skills, capacity, and direction to effectively deliver• Provide realistic estimates, timelines, and work break-down structure for input to work plans• Provide oversight and review of technical deliverables such as specifications, designs, and project plans• Drive Major Incident Management response and communication and assist developers in resolving technical issues/incidents• Provide specification or sizing input for infrastructure needs• Lead production turnover planning, change approval, and deployment process As a plus • Theoretical understanding of Machine Learning and Artificial Intelligence principles is an advantage• Hands-on experience implementing ML / AI concepts in an enterprise environment is a bigger advantage We offer We offer: • competitive compensation• excellent benefit package• flexible schedule• sports activities support• health insurance support• corporate English• free lunches• opportunities to grow professionally• well-equipped workplace• convenient office location (Botanichniy Sad metro station) Responsibilities What we’re looking for in a candidate: • 7+ years of experience developing web technologies; 3+ years of experience using cloud technologies• Software configuration management experience (Gitlab, SVN etc.)• Strong database experience; in-depth knowledge of SQL (beneficial to know Graph, NoSQL, Key Value, etc.)• Practical knowledge of ETL processing with demonstratable skills; advanced knowledge of data preparation for consumption, exposure of analytics, analytical platforms, & OLTP Database design (relational transactional DBs)• Advanced knowledge and practical experience of OLAP Database (Data Warehouse) design (snowflake or star schema — typically column indexed — database design)• Ability to conceive and design analytical schemas to capture base statistical pre-aggregated data• Advanced skills in the design and implementation of ETL procedures and packages to efficiently extract data from parent systems without impacting production.• Debug and analysis skills to diagnose issues with pipelines efficiently and identify missing data• Advanced understanding of Calculated Measures, KPIs, statistics and mathematics.• Dynamic and creative skillset to be able to identify novel solutions to unfamiliar problems• Advanced knowledge of MDX, DAX, SQL; Multidimensional and Tabular modeling and development• Passion to learn, self-motivate, and drive execution in an entrepreneurial setting; resourcefulness, demonstrated problem-solving aptitude and critical thinking; excellent communication skills Project description Role: The Senior IT Architect for Jabil’s Data Warehousing in the Analytics & Intelligence space is responsible for leading the design of data solutions and guiding technical leads to execute on those designs. Works with tech leads, devs, BAs, other architects, management, and product owners to design and maintain new and existing solutions for our executive-facing global Unified Metrics platform as we move toward predictive analytics with machine learning.",dou,2020-11-12,ETL & Data Warehousing Architect@Jabil Software Services,,,"{""Required skills"": [""Responsibilities:""], ""As a plus"": [""Maintain a deep and broad understanding of the technology stack being used by your organization as the lead technical resource for the design development, and maintenance of the program\u2019s software systems"", ""Provide technology direction, technical leadership, and mentoring to tech lead and development resources"", ""Ensure development work product compliance to approved architecture, design patterns, security practices, and other relevant standards"", ""Manage and support the operation of the SDLC spanning research, requirements analysis, prototyping, design, programming, reviews, documentation, and testing"", ""Lead the technical delivery of effective, high standard software development and support services"", ""Assist in coordinating the activities of development resources across various initiatives to meet business objectives and delivery timeline and cost expectations"", ""Implement proactive processes or monitoring to identify/resolve incidents and defects"", ""Perform research, feasibility analysis, and purpose changes to improve technology assets and/or mature processes in alignment with industry standards or best practices"", ""Partner with Program Leadership to plan and execute project(s), ensure the team has appropriate skills, capacity, and direction to effectively deliver"", ""Provide realistic estimates, timelines, and work break-down structure for input to work plans"", ""Provide oversight and review of technical deliverables such as specifications, designs, and project plans"", ""Drive Major Incident Management response and communication and assist developers in resolving technical issues/incidents"", ""Provide specification or sizing input for infrastructure needs"", ""Lead production turnover planning, change approval, and deployment process""], ""We offer"": [""Theoretical understanding of Machine Learning and Artificial Intelligence principles is an advantage"", ""Hands-on experience implementing ML / AI concepts in an enterprise environment is a bigger advantage""], ""Responsibilities"": [""We offer:""], ""Project description"": [""competitive compensation"", ""excellent benefit package"", ""flexible schedule"", ""sports activities support"", ""health insurance support"", ""corporate English"", ""free lunches"", ""opportunities to grow professionally"", ""well-equipped workplace"", ""convenient office location (Botanichniy Sad metro station)""]}"
SocialTech,https://jobs.dou.ua/companies/socialtech/,Senior Data Analyst,https://jobs.dou.ua/companies/socialtech/vacancies/138475/, Kyiv,11 November 2020,,"SocialTech is a multimillion-dollar IT company and a game-changer in the social discovery niche. Our HQ is located in Kyiv.We manage a leading product in the industry, and all our key metrics keep growing exponentially from year to year. The key role of this position is design, development, and maintenance of state-of-the-art in-house data-driven solutions for business growth. Examples of such solutions:● Prescriptive analytical systems for evaluating traffic profitability, retention effectiveness, and split test results under highly volatile conditions.● Automation systems for marketing campaigns.● Recommendation systems for user matching on our product.● Automated reporting ecosystem. Key requirements:● 2+ years of Data / Web / Product Analyst experience in IT.● Proven record of solving complex analytical tasks beyond basic descriptive analytics (such as design & development of decision-making or automation systems).● Advanced data stack: SQL+ R / Python + BI (Tableau / Power BI). ● Strong systemic and algorithmic thinking.● Independence and self-sufficiency in your areas of competence. What you will find in SocialTech:● A powerful team of analysts, stakeholders, and developers to inspire you to constant professional growth and new accomplishments.● The feeling of making an impact on a booming IT industry.● Opportunity to work on topmost tasks in your professional field.● A comfortable office, breakfasts, lunches, and other plyushkas. Some internal workings of SocialTech:● We use common sense as our guiding principle in everything from taking days off to workflow.● We give and take open feedback, and ask each other questions, even the stupid ones.● We enjoy spending some of our free time together, both at the office and outside it. If you think we’re a good fit for each other, don’t hesitate to send us your CV!",dou,2020-11-12,Senior Data Analyst@SocialTech,,,{}
Raiffeisen Bank International AG,https://jobs.dou.ua/companies/raiffeisen-bank-international-ag/,Cloud Engineer — Big Data/Data Lake,https://jobs.dou.ua/companies/raiffeisen-bank-international-ag/vacancies/121624/, Kyiv,11 November 2020,от $3000,"Required skills — About 3 years of experience in implementing integration components and applications, especially in the area of big data / cloud projects or systems integration— 2 or more years of Hadoop experience (Hortonworks, Cloudera preferred, AWS Cloud) in building data ingestion and transformation— Hands-on experiences in data discovery, blending data and data cleansing for analytical purposes from various sources of data— Professional experience in designing and developing solutions in Python— Profound experience with CD / DevOps methodology and good overview of related tools or tool chains— Demonstrated ability to build processes that support data transformation, data structures, metadata, dependency and workload management— Fluent knowledge of English; German or another CEE language is appreciated, but not mandatory— Open and team-minded personality and communication skills— Willingness to work in an agile environment— Willingness to travel (Austria/Vienna) As a plus — Real time processing of data such as KAFKA, etc.— Profound experience in Data Engineering— Understanding of banking business in general We offer — Join our dynamic and motivated team in one of the leading banking groups in Austria and Central and Eastern Europe— Strong support from an international banking & technology team— Competitive Salary (based on EUR NBU rate)— Long-term official employment, sustainable and stable working environment. Sick leave, paid vacation (31 days per year).— Medical insurance— Dedication and commitment to develop and educate our employees— Possibility to travel and to contribute to and benefit from international communities— Comfortable work conditions Responsibilities — Collaborate with data and analytics experts to strive for greater functionality in our data systems— Design, use and test the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS big data technologies (DevOps & Continuous Integration)— Drive the advancement of RBI data infrastructure by designing and implementing the underlying logic and structure for how data is set up, cleansed, and ultimately stored for organizational usage— Assemble large, complex data sets that meet functional / non-functional business requirements— Build data integration from various sources and technologies to the data lake infrastructure as part of an agile delivery team— Monitor the capabilities and react on unplanned interruptions ensuring that environments are provided & loaded in time— Manage incidents reported by data deliverers or data consumers and provide service reporting Project description We are building RBI´s next generation data lake as the back-bone for data science and advanced analytics within RBI group across Europe.As Cloud Engineer you will act hand in hand with data scientists, big data architects and data consumers across CEE to plan and execute data integration. You will be in charge for data definition and the mapping from raw data to final data products running on the data lake eco-system. Additionally, in the revolution of classic ETL you will be part of rapid prototyping, visualization and BI projects for big data consumers.",dou,2020-11-12,Cloud Engineer — Big Data/Data Lake@Raiffeisen Bank International AG,,,"{""Required skills"": [""About 3 years of experience in implementing integration components and applications, especially in the area of big data / cloud projects or systems integration"", ""2 or more years of Hadoop experience (Hortonworks, Cloudera preferred, AWS Cloud) in building data ingestion and transformation"", ""Hands-on experiences in data discovery, blending data and data cleansing for analytical purposes from various sources of data"", ""Professional experience in designing and developing solutions in Python"", ""Profound experience with CD / DevOps methodology and good overview of related tools or tool chains"", ""Demonstrated ability to build processes that support data transformation, data structures, metadata, dependency and workload management"", ""Fluent knowledge of English; German or another CEE language is appreciated, but not mandatory"", ""Open and team-minded personality and communication skills"", ""Willingness to work in an agile environment"", ""Willingness to travel (Austria/Vienna)""], ""As a plus"": [""Real time processing of data such as KAFKA, etc."", ""Profound experience in Data Engineering"", ""Understanding of banking business in general""], ""We offer"": [""Join our dynamic and motivated team in one of the leading banking groups in Austria and Central and Eastern Europe"", ""Strong support from an international banking & technology team"", ""Competitive Salary (based on EUR NBU rate)"", ""Long-term official employment, sustainable and stable working environment. Sick leave, paid vacation (31 days per year)."", ""Medical insurance"", ""Dedication and commitment to develop and educate our employees"", ""Possibility to travel and to contribute to and benefit from international communities"", ""Comfortable work conditions""], ""Responsibilities"": [""Collaborate with data and analytics experts to strive for greater functionality in our data systems"", ""Design, use and test the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS big data technologies (DevOps & Continuous Integration)"", ""Drive the advancement of RBI data infrastructure by designing and implementing the underlying logic and structure for how data is set up, cleansed, and ultimately stored for organizational usage"", ""Assemble large, complex data sets that meet functional / non-functional business requirements"", ""Build data integration from various sources and technologies to the data lake infrastructure as part of an agile delivery team"", ""Monitor the capabilities and react on unplanned interruptions ensuring that environments are provided & loaded in time"", ""Manage incidents reported by data deliverers or data consumers and provide service reporting""], ""Project description"": [""We are building RBI\u00b4s next generation data lake as the back-bone for data science and advanced analytics within RBI group across Europe.As Cloud Engineer you will act hand in hand with data scientists, big data architects and data consumers across CEE to plan and execute data integration. You will be in charge for data definition and the mapping from raw data to final data products running on the data lake eco-system. Additionally, in the revolution of classic ETL you will be part of rapid prototyping, visualization and BI projects for big data consumers.""]}"
Sigma Software,https://jobs.dou.ua/companies/sigma-software/,Python Engineer (Real-Time Data Analysis),https://jobs.dou.ua/companies/sigma-software/vacancies/134708/," Kyiv, Kharkiv, Lviv, Dnipro, Odesa, Vinnytsia",11 November 2020,,"Required skills — 3+ years of experience as a Python Developer— Python 3.7+, boto3, asyncio, aiohttp, SQLite, MySQL As a plus — Practical experience in creatine CI\CD infrastructure— Knowledge of AWS cloud and services (EC2, S3, RDS, DynamoDB, CloudWatch, ECR, Lambda) is a must— Experience in automatization of setting multiple EC2 instances— Understanding of Network domains management— Solid knowledge of monitoring tools— DBA skills for DBs optimization Responsibilities — Developing infrastructure software for motor racing mostly in Python 3.7+. This includes services for data analysis, data streaming, data storage, communication which will be running in Cloud and on Raspberry Pi devices.— Manage domains and subdomains, S3s, EC2s, DBs (RDS & DynamoDB), CloudWatch, maybe also cost optimization— Work with EC2 templates/images, Race Graphics cloud rendering fleet creation and various shortcuts/automation tools for AWS— Introduce and support monitoring tools— Work on recalculation of various processes (Python/Node) on existing data and/or restreaming of this data— Work with track mapping tool from Google Maps or recorded lap— Evaluate migration from RDS to DynamoDB Project description Our project provides the solution for monitoring of motorsports events, which includes remote live streaming units, streaming services, telemetry sensors, telemetry analysis services, and intermediate databases with corresponding access services.We create a unique system that helps monitor all the processes of the race, as well as meet the needs of the most demanding spectators.",dou,2020-11-12,Python Engineer (Real-Time Data Analysis)@Sigma Software,,,"{""Required skills"": [""3+ years of experience as a Python Developer"", ""Python 3.7+, boto3, asyncio, aiohttp, SQLite, MySQL""], ""As a plus"": [""Practical experience in creatine CI\\CD infrastructure"", ""Knowledge of AWS cloud and services (EC2, S3, RDS, DynamoDB, CloudWatch, ECR, Lambda) is a must"", ""Experience in automatization of setting multiple EC2 instances"", ""Understanding of Network domains management"", ""Solid knowledge of monitoring tools"", ""DBA skills for DBs optimization""], ""Responsibilities"": [""Developing infrastructure software for motor racing mostly in Python 3.7+. This includes services for data analysis, data streaming, data storage, communication which will be running in Cloud and on Raspberry Pi devices."", ""Manage domains and subdomains, S3s, EC2s, DBs (RDS & DynamoDB), CloudWatch, maybe also cost optimization"", ""Work with EC2 templates/images, Race Graphics cloud rendering fleet creation and various shortcuts/automation tools for AWS"", ""Introduce and support monitoring tools"", ""Work on recalculation of various processes (Python/Node) on existing data and/or restreaming of this data"", ""Work with track mapping tool from Google Maps or recorded lap"", ""Evaluate migration from RDS to DynamoDB""], ""Project description"": [""Our project provides the solution for monitoring of motorsports events, which includes remote live streaming units, streaming services, telemetry sensors, telemetry analysis services, and intermediate databases with corresponding access services.We create a unique system that helps monitor all the processes of the race, as well as meet the needs of the most demanding spectators.""]}"
StartUs Insights,https://jobs.dou.ua/companies/startus/,Innovation Manager — Research and Data Solutions,https://jobs.dou.ua/companies/startus/vacancies/138382/, remote,11 November 2020,$1000–1500,"Required skills Are you fascinated by the world of startups and disruptive innovation?Join StartUs Insights, an international data science company on a mission to map the world’s information on innovation, emerging companies and technologies. Global leaders such as Samsung, Siemens Gamesa, Nestlé and Altair, among others, work with us to gain actionable innovation intelligence. As Innovation Manager you will gain invaluable inter-disciplinary experience by building a bridge between our clients and teams of analysts who identify the trends, technologies and business opportunities of tomorrow. Become part of our team to impact the global digital transformation! YOUR BACKGROUND:— 3+ years of prior work experience in Project Management, Business Development, Consulting, Project Management, Innovation Management or other relevant fields. Previous experience with Business Intelligence, SaaS or Research Products is highly beneficial— Relevant educational background (e.g. in Innovation Management, Business Administration, or STEM) — Inquisitive about emerging technologies, startups and the digitalization of our society— Highly proficient in English. German and other languages are a plus— Strong project management and problem-solving skills— Proficient user of Microsoft Office products (Excel, Powerpoint, Word) We offer — Work in a company where your input influences further product development— Have constant communication and mentorship from senior analysts— Get a clear view of your KPIs and progression possibilities from day one— Access our personal development programs— Possibility to work 32 hours per week— Enjoy remote work and don’t waste your valuable time in traffic— Enjoy paid work-free time and public holidays according to the Ukrainian calendar Responsibilities — Build a bridge between our clients and analyst teams — Initiate new projects with clients and facilitate the fulfillment of the client’s requirements— Work with our AI-driven tool to recognize innovative solutions and technology trends— Execute client success management and build lasting relationships with our clients— Impact our product development and content strategies for our Research Blog: www.startusinsights.com/innovators-guide",dou,2020-11-12,Innovation Manager — Research and Data Solutions@StartUs Insights,,,"{""Required skills"": [""Are you fascinated by the world of startups and disruptive innovation?Join StartUs Insights, an international data science company on a mission to map the world\u2019s information on innovation, emerging companies and technologies.""], ""We offer"": [""Global leaders such as Samsung, Siemens Gamesa, Nestl\u00e9 and Altair, among others, work with us to gain actionable innovation intelligence. As Innovation Manager you will gain invaluable inter-disciplinary experience by building a bridge between our clients and teams of analysts who identify the trends, technologies and business opportunities of tomorrow.""], ""Responsibilities"": [""Become part of our team to impact the global digital transformation!""]}"
Flatlogic LLC,https://jobs.dou.ua/companies/flatlogic-llc/,Python Developer / Python Data Engineer,https://jobs.dou.ua/companies/flatlogic-llc/vacancies/133243/, remote,11 November 2020,от $1500,"Нам бы хотелось, чтобы Вы: — умели разрабатывать и поддерживать ETL процессы (pandas, apache airflow, scrapy, postgres);— имели 2+ лет опыта (Python)— имели опыт с php— имели хорошие знания sql— английский (Advanced+)",dou,2020-11-12,Python Developer / Python Data Engineer@Flatlogic LLC,,,{}
Data Science UA,https://jobs.dou.ua/companies/data-science-ua/,Social Media Marketing Specialist,https://jobs.dou.ua/companies/data-science-ua/vacancies/138346/, Kyiv,10 November 2020,,"Required skills — Excellent English skills;— 2+ years of experience in marketing and SMM;— Experience managing LinkedIn, YouTube, Twitter, Facebook;— Strong community management skills;— Great taste. We offer — A large amount of creative responsibility;— New and exciting professional challenges;— Working with a dedicated native-speaking editor;— Opportunities to grow within the team;— Options contract after one year with the company ;— All employees have access to the company’s financial reports regardless of their role in the team;— Paid career upgrade courses, conferences for professionals, and English classes are available;— Medical insurance for you and your family. Responsibilities — Develop and implement a social media marketing strategy;— Lead content creation for social channels;— Conduct regular performance audits and competitor analysis;— Create conversations and actively participate in them;— Keep up with product changes to ensure all content is up-to-date and relevant. Project description Our partners automates work processes and empowers people to spend more time doing meaningful things. Their clients are digitization pioneers who want to save time, paper, and money. We are looking for an SMM specialist who balances a creative mindset with a data-driven approach, has demonstrable experience managing social media, and is passionate about technology.",dou,2020-11-12,Social Media Marketing Specialist@Data Science UA,,,"{""Required skills"": [""Excellent English skills;"", ""2+ years of experience in marketing and SMM;"", ""Experience managing LinkedIn, YouTube, Twitter, Facebook;"", ""Strong community management skills;"", ""Great taste.""], ""We offer"": [""A large amount of creative responsibility;"", ""New and exciting professional challenges;"", ""Working with a dedicated native-speaking editor;"", ""Opportunities to grow within the team;"", ""Options contract after one year with the company ;"", ""All employees have access to the company\u2019s financial reports regardless of their role in the team;"", ""Paid career upgrade courses, conferences for professionals, and English classes are available;"", ""Medical insurance for you and your family.""], ""Responsibilities"": [""Develop and implement a social media marketing strategy;"", ""Lead content creation for social channels;"", ""Conduct regular performance audits and competitor analysis;"", ""Create conversations and actively participate in them;"", ""Keep up with product changes to ensure all content is up-to-date and relevant.""], ""Project description"": [""Our partners automates work processes and empowers people to spend more time doing meaningful things. Their clients are digitization pioneers who want to save time, paper, and money.""]}"
Proxet,https://jobs.dou.ua/companies/proxet/,Java Engineer with Data Engineering skills,https://jobs.dou.ua/companies/proxet/vacancies/134338/," Kyiv, remote",10 November 2020,,"Required skills Develop and maintain Roku’s cutting edge advertising planning, delivery, and insights products/solutions.You’ll be working on the next generation of Roku’s DMM (Digital Marketing Management) platform based on a real-time bidding solution with high-load up to 3M QPS used by leading advertisers to manage their online ad campaigns across all media channels, device platforms, and advertising exchanges.You’ll become part of a distributed team developing a product that is used by thousands of businesses worldwide. Skills & Experience— Background in computer science or similar quantitative field— 5+ years of professional software development experience— Expert Knowledge of Java— Proficiency in writing efficient SQL— Experience with data frameworks like Spark SQL, Spark Streaming etc— Experience developing high scale and high performance distributed systems for real-time data processing— Product-focused mindset— Team-player with strong interpersonal skills— English — Upper-intermediate or above As a plus — Experience with Big Data and AWS services is a plus— Experience with Python, Sсala, etc— Experience in the advertising domain is a big plus We offer — Challenging work in an international professional environment— 40-hour working week with flexible working hours— Flexible work-from-home policy— Competitive salary— PE accounting and support— 20 paid vacation days per year— 14 paid sick leave days per year— Medical insurance— Annual 250$ deposit for attending external events (conferences, workshops, etc.)— Long-term employment and real opportunities to change roles and projects within the company— Yoga classes, workout corner— Collaborative and friendly team environment Responsibilities — Work with a highly skilled engineering team in all phases of the Agile development process from design to deployment— Design, develop, and maintain a high scale, highly performant real-time data processing solutions.— Work with quality assurance, release engineering, and product management to deliver quality software— Be part of a continuous improvement atmosphere, proactively suggesting improvements to the platform and development processes; anticipate problems or issues that may arise— Deliver constant value back to the business in a highly agile team approaching near-continuous deployment Project description The project: www.roku.com",dou,2020-11-12,Java Engineer with Data Engineering skills@Proxet,,,"{""Required skills"": [""Develop and maintain Roku\u2019s cutting edge advertising planning, delivery, and insights products/solutions.You\u2019ll be working on the next generation of Roku\u2019s DMM (Digital Marketing Management) platform based on a real-time bidding solution with high-load up to 3M QPS used by leading advertisers to manage their online ad campaigns across all media channels, device platforms, and advertising exchanges.You\u2019ll become part of a distributed team developing a product that is used by thousands of businesses worldwide.""], ""As a plus"": [""Skills & Experience"", ""Background in computer science or similar quantitative field"", ""5+ years of professional software development experience"", ""Expert Knowledge of Java"", ""Proficiency in writing efficient SQL"", ""Experience with data frameworks like Spark SQL, Spark Streaming etc"", ""Experience developing high scale and high performance distributed systems for real-time data processing"", ""Product-focused mindset"", ""Team-player with strong interpersonal skills"", ""English"", ""Upper-intermediate or above""], ""We offer"": [""Experience with Big Data and AWS services is a plus"", ""Experience with Python, S\u0441ala, etc"", ""Experience in the advertising domain is a big plus""], ""Responsibilities"": [""Challenging work in an international professional environment"", ""40-hour working week with flexible working hours"", ""Flexible work-from-home policy"", ""Competitive salary"", ""PE accounting and support"", ""20 paid vacation days per year"", ""14 paid sick leave days per year"", ""Medical insurance"", ""Annual 250$ deposit for attending external events (conferences, workshops, etc.)"", ""Long-term employment and real opportunities to change roles and projects within the company"", ""Yoga classes, workout corner"", ""Collaborative and friendly team environment""], ""Project description"": [""Work with a highly skilled engineering team in all phases of the Agile development process from design to deployment"", ""Design, develop, and maintain a high scale, highly performant real-time data processing solutions."", ""Work with quality assurance, release engineering, and product management to deliver quality software"", ""Be part of a continuous improvement atmosphere, proactively suggesting improvements to the platform and development processes; anticipate problems or issues that may arise"", ""Deliver constant value back to the business in a highly agile team approaching near-continuous deployment""]}"
SocialTech,https://jobs.dou.ua/companies/socialtech/,Product Data Analyst,https://jobs.dou.ua/companies/socialtech/vacancies/134277/, Kyiv,10 November 2020,,"SocialTech is a multimillion IT company and a game-changer in the social discovery niche. Our HQ is located in Kiev. We manage a leading product in the industry, and all our key metrics keep growing exponentially from year to year. What you will find in SocialTech:Exciting tasks in each of the following fields: Marketing, Product, Retention, Monetization.The feeling of making an impact on a booming IT industry.A powerful team of analysts and product managers to inspire you to constant professional growth and new accomplishments. Key requirements:● Data stack: SQL + BI (Tableau / Power BI / QLikView) + R / Python.● Love and understanding of numbers, and a will for finding hidden data patterns.● Strong systemic and algorithmic thinking.● Good comprehension and interest in various parts of a business (marketing, product, etc.).● Independence and self-sufficiency in your areas of competence.What we will do together:● Use data to find answers to complex business challenges.● Create, develop, and maintain analytical solutions to optimize business processes.● Automate traffic buying and traffic quality assessment.● Analyze split test results.● Develop a system for automated reporting. Some internal workings of SocialTech:● We use common sense as our guiding principle in everything from taking days off to workflow.● We dislike bureaucracy, give and take open feedback, and ask each other questions, even the stupid ones.● We enjoy spending some of our free time together, both at the office and outside it. If you think we’re a good fit for each other, don’t hesitate to send us your CV!",dou,2020-11-12,Product Data Analyst@SocialTech,,,{}
SocialTech,https://jobs.dou.ua/companies/socialtech/,Data Analyst (User acquisition),https://jobs.dou.ua/companies/socialtech/vacancies/134272/, Kyiv,10 November 2020,,"SocialTech — глобальная IT-компания в индустрии Social Discovery. Мы развиваем собственные продукты, которыми пользуются миллионы людей по всему миру. Аналитика в SocialTech это 10 специалистов, которые в десятках терабайт данных ежедневно ищут ответы на вопросы бизнеса. Отдел состоит из САО, 4 Data Analysts, 2 Data Scientists, 2 Product Analysts, Bi Officer. Небольшая часть того, что мы разработали за два года, с момента найма первого аналитика: ● Платформу для обучения комплексных регрессионных моделей.● Ряд «точных» и «неточных» систем прогноза LTV, которые позволяют эффективно управлять маркетинговыми бюджетами.● Модель, позволяющую справедливо атрибуцировать выручку между аккаунтами юзеров в условиях Repurchase.● Прототип автоматизированной системы оценки А/Б-тестов. Сегодня мы ищем Data Analyst’a, который усилит аналитику маркетинга (самая крупная зона) и продолжит развивать партнерские отношения между отделами. Мы ожидаем, что ты поможешь нам справиться с: ● Эффективным распределением семизначных маркетинговых бюджетов.● Разработкой новых и развитием существующих моделей прогноза/атрибуции при наличии Repurchase, ReMarketing’a и ультра волатильности.● Дизайном метрик для пользователей, кривая доходимости которых растянута на годы.● Разработкой системы для оценки баланса спроса и предложения в ассиметричном маркетплейсе. ● Факторным, когортным, регрессионным и другими видами анализа.● Правильной постановкой вопросов стейкхолдерам и письменной/графической коммуникацией по итогам выполненных задач. Для этого тебе понадобится: ● 1+ на позиции Data Analyst/Scientist.● Data stack: R/Python + SQL + Tableau/PowerBi/Qlik.● Любовь к цифрам и понимание базовой статистики/теор. вероятности.● Развитое критическое и системное мышление.● Способность «на пальцах» доносить сложные мысли.● Будет плюсом коммерческий опыт в разработке/маркетинге/финансах.● Кроме сложных задач и сформировавшейся команды у нас еще есть что предложить: Неограниченные карьерные возможности.● Уютный офис, страховка, завтраки / обеды / снеки.● Компенсацию любых учебных материалов, занятий спортом и участия в конференциях.● Ежегодные корпоративные поездки за границу.● Сплоченный и дружный коллектив.",dou,2020-11-12,Data Analyst (User acquisition)@SocialTech,,,{}
Genesis,https://jobs.dou.ua/companies/genesis-technology-partners/,Data Security Officer,https://jobs.dou.ua/companies/genesis-technology-partners/vacancies/134150/, Kyiv,10 November 2020,,"Genesis is one of the largest IT companies in Ukraine with more than 1500 people in 9 countries, who create products for 200 million users monthly. We are the most high-loaded company in the country and one of the largest partner of Facebook, Google, Snapchat and Apple in the CEE region. Our team is one of the best high-tech teams in Eastern Europe. Genesis was recognised by DOU.UA as the Best IT Employer in Ukraine in the category 800 — 1500 employees over the last years. We received very high ratings across all major evaluation categories such as professional growth, compensation, working conditions, communication with management and colleagues, etc. We are looking for a structured and experienced Data Security Officer to improve and develop our security systems. You will be responsible for IT security framework creation and implementation, reporting to COO of Genesis and working closely with CTO on projects. Through the framework you will oversee security over 10+ projects inside Genesis group. You will also be working tightly with Head of Legal to ensure that users/employees data is protected. Your primary goal will be to be proactively protect business from external threats, being an owner of security frameworks and principles. Responsibilities:• Oversee security level of projects inside group, perform security audits of projects;• Develop and enhance an information security management framework to ensure business sustainability;• Execute high-level security management framework;• Understand and interact with related disciplines to ensure the consistent application of policies and standards across projects and services;• Assess risks and communicate with business stakeholders across the company to raise awareness of risk management concerns;• Work directly with the business units to facilitate risk assessment and risk management processes. Requirements:• 7+ years in IT security/DevOps/Compliance/IT management;• Extensive, practice-based knowledge of security management frameworks, such as ISO/IEC 27001;• Proven track record of IT security audits/projects implementation;• Excellent written and verbal communication skills and high level of personal integrity;• Innovative thinking and leadership with an ability to get right things done;• Experience with contract and vendor negotiations and management will be an advantage. Genesis is a unique place for the development and growth with:• Expertise in the development of high-loaded products in international markets;• Unique opportunities for learning including various training and seminars within the company, access to a valuable and extensive book library, English courses and participation in key IT industry events worldwide;• Perfect working conditions: an excellent office in a 5 minutes’ walk from Taras Shevchenko metro station, free food and drinks in the office, medical insurance. The company has professional coaches in football, basketball and running and supports various sports initiatives (swim races, marathons, etc).",dou,2020-11-12,Data Security Officer@Genesis,,,{}
Uklon,https://jobs.dou.ua/companies/uklon/,Data Scientist,https://jobs.dou.ua/companies/uklon/vacancies/138286/," Kyiv, remote",10 November 2020,,"Required skills Профессійні знання: — Відмінні знання вищої математики, теорії ймовірностей, математичної та прикладної статистики.— Досвід роботи більше 2-х років з Python (статистичні та ML пакети).— Експертні знання в SQL.— Знання Git та Gitflow Workflow.— Наявність успішних кейсів інтеграції DS логік в роботу продукту.— Наявність прикладів успішних інсайтів виявлення неочевидних патернів роботи продукту. Бажані, але необов’язкові (в разі відсутності можемо навчити): — ML serving tools: Kubeflow, MLFlow.— ETLs Tools (one of): Airflow, AWS Glue.— Cloud (one of): AWS, GCP.— ElasticSearch.— BI tools (Tableau, Superset). Пропонуємо: — Квартальні бонуси.— інші соціальні бонуси та компенсації (народження дитини, відшкодування тестування на COVID-19 та ін.).— Можливість відвідувати оплачувані семінари, майстер-класи, тренінги та конференції; участь у хакатонах, форумах, team building sessions.— Обмін знаннями серед команд (лекції, курси, програма менторства); корпоративна бібліотека та підписка на Pluralsight.— Корпоративне навчання з англійської мови різних рівней.— Лікарняні без обмежень.— Основна відпустка 20 робочих днів на рік, додаткова — у день народження, за донорство.— Можливість вибрати зручний формат роботи (офіс/віддалений/міксований формат), тому ти маєш можливість працювати з будь-якого куточка України та світу.— Гнучкий графік.— Корпоративне дозвілля, спортивні секції (футбол, йога). Бути частиною Uklon Team це: — Працювати в команді експертів.— Широкі можливості навчання та зростання в команді, підтримка ініціатив.— Можливість постійно підвищувати свій рівень, виконуючи тільки цікаві задачі з викликом. Обов’язки: — Імплементація інтелектуальних сервісів та логік у роботі основного продукту.— Пропозиції та опрацювання гіпотез по пошуку і покращенню продукту, виявлення bottleneck.— Використання методів і моделей аналізу даних для вирішення задач та пошуку відповідей на продуктові гіпотези.— Реалізація успішних інструментів в прод, підтримка та навчання моделей на постійній основі;оцінка ризиків.— Пошук патернів в поведінці користувачів, участь у генерації продуктових кейсів як результат знайдених інсайтів.— Участь в процесі формування вимог для збору даних. Project description Uklon — найпопулярніший український райдшерінг-сервіс. Ми продуктова IT-компанія, що розробляє та підтримує одну з найбільших інфраструктур на українському ринку, забезпечуючи взаємодію водія та пасажира в межах здійснюваних поїздок. Uklon розробляє власні додатки та веб-сайти, через які користувачі мають можливість робити замовлення автомобіля, а водії — їх отримувати; створює власну систему управління внутрішніми процесами (CRM), власний картографічний сервіс та власні рішення, що відповідає за інтеграцію з платіжними провайдерами та інше. Продукт побудовано на базі мікросервісної архітектури з використанням хмарних обчислень. Наша команда завжди використовує найновіші технологічні рішення та методології, має мікросервісну архітектуру та хмарні обчислення, впроваджує інтелектуальні інструменти та намагається завжди бути найтехнологічнішим лідером у своєму напрямку. Більше про компанію та нашу команду дивись на сайті: careers.uklon.ua",dou,2020-11-12,Data Scientist@Uklon,,,"{""Required skills"": [""\u041f\u0440\u043e\u0444\u0435\u0441\u0441\u0456\u0439\u043d\u0456 \u0437\u043d\u0430\u043d\u043d\u044f:""], ""Project description"": [""\u0412\u0456\u0434\u043c\u0456\u043d\u043d\u0456 \u0437\u043d\u0430\u043d\u043d\u044f \u0432\u0438\u0449\u043e\u0457 \u043c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u043a\u0438, \u0442\u0435\u043e\u0440\u0456\u0457 \u0439\u043c\u043e\u0432\u0456\u0440\u043d\u043e\u0441\u0442\u0435\u0439, \u043c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u0447\u043d\u043e\u0457 \u0442\u0430 \u043f\u0440\u0438\u043a\u043b\u0430\u0434\u043d\u043e\u0457 \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u043a\u0438."", ""\u0414\u043e\u0441\u0432\u0456\u0434 \u0440\u043e\u0431\u043e\u0442\u0438 \u0431\u0456\u043b\u044c\u0448\u0435 2-\u0445 \u0440\u043e\u043a\u0456\u0432 \u0437 Python (\u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u0447\u043d\u0456 \u0442\u0430 ML \u043f\u0430\u043a\u0435\u0442\u0438)."", ""\u0415\u043a\u0441\u043f\u0435\u0440\u0442\u043d\u0456 \u0437\u043d\u0430\u043d\u043d\u044f \u0432 SQL."", ""\u0417\u043d\u0430\u043d\u043d\u044f Git \u0442\u0430 Gitflow Workflow."", ""\u041d\u0430\u044f\u0432\u043d\u0456\u0441\u0442\u044c \u0443\u0441\u043f\u0456\u0448\u043d\u0438\u0445 \u043a\u0435\u0439\u0441\u0456\u0432 \u0456\u043d\u0442\u0435\u0433\u0440\u0430\u0446\u0456\u0457 DS \u043b\u043e\u0433\u0456\u043a \u0432 \u0440\u043e\u0431\u043e\u0442\u0443 \u043f\u0440\u043e\u0434\u0443\u043a\u0442\u0443."", ""\u041d\u0430\u044f\u0432\u043d\u0456\u0441\u0442\u044c \u043f\u0440\u0438\u043a\u043b\u0430\u0434\u0456\u0432 \u0443\u0441\u043f\u0456\u0448\u043d\u0438\u0445 \u0456\u043d\u0441\u0430\u0439\u0442\u0456\u0432 \u0432\u0438\u044f\u0432\u043b\u0435\u043d\u043d\u044f \u043d\u0435\u043e\u0447\u0435\u0432\u0438\u0434\u043d\u0438\u0445 \u043f\u0430\u0442\u0435\u0440\u043d\u0456\u0432 \u0440\u043e\u0431\u043e\u0442\u0438 \u043f\u0440\u043e\u0434\u0443\u043a\u0442\u0443.""]}"
Metinvest Digital,https://jobs.dou.ua/companies/metinvest-digital/,Аналитик данных/Data Analyst,https://jobs.dou.ua/companies/metinvest-digital/vacancies/138253/," Kyiv, remote",10 November 2020,,"Required skills • 2+ года работы Business Analyst, Data Engineer, Data Analyst;• Опыт работы MS SQL SERVER, SSMS, SAP, 1C, Oracle;• Опыт работы с MS Azure (data factory, Azure DB, Machine learning);• Понимание процессов разработки ПО;• Опыт написания технических бизнес-требований;• Опыт работы с методологиями проектирования услуг / дизайна ориентированного на человека / развития клиентов. As a plus • Наличие сертификата Microsoft Azure Fundamentals;• Опыт работы в одной из областей: data governance, data lake, DWH, мобильные приложения, роботизированная автоматизация процессов, машинное обучение, компьютерное зрение, умный город, цифровая рабочая сила. We offer • Конкурентную заработную плату с гибкой системой премий.• Полное официальное трудоустройство и защищенность согласно КЗоТ.• Медицинское страхование.• Корпоративную мобильную связь, которая оплачивается компанией.• Периодические корпоративные мероприятия, в том числе спортивные. Responsibilities • Подготовка бизнес-требований и технических спецификаций путем преобразования идей бизнес-заказчика;• Коммуникация со смежными командами разработки (SAP, 1C, Oracle);• Предоставление технической экспертизы по структурам хранения данных, интеллектуальному анализу данных, очистке данных, манипуляций с данными и т.д.;• Написание технической документации и инструкций по использованию разработанных сервисов.",dou,2020-11-12,Аналитик данных/Data Analyst@Metinvest Digital,,,"{""Required skills"": [""2+ \u0433\u043e\u0434\u0430 \u0440\u0430\u0431\u043e\u0442\u044b Business Analyst, Data Engineer, Data Analyst;"", ""\u041e\u043f\u044b\u0442 \u0440\u0430\u0431\u043e\u0442\u044b MS SQL SERVER, SSMS, SAP, 1C, Oracle;"", ""\u041e\u043f\u044b\u0442 \u0440\u0430\u0431\u043e\u0442\u044b \u0441 MS Azure (data factory, Azure DB, Machine learning);"", ""\u041f\u043e\u043d\u0438\u043c\u0430\u043d\u0438\u0435 \u043f\u0440\u043e\u0446\u0435\u0441\u0441\u043e\u0432 \u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u041f\u041e;"", ""\u041e\u043f\u044b\u0442 \u043d\u0430\u043f\u0438\u0441\u0430\u043d\u0438\u044f \u0442\u0435\u0445\u043d\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u0431\u0438\u0437\u043d\u0435\u0441-\u0442\u0440\u0435\u0431\u043e\u0432\u0430\u043d\u0438\u0439;"", ""\u041e\u043f\u044b\u0442 \u0440\u0430\u0431\u043e\u0442\u044b \u0441 \u043c\u0435\u0442\u043e\u0434\u043e\u043b\u043e\u0433\u0438\u044f\u043c\u0438 \u043f\u0440\u043e\u0435\u043a\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u0443\u0441\u043b\u0443\u0433 / \u0434\u0438\u0437\u0430\u0439\u043d\u0430 \u043e\u0440\u0438\u0435\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u043e\u0433\u043e \u043d\u0430 \u0447\u0435\u043b\u043e\u0432\u0435\u043a\u0430 / \u0440\u0430\u0437\u0432\u0438\u0442\u0438\u044f \u043a\u043b\u0438\u0435\u043d\u0442\u043e\u0432.""], ""As a plus"": [""\u041d\u0430\u043b\u0438\u0447\u0438\u0435 \u0441\u0435\u0440\u0442\u0438\u0444\u0438\u043a\u0430\u0442\u0430 Microsoft Azure Fundamentals;"", ""\u041e\u043f\u044b\u0442 \u0440\u0430\u0431\u043e\u0442\u044b \u0432 \u043e\u0434\u043d\u043e\u0439 \u0438\u0437 \u043e\u0431\u043b\u0430\u0441\u0442\u0435\u0439: data governance, data lake, DWH, \u043c\u043e\u0431\u0438\u043b\u044c\u043d\u044b\u0435 \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u044f, \u0440\u043e\u0431\u043e\u0442\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u0430\u044f \u0430\u0432\u0442\u043e\u043c\u0430\u0442\u0438\u0437\u0430\u0446\u0438\u044f \u043f\u0440\u043e\u0446\u0435\u0441\u0441\u043e\u0432, \u043c\u0430\u0448\u0438\u043d\u043d\u043e\u0435 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435, \u043a\u043e\u043c\u043f\u044c\u044e\u0442\u0435\u0440\u043d\u043e\u0435 \u0437\u0440\u0435\u043d\u0438\u0435, \u0443\u043c\u043d\u044b\u0439 \u0433\u043e\u0440\u043e\u0434, \u0446\u0438\u0444\u0440\u043e\u0432\u0430\u044f \u0440\u0430\u0431\u043e\u0447\u0430\u044f \u0441\u0438\u043b\u0430.""], ""We offer"": [""\u041a\u043e\u043d\u043a\u0443\u0440\u0435\u043d\u0442\u043d\u0443\u044e \u0437\u0430\u0440\u0430\u0431\u043e\u0442\u043d\u0443\u044e \u043f\u043b\u0430\u0442\u0443 \u0441 \u0433\u0438\u0431\u043a\u043e\u0439 \u0441\u0438\u0441\u0442\u0435\u043c\u043e\u0439 \u043f\u0440\u0435\u043c\u0438\u0439."", ""\u041f\u043e\u043b\u043d\u043e\u0435 \u043e\u0444\u0438\u0446\u0438\u0430\u043b\u044c\u043d\u043e\u0435 \u0442\u0440\u0443\u0434\u043e\u0443\u0441\u0442\u0440\u043e\u0439\u0441\u0442\u0432\u043e \u0438 \u0437\u0430\u0449\u0438\u0449\u0435\u043d\u043d\u043e\u0441\u0442\u044c \u0441\u043e\u0433\u043b\u0430\u0441\u043d\u043e \u041a\u0417\u043e\u0422."", ""\u041c\u0435\u0434\u0438\u0446\u0438\u043d\u0441\u043a\u043e\u0435 \u0441\u0442\u0440\u0430\u0445\u043e\u0432\u0430\u043d\u0438\u0435."", ""\u041a\u043e\u0440\u043f\u043e\u0440\u0430\u0442\u0438\u0432\u043d\u0443\u044e \u043c\u043e\u0431\u0438\u043b\u044c\u043d\u0443\u044e \u0441\u0432\u044f\u0437\u044c, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u043e\u043f\u043b\u0430\u0447\u0438\u0432\u0430\u0435\u0442\u0441\u044f \u043a\u043e\u043c\u043f\u0430\u043d\u0438\u0435\u0439."", ""\u041f\u0435\u0440\u0438\u043e\u0434\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u043a\u043e\u0440\u043f\u043e\u0440\u0430\u0442\u0438\u0432\u043d\u044b\u0435 \u043c\u0435\u0440\u043e\u043f\u0440\u0438\u044f\u0442\u0438\u044f, \u0432 \u0442\u043e\u043c \u0447\u0438\u0441\u043b\u0435 \u0441\u043f\u043e\u0440\u0442\u0438\u0432\u043d\u044b\u0435.""], ""Responsibilities"": [""\u041f\u043e\u0434\u0433\u043e\u0442\u043e\u0432\u043a\u0430 \u0431\u0438\u0437\u043d\u0435\u0441-\u0442\u0440\u0435\u0431\u043e\u0432\u0430\u043d\u0438\u0439 \u0438 \u0442\u0435\u0445\u043d\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u0441\u043f\u0435\u0446\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0439 \u043f\u0443\u0442\u0435\u043c \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u044f \u0438\u0434\u0435\u0439 \u0431\u0438\u0437\u043d\u0435\u0441-\u0437\u0430\u043a\u0430\u0437\u0447\u0438\u043a\u0430;"", ""\u041a\u043e\u043c\u043c\u0443\u043d\u0438\u043a\u0430\u0446\u0438\u044f \u0441\u043e \u0441\u043c\u0435\u0436\u043d\u044b\u043c\u0438 \u043a\u043e\u043c\u0430\u043d\u0434\u0430\u043c\u0438 \u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u043a\u0438 (SAP, 1C, Oracle);"", ""\u041f\u0440\u0435\u0434\u043e\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0442\u0435\u0445\u043d\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u044d\u043a\u0441\u043f\u0435\u0440\u0442\u0438\u0437\u044b \u043f\u043e \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u0430\u043c \u0445\u0440\u0430\u043d\u0435\u043d\u0438\u044f \u0434\u0430\u043d\u043d\u044b\u0445, \u0438\u043d\u0442\u0435\u043b\u043b\u0435\u043a\u0442\u0443\u0430\u043b\u044c\u043d\u043e\u043c\u0443 \u0430\u043d\u0430\u043b\u0438\u0437\u0443 \u0434\u0430\u043d\u043d\u044b\u0445, \u043e\u0447\u0438\u0441\u0442\u043a\u0435 \u0434\u0430\u043d\u043d\u044b\u0445, \u043c\u0430\u043d\u0438\u043f\u0443\u043b\u044f\u0446\u0438\u0439 \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 \u0438 \u0442.\u0434.;"", ""\u041d\u0430\u043f\u0438\u0441\u0430\u043d\u0438\u0435 \u0442\u0435\u0445\u043d\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438 \u0438 \u0438\u043d\u0441\u0442\u0440\u0443\u043a\u0446\u0438\u0439 \u043f\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u044e \u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u0430\u043d\u043d\u044b\u0445 \u0441\u0435\u0440\u0432\u0438\u0441\u043e\u0432.""]}"
DEVrepublik,https://jobs.dou.ua/companies/devrepublik/,Data Engineering Course Instructor,https://jobs.dou.ua/companies/devrepublik/vacancies/102066/, remote,10 November 2020,,"Required skills We are looking for a Data Engineering Instructor to join our team.This is a part-time position. You will have an opportunity to share your knowledge with the ones who also want to master the Data Engineering profession. Qualifications & Experience needed:— Teaching experience or desire to teach/share your knowledge — is a great advantage— 3+ years of work experience— Experience with AWS, Redshift, Athena, Spark, Hive— Familiarity with stream platform Kafka— Proficient SQL skills— Proficiency in one of Python, Java, Scala, or a similar programming language— Familiarity with BI and analytics tools— Independent work according to the deadlines— Excellent communication skills— Good written and spoken English As a plus You enjoy partnering with people and have excellent project management skills and follow through with excellent teaching skills. You’ve got a gift for explaining complicated concepts in a beginner-friendly way. You are fond of teaching and explaining different stuff to other people. We offer We believe this is a win-win situation for us and the instructor since by teaching others you will get a lot of other perks such as public speaking skills, reorganizing your own knowledge and skill set, implementation of the organized skills at work, and of course overall development of soft skills.Sounds like you? Write to us :) Project description IT bootcamps are a unique solution to the problem of providing top-quality training in a short amount of time. Based on a military model, IT bootcamp has different goals but similar methods in creating an intense, low-distraction setting for learning.",dou,2020-11-12,Data Engineering Course Instructor@DEVrepublik,,,"{""Required skills"": [""We are looking for a Data Engineering Instructor to join our team.This is a part-time position. You will have an opportunity to share your knowledge with the ones who also want to master the Data Engineering profession. Qualifications & Experience needed:"", ""Teaching experience or desire to teach/share your knowledge"", ""is a great advantage"", ""3+ years of work experience"", ""Experience with AWS, Redshift, Athena, Spark, Hive"", ""Familiarity with stream platform Kafka"", ""Proficient SQL skills"", ""Proficiency in one of Python, Java, Scala, or a similar programming language"", ""Familiarity with BI and analytics tools"", ""Independent work according to the deadlines"", ""Excellent communication skills"", ""Good written and spoken English""], ""As a plus"": [""You enjoy partnering with people and have excellent project management skills and follow through with excellent teaching skills. You\u2019ve got a gift for explaining complicated concepts in a beginner-friendly way. You are fond of teaching and explaining different stuff to other people.""], ""We offer"": [""We believe this is a win-win situation for us and the instructor since by teaching others you will get a lot of other perks such as public speaking skills, reorganizing your own knowledge and skill set, implementation of the organized skills at work, and of course overall development of soft skills.Sounds like you? Write to us :)""], ""Project description"": [""IT bootcamps are a unique solution to the problem of providing top-quality training in a short amount of time. Based on a military model, IT bootcamp has different goals but similar methods in creating an intense, low-distraction setting for learning.""]}"
Quantum,https://jobs.dou.ua/companies/quantum/,Data Engineer,https://jobs.dou.ua/companies/quantum/vacancies/138186/, Kharkiv,09 November 2020,,"Required skills — 4+ years of experience with Python;— Knowledge and experience with Google Cloud and AWS, Kubernetes;— Strong knowledge and experience with Docker;— Strong knowledge and experience with DB design;— Upper-Intermediate English. We offer — Exchange of experience, professional development;— A strong team, a healthy atmosphere;— Flexible working time;— 20 days paid vacation;— Paid sick leave;— Paid remote work;— 8-hour working day and 5-day working week;— English lessons and massage service in the office (partially paid by the company);— Opportunity to take part in conferences, meetups etc. (fully or partially paid by the company);— Regular company events. Responsibilities — Make architecture for production solutions;— Build data processing pipelines;— Review architecture and suggest solutions for improvement;— Detect and solve performance issues on architectural, design and code levels. Project description The client is Nutanix — a leading cloud computing software company that sells hyper-converged infrastructure (HCI) appliances and software-defined storage. Nutanix was announced an MQ Leader in the November 2018 Magic Quadrant for Hyperconverged Infrastructure.The solution orchestrates services from a provider without intervention and helps customers change the state of their involved resources using simple instructions. Create BMaaS to work with variety of cloud providers (Azure, Google Cloud etc.)",dou,2020-11-12,Data Engineer@Quantum,,,"{""Required skills"": [""4+ years of experience with Python;"", ""Knowledge and experience with Google Cloud and AWS, Kubernetes;"", ""Strong knowledge and experience with Docker;"", ""Strong knowledge and experience with DB design;"", ""Upper-Intermediate English.""], ""We offer"": [""Exchange of experience, professional development;"", ""A strong team, a healthy atmosphere;"", ""Flexible working time;"", ""20 days paid vacation;"", ""Paid sick leave;"", ""Paid remote work;"", ""8-hour working day and 5-day working week;"", ""English lessons and massage service in the office (partially paid by the company);"", ""Opportunity to take part in conferences, meetups etc. (fully or partially paid by the company);"", ""Regular company events.""], ""Responsibilities"": [""Make architecture for production solutions;"", ""Build data processing pipelines;"", ""Review architecture and suggest solutions for improvement;"", ""Detect and solve performance issues on architectural, design and code levels.""], ""Project description"": [""The client is Nutanix"", ""a leading cloud computing software company that sells hyper-converged infrastructure (HCI) appliances and software-defined storage. Nutanix was announced an MQ Leader in the November 2018 Magic Quadrant for Hyperconverged Infrastructure.The solution orchestrates services from a provider without intervention and helps customers change the state of their involved resources using simple instructions. Create BMaaS to work with variety of cloud providers (Azure, Google Cloud etc.)""]}"
Perion,https://jobs.dou.ua/companies/septa-digital-agency/,Data Engineer,https://jobs.dou.ua/companies/septa-digital-agency/vacancies/138169/, Kyiv,09 November 2020,,"Required skills • Deep knowledge and hands-on experience in Python• Experience with Spark• Experience with data warehousing solutions (Redshift, Clickhouse, Bigquery)• Deep knowledge of relational (PostgreSQL) and non-relational (MongoDB) databases• Experience in building ETL / ELT pipelines• Experience with building data lakes• Understanding of machine learning pipelines As a plus • Hands-on experience with AWS infrastructure• Experience using AWS S3 + Athena or similar solutions of different providers• Experience in adtech, understanding the domain data specifics• Experience with the data science stack in Python: pandas, numpy, tensorflow We offer • A skilled tech team across 3 locations (Kyiv, Tel Aviv, Paris)• Challenging data engineering tasks, where solution can be built from scratch• Competitive salary Responsibilities Your mission:As a key member of the development team, you will contribute to high ¬value projects such as:• Design the data management architecture for a reporting tool with various data sources• Implement the solution for raw data storage• Build the data processing pipeline• Design and implement the optimal data storage for various use cases including real-time reporting, machine learning, r&d Project description MakeMeReach is a fast-growing advertising cloud platform empowering world-leading agencies (Havas, Dentsu-Aegis, Social.Lab, OMD...) and advertisers (Disney, Birchbox, Audible, MAPFRE...) to maximize digital advertising performance at lower cost.As an official partner of major networks including Facebook, Instagram, Google, Twitter, Snapchat and Pinterest, we support contemporary channels that help brands and agencies to attract audiences and measure the results of their cross-channel campaigns in one unified, actionable dashboard that cuts across companies, franchises and branches.MakeMeReach is proud to be part of the Perion Network group a global public company, traded at the Nasdaq, incorporated in 1999.",dou,2020-11-12,Data Engineer@Perion,,,"{""Required skills"": [""Deep knowledge and hands-on experience in Python"", ""Experience with Spark"", ""Experience with data warehousing solutions (Redshift, Clickhouse, Bigquery)"", ""Deep knowledge of relational (PostgreSQL) and non-relational (MongoDB) databases"", ""Experience in building ETL / ELT pipelines"", ""Experience with building data lakes"", ""Understanding of machine learning pipelines""], ""As a plus"": [""Hands-on experience with AWS infrastructure"", ""Experience using AWS S3 + Athena or similar solutions of different providers"", ""Experience in adtech, understanding the domain data specifics"", ""Experience with the data science stack in Python: pandas, numpy, tensorflow""], ""We offer"": [""A skilled tech team across 3 locations (Kyiv, Tel Aviv, Paris)"", ""Challenging data engineering tasks, where solution can be built from scratch"", ""Competitive salary""], ""Responsibilities"": [""Your mission:As a key member of the development team, you will contribute to high \u00acvalue projects such as:"", ""Design the data management architecture for a reporting tool with various data sources"", ""Implement the solution for raw data storage"", ""Build the data processing pipeline"", ""Design and implement the optimal data storage for various use cases including real-time reporting, machine learning, r&d""], ""Project description"": [""MakeMeReach is a fast-growing advertising cloud platform empowering world-leading agencies (Havas, Dentsu-Aegis, Social.Lab, OMD...) and advertisers (Disney, Birchbox, Audible, MAPFRE...) to maximize digital advertising performance at lower cost.As an official partner of major networks including Facebook, Instagram, Google, Twitter, Snapchat and Pinterest, we support contemporary channels that help brands and agencies to attract audiences and measure the results of their cross-channel campaigns in one unified, actionable dashboard that cuts across companies, franchises and branches.MakeMeReach is proud to be part of the Perion Network group a global public company, traded at the Nasdaq, incorporated in 1999.""]}"
R&D Center,https://jobs.dou.ua/companies/rnd-center/,Sensor Fusion Research Engineer,https://jobs.dou.ua/companies/rnd-center/vacancies/138138/," Kyiv, Lviv, remote",09 November 2020,,"You will join a team that researches motion localization features for a number of hardware products at different stages. You will be responsible for:— requirements gathering;— scientific paper analysis;— proposal/discussion of ideas;— implementation, integration, and testing of developed solutions. Your everyday activities will be connected to tasks like:— development of motion detection & localization algorithms using a set of sensors;— improvement of existing algorithms;— multi-sensor fusion for better motion detection & localization;— object classification;— depth estimation; — Strong understanding of Probability & Statistics, and Linear Algebra.— Strong analytical skills and creative thinking.— Ability and desire to read and analyze scientific papers.— Excellent knowledge of CS fundamentals, algorithms, and data structures.— Hands-on experience with C++ (11+) development.— Positive attitude, responsibility, and passion for improvement.— Good written and spoken English. — Nice presentation and communication skills.— Experience with Digital Signal Processing, Control Theory, Robotics, SLAM.— Experience with Python (scikit learn, numpy, pandas, scipy, TensorFlow, PyTorch).— Experience with and deep understanding of Machine Learning/Computer Vision. — Working on the world’s most impactful security products. More than that, an opportunity to get some of those for personal usage — Competitive salary and perks— PE accounting and support— WFH and remote working mode possibility. Partial furniture compensation— Social package, including medical insurance available from the start date and sports compensation after the trial period— 18 paid vacation days per year, paid public holidays according to the Ukrainian legislation— Educational possibilities like corporate courses, knowledge hubs, and free English classes. Semiannual performance review— Free meals, fruits, and snacks when working in the office",dou,2020-11-12,Sensor Fusion Research Engineer@R&D Center,,,{}
StartUs Insights,https://jobs.dou.ua/companies/startus/,Business Development Manager — SaaS and Data Products,https://jobs.dou.ua/companies/startus/vacancies/138107/, remote,09 November 2020,$1000–1500,"Required skills We are StartUs Insights, an international data science company on a mission to map the world’s information on innovation, emerging companies and technologies. Global leaders such as Samsung, Siemens Gamesa, Nestlé and Altair, among others, work with us to gain actionable innovation intelligence. As Business Development Manager you will build lasting relationships with existing and potential clients. As your role is critical on our way to growth, you will be mentored by and work closely with our Founders and Senior Business Developers. Become part of our team to impact the global digital transformation! YOUR BACKGROUND:— 3+ years of prior work experience in Business Development, Sales, Consulting or Innovation Management. — - Previous experience with Business Intelligence, SaaS or Research Products is highly beneficial— Strong relevant educational background ( e.g. in Business Administration, Innovation Management or STEM) — A solution-oriented, open approach to work, with strong communication and negotiating skills— Inquisitive about emerging technologies, startups and the digitalization of our society— Highly proficient in English— Experience in virtual sales is beneficial We offer BENEFITS:— Work in a company where your input directly impacts our strategic direction— Get personal & topic-driven mentorship from Founders, Executives and Senior Analysts— Access our personal development programs— Enjoy remote work and don’t waste your valuable time commuting— Package includes 25 paid work-free days per year, public holidays and trips to headquarters in Vienna Responsibilities — Build successful business partnerships with existing and future corporate clients — Present StartUs Insights to industry leaders & turn leads into partners— Build, manage and maintain an active sales funnel— Support us in diversifying our global client portfolio— Shape our strategic development that will provide the best value for our clients",dou,2020-11-12,Business Development Manager — SaaS and Data Products@StartUs Insights,,,"{""Required skills"": [""We are StartUs Insights, an international data science company on a mission to map the world\u2019s information on innovation, emerging companies and technologies.""], ""We offer"": [""Global leaders such as Samsung, Siemens Gamesa, Nestl\u00e9 and Altair, among others, work with us to gain actionable innovation intelligence. As Business Development Manager you will build lasting relationships with existing and potential clients. As your role is critical on our way to growth, you will be mentored by and work closely with our Founders and Senior Business Developers. Become part of our team to impact the global digital transformation!""], ""Responsibilities"": [""YOUR BACKGROUND:"", ""3+ years of prior work experience in Business Development, Sales, Consulting or Innovation Management."", ""- Previous experience with Business Intelligence, SaaS or Research Products is highly beneficial"", ""Strong relevant educational background ( e.g. in Business Administration, Innovation Management or STEM)"", ""A solution-oriented, open approach to work, with strong communication and negotiating skills"", ""Inquisitive about emerging technologies, startups and the digitalization of our society"", ""Highly proficient in English"", ""Experience in virtual sales is beneficial""]}"
Innoteka,https://jobs.dou.ua/companies/innoteka/,React Developer for Music Data Analytics Platform. Remote,https://jobs.dou.ua/companies/innoteka/vacancies/138092/, remote,09 November 2020,$3000–4000,"Required skills Your skills— Strong mid / senior level of experience with JS, React, Responsive web apps— Confident speaking and writing English to structure, specify and implement ideas together— Would be nice — experience with Hybrid apps, design eye, love for great UI We offer Offer— $3,000 — $4,000 /m, can be paid to FOP / Payoneer / Revolut (if you want)— Remote, Flexible schedule as soon as plans and deadlines are met— Direct communications within the team, no PMs etc— 20 days per year paid vacation— Remote outstaffing model (hired by Innoteka, collaborate with Client) Hiring process— Fast, non-bureaucratic, without test tasks— 1 non-technical conversation with Innoteka’s founder— [maybe] 1 technical interview on Innoteka’s side— 1 final conversation with client’s founder and CTO— Possible start date — around 25th Nov — 5th Dec Responsibilities Work / Roadmap— Develop new features for responsive web app utilizing backend and third party APIs— New platforms integrations (TikTok, Twitch, Deezer, Shazam, BandCamp, Amazon Playlists...)— Playlists recommendations, Artists listing page, Widget to display KPIs...— Plans to expand into analytics for social media, video streaming and podcasts Stack— JS, HTML, CSS, SASS, Bootstrap— React (without Redux), Material-UI, Reach Router— Chartist.js, D3.js— Webpack— REST API— GitLab (deployment, wiki and task management) Project description Cross-platform music data analytics service helping 30.000 musicians understand who playlists, uploads and writes about their music. It gathers, simplifies and visualizes data from millions data points and turns it into fun-to-use insights, highly ​valuable by musicians and labels. Team— 10 music and tech enthusiasts, very experienced on both music and engineering sides— Founders worked at Warner Music, Sony Music, now own Independent label & Recording studio— Tech team is currently 3 experienced developers in Europe (you’ll be #4 engineer on the team)",dou,2020-11-12,React Developer for Music Data Analytics Platform. Remote@Innoteka,,,"{""Required skills"": [""Your skills"", ""Strong mid / senior level of experience with JS, React, Responsive web apps"", ""Confident speaking and writing English to structure, specify and implement ideas together"", ""Would be nice"", ""experience with Hybrid apps, design eye, love for great UI""], ""We offer"": [""Offer"", ""$3,000"", ""$4,000 /m, can be paid to FOP / Payoneer / Revolut (if you want)"", ""Remote, Flexible schedule as soon as plans and deadlines are met"", ""Direct communications within the team, no PMs etc"", ""20 days per year paid vacation"", ""Remote outstaffing model (hired by Innoteka, collaborate with Client)""], ""Responsibilities"": [""Hiring process"", ""Fast, non-bureaucratic, without test tasks"", ""1 non-technical conversation with Innoteka\u2019s founder"", ""[maybe] 1 technical interview on Innoteka\u2019s side"", ""1 final conversation with client\u2019s founder and CTO"", ""Possible start date"", ""around 25th Nov"", ""5th Dec""], ""Project description"": [""Work / Roadmap"", ""Develop new features for responsive web app utilizing backend and third party APIs"", ""New platforms integrations (TikTok, Twitch, Deezer, Shazam, BandCamp, Amazon Playlists...)"", ""Playlists recommendations, Artists listing page, Widget to display KPIs..."", ""Plans to expand into analytics for social media, video streaming and podcasts""]}"
UKEESS Software House,https://jobs.dou.ua/companies/ukeess-software-house/,Middle/Senior AI/ML Engineer,https://jobs.dou.ua/companies/ukeess-software-house/vacancies/138089/," Lviv, remote",09 November 2020,,"Required skills UKEESS Software House a Ukrainian software company with 16-years of developing experience is currently looking for a Middle/Senior AI/ML Engineer to join our ambitious team in Lviv (either full-time or remote). Technology Stack:C#/.NET Core 2.0/EF Core 2.1/Ruby/MSSQL/PostgreSQL/Angular6/AWS (EC2, S3, EKS, etc.) You are our ideal candidate if You have:— 3+ years of working experience in data science and machine learning— Strong knowledge and skills in at least one AI/ML domain— Experienced in computer science fundamentals such as object-oriented design, data structures, and algorithms— Experience in the NLP area As a plus Will be a plus:— Familiarity with cloud ML Platforms— Upper-Intermediate English level— Experience with C#— Experience with OCR processing concepts/applications We offer What we offer our new colleague:— Interesting full-time position in a highly professional team and friendly company— Competitive compensation (also depending on the technical level of the candidate)— Flexible work schedule— Individual approach to a profession— Free English classes (online)— 3 health packages to choose from— Annual paid vacation and state holidays celebration— Lack of bureaucracy and micromanagement After the pandemic, in our office you will have:— Modern, comfortable office facilities (a barbecue zone, kitchens, company paid lunches, lounge rooms, coffee machines, etc.)— Foreign business trips— On-site parking lot and charge station for Electric Cars— Corporate gifts, celebrations, and fun activities— Sports activities: ping-pong, soccer, work-out Responsibilities In this position you will:— Bring new ideas and be ready to implement them— Communicate closely with the product team— Train, fine-tune and evaluate ML models— Work with engineers to deploy ready ML models Project description About The Project:This is a big cloud solution for the B2B finance area from the USA. With this platform, companies can optimize and automate their processes. Customers, using our platform, can instantly convert scanned tax returns into complex and comprehensive financial reports and analysis with less human interaction. This enables companies, for example, to make error-free, data-driven, credit decisions in a flash, etc. And our platform doesn’t waste hours for manually spreading deals, that could be declined, and simplifies a lot of manual processes.",dou,2020-11-12,Middle/Senior AI/ML Engineer@UKEESS Software House,,,"{""Required skills"": [""UKEESS Software House a Ukrainian software company with 16-years of developing experience is currently looking for a Middle/Senior AI/ML Engineer to join our ambitious team in Lviv (either full-time or remote).""], ""As a plus"": [""Technology Stack:C#/.NET Core 2.0/EF Core 2.1/Ruby/MSSQL/PostgreSQL/Angular6/AWS (EC2, S3, EKS, etc.)""], ""We offer"": [""You are our ideal candidate if You have:"", ""3+ years of working experience in data science and machine learning"", ""Strong knowledge and skills in at least one AI/ML domain"", ""Experienced in computer science fundamentals such as object-oriented design, data structures, and algorithms"", ""Experience in the NLP area""], ""Responsibilities"": [""Will be a plus:"", ""Familiarity with cloud ML Platforms"", ""Upper-Intermediate English level"", ""Experience with C#"", ""Experience with OCR processing concepts/applications""], ""Project description"": [""What we offer our new colleague:"", ""Interesting full-time position in a highly professional team and friendly company"", ""Competitive compensation (also depending on the technical level of the candidate)"", ""Flexible work schedule"", ""Individual approach to a profession"", ""Free English classes (online)"", ""3 health packages to choose from"", ""Annual paid vacation and state holidays celebration"", ""Lack of bureaucracy and micromanagement""]}"
All company jobs,https://jobs.dou.ua/companies/go-diksi-grup/vacancies/,Data Collection and Analysis Expert,https://jobs.dou.ua/companies/go-diksi-grup/vacancies/138049/, Kyiv,09 November 2020,,"Required skills Experience in R and/or Python, and OpenRefine.Experience in creating information / analytical products based on data. As a plus Portfolio or other information about the experience of providing such services.Recommendations (if available).The presence of a registered private individual (FOP). We offer Term of providing services: 12 months.Participation in interesting projects. Responsibilities Scope of services and expected results: — Data collection:*automated collection (download) of data from websites (web scraping);*manual or semi-automated from unstructured resources (for example, recognition of information from photo, scanned copies, pdf documents).— Data Processing:*restructuring of data sets in a form acceptable for analysis (machine-readable format);*preparation of aggregate data sets;*conversion of unstructured tabular data into machine-readable format;*checking data sets for technical errors, for example, missing punctuation marks, defects in data digitization;*clearing data using regular expressions;*refining the data, for example, enriching the data set with additional indicators, combining it with other data sets, etc.— Quantitative data analysis and participation in the creation of information and / or analytical materials, in particular the preparation of descriptive statistics and data presentation.— Participation in meetings, conferences and project activities.— Preparation of reports on the results of work performed. Project description Online system “Energy Sector Online Map (ESOM)” achieves Developing and Maintaining Online Tools for the Dissemination of Energy Data — easy access to most of the gathered data of the energy sector.",dou,2020-11-12,Data Collection and Analysis Expert@All company jobs,,,"{""Required skills"": [""Experience in R and/or Python, and OpenRefine.Experience in creating information / analytical products based on data.""], ""As a plus"": [""Portfolio or other information about the experience of providing such services.Recommendations (if available).The presence of a registered private individual (FOP).""], ""We offer"": [""Term of providing services: 12 months.Participation in interesting projects.""], ""Responsibilities"": [""Scope of services and expected results:""], ""Project description"": [""Data collection:*automated collection (download) of data from websites (web scraping);*manual or semi-automated from unstructured resources (for example, recognition of information from photo, scanned copies, pdf documents)."", ""Data Processing:*restructuring of data sets in a form acceptable for analysis (machine-readable format);*preparation of aggregate data sets;*conversion of unstructured tabular data into machine-readable format;*checking data sets for technical errors, for example, missing punctuation marks, defects in data digitization;*clearing data using regular expressions;*refining the data, for example, enriching the data set with additional indicators, combining it with other data sets, etc."", ""Quantitative data analysis and participation in the creation of information and / or analytical materials, in particular the preparation of descriptive statistics and data presentation."", ""Participation in meetings, conferences and project activities."", ""Preparation of reports on the results of work performed.""]}"
SoftServe,https://jobs.dou.ua/companies/softserve/,"Middle/Senior Big Data Engineer (Python+GCP), ID 56083",https://jobs.dou.ua/companies/softserve/vacancies/133336/," Kharkiv, remote",09 November 2020,,"WE ARE The largest Big Data & AI Center of Excellence in Eastern Europe, our expertise in Machine Learning, AI, Big Data, IoT, Cloud technologies recognized by Google and Amazon.We are transforming the way thousands of global organizations do business by creating the most innovative solutions using proven architecture design methodologies, innovation frameworks, and experience design approaches. In 2019, our group successfully delivered 50+ discovery & consulting projects and 75+ implementation projects.Together with the SEI, and Carnegie Mellon University, we invented Smart Decisions game, in order to promote architecture design methodologies, intended to facilitate the design of complex architectures.Please read more at smartdecisionsgame.com YOU ARE An expert with • Excellent understanding of distributed computing technologies, approaches, and patterns• Proficiency in one of the following programming languages: Java, Scala, or Python• Cloud experience which is a big plus point: AWS, GCP or Azure• Hands-on experience with Hadoop, NoSQL, MPP technologies, processing frameworks, or other Big Data technology areas: data ingestion, consolidation, streaming, or batching• Skill in at least one of the processing and computation frameworks: Kafka Streams, Storm, Spark, Flink, Beam/DataFlow, Akka, etc.• Experience in at least one of the RDBMS or NoSQL engines: PostgreSQL, MySQL, Cassandra, HBase, Elasticsearch, Redis, MongoDB, Impala, Kudu, etc.• Background of implementing Data Lakes, Data Warehousing, or analytics systems is a big advantage YOU WANT TO WORK WITH Opportunity to • Create sustainable Big Data and AI solutions• Evaluate cutting-edge Big Data technologies, implement PoCs and MVPs• Learn new technologies and obtain certifies• Learn how to design architectures using proven methodologies by SEI, Carnegie Mellon• Become a top expert, or certified Architect• Gain deep expertise in one of the clouds or multiple clouds TOGETHER WE WILL • Deliver discovery and consulting projects, such as solution design, technology assessment and architecture evaluation together with CoE Lead Architects• Implement full-scale high-performance Data Platforms and decision-support systems• Utilize rapid prototyping techniques to accelerate the implementation of new technical solutions• Adopt cutting-edge technologies on a challenging project• Provide high-value services to different range of companies: from startups to Fortune 100• Engage new clients and work closely with the Sales teamOur benefits • Assimilate best practices from experts, working in the team of top-notch Architects• Work closely or be a part of Google or Amazon professional services• Being a part of Center of Excellence, you will have a chance to address different business and technology challenges, drive multiple projects and initiatives• Boost your communication and leadership skills by obtaining experience on different projects• Attend and speak at international events",dou,2020-11-12,"Middle/Senior Big Data Engineer (Python+GCP), ID 56083@SoftServe",,,{}
Gismart,https://jobs.dou.ua/companies/gismart_com/,Data Engineer,https://jobs.dou.ua/companies/gismart_com/vacancies/134402/, Kyiv,09 November 2020,,"Required skills Responsibilities— Develop data pipelines for internal DWH.— Create integrations for 3rd party systems.— Work closely with the BI/DWH team. Requirements— At least 2 years of commercial development experience with Python.— Excellent knowledge of database concepts and practical experience in SQL.— Experience with Docker.— Good knowledge of Linux / shell scripting. Will be a plus— Knowledge of Apache Airflow, Apache Kafka, Amazon Redshift.— Experience in another programming/scripting languages, such as JS, Go, etc. Benefits&Perks— Medical Insurance (100% sick leave coverage).— Paid vacation (18 working days per year, all national holidays, and 3 personal days).— Educational possibilities and free corporate English classes.— Breakfast in the office.— Gifting for major life events.— PE accounting and support.— Trip abroad with all the Gismart employees once a year.— Сompensation for sports activities. Project description Gismart is a leading developer and publisher of mobile games and entertainment-focused apps with over 500 million downloads. The company operates within three primary focus areas — hyper-casual games, third-party game publishing, and entertainment apps. Gismart’s top titles include Cool Goal!, Domino Smash, Cleon, Oil Tycoon, Happy Hockey!, Physics Puzzle Idle, Beat Maker Go, Piano Crush, DJ it!, WeDrum, and others. Today, Gismart unites over 350 professionals. The company is based in London, Minsk, Beijing, and Kyiv. In 2020, the Financial Times named Gismart the fastest-growing company in the games industry placing the company sixth overall out of 1000 European companies. For more information on Gismart, visit gismart.com.",dou,2020-11-12,Data Engineer@Gismart,,,"{""Required skills"": [""Responsibilities"", ""Develop data pipelines for internal DWH."", ""Create integrations for 3rd party systems."", ""Work closely with the BI/DWH team.""], ""Project description"": [""Requirements"", ""At least 2 years of commercial development experience with Python."", ""Excellent knowledge of database concepts and practical experience in SQL."", ""Experience with Docker."", ""Good knowledge of Linux / shell scripting.""]}"
Admixer,https://jobs.dou.ua/companies/admixer-advertising-technologies/,Product Manager DMP / Data Operations,https://jobs.dou.ua/companies/admixer-advertising-technologies/vacancies/120220/, Kyiv,09 November 2020,,"Required skills Давай знакомиться. Мы — команда Admixer. Вот уже 10 лет мы разрабатываем решения для всех участников AdTech рынка:-создаем необходимые инструменты для покупки, оптимизации, измерения и проведения межплатформенных и многоканальных рекламных кампаний;-реализовываем успешные стратегии продаж через RTB и Header Bidding аукционы благодаря нашим партнерствам мирового уровня;-наши офисы и дата-центры расположены по всему миру. Мы очень ценим такие навыки:— Если ты имеешь 1-3 года применимого опыта (опыт работы с любой DMP платформой)— Умеешь качественно проводить анализ данных, есть опыт работы с цифровыми / онлайн-продуктами и средой электронной коммерции— English upper intermediate— Знание инструментов цифрового маркетинга — Google, Adobe и т.д Опыт работы с Google Tag Manager, тегами веб-сайтов, SEO и SEM, DV360 или другими DSP— Полное понимание платформ данных в медиапространстве, как на инфраструктурном (базовом), так и на операционном уровне (эксперт)— Понимание интеграции поставки данных, источников данных, таксономии, пользовательского интерфейса, функциональности платформы— Желательно иметь опыт написания технической документации и практических рекомендаций для оперативных задач We offer — участие в производстве передовых технологических продуктов для клиентов по всему миру;— много интересных задач, которые потребуют технических, аналитических и коммуникативных навыков;— уникальную профессиональную среду (тут дружба и поддержка друг друга закалялась годами!);— наш принцип: развивая себя, ты развиваешь свою команду, развивая свою команду, ты развиваешь компанию. Мы все трудимся над одной целью, и одна из ключевых задач — персонально развитие каждого! — конкурентную зарплату, которая зависит от опыта и навыков;— хорошие условия работы в удобнейшем офисе (если тебе удобно работать вверх тормашками — пожалуйста, для нас главное — результат!— отпуск (28 дней), больничные, страховка, трудоустройство — все как у людей;— каждые 4 квартала — пересмотр твоих навыков и зарплаты;— ну и корпоративы, куда ж без них!:) Responsibilities — Помощь BizDev в Research (анализ конкурентов, сопоставление спецификаций)— Умение понятно сформулировать и объяснить тренды, проблематику и рекомендации для внутренних и клиентских команд в четкой, сжатой и целенаправленной форме— Участие/проведение встреч с ключевыми и стратегическими партнерами и клиентами DSP— Подключение дата провайдеров (переговоры, подключения, интеграции, соблюдение договренностей с ними)— Содействовать в формировании, разработке и реализации кампаний, основанных на данных. Настройка и управление данных из нескольких источников— Помощь во внедрении данных клиентов в платформу Admixer. В сотрудничестве с командами Data Engineering и Customer Success обеспечить бесперебойный и «гладкий» процесс онбординга. — Выявлять и устранять любые проблемы с интеграциями и / или подключением, решая их с командами внутри компании и клиентами— Участие в процессе разработки (Product ownership) — согласование изменений функционала в процессе разработки. Формирование требований. — Декомпозиция задач (разделение требований на итерации). Разработка тех документации— Имплементация. Проверка запущенной функциональности «в бою» — сопоставление фактических результатов с ожидаемыми. Составление отчетов и аналитики. Если ты не до конца уверен(а), насколько ты подходишь под эту роль, попробуй ответить на эти 2 вопроса (для начала себе):1. Как бы ты оценивал(а) перфоманс dmp с помощь статистики, какая статистика тебе для этого нужна будет?2. Распиши, как ты видишь процесс интеграции с дата провайдером. Какие подводные камни могут быть?Если ты сможешь ответить себе — поделить ответом с нами, чтоб мы могли наконец познакомиться поближе:) Project description Сегодня в активе Admixer линейка решений для бизнесов разного масштаба. Больше о том, над чем мы работаем ты можешь узнать тут: admixer.com Больше о нас:Facebook www.facebook.com/AdmixerInstagram www.instagram.com/admixertechnologiesСайт академии: www.admixer.academy Давай поработаем вместе?",dou,2020-11-12,Product Manager DMP / Data Operations@Admixer,,,"{""Required skills"": [""\u0414\u0430\u0432\u0430\u0439 \u0437\u043d\u0430\u043a\u043e\u043c\u0438\u0442\u044c\u0441\u044f. \u041c\u044b"", ""\u043a\u043e\u043c\u0430\u043d\u0434\u0430 Admixer. \u0412\u043e\u0442 \u0443\u0436\u0435 10 \u043b\u0435\u0442 \u043c\u044b \u0440\u0430\u0437\u0440\u0430\u0431\u0430\u0442\u044b\u0432\u0430\u0435\u043c \u0440\u0435\u0448\u0435\u043d\u0438\u044f \u0434\u043b\u044f \u0432\u0441\u0435\u0445 \u0443\u0447\u0430\u0441\u0442\u043d\u0438\u043a\u043e\u0432 AdTech \u0440\u044b\u043d\u043a\u0430:-\u0441\u043e\u0437\u0434\u0430\u0435\u043c \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u044b\u0435 \u0438\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442\u044b \u0434\u043b\u044f \u043f\u043e\u043a\u0443\u043f\u043a\u0438, \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u0438, \u0438\u0437\u043c\u0435\u0440\u0435\u043d\u0438\u044f \u0438 \u043f\u0440\u043e\u0432\u0435\u0434\u0435\u043d\u0438\u044f \u043c\u0435\u0436\u043f\u043b\u0430\u0442\u0444\u043e\u0440\u043c\u0435\u043d\u043d\u044b\u0445 \u0438 \u043c\u043d\u043e\u0433\u043e\u043a\u0430\u043d\u0430\u043b\u044c\u043d\u044b\u0445 \u0440\u0435\u043a\u043b\u0430\u043c\u043d\u044b\u0445 \u043a\u0430\u043c\u043f\u0430\u043d\u0438\u0439;-\u0440\u0435\u0430\u043b\u0438\u0437\u043e\u0432\u044b\u0432\u0430\u0435\u043c \u0443\u0441\u043f\u0435\u0448\u043d\u044b\u0435 \u0441\u0442\u0440\u0430\u0442\u0435\u0433\u0438\u0438 \u043f\u0440\u043e\u0434\u0430\u0436 \u0447\u0435\u0440\u0435\u0437 RTB \u0438 Header Bidding \u0430\u0443\u043a\u0446\u0438\u043e\u043d\u044b \u0431\u043b\u0430\u0433\u043e\u0434\u0430\u0440\u044f \u043d\u0430\u0448\u0438\u043c \u043f\u0430\u0440\u0442\u043d\u0435\u0440\u0441\u0442\u0432\u0430\u043c \u043c\u0438\u0440\u043e\u0432\u043e\u0433\u043e \u0443\u0440\u043e\u0432\u043d\u044f;-\u043d\u0430\u0448\u0438 \u043e\u0444\u0438\u0441\u044b \u0438 \u0434\u0430\u0442\u0430-\u0446\u0435\u043d\u0442\u0440\u044b \u0440\u0430\u0441\u043f\u043e\u043b\u043e\u0436\u0435\u043d\u044b \u043f\u043e \u0432\u0441\u0435\u043c\u0443 \u043c\u0438\u0440\u0443.""], ""We offer"": [""\u041c\u044b \u043e\u0447\u0435\u043d\u044c \u0446\u0435\u043d\u0438\u043c \u0442\u0430\u043a\u0438\u0435 \u043d\u0430\u0432\u044b\u043a\u0438:"", ""\u0415\u0441\u043b\u0438 \u0442\u044b \u0438\u043c\u0435\u0435\u0448\u044c 1-3 \u0433\u043e\u0434\u0430 \u043f\u0440\u0438\u043c\u0435\u043d\u0438\u043c\u043e\u0433\u043e \u043e\u043f\u044b\u0442\u0430 (\u043e\u043f\u044b\u0442 \u0440\u0430\u0431\u043e\u0442\u044b \u0441 \u043b\u044e\u0431\u043e\u0439 DMP \u043f\u043b\u0430\u0442\u0444\u043e\u0440\u043c\u043e\u0439)"", ""\u0423\u043c\u0435\u0435\u0448\u044c \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u043e \u043f\u0440\u043e\u0432\u043e\u0434\u0438\u0442\u044c \u0430\u043d\u0430\u043b\u0438\u0437 \u0434\u0430\u043d\u043d\u044b\u0445, \u0435\u0441\u0442\u044c \u043e\u043f\u044b\u0442 \u0440\u0430\u0431\u043e\u0442\u044b \u0441 \u0446\u0438\u0444\u0440\u043e\u0432\u044b\u043c\u0438 / \u043e\u043d\u043b\u0430\u0439\u043d-\u043f\u0440\u043e\u0434\u0443\u043a\u0442\u0430\u043c\u0438 \u0438 \u0441\u0440\u0435\u0434\u043e\u0439 \u044d\u043b\u0435\u043a\u0442\u0440\u043e\u043d\u043d\u043e\u0439 \u043a\u043e\u043c\u043c\u0435\u0440\u0446\u0438\u0438"", ""English upper intermediate"", ""\u0417\u043d\u0430\u043d\u0438\u0435 \u0438\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442\u043e\u0432 \u0446\u0438\u0444\u0440\u043e\u0432\u043e\u0433\u043e \u043c\u0430\u0440\u043a\u0435\u0442\u0438\u043d\u0433\u0430"", ""Google, Adobe \u0438 \u0442.\u0434 \u041e\u043f\u044b\u0442 \u0440\u0430\u0431\u043e\u0442\u044b \u0441 Google Tag Manager, \u0442\u0435\u0433\u0430\u043c\u0438 \u0432\u0435\u0431-\u0441\u0430\u0439\u0442\u043e\u0432, SEO \u0438 SEM, DV360 \u0438\u043b\u0438 \u0434\u0440\u0443\u0433\u0438\u043c\u0438 DSP"", ""\u041f\u043e\u043b\u043d\u043e\u0435 \u043f\u043e\u043d\u0438\u043c\u0430\u043d\u0438\u0435 \u043f\u043b\u0430\u0442\u0444\u043e\u0440\u043c \u0434\u0430\u043d\u043d\u044b\u0445 \u0432 \u043c\u0435\u0434\u0438\u0430\u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0441\u0442\u0432\u0435, \u043a\u0430\u043a \u043d\u0430 \u0438\u043d\u0444\u0440\u0430\u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u043d\u043e\u043c (\u0431\u0430\u0437\u043e\u0432\u043e\u043c), \u0442\u0430\u043a \u0438 \u043d\u0430 \u043e\u043f\u0435\u0440\u0430\u0446\u0438\u043e\u043d\u043d\u043e\u043c \u0443\u0440\u043e\u0432\u043d\u0435 (\u044d\u043a\u0441\u043f\u0435\u0440\u0442)"", ""\u041f\u043e\u043d\u0438\u043c\u0430\u043d\u0438\u0435 \u0438\u043d\u0442\u0435\u0433\u0440\u0430\u0446\u0438\u0438 \u043f\u043e\u0441\u0442\u0430\u0432\u043a\u0438 \u0434\u0430\u043d\u043d\u044b\u0445, \u0438\u0441\u0442\u043e\u0447\u043d\u0438\u043a\u043e\u0432 \u0434\u0430\u043d\u043d\u044b\u0445, \u0442\u0430\u043a\u0441\u043e\u043d\u043e\u043c\u0438\u0438, \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u0441\u043a\u043e\u0433\u043e \u0438\u043d\u0442\u0435\u0440\u0444\u0435\u0439\u0441\u0430, \u0444\u0443\u043d\u043a\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e\u0441\u0442\u0438 \u043f\u043b\u0430\u0442\u0444\u043e\u0440\u043c\u044b"", ""\u0416\u0435\u043b\u0430\u0442\u0435\u043b\u044c\u043d\u043e \u0438\u043c\u0435\u0442\u044c \u043e\u043f\u044b\u0442 \u043d\u0430\u043f\u0438\u0441\u0430\u043d\u0438\u044f \u0442\u0435\u0445\u043d\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438 \u0438 \u043f\u0440\u0430\u043a\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u0439 \u0434\u043b\u044f \u043e\u043f\u0435\u0440\u0430\u0442\u0438\u0432\u043d\u044b\u0445 \u0437\u0430\u0434\u0430\u0447""], ""Responsibilities"": [""\u0443\u0447\u0430\u0441\u0442\u0438\u0435 \u0432 \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0441\u0442\u0432\u0435 \u043f\u0435\u0440\u0435\u0434\u043e\u0432\u044b\u0445 \u0442\u0435\u0445\u043d\u043e\u043b\u043e\u0433\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u043f\u0440\u043e\u0434\u0443\u043a\u0442\u043e\u0432 \u0434\u043b\u044f \u043a\u043b\u0438\u0435\u043d\u0442\u043e\u0432 \u043f\u043e \u0432\u0441\u0435\u043c\u0443 \u043c\u0438\u0440\u0443;"", ""\u043c\u043d\u043e\u0433\u043e \u0438\u043d\u0442\u0435\u0440\u0435\u0441\u043d\u044b\u0445 \u0437\u0430\u0434\u0430\u0447, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043f\u043e\u0442\u0440\u0435\u0431\u0443\u044e\u0442 \u0442\u0435\u0445\u043d\u0438\u0447\u0435\u0441\u043a\u0438\u0445, \u0430\u043d\u0430\u043b\u0438\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u0438 \u043a\u043e\u043c\u043c\u0443\u043d\u0438\u043a\u0430\u0442\u0438\u0432\u043d\u044b\u0445 \u043d\u0430\u0432\u044b\u043a\u043e\u0432;"", ""\u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u0443\u044e \u043f\u0440\u043e\u0444\u0435\u0441\u0441\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u0443\u044e \u0441\u0440\u0435\u0434\u0443 (\u0442\u0443\u0442 \u0434\u0440\u0443\u0436\u0431\u0430 \u0438 \u043f\u043e\u0434\u0434\u0435\u0440\u0436\u043a\u0430 \u0434\u0440\u0443\u0433 \u0434\u0440\u0443\u0433\u0430 \u0437\u0430\u043a\u0430\u043b\u044f\u043b\u0430\u0441\u044c \u0433\u043e\u0434\u0430\u043c\u0438!);"", ""\u043d\u0430\u0448 \u043f\u0440\u0438\u043d\u0446\u0438\u043f: \u0440\u0430\u0437\u0432\u0438\u0432\u0430\u044f \u0441\u0435\u0431\u044f, \u0442\u044b \u0440\u0430\u0437\u0432\u0438\u0432\u0430\u0435\u0448\u044c \u0441\u0432\u043e\u044e \u043a\u043e\u043c\u0430\u043d\u0434\u0443, \u0440\u0430\u0437\u0432\u0438\u0432\u0430\u044f \u0441\u0432\u043e\u044e \u043a\u043e\u043c\u0430\u043d\u0434\u0443, \u0442\u044b \u0440\u0430\u0437\u0432\u0438\u0432\u0430\u0435\u0448\u044c \u043a\u043e\u043c\u043f\u0430\u043d\u0438\u044e. \u041c\u044b \u0432\u0441\u0435 \u0442\u0440\u0443\u0434\u0438\u043c\u0441\u044f \u043d\u0430\u0434 \u043e\u0434\u043d\u043e\u0439 \u0446\u0435\u043b\u044c\u044e, \u0438 \u043e\u0434\u043d\u0430 \u0438\u0437 \u043a\u043b\u044e\u0447\u0435\u0432\u044b\u0445 \u0437\u0430\u0434\u0430\u0447"", ""\u043f\u0435\u0440\u0441\u043e\u043d\u0430\u043b\u044c\u043d\u043e \u0440\u0430\u0437\u0432\u0438\u0442\u0438\u0435 \u043a\u0430\u0436\u0434\u043e\u0433\u043e!"", ""\u043a\u043e\u043d\u043a\u0443\u0440\u0435\u043d\u0442\u043d\u0443\u044e \u0437\u0430\u0440\u043f\u043b\u0430\u0442\u0443, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u0437\u0430\u0432\u0438\u0441\u0438\u0442 \u043e\u0442 \u043e\u043f\u044b\u0442\u0430 \u0438 \u043d\u0430\u0432\u044b\u043a\u043e\u0432;"", ""\u0445\u043e\u0440\u043e\u0448\u0438\u0435 \u0443\u0441\u043b\u043e\u0432\u0438\u044f \u0440\u0430\u0431\u043e\u0442\u044b \u0432 \u0443\u0434\u043e\u0431\u043d\u0435\u0439\u0448\u0435\u043c \u043e\u0444\u0438\u0441\u0435 (\u0435\u0441\u043b\u0438 \u0442\u0435\u0431\u0435 \u0443\u0434\u043e\u0431\u043d\u043e \u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c \u0432\u0432\u0435\u0440\u0445 \u0442\u043e\u0440\u043c\u0430\u0448\u043a\u0430\u043c\u0438"", ""\u043f\u043e\u0436\u0430\u043b\u0443\u0439\u0441\u0442\u0430, \u0434\u043b\u044f \u043d\u0430\u0441 \u0433\u043b\u0430\u0432\u043d\u043e\u0435"", ""\u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442!"", ""\u043e\u0442\u043f\u0443\u0441\u043a (28 \u0434\u043d\u0435\u0439), \u0431\u043e\u043b\u044c\u043d\u0438\u0447\u043d\u044b\u0435, \u0441\u0442\u0440\u0430\u0445\u043e\u0432\u043a\u0430, \u0442\u0440\u0443\u0434\u043e\u0443\u0441\u0442\u0440\u043e\u0439\u0441\u0442\u0432\u043e"", ""\u0432\u0441\u0435 \u043a\u0430\u043a \u0443 \u043b\u044e\u0434\u0435\u0439;"", ""\u043a\u0430\u0436\u0434\u044b\u0435 4 \u043a\u0432\u0430\u0440\u0442\u0430\u043b\u0430"", ""\u043f\u0435\u0440\u0435\u0441\u043c\u043e\u0442\u0440 \u0442\u0432\u043e\u0438\u0445 \u043d\u0430\u0432\u044b\u043a\u043e\u0432 \u0438 \u0437\u0430\u0440\u043f\u043b\u0430\u0442\u044b;"", ""\u043d\u0443 \u0438 \u043a\u043e\u0440\u043f\u043e\u0440\u0430\u0442\u0438\u0432\u044b, \u043a\u0443\u0434\u0430 \u0436 \u0431\u0435\u0437 \u043d\u0438\u0445!:)""], ""Project description"": [""\u041f\u043e\u043c\u043e\u0449\u044c BizDev \u0432 Research (\u0430\u043d\u0430\u043b\u0438\u0437 \u043a\u043e\u043d\u043a\u0443\u0440\u0435\u043d\u0442\u043e\u0432, \u0441\u043e\u043f\u043e\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0441\u043f\u0435\u0446\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0439)"", ""\u0423\u043c\u0435\u043d\u0438\u0435 \u043f\u043e\u043d\u044f\u0442\u043d\u043e \u0441\u0444\u043e\u0440\u043c\u0443\u043b\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0438 \u043e\u0431\u044a\u044f\u0441\u043d\u0438\u0442\u044c \u0442\u0440\u0435\u043d\u0434\u044b, \u043f\u0440\u043e\u0431\u043b\u0435\u043c\u0430\u0442\u0438\u043a\u0443 \u0438 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u0438 \u0434\u043b\u044f \u0432\u043d\u0443\u0442\u0440\u0435\u043d\u043d\u0438\u0445 \u0438 \u043a\u043b\u0438\u0435\u043d\u0442\u0441\u043a\u0438\u0445 \u043a\u043e\u043c\u0430\u043d\u0434 \u0432 \u0447\u0435\u0442\u043a\u043e\u0439, \u0441\u0436\u0430\u0442\u043e\u0439 \u0438 \u0446\u0435\u043b\u0435\u043d\u0430\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u043d\u043e\u0439 \u0444\u043e\u0440\u043c\u0435"", ""\u0423\u0447\u0430\u0441\u0442\u0438\u0435/\u043f\u0440\u043e\u0432\u0435\u0434\u0435\u043d\u0438\u0435 \u0432\u0441\u0442\u0440\u0435\u0447 \u0441 \u043a\u043b\u044e\u0447\u0435\u0432\u044b\u043c\u0438 \u0438 \u0441\u0442\u0440\u0430\u0442\u0435\u0433\u0438\u0447\u0435\u0441\u043a\u0438\u043c\u0438 \u043f\u0430\u0440\u0442\u043d\u0435\u0440\u0430\u043c\u0438 \u0438 \u043a\u043b\u0438\u0435\u043d\u0442\u0430\u043c\u0438 DSP"", ""\u041f\u043e\u0434\u043a\u043b\u044e\u0447\u0435\u043d\u0438\u0435 \u0434\u0430\u0442\u0430 \u043f\u0440\u043e\u0432\u0430\u0439\u0434\u0435\u0440\u043e\u0432 (\u043f\u0435\u0440\u0435\u0433\u043e\u0432\u043e\u0440\u044b, \u043f\u043e\u0434\u043a\u043b\u044e\u0447\u0435\u043d\u0438\u044f, \u0438\u043d\u0442\u0435\u0433\u0440\u0430\u0446\u0438\u0438, \u0441\u043e\u0431\u043b\u044e\u0434\u0435\u043d\u0438\u0435 \u0434\u043e\u0433\u043e\u0432\u0440\u0435\u043d\u043d\u043e\u0441\u0442\u0435\u0439 \u0441 \u043d\u0438\u043c\u0438)"", ""\u0421\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u043e\u0432\u0430\u0442\u044c \u0432 \u0444\u043e\u0440\u043c\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0438, \u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u043a\u0435 \u0438 \u0440\u0435\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 \u043a\u0430\u043c\u043f\u0430\u043d\u0438\u0439, \u043e\u0441\u043d\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u043d\u0430 \u0434\u0430\u043d\u043d\u044b\u0445. \u041d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0430 \u0438 \u0443\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0445 \u0438\u0437 \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u0438\u0445 \u0438\u0441\u0442\u043e\u0447\u043d\u0438\u043a\u043e\u0432"", ""\u041f\u043e\u043c\u043e\u0449\u044c \u0432\u043e \u0432\u043d\u0435\u0434\u0440\u0435\u043d\u0438\u0438 \u0434\u0430\u043d\u043d\u044b\u0445 \u043a\u043b\u0438\u0435\u043d\u0442\u043e\u0432 \u0432 \u043f\u043b\u0430\u0442\u0444\u043e\u0440\u043c\u0443 Admixer. \u0412 \u0441\u043e\u0442\u0440\u0443\u0434\u043d\u0438\u0447\u0435\u0441\u0442\u0432\u0435 \u0441 \u043a\u043e\u043c\u0430\u043d\u0434\u0430\u043c\u0438 Data Engineering \u0438 Customer Success \u043e\u0431\u0435\u0441\u043f\u0435\u0447\u0438\u0442\u044c \u0431\u0435\u0441\u043f\u0435\u0440\u0435\u0431\u043e\u0439\u043d\u044b\u0439 \u0438 \u00ab\u0433\u043b\u0430\u0434\u043a\u0438\u0439\u00bb \u043f\u0440\u043e\u0446\u0435\u0441\u0441 \u043e\u043d\u0431\u043e\u0440\u0434\u0438\u043d\u0433\u0430."", ""\u0412\u044b\u044f\u0432\u043b\u044f\u0442\u044c \u0438 \u0443\u0441\u0442\u0440\u0430\u043d\u044f\u0442\u044c \u043b\u044e\u0431\u044b\u0435 \u043f\u0440\u043e\u0431\u043b\u0435\u043c\u044b \u0441 \u0438\u043d\u0442\u0435\u0433\u0440\u0430\u0446\u0438\u044f\u043c\u0438 \u0438 / \u0438\u043b\u0438 \u043f\u043e\u0434\u043a\u043b\u044e\u0447\u0435\u043d\u0438\u0435\u043c, \u0440\u0435\u0448\u0430\u044f \u0438\u0445 \u0441 \u043a\u043e\u043c\u0430\u043d\u0434\u0430\u043c\u0438 \u0432\u043d\u0443\u0442\u0440\u0438 \u043a\u043e\u043c\u043f\u0430\u043d\u0438\u0438 \u0438 \u043a\u043b\u0438\u0435\u043d\u0442\u0430\u043c\u0438"", ""\u0423\u0447\u0430\u0441\u0442\u0438\u0435 \u0432 \u043f\u0440\u043e\u0446\u0435\u0441\u0441\u0435 \u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u043a\u0438 (Product ownership)"", ""\u0441\u043e\u0433\u043b\u0430\u0441\u043e\u0432\u0430\u043d\u0438\u0435 \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0439 \u0444\u0443\u043d\u043a\u0446\u0438\u043e\u043d\u0430\u043b\u0430 \u0432 \u043f\u0440\u043e\u0446\u0435\u0441\u0441\u0435 \u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u043a\u0438. \u0424\u043e\u0440\u043c\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u0442\u0440\u0435\u0431\u043e\u0432\u0430\u043d\u0438\u0439."", ""\u0414\u0435\u043a\u043e\u043c\u043f\u043e\u0437\u0438\u0446\u0438\u044f \u0437\u0430\u0434\u0430\u0447 (\u0440\u0430\u0437\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0442\u0440\u0435\u0431\u043e\u0432\u0430\u043d\u0438\u0439 \u043d\u0430 \u0438\u0442\u0435\u0440\u0430\u0446\u0438\u0438). \u0420\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0442\u0435\u0445 \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438"", ""\u0418\u043c\u043f\u043b\u0435\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u044f. \u041f\u0440\u043e\u0432\u0435\u0440\u043a\u0430 \u0437\u0430\u043f\u0443\u0449\u0435\u043d\u043d\u043e\u0439 \u0444\u0443\u043d\u043a\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e\u0441\u0442\u0438 \u00ab\u0432 \u0431\u043e\u044e\u00bb"", ""\u0441\u043e\u043f\u043e\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0444\u0430\u043a\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432 \u0441 \u043e\u0436\u0438\u0434\u0430\u0435\u043c\u044b\u043c\u0438. \u0421\u043e\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u043e\u0442\u0447\u0435\u0442\u043e\u0432 \u0438 \u0430\u043d\u0430\u043b\u0438\u0442\u0438\u043a\u0438.""]}"
N-iX,https://jobs.dou.ua/companies/n-ix/,Data Architect/Lead Data Engineer,https://jobs.dou.ua/companies/n-ix/vacancies/134086/," Kyiv, Lviv, remote",06 November 2020,,"N-iX is looking for a Data Architect/Lead Data Engineer to join our team. Our customer, as a proven leader in global commerce, provides flexible E-commerce solutions for monetizing digital goods, online services and SaaS. Their cloud-based E-commerce platform simplifies recurring billing, optimizes the customer experience and offers comprehensive global payment capabilities. Leveraging customer’s expertise, technology and services, clients effectively increase customer acquisition and retention while rapidly expanding into international markets for revenue growth.The ideal candidate will drive the implementation on best practices for design, architecture, and the execution of high-performance, scalable and optimized data solutions for internal and external stakeholders. As a member of our Agile development team, you will be collaborating with data engineers, software developers, product owners and project managers to select the appropriate design solution and ensure the compatibility of various system components. Responsibilities:• You will work with our Director of Finance Applications and serve as a pivotal role in the oversight, architecture and implementation of our ERP Project.• You will also collaborate with our Data Development Team to understand current-state data schemas and data models then contribute to define future-state data schema, model and flow.• You will provide input in each cycle of the development phase (develop, test, and release) to ensure we produce a leading-edge solution and we continually learn and evolve, adapting to changing business landscapes.• You will verify the stability, inter-operability, portability, security, or scalability of existing and new data architectures.• You will establish data quality check approaches, tools, and data governance where ever necessary, to ensure the delivery of trusted and accurate data.• You will drive to establish a process for documenting data models, data schemas, lineages and data dictionaries across the data stores of our project.• Provide mentorship, technical expertise and recommendations on the current and emerging data strategies and platform trends to our team of data engineers and scientists. Skills/Requirements:• Computer science or related engineering degree and 6+ years of professional experience as a Data Engineer/Data Architect• Worked with large volumes of structured and semi-structured data on the cloud, implemented real-time and/or batch data processing and analytics• Proficiency with Azure Ecosystem (Data Factories, Cosmos DB, DataBricks)• Experience in creating and maintaining ETL/ELT pipeline architecture• Used relational and/or NoSQL databases, with strong skills with Microsoft SQL Server technology.• Experience with DevOps and engineering best practices, especially Docker, Git, testing in Python, and SQL (T-SQL)• Working knowledge of using a Business Intelligence platform (Tableau, PowerBI, Qlik) Would be a plus:• AWS: Redshift, Athena, S3, EMR• Industry-specific knowledge and experience (e-commerce)• Knowledge in statistics and understanding of machine learning algorithms.• Worked in an agile environment• Data visualization experience, exposure to the UI/UX field• Worked on predictive modeling for e-commerce-relevant use cases, such as: Customer Churn Prediction, Sales Forecasting, Predictive Marketing, Fraud Detection. We offer:• Flexible working hours• A competitive salary and good compensation package• Best hardware• A masseur and a corporate doctor• Healthcare & sport benefits• An inspiring and comfy office Professional growth:• Challenging tasks and innovative projects• Meetups and events for professional development• An individual development plan• Mentorship program Fun:• Corporate events and outstanding parties• Exciting team buildings• Memorable anniversary presents• A fun zone where you can play video games, foosball, ping pong, and more",dou,2020-11-12,Data Architect/Lead Data Engineer@N-iX,,,{}
Vodafone Україна,https://jobs.dou.ua/companies/vodafone-ukraine/,Data Engineer,https://jobs.dou.ua/companies/vodafone-ukraine/vacancies/128244/, Kyiv,06 November 2020,,"Required skills • Знання та розуміння технологій OLAP• Досвід в галузі будування сховищ даних та процесів видобутку, обробки та трансформування даних• Досвід у database schema design та dimensional data modeling• Досвід та глибокі знання системи SAS (SAS EG, SAS code, macro language)• Досвід в розробці ETL процесів• Знання та досвід роботи з UNIX, Oracle• Знання технічної англійської мови As a plus • Знання мов програмування SQL, Python, Shell• Досвід у роботі з СУБД для аналізу великих даних HP Vertica• Знання Agile та принципів управління проектами, розуміння предметної галузі мобільного зв’язку We offer • Колектив однодумців, прозорість рішень та відкритість в комунікації• Офіційне працевлаштування, соціальні гарантії• Гнучкий графік роботи та віддалені робочі дні• 31 календарний день відпустки• Медичне корпоративне страхування та оздоровлення• Безкоштовний мобільний зв’язок• Вигідний тарифний план від Vodafone для сім’ї• Професійне навчання за рахунок роботодавця• Активне корпоративне життя Якщо тебе зацікавила дана позиція, надсилай нам своє резюме з очікуваннями заробітної плати. Responsibilities • Визначення зовнішніх та внутрішніх джерел великих даних для проведення аналітичних робіт• Отримання, обробка великих об’ємів даних із гетерогенних джерел даних, розробка методів та регламентів необхідних ETL-процесів• Розробка моделей даних, адаптованих до технологій великих даних• Розробка необхідних аналітичних звітів, розрізів і агрегатів великих даних• Оцінка відповідності наборів даних предметній галузі та задачам аналітичних робіт• Керування життєвим циклом даних (процеси отримання, розміщення, зберігання, розподілення, міграції, архівуваннята видалення великих даних) Project description Ми віримо, що ти зможеш:• Взаємодіяти з користувачами інформації та внутрішніми і зовнішніми постачальниками даних із гетерогенних джерел• Визначати вимоги до постачальників даних• Проводити інтеграцію джерел великих об’ємів даних• Працювати з MPP обчислювальними системами• Постійно вивчати сучасні методи та технології добування та використання великих даних",dou,2020-11-12,Data Engineer@Vodafone Україна,,,"{""Required skills"": [""\u0417\u043d\u0430\u043d\u043d\u044f \u0442\u0430 \u0440\u043e\u0437\u0443\u043c\u0456\u043d\u043d\u044f \u0442\u0435\u0445\u043d\u043e\u043b\u043e\u0433\u0456\u0439 OLAP"", ""\u0414\u043e\u0441\u0432\u0456\u0434 \u0432 \u0433\u0430\u043b\u0443\u0437\u0456 \u0431\u0443\u0434\u0443\u0432\u0430\u043d\u043d\u044f \u0441\u0445\u043e\u0432\u0438\u0449 \u0434\u0430\u043d\u0438\u0445 \u0442\u0430 \u043f\u0440\u043e\u0446\u0435\u0441\u0456\u0432 \u0432\u0438\u0434\u043e\u0431\u0443\u0442\u043a\u0443, \u043e\u0431\u0440\u043e\u0431\u043a\u0438 \u0442\u0430 \u0442\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0443\u0432\u0430\u043d\u043d\u044f \u0434\u0430\u043d\u0438\u0445"", ""\u0414\u043e\u0441\u0432\u0456\u0434 \u0443 database schema design \u0442\u0430 dimensional data modeling"", ""\u0414\u043e\u0441\u0432\u0456\u0434 \u0442\u0430 \u0433\u043b\u0438\u0431\u043e\u043a\u0456 \u0437\u043d\u0430\u043d\u043d\u044f \u0441\u0438\u0441\u0442\u0435\u043c\u0438 SAS (SAS EG, SAS code, macro language)"", ""\u0414\u043e\u0441\u0432\u0456\u0434 \u0432 \u0440\u043e\u0437\u0440\u043e\u0431\u0446\u0456 ETL \u043f\u0440\u043e\u0446\u0435\u0441\u0456\u0432"", ""\u0417\u043d\u0430\u043d\u043d\u044f \u0442\u0430 \u0434\u043e\u0441\u0432\u0456\u0434 \u0440\u043e\u0431\u043e\u0442\u0438 \u0437 UNIX, Oracle"", ""\u0417\u043d\u0430\u043d\u043d\u044f \u0442\u0435\u0445\u043d\u0456\u0447\u043d\u043e\u0457 \u0430\u043d\u0433\u043b\u0456\u0439\u0441\u044c\u043a\u043e\u0457 \u043c\u043e\u0432\u0438""], ""As a plus"": [""\u0417\u043d\u0430\u043d\u043d\u044f \u043c\u043e\u0432 \u043f\u0440\u043e\u0433\u0440\u0430\u043c\u0443\u0432\u0430\u043d\u043d\u044f SQL, Python, Shell"", ""\u0414\u043e\u0441\u0432\u0456\u0434 \u0443 \u0440\u043e\u0431\u043e\u0442\u0456 \u0437 \u0421\u0423\u0411\u0414 \u0434\u043b\u044f \u0430\u043d\u0430\u043b\u0456\u0437\u0443 \u0432\u0435\u043b\u0438\u043a\u0438\u0445 \u0434\u0430\u043d\u0438\u0445 HP Vertica"", ""\u0417\u043d\u0430\u043d\u043d\u044f Agile \u0442\u0430 \u043f\u0440\u0438\u043d\u0446\u0438\u043f\u0456\u0432 \u0443\u043f\u0440\u0430\u0432\u043b\u0456\u043d\u043d\u044f \u043f\u0440\u043e\u0435\u043a\u0442\u0430\u043c\u0438, \u0440\u043e\u0437\u0443\u043c\u0456\u043d\u043d\u044f \u043f\u0440\u0435\u0434\u043c\u0435\u0442\u043d\u043e\u0457 \u0433\u0430\u043b\u0443\u0437\u0456 \u043c\u043e\u0431\u0456\u043b\u044c\u043d\u043e\u0433\u043e \u0437\u0432\u2019\u044f\u0437\u043a\u0443""], ""We offer"": [""\u041a\u043e\u043b\u0435\u043a\u0442\u0438\u0432 \u043e\u0434\u043d\u043e\u0434\u0443\u043c\u0446\u0456\u0432, \u043f\u0440\u043e\u0437\u043e\u0440\u0456\u0441\u0442\u044c \u0440\u0456\u0448\u0435\u043d\u044c \u0442\u0430 \u0432\u0456\u0434\u043a\u0440\u0438\u0442\u0456\u0441\u0442\u044c \u0432 \u043a\u043e\u043c\u0443\u043d\u0456\u043a\u0430\u0446\u0456\u0457"", ""\u041e\u0444\u0456\u0446\u0456\u0439\u043d\u0435 \u043f\u0440\u0430\u0446\u0435\u0432\u043b\u0430\u0448\u0442\u0443\u0432\u0430\u043d\u043d\u044f, \u0441\u043e\u0446\u0456\u0430\u043b\u044c\u043d\u0456 \u0433\u0430\u0440\u0430\u043d\u0442\u0456\u0457"", ""\u0413\u043d\u0443\u0447\u043a\u0438\u0439 \u0433\u0440\u0430\u0444\u0456\u043a \u0440\u043e\u0431\u043e\u0442\u0438 \u0442\u0430 \u0432\u0456\u0434\u0434\u0430\u043b\u0435\u043d\u0456 \u0440\u043e\u0431\u043e\u0447\u0456 \u0434\u043d\u0456"", ""31 \u043a\u0430\u043b\u0435\u043d\u0434\u0430\u0440\u043d\u0438\u0439 \u0434\u0435\u043d\u044c \u0432\u0456\u0434\u043f\u0443\u0441\u0442\u043a\u0438"", ""\u041c\u0435\u0434\u0438\u0447\u043d\u0435 \u043a\u043e\u0440\u043f\u043e\u0440\u0430\u0442\u0438\u0432\u043d\u0435 \u0441\u0442\u0440\u0430\u0445\u0443\u0432\u0430\u043d\u043d\u044f \u0442\u0430 \u043e\u0437\u0434\u043e\u0440\u043e\u0432\u043b\u0435\u043d\u043d\u044f"", ""\u0411\u0435\u0437\u043a\u043e\u0448\u0442\u043e\u0432\u043d\u0438\u0439 \u043c\u043e\u0431\u0456\u043b\u044c\u043d\u0438\u0439 \u0437\u0432\u2019\u044f\u0437\u043e\u043a"", ""\u0412\u0438\u0433\u0456\u0434\u043d\u0438\u0439 \u0442\u0430\u0440\u0438\u0444\u043d\u0438\u0439 \u043f\u043b\u0430\u043d \u0432\u0456\u0434 Vodafone \u0434\u043b\u044f \u0441\u0456\u043c\u2019\u0457"", ""\u041f\u0440\u043e\u0444\u0435\u0441\u0456\u0439\u043d\u0435 \u043d\u0430\u0432\u0447\u0430\u043d\u043d\u044f \u0437\u0430 \u0440\u0430\u0445\u0443\u043d\u043e\u043a \u0440\u043e\u0431\u043e\u0442\u043e\u0434\u0430\u0432\u0446\u044f"", ""\u0410\u043a\u0442\u0438\u0432\u043d\u0435 \u043a\u043e\u0440\u043f\u043e\u0440\u0430\u0442\u0438\u0432\u043d\u0435 \u0436\u0438\u0442\u0442\u044f""], ""Responsibilities"": [""\u042f\u043a\u0449\u043e \u0442\u0435\u0431\u0435 \u0437\u0430\u0446\u0456\u043a\u0430\u0432\u0438\u043b\u0430 \u0434\u0430\u043d\u0430 \u043f\u043e\u0437\u0438\u0446\u0456\u044f, \u043d\u0430\u0434\u0441\u0438\u043b\u0430\u0439 \u043d\u0430\u043c \u0441\u0432\u043e\u0454 \u0440\u0435\u0437\u044e\u043c\u0435 \u0437 \u043e\u0447\u0456\u043a\u0443\u0432\u0430\u043d\u043d\u044f\u043c\u0438 \u0437\u0430\u0440\u043e\u0431\u0456\u0442\u043d\u043e\u0457 \u043f\u043b\u0430\u0442\u0438.""], ""Project description"": [""\u0412\u0438\u0437\u043d\u0430\u0447\u0435\u043d\u043d\u044f \u0437\u043e\u0432\u043d\u0456\u0448\u043d\u0456\u0445 \u0442\u0430 \u0432\u043d\u0443\u0442\u0440\u0456\u0448\u043d\u0456\u0445 \u0434\u0436\u0435\u0440\u0435\u043b \u0432\u0435\u043b\u0438\u043a\u0438\u0445 \u0434\u0430\u043d\u0438\u0445 \u0434\u043b\u044f \u043f\u0440\u043e\u0432\u0435\u0434\u0435\u043d\u043d\u044f \u0430\u043d\u0430\u043b\u0456\u0442\u0438\u0447\u043d\u0438\u0445 \u0440\u043e\u0431\u0456\u0442"", ""\u041e\u0442\u0440\u0438\u043c\u0430\u043d\u043d\u044f, \u043e\u0431\u0440\u043e\u0431\u043a\u0430 \u0432\u0435\u043b\u0438\u043a\u0438\u0445 \u043e\u0431\u2019\u0454\u043c\u0456\u0432 \u0434\u0430\u043d\u0438\u0445 \u0456\u0437 \u0433\u0435\u0442\u0435\u0440\u043e\u0433\u0435\u043d\u043d\u0438\u0445 \u0434\u0436\u0435\u0440\u0435\u043b \u0434\u0430\u043d\u0438\u0445, \u0440\u043e\u0437\u0440\u043e\u0431\u043a\u0430 \u043c\u0435\u0442\u043e\u0434\u0456\u0432 \u0442\u0430 \u0440\u0435\u0433\u043b\u0430\u043c\u0435\u043d\u0442\u0456\u0432 \u043d\u0435\u043e\u0431\u0445\u0456\u0434\u043d\u0438\u0445 ETL-\u043f\u0440\u043e\u0446\u0435\u0441\u0456\u0432"", ""\u0420\u043e\u0437\u0440\u043e\u0431\u043a\u0430 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u0434\u0430\u043d\u0438\u0445, \u0430\u0434\u0430\u043f\u0442\u043e\u0432\u0430\u043d\u0438\u0445 \u0434\u043e \u0442\u0435\u0445\u043d\u043e\u043b\u043e\u0433\u0456\u0439 \u0432\u0435\u043b\u0438\u043a\u0438\u0445 \u0434\u0430\u043d\u0438\u0445"", ""\u0420\u043e\u0437\u0440\u043e\u0431\u043a\u0430 \u043d\u0435\u043e\u0431\u0445\u0456\u0434\u043d\u0438\u0445 \u0430\u043d\u0430\u043b\u0456\u0442\u0438\u0447\u043d\u0438\u0445 \u0437\u0432\u0456\u0442\u0456\u0432, \u0440\u043e\u0437\u0440\u0456\u0437\u0456\u0432 \u0456 \u0430\u0433\u0440\u0435\u0433\u0430\u0442\u0456\u0432 \u0432\u0435\u043b\u0438\u043a\u0438\u0445 \u0434\u0430\u043d\u0438\u0445"", ""\u041e\u0446\u0456\u043d\u043a\u0430 \u0432\u0456\u0434\u043f\u043e\u0432\u0456\u0434\u043d\u043e\u0441\u0442\u0456 \u043d\u0430\u0431\u043e\u0440\u0456\u0432 \u0434\u0430\u043d\u0438\u0445 \u043f\u0440\u0435\u0434\u043c\u0435\u0442\u043d\u0456\u0439 \u0433\u0430\u043b\u0443\u0437\u0456 \u0442\u0430 \u0437\u0430\u0434\u0430\u0447\u0430\u043c \u0430\u043d\u0430\u043b\u0456\u0442\u0438\u0447\u043d\u0438\u0445 \u0440\u043e\u0431\u0456\u0442"", ""\u041a\u0435\u0440\u0443\u0432\u0430\u043d\u043d\u044f \u0436\u0438\u0442\u0442\u0454\u0432\u0438\u043c \u0446\u0438\u043a\u043b\u043e\u043c \u0434\u0430\u043d\u0438\u0445 (\u043f\u0440\u043e\u0446\u0435\u0441\u0438 \u043e\u0442\u0440\u0438\u043c\u0430\u043d\u043d\u044f, \u0440\u043e\u0437\u043c\u0456\u0449\u0435\u043d\u043d\u044f, \u0437\u0431\u0435\u0440\u0456\u0433\u0430\u043d\u043d\u044f, \u0440\u043e\u0437\u043f\u043e\u0434\u0456\u043b\u0435\u043d\u043d\u044f, \u043c\u0456\u0433\u0440\u0430\u0446\u0456\u0457, \u0430\u0440\u0445\u0456\u0432\u0443\u0432\u0430\u043d\u043d\u044f\u0442\u0430 \u0432\u0438\u0434\u0430\u043b\u0435\u043d\u043d\u044f \u0432\u0435\u043b\u0438\u043a\u0438\u0445 \u0434\u0430\u043d\u0438\u0445)""]}"
Brainstack_,https://jobs.dou.ua/companies/brainstack/,Product/Data analyst (Mobile apps),https://jobs.dou.ua/companies/brainstack/vacancies/137880/, Kyiv,06 November 2020,,"Required skills — Техническое или экономическое образование.— 2+ лет профессионального опыта в роли аналитика.— Хорошие знания Python/R, SQL (MySQL, PostgreSQL).— Хороший теоретический бекграунд в статистике, тестировании гипотез и знания в регрессионном анализе (Logistic Regression, и т.п.)— Опыт работы с хранилищами данных и инструментами визуализации.— Знание mobile аналитики— Опыт работы с AppsFlyer, Facebook, Firebase— Высокий уровень владения Excel (сводные таблицы, подключение к внешним источникам).— Аналитическое мышление, способность находить закономерность в изученных наборах данных, причины отклонений показателей, описывающих бизнес-процессы, кратко и точно акцентировать внимание на основных причинах отклонений.— Знание английского на уровне Pre-Intermediate или выше As a plus — Хорошее понимание Mobile маркетинга, понимание Mobile бизнеса, знание его специфических показателей— Опыт в прогнозной аналитике (анализ и прогнозирование временных рядов, анализ выживаемости и т. д.)— Опыт с сервисами облачных вычислений (GCP, etc.)— Опыт работы с платежными сервисами / электронной коммерцией / банками / провайдерами платежей We offer — Работу в стабильной международной IT компании в команде 125+ специалистов;— Медицинскую страховка после выхода на работу;— Конкурентный уровень вознаграждения;— Сотрудничество по ФЛП 3 группа (100% оплата налогов компанией);— Частичное покрытие курсов английского языка в любой языковой школе Киева— 24 календарных дней отпуска;— Оплата больничных— 3 недокументированных больничных дня в году;— Всегда свежая еда в офисе: фрукты, печенье, завтраки и много других вкусностей). Responsibilities — Поддержка Business Intelligent (автоматизированной отчетности в Google Data Studio и Tableau, работа с дата-инженерами для разработки схемы DWH и процессов ETL).— Анализ продуктовых метрик— Анализ и построение продуктовых воронок— Построение дешбордов— Анализ данных для выявления причин отклонений бизнес-метрик, а также оптимизации деятельности.— Настройка и анализ результатов экспериментов (A/B-тесты).— Прогнозирование финансовых результатов.— Участие в проектах по оптимизации бизнеса Project description Brainstack_ - это продуктовая IT компания полного цикла в 130+ специалистов, продвигающая свои продукты на рынки США и западной Европы.Наши продукты в нише — Monitoring Software, а также мобильные приложения в различных нишах (Parental control, Fitness&Health, также ресерч новых ниш).",dou,2020-11-12,Product/Data analyst (Mobile apps)@Brainstack_,,,"{""Required skills"": [""\u0422\u0435\u0445\u043d\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u0438\u043b\u0438 \u044d\u043a\u043e\u043d\u043e\u043c\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u0435."", ""2+ \u043b\u0435\u0442 \u043f\u0440\u043e\u0444\u0435\u0441\u0441\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e\u0433\u043e \u043e\u043f\u044b\u0442\u0430 \u0432 \u0440\u043e\u043b\u0438 \u0430\u043d\u0430\u043b\u0438\u0442\u0438\u043a\u0430."", ""\u0425\u043e\u0440\u043e\u0448\u0438\u0435 \u0437\u043d\u0430\u043d\u0438\u044f Python/R, SQL (MySQL, PostgreSQL)."", ""\u0425\u043e\u0440\u043e\u0448\u0438\u0439 \u0442\u0435\u043e\u0440\u0435\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u0431\u0435\u043a\u0433\u0440\u0430\u0443\u043d\u0434 \u0432 \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u043a\u0435, \u0442\u0435\u0441\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0438 \u0433\u0438\u043f\u043e\u0442\u0435\u0437 \u0438 \u0437\u043d\u0430\u043d\u0438\u044f \u0432 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u043e\u043d\u043d\u043e\u043c \u0430\u043d\u0430\u043b\u0438\u0437\u0435 (Logistic Regression, \u0438 \u0442.\u043f.)"", ""\u041e\u043f\u044b\u0442 \u0440\u0430\u0431\u043e\u0442\u044b \u0441 \u0445\u0440\u0430\u043d\u0438\u043b\u0438\u0449\u0430\u043c\u0438 \u0434\u0430\u043d\u043d\u044b\u0445 \u0438 \u0438\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442\u0430\u043c\u0438 \u0432\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438."", ""\u0417\u043d\u0430\u043d\u0438\u0435 mobile \u0430\u043d\u0430\u043b\u0438\u0442\u0438\u043a\u0438"", ""\u041e\u043f\u044b\u0442 \u0440\u0430\u0431\u043e\u0442\u044b \u0441 AppsFlyer, Facebook, Firebase"", ""\u0412\u044b\u0441\u043e\u043a\u0438\u0439 \u0443\u0440\u043e\u0432\u0435\u043d\u044c \u0432\u043b\u0430\u0434\u0435\u043d\u0438\u044f Excel (\u0441\u0432\u043e\u0434\u043d\u044b\u0435 \u0442\u0430\u0431\u043b\u0438\u0446\u044b, \u043f\u043e\u0434\u043a\u043b\u044e\u0447\u0435\u043d\u0438\u0435 \u043a \u0432\u043d\u0435\u0448\u043d\u0438\u043c \u0438\u0441\u0442\u043e\u0447\u043d\u0438\u043a\u0430\u043c)."", ""\u0410\u043d\u0430\u043b\u0438\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u043c\u044b\u0448\u043b\u0435\u043d\u0438\u0435, \u0441\u043f\u043e\u0441\u043e\u0431\u043d\u043e\u0441\u0442\u044c \u043d\u0430\u0445\u043e\u0434\u0438\u0442\u044c \u0437\u0430\u043a\u043e\u043d\u043e\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u044c \u0432 \u0438\u0437\u0443\u0447\u0435\u043d\u043d\u044b\u0445 \u043d\u0430\u0431\u043e\u0440\u0430\u0445 \u0434\u0430\u043d\u043d\u044b\u0445, \u043f\u0440\u0438\u0447\u0438\u043d\u044b \u043e\u0442\u043a\u043b\u043e\u043d\u0435\u043d\u0438\u0439 \u043f\u043e\u043a\u0430\u0437\u0430\u0442\u0435\u043b\u0435\u0439, \u043e\u043f\u0438\u0441\u044b\u0432\u0430\u044e\u0449\u0438\u0445 \u0431\u0438\u0437\u043d\u0435\u0441-\u043f\u0440\u043e\u0446\u0435\u0441\u0441\u044b, \u043a\u0440\u0430\u0442\u043a\u043e \u0438 \u0442\u043e\u0447\u043d\u043e \u0430\u043a\u0446\u0435\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0432\u043d\u0438\u043c\u0430\u043d\u0438\u0435 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u043d\u044b\u0445 \u043f\u0440\u0438\u0447\u0438\u043d\u0430\u0445 \u043e\u0442\u043a\u043b\u043e\u043d\u0435\u043d\u0438\u0439."", ""\u0417\u043d\u0430\u043d\u0438\u0435 \u0430\u043d\u0433\u043b\u0438\u0439\u0441\u043a\u043e\u0433\u043e \u043d\u0430 \u0443\u0440\u043e\u0432\u043d\u0435 Pre-Intermediate \u0438\u043b\u0438 \u0432\u044b\u0448\u0435""], ""As a plus"": [""\u0425\u043e\u0440\u043e\u0448\u0435\u0435 \u043f\u043e\u043d\u0438\u043c\u0430\u043d\u0438\u0435 Mobile \u043c\u0430\u0440\u043a\u0435\u0442\u0438\u043d\u0433\u0430, \u043f\u043e\u043d\u0438\u043c\u0430\u043d\u0438\u0435 Mobile \u0431\u0438\u0437\u043d\u0435\u0441\u0430, \u0437\u043d\u0430\u043d\u0438\u0435 \u0435\u0433\u043e \u0441\u043f\u0435\u0446\u0438\u0444\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u043f\u043e\u043a\u0430\u0437\u0430\u0442\u0435\u043b\u0435\u0439"", ""\u041e\u043f\u044b\u0442 \u0432 \u043f\u0440\u043e\u0433\u043d\u043e\u0437\u043d\u043e\u0439 \u0430\u043d\u0430\u043b\u0438\u0442\u0438\u043a\u0435 (\u0430\u043d\u0430\u043b\u0438\u0437 \u0438 \u043f\u0440\u043e\u0433\u043d\u043e\u0437\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0445 \u0440\u044f\u0434\u043e\u0432, \u0430\u043d\u0430\u043b\u0438\u0437 \u0432\u044b\u0436\u0438\u0432\u0430\u0435\u043c\u043e\u0441\u0442\u0438 \u0438 \u0442. \u0434.)"", ""\u041e\u043f\u044b\u0442 \u0441 \u0441\u0435\u0440\u0432\u0438\u0441\u0430\u043c\u0438 \u043e\u0431\u043b\u0430\u0447\u043d\u044b\u0445 \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u0439 (GCP, etc.)"", ""\u041e\u043f\u044b\u0442 \u0440\u0430\u0431\u043e\u0442\u044b \u0441 \u043f\u043b\u0430\u0442\u0435\u0436\u043d\u044b\u043c\u0438 \u0441\u0435\u0440\u0432\u0438\u0441\u0430\u043c\u0438 / \u044d\u043b\u0435\u043a\u0442\u0440\u043e\u043d\u043d\u043e\u0439 \u043a\u043e\u043c\u043c\u0435\u0440\u0446\u0438\u0435\u0439 / \u0431\u0430\u043d\u043a\u0430\u043c\u0438 / \u043f\u0440\u043e\u0432\u0430\u0439\u0434\u0435\u0440\u0430\u043c\u0438 \u043f\u043b\u0430\u0442\u0435\u0436\u0435\u0439""], ""We offer"": [""\u0420\u0430\u0431\u043e\u0442\u0443 \u0432 \u0441\u0442\u0430\u0431\u0438\u043b\u044c\u043d\u043e\u0439 \u043c\u0435\u0436\u0434\u0443\u043d\u0430\u0440\u043e\u0434\u043d\u043e\u0439 IT \u043a\u043e\u043c\u043f\u0430\u043d\u0438\u0438 \u0432 \u043a\u043e\u043c\u0430\u043d\u0434\u0435 125+ \u0441\u043f\u0435\u0446\u0438\u0430\u043b\u0438\u0441\u0442\u043e\u0432;"", ""\u041c\u0435\u0434\u0438\u0446\u0438\u043d\u0441\u043a\u0443\u044e \u0441\u0442\u0440\u0430\u0445\u043e\u0432\u043a\u0430 \u043f\u043e\u0441\u043b\u0435 \u0432\u044b\u0445\u043e\u0434\u0430 \u043d\u0430 \u0440\u0430\u0431\u043e\u0442\u0443;"", ""\u041a\u043e\u043d\u043a\u0443\u0440\u0435\u043d\u0442\u043d\u044b\u0439 \u0443\u0440\u043e\u0432\u0435\u043d\u044c \u0432\u043e\u0437\u043d\u0430\u0433\u0440\u0430\u0436\u0434\u0435\u043d\u0438\u044f;"", ""\u0421\u043e\u0442\u0440\u0443\u0434\u043d\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043f\u043e \u0424\u041b\u041f 3 \u0433\u0440\u0443\u043f\u043f\u0430 (100% \u043e\u043f\u043b\u0430\u0442\u0430 \u043d\u0430\u043b\u043e\u0433\u043e\u0432 \u043a\u043e\u043c\u043f\u0430\u043d\u0438\u0435\u0439);"", ""\u0427\u0430\u0441\u0442\u0438\u0447\u043d\u043e\u0435 \u043f\u043e\u043a\u0440\u044b\u0442\u0438\u0435 \u043a\u0443\u0440\u0441\u043e\u0432 \u0430\u043d\u0433\u043b\u0438\u0439\u0441\u043a\u043e\u0433\u043e \u044f\u0437\u044b\u043a\u0430 \u0432 \u043b\u044e\u0431\u043e\u0439 \u044f\u0437\u044b\u043a\u043e\u0432\u043e\u0439 \u0448\u043a\u043e\u043b\u0435 \u041a\u0438\u0435\u0432\u0430"", ""24 \u043a\u0430\u043b\u0435\u043d\u0434\u0430\u0440\u043d\u044b\u0445 \u0434\u043d\u0435\u0439 \u043e\u0442\u043f\u0443\u0441\u043a\u0430;"", ""\u041e\u043f\u043b\u0430\u0442\u0430 \u0431\u043e\u043b\u044c\u043d\u0438\u0447\u043d\u044b\u0445"", ""3 \u043d\u0435\u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u0431\u043e\u043b\u044c\u043d\u0438\u0447\u043d\u044b\u0445 \u0434\u043d\u044f \u0432 \u0433\u043e\u0434\u0443;"", ""\u0412\u0441\u0435\u0433\u0434\u0430 \u0441\u0432\u0435\u0436\u0430\u044f \u0435\u0434\u0430 \u0432 \u043e\u0444\u0438\u0441\u0435: \u0444\u0440\u0443\u043a\u0442\u044b, \u043f\u0435\u0447\u0435\u043d\u044c\u0435, \u0437\u0430\u0432\u0442\u0440\u0430\u043a\u0438 \u0438 \u043c\u043d\u043e\u0433\u043e \u0434\u0440\u0443\u0433\u0438\u0445 \u0432\u043a\u0443\u0441\u043d\u043e\u0441\u0442\u0435\u0439).""], ""Responsibilities"": [""\u041f\u043e\u0434\u0434\u0435\u0440\u0436\u043a\u0430 Business Intelligent (\u0430\u0432\u0442\u043e\u043c\u0430\u0442\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u043e\u0439 \u043e\u0442\u0447\u0435\u0442\u043d\u043e\u0441\u0442\u0438 \u0432 Google Data Studio \u0438 Tableau, \u0440\u0430\u0431\u043e\u0442\u0430 \u0441 \u0434\u0430\u0442\u0430-\u0438\u043d\u0436\u0435\u043d\u0435\u0440\u0430\u043c\u0438 \u0434\u043b\u044f \u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u0441\u0445\u0435\u043c\u044b DWH \u0438 \u043f\u0440\u043e\u0446\u0435\u0441\u0441\u043e\u0432 ETL)."", ""\u0410\u043d\u0430\u043b\u0438\u0437 \u043f\u0440\u043e\u0434\u0443\u043a\u0442\u043e\u0432\u044b\u0445 \u043c\u0435\u0442\u0440\u0438\u043a"", ""\u0410\u043d\u0430\u043b\u0438\u0437 \u0438 \u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u0435 \u043f\u0440\u043e\u0434\u0443\u043a\u0442\u043e\u0432\u044b\u0445 \u0432\u043e\u0440\u043e\u043d\u043e\u043a"", ""\u041f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u0435 \u0434\u0435\u0448\u0431\u043e\u0440\u0434\u043e\u0432"", ""\u0410\u043d\u0430\u043b\u0438\u0437 \u0434\u0430\u043d\u043d\u044b\u0445 \u0434\u043b\u044f \u0432\u044b\u044f\u0432\u043b\u0435\u043d\u0438\u044f \u043f\u0440\u0438\u0447\u0438\u043d \u043e\u0442\u043a\u043b\u043e\u043d\u0435\u043d\u0438\u0439 \u0431\u0438\u0437\u043d\u0435\u0441-\u043c\u0435\u0442\u0440\u0438\u043a, \u0430 \u0442\u0430\u043a\u0436\u0435 \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u0438 \u0434\u0435\u044f\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438."", ""\u041d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0430 \u0438 \u0430\u043d\u0430\u043b\u0438\u0437 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432 \u044d\u043a\u0441\u043f\u0435\u0440\u0438\u043c\u0435\u043d\u0442\u043e\u0432 (A/B-\u0442\u0435\u0441\u0442\u044b)."", ""\u041f\u0440\u043e\u0433\u043d\u043e\u0437\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u0444\u0438\u043d\u0430\u043d\u0441\u043e\u0432\u044b\u0445 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432."", ""\u0423\u0447\u0430\u0441\u0442\u0438\u0435 \u0432 \u043f\u0440\u043e\u0435\u043a\u0442\u0430\u0445 \u043f\u043e \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u0438 \u0431\u0438\u0437\u043d\u0435\u0441\u0430""], ""Project description"": [""Brainstack_ - \u044d\u0442\u043e \u043f\u0440\u043e\u0434\u0443\u043a\u0442\u043e\u0432\u0430\u044f IT \u043a\u043e\u043c\u043f\u0430\u043d\u0438\u044f \u043f\u043e\u043b\u043d\u043e\u0433\u043e \u0446\u0438\u043a\u043b\u0430 \u0432 130+ \u0441\u043f\u0435\u0446\u0438\u0430\u043b\u0438\u0441\u0442\u043e\u0432, \u043f\u0440\u043e\u0434\u0432\u0438\u0433\u0430\u044e\u0449\u0430\u044f \u0441\u0432\u043e\u0438 \u043f\u0440\u043e\u0434\u0443\u043a\u0442\u044b \u043d\u0430 \u0440\u044b\u043d\u043a\u0438 \u0421\u0428\u0410 \u0438 \u0437\u0430\u043f\u0430\u0434\u043d\u043e\u0439 \u0415\u0432\u0440\u043e\u043f\u044b.\u041d\u0430\u0448\u0438 \u043f\u0440\u043e\u0434\u0443\u043a\u0442\u044b \u0432 \u043d\u0438\u0448\u0435"", ""Monitoring Software, \u0430 \u0442\u0430\u043a\u0436\u0435 \u043c\u043e\u0431\u0438\u043b\u044c\u043d\u044b\u0435 \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u044f \u0432 \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u0445 \u043d\u0438\u0448\u0430\u0445 (Parental control, Fitness&Health, \u0442\u0430\u043a\u0436\u0435 \u0440\u0435\u0441\u0435\u0440\u0447 \u043d\u043e\u0432\u044b\u0445 \u043d\u0438\u0448).""]}"
N-iX,https://jobs.dou.ua/companies/n-ix/,Senior Data Quality Engineer,https://jobs.dou.ua/companies/n-ix/vacancies/137867/," Kyiv, Lviv, remote",06 November 2020,,"We are looking for Senior Data Quality Engineer to join our team. The Data Quality Engineer is responsible for the data consistency, quality, and transparency of the data collected from all internal and external sources, and will primarily support the Data Engineering Team. This individual will also be responsible for the QA of data flow or ensuring there is a consistent QA procedure in place for all teams to follow when enhancing existing applications or the development of new products. Duties and responsibilities:• Work with data warehouse and development teams to help drive consistency across our varied data domains• Document data capture requirements as required• Perform QA data testing and validation on all changes, and periodic QA reviews of existing data feeds using SQL and other tools• Assist in the creation of core enterprise data governance guidelines• Work collaboratively in an Agile/Scrum team guiding the testing process for both development and other quality engineers• Evaluating and approving data for validity and impact on deliverables• Responding to internal data inquiries• Compiling and analyzing data reports• Desired Skills and Competencies • Solid SQL knowledge• Exposure to agile/scrum methodologies• QA experience — specifically testing data pipelines, data• Knowledge of performance test tools is a plus• Optionally — experience with test automation tools, such as JUnit and other open-source test automation tools• Understanding of data analysis and interpretation Qualifications:• BS/MS degree in Computer Science, Engineering or a related subject• 2+ years of software testing and QE experience in data space• 2+ years’ experience with data processing or ETL• 2+ years of QA or data validation using tools such as SQL or PythonAgile / Scrum We offer:• Flexible working hours• A competitive salary and good compensation package• Possibility of partial remote work• Best hardware• A masseur and a corporate doctor• Healthcare & sport benefits• An inspiring and comfy office Professional growth:• Challenging tasks and innovative projects• Meetups and events for professional development• An individual development plan• Mentorship program Fun:• Corporate events and outstanding parties• Exciting team buildings• Memorable anniversary presents• A fun zone where you can play video games, foosball, ping pong, and more.",dou,2020-11-12,Senior Data Quality Engineer@N-iX,,,{}
Solid - Fintech Company,https://jobs.dou.ua/companies/solid/,Middle Data Analyst,https://jobs.dou.ua/companies/solid/vacancies/130407/, Kyiv,06 November 2020,,"Solid — це фінтех-компанія, яка допомагає бізнесам приймати платежі онлайн по всьому світу за допомогою плеяди різних методів та технологій. З року в рік ми зростаємо на сотні відсотків та спільно з міжнародними платіжними системами Visa, Mastercard i банками-партнерами з усього світу, створюємо власні сервіси. Наш продукт відповідає міжнародним стандартам безпеки даних індустрії платіжних карт, є багатошаровим та може бути адаптований під найвибагливішого клієнта. Solid — це команда, яка має колосальний досвід в фінтех, еквайрингу, аналітиці та ризик-менеджменті і, яка підтримує стрімке зростання партнерів на всіх ключових ринках. Ми шукаємо активного і цілеспрямованого Middle Data Analyst , який може зміцнити наш Data Squad, і візьме участь в покращенні наступних сервісів:• Automatic Routing System — система маршрутизації транзакцій між банками;• Risk Scoring Engine — скорингова модель оцінки підозрілих транзакцій;• Subscription Booster — сервіс оптимізації регулярних списань для клієнтів, що працюють по моделі підписок. Ви можете бути саме тим, кого ми шукаємо! У тебе будуть такі задачі :• участь в побудові аналітики метрик і процесів роботи платіжного провайдера;створення та покращення аналітичних сервісів з використанням ML алгоритмів — від ідеї до запуску в продакшині;• проведення досліджень для перевірки та пошуку нових продуктових гіпотез;• покращення існуючої системи моніторингу основних метрик;• розробка системи контролю якості даних;• тісна робота з продуктовою, технічною командами проекту з ціллю покращення робочих інструментів, функціоналу системи, джерел даних та методів виявлення аномалій. Для нас важливо, щоб ти мав :• 1,5 роки комерційного досвіду на аналогічній позиції;• досвід роботи з BI системами (Tableau/PowerBI) або іншими засобами візуалізації Mode, plot.ly;• чудові знання SQL і досвід роботи з реляційними базами даних;• досвід реалізації задач на Python: впевнені знання основних бібліотек — pandas, numpy, scikit, xgboost, catboost, etc.;• навички роботи з Shell, Git;• English — Intermediate. Буде плюсом :• знання Elastic стеку — Search, Kibana;• досвід вирішення практичних задач Data Science;• досвід побудови Data Pipeline. Ми створили всі умови для твого успіху Комфортні умови та гнучкий графік Класний офіс в 5 хвилинах від метро Тараса Шевченко з терасою, лаунж зонами, кухнею, PlayStation. В наших офісах багато рослин та спеціальна система вентиляції і кондиціонування. Ми забезпечуємо 20 днів оплачуваної відпустки в рік і зручний робочий графік. Турбота про здоров’я і спорт Сніданки, обіди, безмежна кількість фруктів, снеків, смузі та йогуртів в офісі. Корпоративний лікар і медичне страхування. Безкоштовні тренування з бігу, футболу, баскетболу, волейболу та йоги. Знижки в найближчі спортзали та оплата участі у спортивних змаганнях. Навчання та розвиток Business і Management School для співробітників компанії. Велика електронна бібліотека та доступ до платних онлайн-курсів і конференцій, внутрішні бесіди і воркшопи, курси англійської і speaking club. Компенсація додаткового навчання на зовнішніх тренінгах і семінарах. Гучні корпоративи Два рази на рік — влітку і взимку — ми проводимо корпоративи з шаленим лайнапом в крутих локаціях. Щомісяця офіси зустрічаються на локальні тусовки та виїзні заходи, а після квартального звіту для всієї команди влаштовують вечірку. Приєднуйся до нас в команду!",dou,2020-11-12,Middle Data Analyst@Solid - Fintech Company,,,{}
Solid - Fintech Company,https://jobs.dou.ua/companies/solid/,Senior Data Analyst,https://jobs.dou.ua/companies/solid/vacancies/130409/, Kyiv,06 November 2020,,"Solid — це фінтех-компанія, яка допомагає бізнесам приймати платежі онлайн по всьому світу за допомогою плеяди різних методів та технологій. З року в рік ми зростаємо на сотні відсотків та спільно з міжнародними платіжними системами Visa, Mastercard i банками-партнерами з усього світу, створюємо власні сервіси. Наш продукт відповідає міжнародним стандартам безпеки даних індустрії платіжних карт, є багатошаровим та може бути адаптований під найвибагливішого клієнта. Solid — це команда, яка має колосальний досвід в фінтех, еквайрингу, аналітиці та ризик-менеджменті і, яка підтримує стрімке зростання партнерів на всіх ключових ринках. Ми шукаємо активного і цілеспрямованого Senior Data Analyst, який може зміцнити наш Data Squad, і візьме участь в покращенні наступних сервісів:• Automatic Routing System — система маршрутизації транзакцій між банками;• Risk Scoring Engine — скорингова модель оцінки підозрілих транзакцій;• Subscription Booster — сервіс оптимізації регулярних списань для клієнтів, що працюють по моделі підписок. Ви можете бути саме тим, кого ми шукаємо! У тебе будуть такі задачі:• участь в побудові аналітики метрик і процесів роботи платіжного провайдера;• створення та покращення аналітичних сервісів з використанням ML алгоритмів — від ідеї до запуску в продакшині;• проведення досліджень для перевірки та пошуку нових продуктових гіпотез;• покращення існуючої системи моніторингу основних метрик;• розробка системи контролю якості даних;• тісна робота з продуктовою, технічною командами проекту з ціллю покращення робочих інструментів, функціоналу системи, джерел даних та методів виявлення аномалій. Для нас важливо, щоб ти мав:• комерційний досвід на аналогічній позиції від 3х років;• досвід роботи з BI системами (Tableau/PowerBI) або іншими засобами візуалізації Mode, plot.ly;• чудові знання SQL і досвід роботи з реляційними базами даних;• досвід реалізації задач на Python: впевнені знання основних бібліотек — pandas, numpy, scikit, xgboost, catboost, etc.;• навички роботи з Shell, Git;• English — Intermediate. Буде плюсом:• знання Elastic стеку — Search, Kibana;• досвід вирішення практичних задач Data Science;• досвід побудови Data Pipeline. Ми створили всі умови для твого успіху Комфортні умови та гнучкий графік Класний офіс в 5 хвилинах від метро Тараса Шевченко з терасою, лаунж зонами, кухнею, PlayStation. В наших офісах багато рослин та спеціальна система вентиляції і кондиціонування. Ми забезпечуємо 20 днів оплачуваної відпустки в рік і зручний робочий графік. Турбота про здоров’я і спорт Сніданки, обіди, безмежна кількість фруктів, снеків, смузі та йогуртів в офісі. Корпоративний лікар і медичне страхування. Безкоштовні тренування з бігу, футболу, баскетболу, волейболу та йоги. Знижки в найближчі спортзали та оплата участі у спортивних змаганнях. Навчання та розвиток Business і Management School для співробітників компанії. Велика електронна бібліотека та доступ до платних онлайн-курсів і конференцій, внутрішні бесіди і воркшопи, курси англійської і speaking club. Компенсація додаткового навчання на зовнішніх тренінгах і семінарах. Гучні корпоративи Два рази на рік — влітку і взимку — ми проводимо корпоративи з шаленим лайнапом в крутих локаціях. Щомісяця офіси зустрічаються на локальні тусовки та виїзні заходи, а після квартального звіту для всієї команди влаштовують вечірку. Приєднуйся до нас в команду!",dou,2020-11-12,Senior Data Analyst@Solid - Fintech Company,,,{}
Vodafone Україна,https://jobs.dou.ua/companies/vodafone-ukraine/,Data Analyst,https://jobs.dou.ua/companies/vodafone-ukraine/vacancies/137861/, Kyiv,06 November 2020,,Обов’язки: Вимоги: Ми пропонуємо:,dou,2020-11-12,Data Analyst@Vodafone Україна,,,{}
WiX,https://jobs.dou.ua/companies/wix/,Data Science Team Leader,https://jobs.dou.ua/companies/wix/vacancies/124350/, Kyiv,06 November 2020,,"A top-notch data-driven company. We’re growing so fast (180M users) that we’re collecting more data than we can monetize. Our Data Science group consists of ~50 people, of which there are 20+ data scientists, and others (engineering, curation and labeling) that help us make our projects successful and impactful for Wix. We apply SOTA Machine Learning techniques to Wix’s data to improve the product, internal processes and personalization for users, which in turn improves our profitability. We’re looking for a Data Science Team Leader who is passionate about data science and analytical problem-solving to join our Kyiv office. An extremely talented data scientist with at least 5 years’ experience with data science, machine learning, and/or deep learning and at least 2 years’ experience managing a successful team of data scientists. You have a deep understanding of classical ML & DL algs for a wide spectrum of problems and domains: Computer Vision, NLP, Recommendation Systems, Supervised & Unsupervised Learning. You have the technical know-how to advise team members, the managerial skills to prioritize and advance projects and the interpersonal skills to communicate data science to non-technical management. You also have hands-on machine learning experience with Python. You’re willing to work hands-on, potentially working on your own project while managing the team. Have an M.Sc or PhD in Math, Statistics, Computer Science, Physics, or an equivalent field.Are familiar with Deep Learning frameworks and applications Get to work on extremely diverse problems and domains.Manage and mentor a team of talented data scientists with experience in various machine learning applications.Serve as the technical expert of the team, providing high level guidance on all projects.Direct and define projects based on the business requirements of various product managers.Conduct advanced analysis, then design and code appropriate solutions using machine-learning, and communicating with all business partners.Be responsible for building relationships with stakeholders in other business units, to identify aspects of the business/product that can benefit from data science and machine learning.",dou,2020-11-12,Data Science Team Leader@WiX,,,{}
Innovecs,https://jobs.dou.ua/companies/innovecs/,Data Engineer/DBA,https://jobs.dou.ua/companies/innovecs/vacancies/137833/, Kyiv,05 November 2020,,"Required skills — At least 3 years in a Big Data engineering position / DBA;— Advanced coding experience in Python; — Excellent hands-on SQL skills;— Hands-on experience with MySQL. As a plus — Knowledge and experience with AWS services (e.g. Redshift, S3, RDS); — Hands-on experience with Hadoop stack technologies (Spark, HIVE) ;— Hands-on experience with Data Streaming technologies: Apache Kafka/KStream/KSQL/Apache Flink;— Experience with Presto, Apache Druid;— Experience with Java / Scala coding. We offer — An environment that allows you to maximize your productivity and gives you the freedom to think and collaborate beyond the next line of code or deadline;— We like to have fun, we love what we do, we relax when we need to, we are a great team and we deliver;— High-level compensation and regular performance-based salary and career development reviews;— Medical insurance (health), employee assistance program;— Paid vacation, holidays and sick leaves;— Gym 24/7, personal fitness instructor;— Massage in the office, personal wellness consultant;— English classes with native speakers and partially or fully reimbursed personal trainings and conferences;— Referral program;— Team building and a lot of fun to take a break, relax, and give you the freedom to think beyond the next line of code. Responsibilities — Maintain the integrity and performance of the company data lake and databases;— Be the main expert and focal point of our data technologies and tools;— Manage and maintain cross R&D data related to AWS services;— Hands on work on our core Data Infrastructure, tools and automation;— Take part in POCs and innovation for new data technologies, databases, tools and services that can significantly push the division goals. Project description About the ClientWe are looking for an experienced Data Engineer / DBA with high technical skills and passion for data. You will be part of a team responsible for Big data technologies, data pipelines and applications.You will work in a high paced, data-driven environment as part of the division which creates petabytes of data each day.You will be part of a team in charge of introducing new technologies and overcoming data challenges.",dou,2020-11-12,Data Engineer/DBA@Innovecs,,,"{""Required skills"": [""At least 3 years in a Big Data engineering position / DBA;"", ""Advanced coding experience in Python;"", ""Excellent hands-on SQL skills;"", ""Hands-on experience with MySQL.""], ""As a plus"": [""Knowledge and experience with AWS services (e.g. Redshift, S3, RDS);"", ""Hands-on experience with Hadoop stack technologies (Spark, HIVE) ;"", ""Hands-on experience with Data Streaming technologies: Apache Kafka/KStream/KSQL/Apache Flink;"", ""Experience with Presto, Apache Druid;"", ""Experience with Java / Scala coding.""], ""We offer"": [""An environment that allows you to maximize your productivity and gives you the freedom to think and collaborate beyond the next line of code or deadline;"", ""We like to have fun, we love what we do, we relax when we need to, we are a great team and we deliver;"", ""High-level compensation and regular performance-based salary and career development reviews;"", ""Medical insurance (health), employee assistance program;"", ""Paid vacation, holidays and sick leaves;"", ""Gym 24/7, personal fitness instructor;"", ""Massage in the office, personal wellness consultant;"", ""English classes with native speakers and partially or fully reimbursed personal trainings and conferences;"", ""Referral program;"", ""Team building and a lot of fun to take a break, relax, and give you the freedom to think beyond the next line of code.""], ""Responsibilities"": [""Maintain the integrity and performance of the company data lake and databases;"", ""Be the main expert and focal point of our data technologies and tools;"", ""Manage and maintain cross R&D data related to AWS services;"", ""Hands on work on our core Data Infrastructure, tools and automation;"", ""Take part in POCs and innovation for new data technologies, databases, tools and services that can significantly push the division goals.""], ""Project description"": [""About the ClientWe are looking for an experienced Data Engineer / DBA with high technical skills and passion for data. You will be part of a team responsible for Big data technologies, data pipelines and applications.You will work in a high paced, data-driven environment as part of the division which creates petabytes of data each day.You will be part of a team in charge of introducing new technologies and overcoming data challenges.""]}"
SoftServe,https://jobs.dou.ua/companies/softserve/,Middle DB Engineer (ID 57671),https://jobs.dou.ua/companies/softserve/vacancies/137832/, Dnipro,05 November 2020,,"WE ARE Our Customer is an American technology company that develops cloud-based human capital management (HCM) software system for business. The solutions designed to improve the employee experience in HR, payroll, talent, time and scheduling, engagement surveys, HR service delivery, and more.As a Technical Consultant (MSSQL 2016 Data Engineer), you will be a member of a dynamic distributed team responsible for the high-quality launch of our Human Capital Management products. You will be also responsible for delivering high-quality Development and Consulting Services to our Customers and Partners with a focus on creating and supporting new integrations. You will strive to identify new business opportunities and enrich the service experience for all customers. YOU ARE • Experienced in SQL Relational Database Programming (MS Transact SQL is preferred)• The one who graduated with Bachelor’s degree or equivalent experience• Skilled in writing stored procedures/functions• Able to do Performance Tuning /Handle large volumes of data• Familiar with FTP and file transfer technologies including encryption• Knowledgeable about building integration-related scripts, ETL/ELT tools• Familiar with Microsoft tool SSIS• Showing strong communication skills• Presenting good interpersonal skills and dedication to a team environment• Organized, detail-oriented, accurate and responsive• Able to manage multiple concurrent projects in a fast-paced environment• Capable of working independently with minimal supervision, self-motivated, and a quick learner YOU WANT TO WORK WITH • Developing and/or enhancing custom Integrations to meet the internal and external requirements of customers along with their third-party carriers• Utilizing internal tools and documentation to promote efficiency and standardization• Providing input on methodology and process improvements• Creating, debugging, and optimizing SQL queries and stored procedures involving large data volumes• Planning, designing, implementing, and testing data integration processes• Internal and external personnel in own area of expertise• Utilizing consulting experience and skills to guide internal and external customers through the development process using the team’s best practices TOGETHER WE WILL • Support your technical and personal growth• Process dynamic projects and still have a stable place to work• Enjoy flexible working hours• Give you access to experienced specialists willing to share their knowledge• Have an opportunity to participate in internal and external events where you can build and promote your personal brand• Share many other advantages with you such as attractive salary, modern office, a package of benefits, language classes, insurance",dou,2020-11-12,Middle DB Engineer (ID 57671)@SoftServe,,,{}
SoftServe,https://jobs.dou.ua/companies/softserve/,Big Data Competence Manager (ID 58135),https://jobs.dou.ua/companies/softserve/vacancies/137817/, Kyiv,05 November 2020,,"WE ARE The largest Big Data & AI Center of Excellence in Eastern Europe. Our expertise in Machine Learning, AI, Big Data, IoT, Cloud technologies is recognized by Google and Amazon.We are transforming the way thousands of global organizations do business by creating the most innovative solutions using proven architecture design methodologies, innovation frameworks and experience design approaches. Our group successfully delivered 50+ discovery & consulting projects and 75+ implementation projects in 2019. Together with the SEI, Carnegie Mellon University, we invented Smart Decisions game, in order to promote architecture design methodologies, intended to facilitate the design of complex architectures.Please read more at smartdecisionsgame.com YOU ARE A person with • Strong interpersonal, presentation and negotiation skills• Experience in driving enterprise-level strategic initiatives and partnership programs• Excellent organizational and time-management skills• Experience in driving communities or speaking at events• Expertise in leading dev teams, defining career development plan for your team members• Excellent coaching and supervisory skills, people motivation skills, human resources management• A proven record of experience building large-scale Data Platforms and decision-support systems• Extended experience in implementing data processing pipelines and distributed computing systems, data streaming, Data Warehousing, reporting or analytics solutions• Clouds: AWS, Google Cloud or Azure• Extensive experience with applying architectural tactics and patterns on different projects,• Experience with Hadoop stack, NoSQL, MPP technologies, processing frameworks• A proven record of success in delivering solutions for different businesses• Industry certificates YOU WANT TO • Lead Big Data & Analytics practice, define targets and execution plan company-wide• Ensure competitiveness of Big Data & Analytics practice worldwide• Represent the company at external events• Establish and maintain knowledge model for Big Data engineers (qualification standards and evaluation criteria)• Develop and maintain training programs and labs covering Big Data best practices, frameworks and technologies• Develop mentorship program for Big Data engineers• Drive research of market trends, emerging Big Data approaches, frameworks and technologies • Contribute to SS marketing programs by publishing tech articles, blogs in the media and create Case Studies based on delivered projects success stories• Engage and build relationships with other companies and their business stakeholders TOGETHER WE WILL • Multiply success and achievements of Big Data practice, get practice to the new level of maturity• Develop new best practices and standards in order to ensure high business value and high quality of services on a bigger scale• Define financial targets for entire practice, develop roadmap and grow Big Data & Analytics competency companywide• Create new Big Data & Analytics knowledge model and skills map for Big Data experts, which will allow them to meet their professional growthYour benefits • Assimilate best practices from experts, working in the team of top-notch Architects• Work with the world-leading companies• Being a part of Center of Excellence, you will have a chance to address different business and technology challenges, drive multiple projects and initiatives• Speak at international events",dou,2020-11-12,Big Data Competence Manager (ID 58135)@SoftServe,,,{}
TapOK,https://jobs.dou.ua/companies/tapok/,Senior Data Scientist,https://jobs.dou.ua/companies/tapok/vacancies/133855/, Vinnytsia,05 November 2020,,"Required skills — вміти аналізувати данні— зання SQL (MySQL, Clickhouse)— знання Python (catboost, tensor flow, scikit learn, xlearn, etc.)— поглиблене розуміння принципів та основних алгоритмів машинного навчання, оцінок якості моделей— досвід роботи з інтернет-рекламою та виведенням моделей в продакшн буде плюсом We offer — гнучкий графік, можливість працювати віддалено, акцент на результат та залученість— кар’єрне зростання, навчання, тренінги, необхідна література— професійна команда, можливість ділитися досвідом— відсутність бюрократії— сучасний, зручний офіс в самому серці міста— прив’язка ЗП до USD, оплачувані відпустки та лікарняні— повноцінний обід за рахунок компанії + чай, кава, фрукти, перекуси в офісі— абонемент в спортзал і участь в різних спортивних активностях— незабутні корпоративи та тімбілдинги (в Україні та інших країнах). Responsibilities — провести аудит існуючих моделей та роботу над їх вдосконаленням— проводити feature engineering з існуючими даними, також продумувати та пропонувати збір яких даних можно розпочати— створювати моделі машинного навчання та реалізовувати їх на базі Python (catboost, tensor flow, scikit learn, xlearn, etc.) Project description TapOK- міжнародна продуктова IT компанія повного циклу.Понад сім років реалізовуємо проекти у напрямках: Affiliate Marketing, Advertise Networks, Machine Learning, BigData, E-commerce, CRM systems. Шукаємо Senior Data Scientist з сильною експертизою в Machine Learning та Data Science, готового ділитись знаннями, вдосконалювати проекти та продукти компанії.",dou,2020-11-12,Senior Data Scientist@TapOK,,,"{""Required skills"": [""\u0432\u043c\u0456\u0442\u0438 \u0430\u043d\u0430\u043b\u0456\u0437\u0443\u0432\u0430\u0442\u0438 \u0434\u0430\u043d\u043d\u0456"", ""\u0437\u0430\u043d\u043d\u044f SQL (MySQL, Clickhouse)"", ""\u0437\u043d\u0430\u043d\u043d\u044f Python (catboost, tensor flow, scikit learn, xlearn, etc.)"", ""\u043f\u043e\u0433\u043b\u0438\u0431\u043b\u0435\u043d\u0435 \u0440\u043e\u0437\u0443\u043c\u0456\u043d\u043d\u044f \u043f\u0440\u0438\u043d\u0446\u0438\u043f\u0456\u0432 \u0442\u0430 \u043e\u0441\u043d\u043e\u0432\u043d\u0438\u0445 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u0456\u0432 \u043c\u0430\u0448\u0438\u043d\u043d\u043e\u0433\u043e \u043d\u0430\u0432\u0447\u0430\u043d\u043d\u044f, \u043e\u0446\u0456\u043d\u043e\u043a \u044f\u043a\u043e\u0441\u0442\u0456 \u043c\u043e\u0434\u0435\u043b\u0435\u0439"", ""\u0434\u043e\u0441\u0432\u0456\u0434 \u0440\u043e\u0431\u043e\u0442\u0438 \u0437 \u0456\u043d\u0442\u0435\u0440\u043d\u0435\u0442-\u0440\u0435\u043a\u043b\u0430\u043c\u043e\u044e \u0442\u0430 \u0432\u0438\u0432\u0435\u0434\u0435\u043d\u043d\u044f\u043c \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u0432 \u043f\u0440\u043e\u0434\u0430\u043a\u0448\u043d \u0431\u0443\u0434\u0435 \u043f\u043b\u044e\u0441\u043e\u043c""], ""We offer"": [""\u0433\u043d\u0443\u0447\u043a\u0438\u0439 \u0433\u0440\u0430\u0444\u0456\u043a, \u043c\u043e\u0436\u043b\u0438\u0432\u0456\u0441\u0442\u044c \u043f\u0440\u0430\u0446\u044e\u0432\u0430\u0442\u0438 \u0432\u0456\u0434\u0434\u0430\u043b\u0435\u043d\u043e, \u0430\u043a\u0446\u0435\u043d\u0442 \u043d\u0430 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 \u0442\u0430 \u0437\u0430\u043b\u0443\u0447\u0435\u043d\u0456\u0441\u0442\u044c"", ""\u043a\u0430\u0440\u2019\u0454\u0440\u043d\u0435 \u0437\u0440\u043e\u0441\u0442\u0430\u043d\u043d\u044f, \u043d\u0430\u0432\u0447\u0430\u043d\u043d\u044f, \u0442\u0440\u0435\u043d\u0456\u043d\u0433\u0438, \u043d\u0435\u043e\u0431\u0445\u0456\u0434\u043d\u0430 \u043b\u0456\u0442\u0435\u0440\u0430\u0442\u0443\u0440\u0430"", ""\u043f\u0440\u043e\u0444\u0435\u0441\u0456\u0439\u043d\u0430 \u043a\u043e\u043c\u0430\u043d\u0434\u0430, \u043c\u043e\u0436\u043b\u0438\u0432\u0456\u0441\u0442\u044c \u0434\u0456\u043b\u0438\u0442\u0438\u0441\u044f \u0434\u043e\u0441\u0432\u0456\u0434\u043e\u043c"", ""\u0432\u0456\u0434\u0441\u0443\u0442\u043d\u0456\u0441\u0442\u044c \u0431\u044e\u0440\u043e\u043a\u0440\u0430\u0442\u0456\u0457"", ""\u0441\u0443\u0447\u0430\u0441\u043d\u0438\u0439, \u0437\u0440\u0443\u0447\u043d\u0438\u0439 \u043e\u0444\u0456\u0441 \u0432 \u0441\u0430\u043c\u043e\u043c\u0443 \u0441\u0435\u0440\u0446\u0456 \u043c\u0456\u0441\u0442\u0430"", ""\u043f\u0440\u0438\u0432\u2019\u044f\u0437\u043a\u0430 \u0417\u041f \u0434\u043e USD, \u043e\u043f\u043b\u0430\u0447\u0443\u0432\u0430\u043d\u0456 \u0432\u0456\u0434\u043f\u0443\u0441\u0442\u043a\u0438 \u0442\u0430 \u043b\u0456\u043a\u0430\u0440\u043d\u044f\u043d\u0456"", ""\u043f\u043e\u0432\u043d\u043e\u0446\u0456\u043d\u043d\u0438\u0439 \u043e\u0431\u0456\u0434 \u0437\u0430 \u0440\u0430\u0445\u0443\u043d\u043e\u043a \u043a\u043e\u043c\u043f\u0430\u043d\u0456\u0457 + \u0447\u0430\u0439, \u043a\u0430\u0432\u0430, \u0444\u0440\u0443\u043a\u0442\u0438, \u043f\u0435\u0440\u0435\u043a\u0443\u0441\u0438 \u0432 \u043e\u0444\u0456\u0441\u0456"", ""\u0430\u0431\u043e\u043d\u0435\u043c\u0435\u043d\u0442 \u0432 \u0441\u043f\u043e\u0440\u0442\u0437\u0430\u043b \u0456 \u0443\u0447\u0430\u0441\u0442\u044c \u0432 \u0440\u0456\u0437\u043d\u0438\u0445 \u0441\u043f\u043e\u0440\u0442\u0438\u0432\u043d\u0438\u0445 \u0430\u043a\u0442\u0438\u0432\u043d\u043e\u0441\u0442\u044f\u0445"", ""\u043d\u0435\u0437\u0430\u0431\u0443\u0442\u043d\u0456 \u043a\u043e\u0440\u043f\u043e\u0440\u0430\u0442\u0438\u0432\u0438 \u0442\u0430 \u0442\u0456\u043c\u0431\u0456\u043b\u0434\u0438\u043d\u0433\u0438 (\u0432 \u0423\u043a\u0440\u0430\u0457\u043d\u0456 \u0442\u0430 \u0456\u043d\u0448\u0438\u0445 \u043a\u0440\u0430\u0457\u043d\u0430\u0445).""], ""Responsibilities"": [""\u043f\u0440\u043e\u0432\u0435\u0441\u0442\u0438 \u0430\u0443\u0434\u0438\u0442 \u0456\u0441\u043d\u0443\u044e\u0447\u0438\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u0442\u0430 \u0440\u043e\u0431\u043e\u0442\u0443 \u043d\u0430\u0434 \u0457\u0445 \u0432\u0434\u043e\u0441\u043a\u043e\u043d\u0430\u043b\u0435\u043d\u043d\u044f\u043c"", ""\u043f\u0440\u043e\u0432\u043e\u0434\u0438\u0442\u0438 feature engineering \u0437 \u0456\u0441\u043d\u0443\u044e\u0447\u0438\u043c\u0438 \u0434\u0430\u043d\u0438\u043c\u0438, \u0442\u0430\u043a\u043e\u0436 \u043f\u0440\u043e\u0434\u0443\u043c\u0443\u0432\u0430\u0442\u0438 \u0442\u0430 \u043f\u0440\u043e\u043f\u043e\u043d\u0443\u0432\u0430\u0442\u0438 \u0437\u0431\u0456\u0440 \u044f\u043a\u0438\u0445 \u0434\u0430\u043d\u0438\u0445 \u043c\u043e\u0436\u043d\u043e \u0440\u043e\u0437\u043f\u043e\u0447\u0430\u0442\u0438"", ""\u0441\u0442\u0432\u043e\u0440\u044e\u0432\u0430\u0442\u0438 \u043c\u043e\u0434\u0435\u043b\u0456 \u043c\u0430\u0448\u0438\u043d\u043d\u043e\u0433\u043e \u043d\u0430\u0432\u0447\u0430\u043d\u043d\u044f \u0442\u0430 \u0440\u0435\u0430\u043b\u0456\u0437\u043e\u0432\u0443\u0432\u0430\u0442\u0438 \u0457\u0445 \u043d\u0430 \u0431\u0430\u0437\u0456 Python (catboost, tensor flow, scikit learn, xlearn, etc.)""], ""Project description"": [""TapOK- \u043c\u0456\u0436\u043d\u0430\u0440\u043e\u0434\u043d\u0430 \u043f\u0440\u043e\u0434\u0443\u043a\u0442\u043e\u0432\u0430 IT \u043a\u043e\u043c\u043f\u0430\u043d\u0456\u044f \u043f\u043e\u0432\u043d\u043e\u0433\u043e \u0446\u0438\u043a\u043b\u0443.\u041f\u043e\u043d\u0430\u0434 \u0441\u0456\u043c \u0440\u043e\u043a\u0456\u0432 \u0440\u0435\u0430\u043b\u0456\u0437\u043e\u0432\u0443\u0454\u043c\u043e \u043f\u0440\u043e\u0435\u043a\u0442\u0438 \u0443 \u043d\u0430\u043f\u0440\u044f\u043c\u043a\u0430\u0445: Affiliate Marketing, Advertise Networks, Machine Learning, BigData, E-commerce, CRM systems.""]}"
Ciklum,https://jobs.dou.ua/companies/ciklum/,Enterprise Data Architect for Ciklum Digital,https://jobs.dou.ua/companies/ciklum/vacancies/137805/, Kyiv,05 November 2020,,"On behalf of Ciklum Digital, we are looking for an Enterprise Data Architect to join our team on full-time basis to create innovative solutions that exceed the needs of our customers. Technology Stack: Kafka, Cassandra, Oracle, Pentaho DI, AWS Amazon. Project description: Ciklum will build the collaborative sportswear design platform for TOP-5 company of the sportswear market. Scalable solution that will be used by famous worldwide sportswear designers to create and bring to market new sports fashion collections. Part of the larger Product Lifecycle Management ecosystem the platform delivered within the current project will become a market differentiator and key comparative advantage for multi-billion business. Project will use proven and edge technologies and concepts, such as SAFe, Behaviour-Driven Development, DevOps, Event-Driven architecture.",dou,2020-11-12,Enterprise Data Architect for Ciklum Digital@Ciklum,,,{}
R&D Center,https://jobs.dou.ua/companies/rnd-center/,BI Data Analyst,https://jobs.dou.ua/companies/rnd-center/vacancies/109290/, Kyiv,05 November 2020,,"We are looking for a passionate and experienced Data Analyst to join our Engineering Support team. In this role, you will implement interactive dashboards for monitoring the status and metrics of services and devices, perform ad hoc analytic requests to transform data into information, analyze this information to help engineering teams make business decisions. The candidate would have to handle multiple reports on devices and sort them according to the categories. — Implement interactive dashboards— Interact with customer support, product, and engineering teams— Conduct quantitative and qualitative analysis— Perform ad hoc analytics requests— Provide result interpretations— Monitor and report metrics and performance of production cloud services and devices on a daily basis — 2+ years of experience in data analytics and engineering support— Working experience with SQL — Familiarity with data storage and ETL pipelines— Knowledge of basic Linux commands— Familiarity with task management systems (Jira)— Advanced level of MS Excel— Passionate team-player— Good written and spoken English— Ability to learn quickly Nice to have — Computer science fundamentals— Experience with Redshift, Athena, Tableau, Splunk— Knowledge of data structures, algorithms, design patterns— Knowledge of AWS services, NoSQL — Working on the world’s most impactful security products. More than that, an opportunity to get some of those for personal usage— Competitive salary and perks— PE accounting and support— WFH and remote working mode possibility. Partial furniture compensation— Social package, including medical insurance available from the start date and sports compensation after the trial period— 18 paid vacation days per year, paid public holidays according to the Ukrainian legislation— Educational possibilities like corporate courses, knowledge hubs, and free English classes. Semiannual performance review— Free meals, fruits, and snacks when working in the office.",dou,2020-11-12,BI Data Analyst@R&D Center,,,{}
SciForce,https://jobs.dou.ua/companies/sciforce/,Middle Machine Learning Engineer,https://jobs.dou.ua/companies/sciforce/vacancies/103155/," Kharkiv, Lviv",05 November 2020,,"Required skills — No less than 2 years in ML development;— Strong technical skills regarding data analysis, statistics and programming;— Working experience in Python and/or C++;— Experience with DNN frameworks as PyTorch, or TensorFlow;— Knowledge of machine learning and deep learning technologies;— Research mindset and math-oriented expertise;— Good technical written and spoken English. As a plus — Ability to work on AWS stack;— Familiarity with Linux/Unix/Shell environments We offer — Transparency in communication between the company and an employee;— Paid vacation (20 business days) and paid sick leave;— Opportunity to join advanced, innovative projects;— Accounting as a service;— Competitive salary;— Friendly working environment;— Language classes; Responsibilities — Designing and implementation of ML models and solutions;— Developing best Machine Learning pipelines and proof-of-concept prototypes for a variety of use cases;— Creating API endpoints and wrapping models into Docker containers. Project description SciForce is looking for an experienced Machine Learning Engineer to strengthen our ML experts team. You will be a part of a cross-functional team focused on product innovations and new technologies in such areas as Speech Processing, NLP, Computer Vision, Anomaly Detection/Prediction, Data Science.",dou,2020-11-12,Middle Machine Learning Engineer@SciForce,,,"{""Required skills"": [""No less than 2 years in ML development;"", ""Strong technical skills regarding data analysis, statistics and programming;"", ""Working experience in Python and/or C++;"", ""Experience with DNN frameworks as PyTorch, or TensorFlow;"", ""Knowledge of machine learning and deep learning technologies;"", ""Research mindset and math-oriented expertise;"", ""Good technical written and spoken English.""], ""As a plus"": [""Ability to work on AWS stack;"", ""Familiarity with Linux/Unix/Shell environments""], ""We offer"": [""Transparency in communication between the company and an employee;"", ""Paid vacation (20 business days) and paid sick leave;"", ""Opportunity to join advanced, innovative projects;"", ""Accounting as a service;"", ""Competitive salary;"", ""Friendly working environment;"", ""Language classes;""], ""Responsibilities"": [""Designing and implementation of ML models and solutions;"", ""Developing best Machine Learning pipelines and proof-of-concept prototypes for a variety of use cases;"", ""Creating API endpoints and wrapping models into Docker containers.""], ""Project description"": [""SciForce is looking for an experienced Machine Learning Engineer to strengthen our ML experts team. You will be a part of a cross-functional team focused on product innovations and new technologies in such areas as Speech Processing, NLP, Computer Vision, Anomaly Detection/Prediction, Data Science.""]}"
NIX Solutions,https://jobs.dou.ua/companies/nix-solutions-ltd/,Lead Data Architect / Data Modeler (AWS) (Python),https://jobs.dou.ua/companies/nix-solutions-ltd/vacancies/137766/," Kharkiv, remote",05 November 2020,,"Required skills Required skills:• Production experience with Spark at scale• Building data pipelines, CICD pipelines, and fit for purpose data stores• Dimensional Data Modeling• Building data pipelines that process more than 1TB both in streaming and batch mode• Working with data consumption patterns• Working with automated build and continuous integration systemsTechnologies:• Microservices development in two of these languages: Python, Java, Scala• Big Data Technologies: Apache Spark, Hadoop• Relational Databases: Postgres, MySQL• NoSQL: MongoDB, DynamoDB• Data-warehousing products: Snowflake or Redshift• Cloud technologies: AWS (Terraform, S3, EMR, EC2, Glue, Athena)• Orchestration: Apache Airflow and MLFlow As a plus Nice to have:• KubeFlow• Spark on Kubernetes Responsibilities We need a hero:• someone who has played a role at a Lead /arch level and has been involved with dealing with data at scale in PROD• who is able to analyze, assess, document, and design scalable and sustainable data architecture and data transformation processes within a large analytics pipeline• who will review the implementation of improvements to the data architecture, providing feedback and guidance to product developers, data scientists, and product owners• is comfortable collaborating with various teams/regions in driving facilitating data design, identifying architectural risks, and developing and refining data models and architecture frameworks. Project description Product: Сloud-based, AI-driven SaaS solution using advanced predictive analytics and proactive device insights to monitor your fleet’s performance. The solution gives IT teams the tools to predict, diagnose, and prevent common PC health and performance problems, at scale, improving employee uptime and productivity.The platform collects analytics from different devices and predicts when they need maintenance using Data science.",dou,2020-11-12,Lead Data Architect / Data Modeler (AWS) (Python)@NIX Solutions,,,"{""Required skills"": [""Required skills:"", ""Production experience with Spark at scale"", ""Building data pipelines, CICD pipelines, and fit for purpose data stores"", ""Dimensional Data Modeling"", ""Building data pipelines that process more than 1TB both in streaming and batch mode"", ""Working with data consumption patterns"", ""Working with automated build and continuous integration systemsTechnologies:"", ""Microservices development in two of these languages: Python, Java, Scala"", ""Big Data Technologies: Apache Spark, Hadoop"", ""Relational Databases: Postgres, MySQL"", ""NoSQL: MongoDB, DynamoDB"", ""Data-warehousing products: Snowflake or Redshift"", ""Cloud technologies: AWS (Terraform, S3, EMR, EC2, Glue, Athena)"", ""Orchestration: Apache Airflow and MLFlow""], ""As a plus"": [""Nice to have:"", ""KubeFlow"", ""Spark on Kubernetes""], ""Responsibilities"": [""We need a hero:"", ""someone who has played a role at a Lead /arch level and has been involved with dealing with data at scale in PROD"", ""who is able to analyze, assess, document, and design scalable and sustainable data architecture and data transformation processes within a large analytics pipeline"", ""who will review the implementation of improvements to the data architecture, providing feedback and guidance to product developers, data scientists, and product owners"", ""is comfortable collaborating with various teams/regions in driving facilitating data design, identifying architectural risks, and developing and refining data models and architecture frameworks.""], ""Project description"": [""Product: \u0421loud-based, AI-driven SaaS solution using advanced predictive analytics and proactive device insights to monitor your fleet\u2019s performance. The solution gives IT teams the tools to predict, diagnose, and prevent common PC health and performance problems, at scale, improving employee uptime and productivity.The platform collects analytics from different devices and predicts when they need maintenance using Data science.""]}"
Data Science UA,https://jobs.dou.ua/companies/data-science-ua/,"CTO, Team Lead Developer (Location — Dnipro or remote, fulltime)",https://jobs.dou.ua/companies/data-science-ua/vacancies/137747/, remote,05 November 2020,,"Required skills — Experience in implementing successful e-commerce projects as CTO / TEAM Lead;— Experience in programming from 5 years, development from scratch of high-load systems with traffic of more than 1 million users per day;— Experience with several programming languages: Ruby, PHP, Python, Java, Swift, Kotlin (no, we don’t expect you to know all of them, but at least some);— Experience with complex infrastructure (Kubernetes, Docker, AWS, Google Cloud);— English: Upper-Intermediate;— Experience in managing a team of 15+ developers;— Proactivity;— System design and solution architecture. As a plus — GCP Solutions Architect or similar AWS certification is a big plus;— Experience with CMS Magenta2;— Leadership qualities, ability to influence, motivate, involve the team;— Customer focus, think like a client, and at the same time act like a business owner (take responsibility for yourself, make decisions, look for opportunities). We offer — Work within one product;— Complex projects;— Ability to influence business growth;— Bonus system that will be adapted for you;— Flexible remote policy;— The Company compensates for the payment of conferences, seminars, courses and other events (including foreign ones) that improve the professional skills of our employees;— An ambitious friendly team. Responsibilities We are looking for a professional with strong strategic thinking for:— Realization of technical and business plans;— Making key architectural and infrastructural decisions in the direction of e-commerce;— Development of a long-term strategy for the development of technologies and products;— Also a priority is the ability to form a strong development team around you (to realization ideas). Responsibilities:— Designing architecture for a new product;— Planning and implementation of development solutions;— Code review and practical development;— Construction of technological processes in the development team;— Managing a development team (contractors), as well as creating your own team (recruiting / developing new employees);— Participation in strategic meetings of the Company, offering new markets / services from a technical point of view;— Help with discussion and assessment process (we have a BA for general assessments, you will only need to research technical constraints and challenging tasks);— To offer systemic, technological solutions to internal customers in their specific situation;— Explain complex decisions in simple non-technical language. Project description Our partner has tens of thousands of orders every day and a place among the leaders of the Ukrainian drogerie online. Without exaggeration, the strongest e-commerce team is behind this project. The team that delivered 680% + growth in a year and 50,000 new items on the site that are not on store shelves.",dou,2020-11-12,"CTO, Team Lead Developer (Location — Dnipro or remote, fulltime)@Data Science UA",,,"{""Required skills"": [""Experience in implementing successful e-commerce projects as CTO / TEAM Lead;"", ""Experience in programming from 5 years, development from scratch of high-load systems with traffic of more than 1 million users per day;"", ""Experience with several programming languages: Ruby, PHP, Python, Java, Swift, Kotlin (no, we don\u2019t expect you to know all of them, but at least some);"", ""Experience with complex infrastructure (Kubernetes, Docker, AWS, Google Cloud);"", ""English: Upper-Intermediate;"", ""Experience in managing a team of 15+ developers;"", ""Proactivity;"", ""System design and solution architecture.""], ""As a plus"": [""GCP Solutions Architect or similar AWS certification is a big plus;"", ""Experience with CMS Magenta2;"", ""Leadership qualities, ability to influence, motivate, involve the team;"", ""Customer focus, think like a client, and at the same time act like a business owner (take responsibility for yourself, make decisions, look for opportunities).""], ""We offer"": [""Work within one product;"", ""Complex projects;"", ""Ability to influence business growth;"", ""Bonus system that will be adapted for you;"", ""Flexible remote policy;"", ""The Company compensates for the payment of conferences, seminars, courses and other events (including foreign ones) that improve the professional skills of our employees;"", ""An ambitious friendly team.""], ""Responsibilities"": [""We are looking for a professional with strong strategic thinking for:"", ""Realization of technical and business plans;"", ""Making key architectural and infrastructural decisions in the direction of e-commerce;"", ""Development of a long-term strategy for the development of technologies and products;"", ""Also a priority is the ability to form a strong development team around you (to realization ideas).""], ""Project description"": [""Responsibilities:"", ""Designing architecture for a new product;"", ""Planning and implementation of development solutions;"", ""Code review and practical development;"", ""Construction of technological processes in the development team;"", ""Managing a development team (contractors), as well as creating your own team (recruiting / developing new employees);"", ""Participation in strategic meetings of the Company, offering new markets / services from a technical point of view;"", ""Help with discussion and assessment process (we have a BA for general assessments, you will only need to research technical constraints and challenging tasks);"", ""To offer systemic, technological solutions to internal customers in their specific situation;"", ""Explain complex decisions in simple non-technical language.""]}"
EPAM,https://jobs.dou.ua/companies/epam-systems/,Senior Data Analyst [Lviv],https://jobs.dou.ua/companies/epam-systems/vacancies/137689/, Lviv,04 November 2020,,"Striving for excellence is in our DNA. Since 1993, we have been helping the world’s leading companies imagine, design, engineer, and deliver software and digital experiences that change the world. We are more than just specialists, we are experts. DESCRIPTIONEPAM is committed to providing our global team of 36,700+ EPAMers with inspiring careers from day one. EPAMers lead with passion and honesty and think creatively. Our people are the source of our success and we value collaboration, try to always understand our customers’ business, and strive for the highest standards of excellence. In today’s new market conditions, around the world remotely, with the vast majority of our teams working from home. No matter where you are located, you’ll join a dedicated, diverse community that will help you discover your fullest potential. PROJECT/CUSTOMER DESCRIPTIONOur client is a business-focused data technologist who help our clients turn their Cloud Data & Analytics Program into a profit center. Building and leveraging business and technology capabilities they help companies across the USA turn their data into a strategic asset to achieve critical business outcomes of improved profits, increased revenues, and competitive advantages. RESPONSIBILITIES• Gathering and documenting needs and use of data from internal and external stakeholders, including data rules, data usage and BI reporting requirements;• Identifying and interpreting patterns within data;• Writing complex SQL queries, stored procedures, etc.;• Preparing and modeling of data sets that can be used by business and BI users;• Cleansing and ensuring the quality of data sets;• Preparing reports and presentations to convey the understanding and meaning of data. REQUIREMENTS• 2+ years’ experience deploying scalable data solutions in using large-scale high performing MPP databases (Redshift, Snowflake, or Azure Datawarehouse), data storage (S3 or Azure Blob Storage) and analytics platforms (i.e. Spark, Databricks, etc);• 2+ years’ experience writing complex SQL queries, stored procedures, etc.;• Knowledge of database structures, normalization, de-normalization and entity relationships;• Strong data manipulation and data analysis skills;• Proficient understanding of development methodologies, SCRUM;• Strong team player. WILL BE A PLUS• Experience with ELT/ETL tools (such as Matillion, DBT, Informatica, Talend, etc.), Cloud Data;• Experience with Python, Spark, or PySpark;• Experience with AWS and/or Azure Cloud;• Experience with Transportation Logistics;• Experience with EDI. BENEFITS• Competitive compensation depending on experience and skills;• Individual career path;• Social package — medical insurance, sports;• Sick leave and regular vacation;• Unlimited access to Linkedin learning solutions;• English classes with certified English teachers;• Flexible work schedule. EVEN MORE EPAM BENEFITS • We offer the possibility to work on full product lifecycle —from concept to delivery into production;• We offer mentorship program;• We offer guaranteed professional growth through the technology trainings and technology communities inside EPAM;• We are proactive Agile/Scrum/XP practitioners. ABOUT EPAMEPAM Systems, Inc., delivers product development and platform engineering services to Global 2000 companies by solving complex business challenges with breakthrough technical innovations. EPAM specializes in digital transformation, business intelligence, big data and analytics, testing, next-gen architecture, digital engagement, service design and business consulting; all supported by over 25 years of software engineering expertise. We are among the fastest growing technology firms globally, with over 200 clients and offices in over 25 countries. We have successfully helped our clients achieve measurable business results through best-in-class software engineering, combined with innovative strategy, consulting and design capabilities. As a team — we bring in all our empathy, synergy and drive to focus on the client — bringing world-class experience online. If you are interested to join us in our efforts to build the application of a future, or would like to receive additional details on open position — please send a detailed CV (in English) to ua_career@epam.com",dou,2020-11-12,Senior Data Analyst [Lviv]@EPAM,,,{}
Daxx,https://jobs.dou.ua/companies/daxx-group/,Senior Big Data Engineer for Exabeam,https://jobs.dou.ua/companies/daxx-group/vacancies/137670/," Kyiv, Kharkiv, Lviv, Dnipro, remote",04 November 2020,,"Required skills — Expert level with at least one of the languages Scala / Java / Go / Python. Scala is highly preferable.— Experience with at least one of the next technologies: Elasticsearch / Hadoop / MongoDB / Kafka / Zookeeper / Prometheus / Spark / Docker.— Experience with Linux, Bash, Ansible, great to have k8s.— Experience with building high performance streaming pipelines, CI/CD.— Experience with PaaS/SaaS and microservice architecture.— Understanding of algorithms and data structures.— Experience with containerization, service discovery, orchestration.— Experience in leading the team, initiating product changes and improvements, and the startup approach are highly desirable.— If you work remotely — willingness to visit our Kyiv office at least once a month We offer — Opportunity to change the future of cybersecurity — Direct cooperation with the customer— Business trips to the US— Dedicated HR/ Client Manager— Regular performance reviews— Competitive Salary, medical insurance, 20 working vacation days— Regular corporate events, team buildings, etc Responsibilities — You are a mature professional looking for a challenging role of BigData Infrastructure engineer at a fast-growing company, ready to be a leader and initiate positive changes in the code, product, and your team— You will create, own, and develop mission-critical pieces of our large scale data pipeline that processes hundreds of thousands of events per second— - You will create features and maximize performance at multiple levels of the stack: Scala applications, orchestration, scripting, and bare metal— You will get the opportunity to work on clusters that can ingest multiple terabytes/day, can scale to 100s of nodes— You will be responsible for the end-to-end delivery of features and provide proper maintenance of the codebase— You will solve the incoming issues, notice the positive and negative trends, and communicate possible problems and achievements clearly and loudly Project description Exabeam is the leader in user and entity behavior analytics. We deliver a complete Security Intelligence Platform that enables customers to easily collect all of their security-relevant data, to accurately detect complex threats, and to effectively respond to security incidents. We have built the best threat detection solution in the market to detect in real-time advanced attacks using Big Data, machine learning, and security expertise.Kyiv is our 2nd largest engineering office, where you will work with dozens of other talented and like-minded people.",dou,2020-11-12,Senior Big Data Engineer for Exabeam@Daxx,,,"{""Required skills"": [""Expert level with at least one of the languages Scala / Java / Go / Python. Scala is highly preferable."", ""Experience with at least one of the next technologies: Elasticsearch / Hadoop / MongoDB / Kafka / Zookeeper / Prometheus / Spark / Docker."", ""Experience with Linux, Bash, Ansible, great to have k8s."", ""Experience with building high performance streaming pipelines, CI/CD."", ""Experience with PaaS/SaaS and microservice architecture."", ""Understanding of algorithms and data structures."", ""Experience with containerization, service discovery, orchestration."", ""Experience in leading the team, initiating product changes and improvements, and the startup approach are highly desirable."", ""If you work remotely"", ""willingness to visit our Kyiv office at least once a month""], ""We offer"": [""Opportunity to change the future of cybersecurity"", ""Direct cooperation with the customer"", ""Business trips to the US"", ""Dedicated HR/ Client Manager"", ""Regular performance reviews"", ""Competitive Salary, medical insurance, 20 working vacation days"", ""Regular corporate events, team buildings, etc""], ""Responsibilities"": [""You are a mature professional looking for a challenging role of BigData Infrastructure engineer at a fast-growing company, ready to be a leader and initiate positive changes in the code, product, and your team"", ""You will create, own, and develop mission-critical pieces of our large scale data pipeline that processes hundreds of thousands of events per second"", ""- You will create features and maximize performance at multiple levels of the stack: Scala applications, orchestration, scripting, and bare metal"", ""You will get the opportunity to work on clusters that can ingest multiple terabytes/day, can scale to 100s of nodes"", ""You will be responsible for the end-to-end delivery of features and provide proper maintenance of the codebase"", ""You will solve the incoming issues, notice the positive and negative trends, and communicate possible problems and achievements clearly and loudly""], ""Project description"": [""Exabeam is the leader in user and entity behavior analytics. We deliver a complete Security Intelligence Platform that enables customers to easily collect all of their security-relevant data, to accurately detect complex threats, and to effectively respond to security incidents. We have built the best threat detection solution in the market to detect in real-time advanced attacks using Big Data, machine learning, and security expertise.Kyiv is our 2nd largest engineering office, where you will work with dozens of other talented and like-minded people.""]}"
Synergetica,https://jobs.dou.ua/companies/synergetica/,Full Stack Developer for Data Science Platform,https://jobs.dou.ua/companies/synergetica/vacancies/137660/, Kyiv,04 November 2020,,"Required skills • Experience with end-to-end development, deployment, and maintenance of complex web applications in Linux and cloud environments;• Server-side languages like Java, NodeJS, Python;• Strong passion for great user experience, performance, and stability;• Experience with SQL databases;• Rails experience — an advantage;• Good Knowledge of docker, Kubernetes — an advantage;• BSc. in Computer Science or equivalent — an advantage;• Experience with one of the following frameworks: React, Angular.js, Angular, or Vue.js — advantage;• Problem-solving skills, thinking outside the box;• Type S(startup) personality: smart, ethical, friendly, cross-functional, hardworking, and proactive. We offer • Remote work during the quarantine;• Competitive compensation depending on experience and skills;• Opportunities for self-realization, professional and career growth;• Studying and practice of English: courses and communication with foreign colleagues and clients;• Compensation package (paid vacation, sick leaves);• Flexible working hours (from 9-11 to 18-20);• Office near the Teatral’na metro station (Bohdana Khmelnytskogo Str.);• Yoga classes. Responsibilities • Hands-on server-side development using rails and Postgres;• Write high quality, testable and efficient code;• Take part in all aspects of the project life-cycle;• Collaborate with UX/UI and Front-End development using VueJS 2.0;• Initiate and promote new ideas for continuous improvement of the product functionality. Project description The project is transforming AI and data science with a code-first platform helping the world’s largest companies scale their machine learning with an unparalleled end-to-end solution. From Fortune 500 companies to startups, it helps data scientists solve complex problems, by building intelligent machines. The platform is used across industries by leading companies in finance, gaming, BI, automotive, manufacturing, e-commerce, and more. We are looking for an experienced Fullstack developer who will develop the product and features end-to-end, from architecture design to implementation.If you are interested in joining a growing, dynamic, and successful startup where besides a wild ride you will be using the latest frameworks and technologies and be part of shaping the world of data science. we want to talk with you.",dou,2020-11-12,Full Stack Developer for Data Science Platform@Synergetica,,,"{""Required skills"": [""Experience with end-to-end development, deployment, and maintenance of complex web applications in Linux and cloud environments;"", ""Server-side languages like Java, NodeJS, Python;"", ""Strong passion for great user experience, performance, and stability;"", ""Experience with SQL databases;"", ""Rails experience"", ""an advantage;"", ""Good Knowledge of docker, Kubernetes"", ""an advantage;"", ""BSc. in Computer Science or equivalent"", ""an advantage;"", ""Experience with one of the following frameworks: React, Angular.js, Angular, or Vue.js"", ""advantage;"", ""Problem-solving skills, thinking outside the box;"", ""Type S(startup) personality: smart, ethical, friendly, cross-functional, hardworking, and proactive.""], ""We offer"": [""Remote work during the quarantine;"", ""Competitive compensation depending on experience and skills;"", ""Opportunities for self-realization, professional and career growth;"", ""Studying and practice of English: courses and communication with foreign colleagues and clients;"", ""Compensation package (paid vacation, sick leaves);"", ""Flexible working hours (from 9-11 to 18-20);"", ""Office near the Teatral\u2019na metro station (Bohdana Khmelnytskogo Str.);"", ""Yoga classes.""], ""Responsibilities"": [""Hands-on server-side development using rails and Postgres;"", ""Write high quality, testable and efficient code;"", ""Take part in all aspects of the project life-cycle;"", ""Collaborate with UX/UI and Front-End development using VueJS 2.0;"", ""Initiate and promote new ideas for continuous improvement of the product functionality.""], ""Project description"": [""The project is transforming AI and data science with a code-first platform helping the world\u2019s largest companies scale their machine learning with an unparalleled end-to-end solution. From Fortune 500 companies to startups, it helps data scientists solve complex problems, by building intelligent machines. The platform is used across industries by leading companies in finance, gaming, BI, automotive, manufacturing, e-commerce, and more. We are looking for an experienced Fullstack developer who will develop the product and features end-to-end, from architecture design to implementation.If you are interested in joining a growing, dynamic, and successful startup where besides a wild ride you will be using the latest frameworks and technologies and be part of shaping the world of data science. we want to talk with you.""]}"
Cpamatica,https://jobs.dou.ua/companies/cpamatica/,Middle/Senior Data Analyst,https://jobs.dou.ua/companies/cpamatica/vacancies/131404/, Kyiv,04 November 2020,,"Required skills — 1-3 years of experience doing quantitative product/marketing analysis;— 1-3 years of initiating and driving projects to completion with minimal guidance;— Highly skilled in data visualization tools such as Tableau;— Experience using product analytics tools like Grafana, Google Analytics;— Proficiency in using SQL;— Proficiency in using Python. As a plus — You have a passion for creating and supporting new great products;— You are highly motivated and hard-working as well as curious and creative at problem-solving;— You have strong verbal and written communication skills;— You thrive on collaboration, working side by side with people of all backgrounds and disciplines;— You have analytical mindset;— Ability to structure information and dive deeper when it is needed;— You have passion for digital advertising;— You are organised and self-motivated;— You have strong analytical skills and ability to make data-driven decisions;— You have entrepreneurial Mindset;— Your English — Upper Intermediate or higher. We offer — Expertise in the development of high-loaded products in international markets;— Unique opportunities for learning including various training and seminars within the company, access to a valuable and extensive book library, English courses and participation in key IT industry events worldwide;— Perfect working conditions: an excellent office in a 5 minutes’ walk from Taras Shevchenko metro station, free food and drinks in the office, medical insurance. The company has professional coaches in football, basketball and running and supports various sports initiatives (swim races, marathons, etc). Responsibilities — Primarily work with marketing managers and help them to make correct data-informed decisions;— Research and conclude on results of A/B tests and product improvements;— Enhance data gathering flows and procedures to include information that is relevant for building product analytics systems and further reporting;— Evolve data analytics processes and architecture;— Automate entire analytics process and create dashboards for different stakeholders (product and marketing);— Advocate importance of data in product management and growth of the entire business;— Upgrade marketing and product analytics tools;— Help make decisions on marketing purchases;— Analyze user behavior on the product;— Design and analyze the results of A/B tests;— Explore the marketing environment and make changes with a product.— Develop forecasts for the traffic ROI.",dou,2020-11-12,Middle/Senior Data Analyst@Cpamatica,,,"{""Required skills"": [""1-3 years of experience doing quantitative product/marketing analysis;"", ""1-3 years of initiating and driving projects to completion with minimal guidance;"", ""Highly skilled in data visualization tools such as Tableau;"", ""Experience using product analytics tools like Grafana, Google Analytics;"", ""Proficiency in using SQL;"", ""Proficiency in using Python.""], ""As a plus"": [""You have a passion for creating and supporting new great products;"", ""You are highly motivated and hard-working as well as curious and creative at problem-solving;"", ""You have strong verbal and written communication skills;"", ""You thrive on collaboration, working side by side with people of all backgrounds and disciplines;"", ""You have analytical mindset;"", ""Ability to structure information and dive deeper when it is needed;"", ""You have passion for digital advertising;"", ""You are organised and self-motivated;"", ""You have strong analytical skills and ability to make data-driven decisions;"", ""You have entrepreneurial Mindset;"", ""Your English"", ""Upper Intermediate or higher.""], ""We offer"": [""Expertise in the development of high-loaded products in international markets;"", ""Unique opportunities for learning including various training and seminars within the company, access to a valuable and extensive book library, English courses and participation in key IT industry events worldwide;"", ""Perfect working conditions: an excellent office in a 5 minutes\u2019 walk from Taras Shevchenko metro station, free food and drinks in the office, medical insurance. The company has professional coaches in football, basketball and running and supports various sports initiatives (swim races, marathons, etc).""], ""Responsibilities"": [""Primarily work with marketing managers and help them to make correct data-informed decisions;"", ""Research and conclude on results of A/B tests and product improvements;"", ""Enhance data gathering flows and procedures to include information that is relevant for building product analytics systems and further reporting;"", ""Evolve data analytics processes and architecture;"", ""Automate entire analytics process and create dashboards for different stakeholders (product and marketing);"", ""Advocate importance of data in product management and growth of the entire business;"", ""Upgrade marketing and product analytics tools;"", ""Help make decisions on marketing purchases;"", ""Analyze user behavior on the product;"", ""Design and analyze the results of A/B tests;"", ""Explore the marketing environment and make changes with a product."", ""Develop forecasts for the traffic ROI.""]}"
Reface,https://jobs.dou.ua/companies/reface/,Data Moderator,https://jobs.dou.ua/companies/reface/vacancies/137634/, Kyiv,04 November 2020,,"We are looking for a Data moderator to join our Customer support team. In this role you will be responsible for content moderation working with big amounts of videos and images. Our goal is to be sure that our users get only trusted content. At Reface, a Data moderator means that you will:Pre-moderate incoming content: videos, imagesCollect and analyze data for the other departmentsIdentify ways to improve the workflows and suggest solutions Apply, don’t overthink. This is The Job you were looking for.",dou,2020-11-12,Data Moderator@Reface,,,{}
N-iX,https://jobs.dou.ua/companies/n-ix/,Intermediate Data Analyst,https://jobs.dou.ua/companies/n-ix/vacancies/137628/," Kyiv, Lviv, remote",04 November 2020,,"The ideal candidate will be an experienced Data Analysis that demonstrates in-depth knowledge and understanding of data analysis through the development life cycle. Open-minded and flexible and prepared to work in a very dynamic environment, very interested in database management systems, data analysis, reporting tools and Cloud technologies from the back-end services and client/server applications point of view. Job Responsibilities:• Design of data models necessary to support Tax system requirements• Help team members to design and develop high performance data manipulation processes• Develop understanding of information sources and correct interpretation of data• Work with managers and Product Owners to understand trends and make recommendations for improvements• Attention to details, in particular as it relates to compliance and accuracy of data.• Improving and streamlining processes regarding data flow and data quality to improve data accuracy, viability and value.• Extract data and perform analysis on activities of the business• Gather, document and analyze requirements from stakeholders on existing and new data models Qualifications:• BS in computer science or related field• Minimum 3 years of related experience.• Working experience with relational, multidimensional, tabular data sources, non-SQL databases. • Excellent proficiency in writing SQL and T-SQL• Knowledge of Cloud technologies is a very important asset• Ability to create, maintain and monitor reporting systems and decision support tools to report and analyze performance• Strong skills at performance application tuning.• Excellent analytical skills with the ability to investigate and research multiple sources• Have good interpersonal and oral/written communications skills.• Knowledge of trading systems and trading concepts, particularly within the equities realm, is considered an asset• Working knowledge of LINUX/UNIX and WINDOWS environments• Programming skill is an asset• Strong knowledge and comprehension of technology and data management used in the process of collecting, storing and retrieving data.• Experience and/or personal interest in the financial industry an asset. Preferred technical stack:• Data: mysql, SQL Server, Redis• Streaming: Google Pub/Sub, Kafka.• Infrastructure: Google Kubernetes Engine, and other Google Cloud Platform components.Tools: Gitlab, Jira. We offer:• Flexible working hours• A competitive salary and good compensation package• Possibility of partial remote work• Best hardware• A masseur and a corporate doctor• Healthcare & sport benefits• An inspiring and comfy office Professional growth:• Challenging tasks and innovative projects• Meetups and events for professional development• An individual development plan• Mentorship program Fun:• Corporate events and outstanding parties• Exciting team buildings• Memorable anniversary presents• A fun zone where you can play video games, foosball, ping pong, and more.",dou,2020-11-12,Intermediate Data Analyst@N-iX,,,{}
WiX,https://jobs.dou.ua/companies/wix/,Data Developer,https://jobs.dou.ua/companies/wix/vacancies/61146/, Kyiv,04 November 2020,,"Wix’s Data Developers Guild. We’re a team of 30+ data devs who use cutting-edge technology to collect, transform, and democratize Wix’s data. Wix is a highly data-driven company, so we’re the ones who make it possible for employees to easily access data cross-company. We use data to understand how users interact with our products, and how to prioritize & strategize the changes that make us the innovative company we are. With over 160 million users (and 2 million new users a month), we collect over a billion events a day. That’s over 1TB of data added daily. A data developer with at least 3 years’ experience designing and implementing Data/BI solutions, preferably in a consumer web company. You’re familiar with Big Data solutions, processing unstructured and structured data into Columnstore DBs. Plus, you have 2+ years’ experience in implementing data processing systems (ETL). Your SQL skills are strong — you can write complex queries and optimize them for performance. You have experience with Python. You’re great at dimensional modeling and familiar with DWH concepts. You have experience in designing and implementing BI solutions. Plus, you’re fluent in English. Bonus points if you have experience with the Hadoop ecosystem (Hive, Presto, PIG), Columnstore DBs (Redshift, Vertica), Tableau and/or Amazon solutions. Contribute to building stunning products, based on a data-driven approach.Provide technical and business support to Data Developers in product verticals. Be responsible for gathering data from product verticals for further use across WIX.Work with business analysts to understand business priorities and translate requirements to data models.Be responsible for the data pipeline of a major data flow in a product and satellite areas. Design, implement and support robust, scalable solutions to enhance business analysis capabilities, identify gaps and design processes to fill them.Collaborate with a broad forum of experts and stakeholders, including architects and engineers, to create high-quality deliverables.Be a technical curator and set querying methodologies across the product.",dou,2020-11-12,Data Developer@WiX,,,{}
Shelf,https://jobs.dou.ua/companies/shelf-io/,Data Scientist NLP Specialist (Product. Startup. Equity),https://jobs.dou.ua/companies/shelf-io/vacancies/137596/, Lviv,04 November 2020,,"Required skills Requirements:* M.Sс. in Computer Science, Mathematics, Electrical Engineering, Statistics, or equivalent fields* Solid programming experience in Python* 2+ years of experience in NLP, Recommendation System domains* Strong knowledge of NLP tools and libraries such as NLTK, Spacy, scikit-learn * Understanding theoretical concepts of Recommendation System, Language modeling, Named Entity Recognition, text classification and clustering, topic modeling.* Hands on experience in conceptual modeling, statistical analysis, predictive modeling, and hypothesis testing.* Experience in developing machine learning models and applying advanced analytics solutions to solve complex business problems.* Strong English verbal and written communication. As a plus As a plus:+ Experience with cloud computing stacks such as Amazon Web Services preferred.+ Experience deploying and maintaining AI solutions in production. We offer We offer:This is an opportunity to break through the noise and drudgery of everyday work and become a thought leader who is making an impact on the world and the future of work itself.* Office, located in the city center* Flexible work schedule* Official Ukrainian holidays are non-working days* Competitive salaryWe have been incubated out of New York’s bustling technology scene.Joining us will let you enjoy:+ Rapid self-development and learning+ Working with the latest technologies+ Challenging tasks from day 1+ Building a product that’s used by leading companies across the globe+ Young, smart, friendly Team to help you rise to the challenges Also, this position will include:+ Equity in the company+ Opportunity to work in an internationally established startup that has already raised Venture Capital+ Travel to Europe and US for conventions, trainings, and teambuilding If you are looking for the adventure of a lifetime and are interested in this opportunity, please send the following information to join@shelf.io:1) Resume and link to your LinkedIn profile2) Some good references or links to previous projects you’ve worked on (would be a plus) Responsibilities Responsibilities:The Data Science Team plays a critical role in Shelf’s future development and direction. This Team works directly with the executive leadership team and is responsible for designing and implementing strategic R&D initiatives, using Natural Language Processing, Machine learning, Deep learning, Artificial Intelligence, Conceptual modeling, Statistical analysis, Predictive modeling, and Hypothesis testing. Project description Shelf is an award-winning, intelligent knowledge management platform for companies and their customer support operations. Advanced AI and powerful search helps agents solve customer issues on the first call. Advanced analytics and task automation significantly reduce admin overhead.The workplace has changed. Teams now live in different parts of the country and, possibly, even the world. Collaborating across projects and continents is becoming the new norm. Success in this new distributed workplace means greater access to specialized skills, faster project ramp-up time, quicker entry into new markets, and greater competitiveness with lower costs.Shelf’s mission is to create a platform that enables millions of distributed teams to accelerate their ability to share, learn and succeed. We believe Shelf is the missing piece that connects the tools of the distributed workplace of the 21st century.",dou,2020-11-12,Data Scientist NLP Specialist (Product. Startup. Equity)@Shelf,,,"{""Required skills"": [""Requirements:* M.S\u0441. in Computer Science, Mathematics, Electrical Engineering, Statistics, or equivalent fields* Solid programming experience in Python* 2+ years of experience in NLP, Recommendation System domains* Strong knowledge of NLP tools and libraries such as NLTK, Spacy, scikit-learn * Understanding theoretical concepts of Recommendation System, Language modeling, Named Entity Recognition, text classification and clustering, topic modeling.* Hands on experience in conceptual modeling, statistical analysis, predictive modeling, and hypothesis testing.* Experience in developing machine learning models and applying advanced analytics solutions to solve complex business problems.* Strong English verbal and written communication.""], ""As a plus"": [""As a plus:+ Experience with cloud computing stacks such as Amazon Web Services preferred.+ Experience deploying and maintaining AI solutions in production.""], ""We offer"": [""We offer:This is an opportunity to break through the noise and drudgery of everyday work and become a thought leader who is making an impact on the world and the future of work itself.* Office, located in the city center* Flexible work schedule* Official Ukrainian holidays are non-working days* Competitive salaryWe have been incubated out of New York\u2019s bustling technology scene.Joining us will let you enjoy:+ Rapid self-development and learning+ Working with the latest technologies+ Challenging tasks from day 1+ Building a product that\u2019s used by leading companies across the globe+ Young, smart, friendly Team to help you rise to the challenges""], ""Responsibilities"": [""Also, this position will include:+ Equity in the company+ Opportunity to work in an internationally established startup that has already raised Venture Capital+ Travel to Europe and US for conventions, trainings, and teambuilding""], ""Project description"": [""If you are looking for the adventure of a lifetime and are interested in this opportunity, please send the following information to join@shelf.io:1) Resume and link to your LinkedIn profile2) Some good references or links to previous projects you\u2019ve worked on (would be a plus)""]}"
Data Science UA,https://jobs.dou.ua/companies/data-science-ua/,Fullstack Software Engineer,https://jobs.dou.ua/companies/data-science-ua/vacancies/137564/, Kyiv,04 November 2020,,"Required skills What you need to be successful:● 2+ years Required technical experience: React JS (with Redux or MobX), + react hooks;● Node JS, Express JS and Type Script/Flow;● SQL databases (PostgreSQL);● Have knowledge of good database design and query optimisation;● Have good experience with Promise/A+; async/await is must;● Are passionate about writing good code As a plus You earn bonus points for the following:● Experience using test frameworks (Jest, Enzyme etc);● Lodash experience;● Knowledge of any functional programming language. We offer What is in it for you:● Flexible working● Sharing culture ● Diversity● Benefit package Responsibilities What you will be doing:● You will help architect and build solutions to business-critical problems;● You will participate in interesting projects such as:— Building a self-service solution for clients and partners for planning, activation, measurement and insights, tapping into the best of proprietary data and analysis;— Enable the efficient management of their demand- and publisher-side activity and allow all areas of their business access to reporting and insights;— Providing continuous improvements in the way they collect and provide data for all teams to use when making decisions;● You will experiment with new tools and technologies to produce cutting-edge solutions to business problems;● You will be a part of a self-organising, results-oriented agile team. Project description We are looking for a Full Stack Software Engineer for our partner company. This person will work as a part of the team responsible for providing a full-stack self-service solution to clients and partners for planning, activation, measurement, and insights, tapping into the best of proprietary data and analysis. With exciting projects, technologies and services in the pipeline now is a great time to be part of them.",dou,2020-11-12,Fullstack Software Engineer@Data Science UA,,,"{""Required skills"": [""What you need to be successful:\u25cf 2+ years Required technical experience: React JS (with Redux or MobX), + react hooks;\u25cf Node JS, Express JS and Type Script/Flow;\u25cf SQL databases (PostgreSQL);\u25cf Have knowledge of good database design and query optimisation;\u25cf Have good experience with Promise/A+; async/await is must;\u25cf Are passionate about writing good code""], ""As a plus"": [""You earn bonus points for the following:\u25cf Experience using test frameworks (Jest, Enzyme etc);\u25cf Lodash experience;\u25cf Knowledge of any functional programming language.""], ""We offer"": [""What is in it for you:\u25cf Flexible working\u25cf Sharing culture \u25cf Diversity\u25cf Benefit package""], ""Responsibilities"": [""What you will be doing:\u25cf You will help architect and build solutions to business-critical problems;\u25cf You will participate in interesting projects such as:"", ""Building a self-service solution for clients and partners for planning, activation, measurement and insights, tapping into the best of proprietary data and analysis;"", ""Enable the efficient management of their demand- and publisher-side activity and allow all areas of their business access to reporting and insights;"", ""Providing continuous improvements in the way they collect and provide data for all teams to use when making decisions;\u25cf You will experiment with new tools and technologies to produce cutting-edge solutions to business problems;\u25cf You will be a part of a self-organising, results-oriented agile team.""], ""Project description"": [""We are looking for a Full Stack Software Engineer for our partner company. This person will work as a part of the team responsible for providing a full-stack self-service solution to clients and partners for planning, activation, measurement, and insights, tapping into the best of proprietary data and analysis. With exciting projects, technologies and services in the pipeline now is a great time to be part of them.""]}"
Intersog,https://jobs.dou.ua/companies/intersog/,Data Engineer,https://jobs.dou.ua/companies/intersog/vacancies/137553/," Kyiv, Odesa, remote",04 November 2020,,"Required skills — At least 3 years of experience in development of data pipelines; — Proficiency in SQL and at least one scripting language, preferably in Python or R, to clean, transform, and denormalise datasets;— Experience with cloud infrastructure (e.g. AWS, GCP) and containerisation;— Experience using shell scripting, big data frameworks (e.g. Hadoop, Spark, Kafka); relational databases (e.g. BigQuery, Redshift) and data lakes (e.g. Google Buckets);— Familiarity with web and customer analytics;— Experience in machine learning is a plus;— At least an Intermediate level of English (written and spoken). We offer — Competitive compensation based on your skills, experience, and customer satisfaction;— Opportunity to work on challenging and exciting international projects;— Flexible working hours and the possibility to work remotely when needed;— Regular performance evaluation twice a year;— Long-term contract with 20-25 paid time off working days (for vacation, sick and personal leave);— Corporate English courses from A1 to C1 level and monthly English speaking clubs;— Compensation of professional conference attendance according to the corporate policy;— Compensation of medical insurance/gym according to the corporate policy;— Casual, friendly and family work environment, flat organizational structure;— Regular knowledge sharing meetups and various corporate events (such as the company’s Birthday celebration and summer family party);— Newborn and wedding bonuses;— Gameroom with a lounge zone, PlayStation and sports equipment;— Travel and visa assistance for employees;— Coffee, cookies, nuts and dried fruits for a productive day;— Friday brunches with fresh bakery;— Yoga lessons;— Secure bike storage (Odessa). Responsibilities — Define analytical questions & approaches based on business requirements/direction;— Design, implement, and maintain data governance, dictionaries, and models;— Identify, define, and compile disparate data sources (local, cloud, 3rd party) for ingestion in data pipelines;— Design and build a reliable, scalable, and secure data pipeline (cloud) to ingest, transform, and push datasets to stakeholders;— Collaborate with data analyst to generate actionable insights based on your soon-to-be intimate understanding of our data;— Collaborate with development team to build data products (e.g. personalization, recommendation system). Project description We’re looking for a strong Data Engineer, to join a Data team of our client from Singapore that works in the Digital Marketing domain. Intersog® is a Chicago-based provider of ROI-driven custom web and mobile development specializing in the delivery of full-service, end-to-end solutions, and project resources to Fortune 500 companies, SMEs, and startups. We help our clients attack their ambitious business goals, solve skills shortage issues and become innovative by building Dedicated Software Development Teams in Ukraine and/or providing on-demand IT project resources to complete required skills on their in-house teams. Our primary goal in partnering with our clients is to exceed their expectations and foster an ongoing relationship that envelopes innovation, industry leadership, and business strategy while delivering products that bring exceptional user experience, brand elevation, and market dominance.",dou,2020-11-12,Data Engineer@Intersog,,,"{""Required skills"": [""At least 3 years of experience in development of data pipelines;"", ""Proficiency in SQL and at least one scripting language, preferably in Python or R, to clean, transform, and denormalise datasets;"", ""Experience with cloud infrastructure (e.g. AWS, GCP) and containerisation;"", ""Experience using shell scripting, big data frameworks (e.g. Hadoop, Spark, Kafka); relational databases (e.g. BigQuery, Redshift) and data lakes (e.g. Google Buckets);"", ""Familiarity with web and customer analytics;"", ""Experience in machine learning is a plus;"", ""At least an Intermediate level of English (written and spoken).""], ""We offer"": [""Competitive compensation based on your skills, experience, and customer satisfaction;"", ""Opportunity to work on challenging and exciting international projects;"", ""Flexible working hours and the possibility to work remotely when needed;"", ""Regular performance evaluation twice a year;"", ""Long-term contract with 20-25 paid time off working days (for vacation, sick and personal leave);"", ""Corporate English courses from A1 to C1 level and monthly English speaking clubs;"", ""Compensation of professional conference attendance according to the corporate policy;"", ""Compensation of medical insurance/gym according to the corporate policy;"", ""Casual, friendly and family work environment, flat organizational structure;"", ""Regular knowledge sharing meetups and various corporate events (such as the company\u2019s Birthday celebration and summer family party);"", ""Newborn and wedding bonuses;"", ""Gameroom with a lounge zone, PlayStation and sports equipment;"", ""Travel and visa assistance for employees;"", ""Coffee, cookies, nuts and dried fruits for a productive day;"", ""Friday brunches with fresh bakery;"", ""Yoga lessons;"", ""Secure bike storage (Odessa).""], ""Responsibilities"": [""Define analytical questions & approaches based on business requirements/direction;"", ""Design, implement, and maintain data governance, dictionaries, and models;"", ""Identify, define, and compile disparate data sources (local, cloud, 3rd party) for ingestion in data pipelines;"", ""Design and build a reliable, scalable, and secure data pipeline (cloud) to ingest, transform, and push datasets to stakeholders;"", ""Collaborate with data analyst to generate actionable insights based on your soon-to-be intimate understanding of our data;"", ""Collaborate with development team to build data products (e.g. personalization, recommendation system).""], ""Project description"": [""We\u2019re looking for a strong Data Engineer, to join a Data team of our client from Singapore that works in the Digital Marketing domain.""]}"
Sigma Software,https://jobs.dou.ua/companies/sigma-software/,Middle Big Data Developer,https://jobs.dou.ua/companies/sigma-software/vacancies/137550/," Kyiv, Kharkiv, Lviv, Dnipro, Odesa, Vinnytsia, Sumy, remote",04 November 2020,,"Required skills This position is a perfect match for an enthusiastic Big Data Engineer who would like to develop a Data Processing platform. As a Big Data Engineer, you will design and develop a robust and scalable solution for data versioning and distribution, orchestration of data merge & conflict handling processes, as well as for providing Data Dashboards and analytical reports to end-users. You are going to be a part of the distributed team that use agile methodologies best practices, Continuous Integration, performance testing, capacity planning, and documentation. REQUIREMENTSAt least 3 years of experience in Python developmentExperience with Apache Spark at least 2 yearsAt least 1 year experience of KafkaGood knowledge of designing DBsGood experience with AWS Serverless (Glue, Lambda, Step Functions, S3, SNS/SQS)Result-oriented standpointAbility to perform in self-manageable manner within set time-boxingAt least Intermediate level of English As a plus NICE TO HAVE:Experience with Apache HudiExperience with serverless.com We offer career.sigma.software/what-we-offer Responsibilities RESPONSIBILITIESBuilding up an infrastructure for data distribution platform with the level of customization and scalability that allows applying quick changes and creating Proof-of-Concept prototypesDesigning and developing data pipelinesCreating and customizing ETL scripts with AWS serverless technologies and frameworksLiving by agile principles and collaborating with team members using agile techniques Project description CUSTOMEROur customer is a company with more than 20 years’ experience in setting standards in the automotive aftermarket worldwide as a digital innovator and a provider of leading expert solutions. The client is distinguished by in-depth knowledge of the industry and requirement for the highest levels of quality and security based on sustainable technologies. The primary company goal is to support the client during the digital transformation with data-driven solutions and comprehensive consulting services for effective and efficient business processes and practical digital solutions and innovative services implementation to make day-to-day business easier. PROJECTYou will work with the Data Warehouse solution designed to become a unified storage tool for structured and unstructured data generated by the various solutions. The solution is a data processing tool aimed at collecting the data and sending it to the Data Distribution Team.",dou,2020-11-12,Middle Big Data Developer@Sigma Software,,,"{""Required skills"": [""This position is a perfect match for an enthusiastic Big Data Engineer who would like to develop a Data Processing platform.""], ""As a plus"": [""As a Big Data Engineer, you will design and develop a robust and scalable solution for data versioning and distribution, orchestration of data merge & conflict handling processes, as well as for providing Data Dashboards and analytical reports to end-users.""], ""We offer"": [""You are going to be a part of the distributed team that use agile methodologies best practices, Continuous Integration, performance testing, capacity planning, and documentation.""], ""Responsibilities"": [""REQUIREMENTSAt least 3 years of experience in Python developmentExperience with Apache Spark at least 2 yearsAt least 1 year experience of KafkaGood knowledge of designing DBsGood experience with AWS Serverless (Glue, Lambda, Step Functions, S3, SNS/SQS)Result-oriented standpointAbility to perform in self-manageable manner within set time-boxingAt least Intermediate level of English""], ""Project description"": [""NICE TO HAVE:Experience with Apache HudiExperience with serverless.com""]}"
Data Science UA,https://jobs.dou.ua/companies/data-science-ua/,Software Engineer (Scala),https://jobs.dou.ua/companies/data-science-ua/vacancies/108184/, Kyiv,04 November 2020,,"Required skills What you need to be successful:— At least 3-4 years in software development;— Experience with Scala OR Java with strong desire to learn Scala;— Experience with Hadoop, Spark;— Experience with streaming algorithms and practical analysis of real-time data streams. Big data/scaling experience;— B.Sc. in Computer Science or related field;— Excellent communication skills and ability to work directly with English native speakers. As a plus You earn bonus points for the following:— Experience with Flink, Spark Streaming, KafkaStreams, or any other Event Stream Processing engine;— Experience with Python;— Experience with Aerospike;— Experience with AWS. We offer What is in it for you:— Flexible working;— Sharing culture — Diversity— Benefit package Responsibilities What you will be doing:— Develop optimal, well-monitored, reliable ETL pipelines for streaming data flows;— Working with cutting edge technologies in a fast-paced, start-up like environment;— Experimenting with new tools and technologies, producing POC to address business needs;— Carrying out efficient integration with our data providers via various API endpoints for real-time ingestion and batch data in cloud. Project description We are looking for a Software Engineer for our partner company. This person will work as a part of the team responsible for enabling the efficient, well-organised and robust flow of data through their systems, all the way from capture through to processing and delivery. With exciting projects, technologies and services in the pipeline now is a great time to be part of them.",dou,2020-11-12,Software Engineer (Scala)@Data Science UA,,,"{""Required skills"": [""What you need to be successful:"", ""At least 3-4 years in software development;"", ""Experience with Scala OR Java with strong desire to learn Scala;"", ""Experience with Hadoop, Spark;"", ""Experience with streaming algorithms and practical analysis of real-time data streams. Big data/scaling experience;"", ""B.Sc. in Computer Science or related field;"", ""Excellent communication skills and ability to work directly with English native speakers.""], ""As a plus"": [""You earn bonus points for the following:"", ""Experience with Flink, Spark Streaming, KafkaStreams, or any other Event Stream Processing engine;"", ""Experience with Python;"", ""Experience with Aerospike;"", ""Experience with AWS.""], ""We offer"": [""What is in it for you:"", ""Flexible working;"", ""Sharing culture"", ""Diversity"", ""Benefit package""], ""Responsibilities"": [""What you will be doing:"", ""Develop optimal, well-monitored, reliable ETL pipelines for streaming data flows;"", ""Working with cutting edge technologies in a fast-paced, start-up like environment;"", ""Experimenting with new tools and technologies, producing POC to address business needs;"", ""Carrying out efficient integration with our data providers via various API endpoints for real-time ingestion and batch data in cloud.""], ""Project description"": [""We are looking for a Software Engineer for our partner company. This person will work as a part of the team responsible for enabling the efficient, well-organised and robust flow of data through their systems, all the way from capture through to processing and delivery. With exciting projects, technologies and services in the pipeline now is a great time to be part of them.""]}"
CloudLinux,https://jobs.dou.ua/companies/cloud-linux/,Data Analyst [Remote],https://jobs.dou.ua/companies/cloud-linux/vacancies/137537/, remote,04 November 2020,,"Required skills 5+ years’ experience as an analyst, data scientist, or related quantitative role within high transactions volume business3+ years’ experience with SQL3+ year experience in gathering data from various data sources, experience of work with big databases1+year experience in Python.1+ experience in finance (understanding of accruals concept, familiarity with the structure of financial statements and basic accounting principles and rules)High proficiency with TableauAdvanced user of MS Excel and Google DocsExperience with Spark or similar frameworkExcellent communicator that can both — ask questions to grasp complex concepts and explain complex concepts to a general audienceProficient in English.Excellent presentation skillsDesirable- understanding of SaaS concepts and terminology (MRR, churn, retention rates, cohort analysis)Desirable- Jira experience As a plus An experienced data analyst who can work with high volumes of dataAn analytical and critical thinker who can model and interpret data to drive decisionsSomeone who can work independently and as a teamSomeone with a flexible mindset who can adjust to the fast-growing dynamics of the business and changing business requirementsAn innovative individual with the ability to develop and deliver creative solutions within fragmented or incomplete data We offer the chance of a lifetime to get onboard a young, dynamic, and fast-growing technology company;access to ground-breaking technology, and the freedom to expand your skills;a full-time, regular monthly contract, with a competitive compensation paid in US dollars;paid vacation, compensation for coworking, English language training, individual coaching, paid sick leave, medical insurance (if based in Russia or Ukraine), additional national holidays for your region;The freedom to choose where to work, anywhere in the world, so long as you’re online. Responsibilities Having a good understanding of the company’s business metrics, KPIs and sales drivers provide proactive insights on various business factors and their potential impact on the businessUsing Tableau build dashboards and analytical reports covering financial and non-financial business metrics to allow management take data-driven business decisions.Take a lead role in revenue reporting by pulling transactional data from various data sources ( billing systems, accounting software, PayPal , internal databases and others) and apply accounting rules to produce various revenue reports.Responsible for importing, transforming, validating, aggregating, and analyzing data from various sources for making conclusions about trends and best practices for development, sales&marketing, operational, finance functions of the business Project description As CloudLinux rapidly expands, we are making more and more decisions based on data. We need an experienced Data Analyst to deliver key metrics for our executive, marketing, sales, finance & product management teams to make decisions. We are the maker of the #1 OS for web hosting providers. We develop our products — CloudLinux OS, KernelCare, and Imunify360 — using the most innovative technologies. Our products are used by thousands of companies around the world, including Dell, GoDaddy, IBM, 1&1, Endurance, and many others. Work is fully remote, with flexible hours, where you can plan your day and work from anywhere. More details about the company on Cloudlinux.com. Join us to make the difference!",dou,2020-11-12,Data Analyst [Remote]@CloudLinux,,,"{""Required skills"": [""5+ years\u2019 experience as an analyst, data scientist, or related quantitative role within high transactions volume business3+ years\u2019 experience with SQL3+ year experience in gathering data from various data sources, experience of work with big databases1+year experience in Python.1+ experience in finance (understanding of accruals concept, familiarity with the structure of financial statements and basic accounting principles and rules)High proficiency with TableauAdvanced user of MS Excel and Google DocsExperience with Spark or similar frameworkExcellent communicator that can both"", ""ask questions to grasp complex concepts and explain complex concepts to a general audienceProficient in English.Excellent presentation skillsDesirable- understanding of SaaS concepts and terminology (MRR, churn, retention rates, cohort analysis)Desirable- Jira experience""], ""As a plus"": [""An experienced data analyst who can work with high volumes of dataAn analytical and critical thinker who can model and interpret data to drive decisionsSomeone who can work independently and as a teamSomeone with a flexible mindset who can adjust to the fast-growing dynamics of the business and changing business requirementsAn innovative individual with the ability to develop and deliver creative solutions within fragmented or incomplete data""], ""We offer"": [""the chance of a lifetime to get onboard a young, dynamic, and fast-growing technology company;access to ground-breaking technology, and the freedom to expand your skills;a full-time, regular monthly contract, with a competitive compensation paid in US dollars;paid vacation, compensation for coworking, English language training, individual coaching, paid sick leave, medical insurance (if based in Russia or Ukraine), additional national holidays for your region;The freedom to choose where to work, anywhere in the world, so long as you\u2019re online.""], ""Responsibilities"": [""Having a good understanding of the company\u2019s business metrics, KPIs and sales drivers provide proactive insights on various business factors and their potential impact on the businessUsing Tableau build dashboards and analytical reports covering financial and non-financial business metrics to allow management take data-driven business decisions.Take a lead role in revenue reporting by pulling transactional data from various data sources ( billing systems, accounting software, PayPal , internal databases and others) and apply accounting rules to produce various revenue reports.Responsible for importing, transforming, validating, aggregating, and analyzing data from various sources for making conclusions about trends and best practices for development, sales&marketing, operational, finance functions of the business""], ""Project description"": [""As CloudLinux rapidly expands, we are making more and more decisions based on data. We need an experienced Data Analyst to deliver key metrics for our executive, marketing, sales, finance & product management teams to make decisions.""]}"
Playtech,https://jobs.dou.ua/companies/playtech/,BI Developer,https://jobs.dou.ua/companies/playtech/vacancies/133830/, Kyiv,04 November 2020,,"Required skills • 3+ years of development experience with Business Intelligence tools• A Bachelor’s Degree in Computer Science, Engineering, Information Technology, Information Systems or equivalent• Understanding of ETL (Extract, Transform, Load) techniques and processes• Knowledge of databases and data warehouse design• Experience with developing dashboards and reports• Experience working with MicroStrategy — advantage• Experience with fundamental SQL and data modeling • Understanding and knowledge of Big Data technologies such as Presto/Hive/HBase/Redis/MemSQL/MySQL/Kafka/Spark processes — advantage• Experience and knowledge working with SQL Server Integration Services (SSIS)• Knowledge of working with large datasets • Knowledge of data modeling and data visualization• Prior experience with SCRUM/Agile methodologies • Strong technical, analytical, and mathematical skills desired • Strong self-initiative • English as a native language • Great communication skills and strong customer facing abilities We offer · Possibility to cooperate with a product company· Professional growth· Educational possibilities· Competitive compensation· Fully-equipped perfect office space located in the city center (“Palats Sportu” metro station)· Warm and friendly attitude to every specialist Responsibilities • Creating and managing BI solutions. Working with business unit stakeholders to define Playtech’s BI delivery roadmap and scope• Analyzing business requirements and developing Business Intelligence solutions that provide business value to users• Developing views, custom reports, and dashboards pursuant to business requirements• Recommending and developing extendable and state of the art information delivery solutions to optimize ease of use and user adoption• Providing training and assistance to business users",dou,2020-11-12,BI Developer@Playtech,,,"{""Required skills"": [""3+ years of development experience with Business Intelligence tools"", ""A Bachelor\u2019s Degree in Computer Science, Engineering, Information Technology, Information Systems or equivalent"", ""Understanding of ETL (Extract, Transform, Load) techniques and processes"", ""Knowledge of databases and data warehouse design"", ""Experience with developing dashboards and reports"", ""Experience working with MicroStrategy"", ""advantage"", ""Experience with fundamental SQL and data modeling"", ""Understanding and knowledge of Big Data technologies such as Presto/Hive/HBase/Redis/MemSQL/MySQL/Kafka/Spark processes"", ""advantage"", ""Experience and knowledge working with SQL Server Integration Services (SSIS)"", ""Knowledge of working with large datasets"", ""Knowledge of data modeling and data visualization"", ""Prior experience with SCRUM/Agile methodologies"", ""Strong technical, analytical, and mathematical skills desired"", ""Strong self-initiative"", ""English as a native language"", ""Great communication skills and strong customer facing abilities""], ""We offer"": [""\u00b7 Possibility to cooperate with a product company\u00b7 Professional growth\u00b7 Educational possibilities\u00b7 Competitive compensation\u00b7 Fully-equipped perfect office space located in the city center (\u201cPalats Sportu\u201d metro station)\u00b7 Warm and friendly attitude to every specialist""], ""Responsibilities"": [""Creating and managing BI solutions. Working with business unit stakeholders to define Playtech\u2019s BI delivery roadmap and scope"", ""Analyzing business requirements and developing Business Intelligence solutions that provide business value to users"", ""Developing views, custom reports, and dashboards pursuant to business requirements"", ""Recommending and developing extendable and state of the art information delivery solutions to optimize ease of use and user adoption"", ""Providing training and assistance to business users""]}"
SocialTech,https://jobs.dou.ua/companies/socialtech/,Data Analyst (Scientist),https://jobs.dou.ua/companies/socialtech/vacancies/123061/, Kyiv,04 November 2020,,"SocialTech— глобальная IT-компания в индустрии Social Discovery. Мы развиваем собственные продукты, которыми пользуются миллионы людей по всему миру. У тебя аналитический склад ума, ты любишь качественно решать сложные задачи — присоединяйся к нам и мы вместе продолжим строить одну из сильнейших команд аналитики в стране. Чем предстоит заниматься:● Проектировать метрики из множества разрозненных, контринтуитивно связанных данных. ● Определять паттерны в поведении пользователей и извлекать из них пользу. ● Разбираться, за счет каких пользователей мы получаем основную часть прибыли, сколько и как они живут на продукте, растет или падает их активность. ● Автоматизировать решения повторяющихся задач: оценка A/B-тестов, наливка трафика в AdWords и т.д.● Разрабатывать инструменты для поиска узких мест в продуктовых воронках.● Продумать как обеспечить снижение волатильности или повышение точности метрик.● Проводить факторный, регрессионный, когортный и другие виды анализа. Чего в работе не будет:● ML ради ML. Мы не хотим строить космолеты там, где можно обойтись самокатом (но иногда приходится!).● Бюрократии. От тебя мы ожидаем:● Умение работать с цифрами и способность мыслить системно.● Желание быть экспертом своего дела и применять свои навыки с максимальной эффективностью.● Уверенное владение SQL.● Опыт работы с Python (pandas, seaborn) / R (tidyverse, ggplot).● Опыт с Tableau / PowerBI / QlikView будет плюсом. Что еще мы можем предложить:● Уютный офис, страховка, завтраки / обеды / снеки.● Компенсацию любых учебных материалов, занятий спортом и участия в конференциях.● Сплоченный и дружный коллектив.",dou,2020-11-12,Data Analyst (Scientist)@SocialTech,,,{}
Edifecs,https://jobs.dou.ua/companies/edifecs/,Инженер по работе с данными — Data engineer,https://jobs.dou.ua/companies/edifecs/vacancies/137483/, remote,04 November 2020,,"Required skills Ты станешь нашим лучшим кандидатом, если:Имеешь высшее техническое образование;Понимаешь, что такое IT, как устроен процесс разработки продукта, кто в него вовлеченПонимаешь основные стандарты представления информации — XML, JSON, CSV, PFF etc.Знаешь основы программирования на любом языке (JS, Java, Groovy etc.)Имеешь уровень английского языка, достаточный для чтения технической литературы, составления описаний и ведения переписки с коллегами;Имеешь хорошие аналитические способности и системный подход к решению задач, умение работать с большими объемами информации, способность предлагать решения, а не описывать проблемы. We offer Став частью нашей команды, ты получишь:ежемесячную оплату в долларах США с ежегодным пересмотромофициальные отношения на контрактной основе (ИП)постоянный проект с одним заказчикомопыт с наиболее востребованными и современными инструментами и технологиямигибкий график включая компенсацию больничного и отпускаиндивидуальный подход к профессиональному росту и развитию внутри компании Responsibilities В твои задачи будет входить:Анализ стандартов и структур представления данных на английском языкеСоздание структур данных, необходимых для бизнес процессов/бизнес задачСоздание систем трансляции (конвертирования) из одних структур в другие, используя модуль MapBuilder продукта SpecBuilder компании Edifecs. У продукта есть полноценный UI, программирование не требуется. Продукт содержит огромное количество различных компонентов, позволяющих преобразовать данные из одного вида в другой. В редких случаях, когда необходимого компонента нет, логику его работы нужно написать на JavaScript;Создание документации для полученных систем трансляции на английском языке в понятной для менеджмента и клиентов формеТесное взаимодействие с департаментами разработки и тестирования: разъяснение бизнес смысла разрабатываемых структур и систем трансляцииУстранение проблем клиентов, связанных с ошибками трансляции данных. Project description В связи с расширением пула интересных и уникальных задач в крупную американскую компанию Edifecs (www.edifecs.com) разыскиваем Инженер по работе с данными — Data engineer, способного посвятить себя любимой работе и стать преданным членом нашей дружной команды, раз и навсегда :) Жизнь в мире Edifecs похожа на любимый фильм: каждый день на протяжении 24 лет мы рассказываем нашу IT-историю. В десятках продуктовых командах кипит работа. Наши продукты используют в компаниях пула 550+Global Associates: в Северной Америке, Европе и Азии. Самое основное — в Edifecs каждый сотрудник играет главную роль. Мы являемся ведущей компанией в сфере автоматизации бизнес-процессов в отрасли Healthcare Insurance на рынке США. Имеем устойчивую линейку продуктов, которые пользуются успехом среди компаний, предоставляющих услуги по медицинскому страхованию. Рассматриваем коллегу из России или Украины. У нас распределенные команды и приятное сотрудничество будет в удаленном формате. Уважаемый наш потенциальный будущий коллега, если все наши пожелания совпали с вашими возможностями, ждём резюме, с ожиданиями по з.п. и вашим telegram в сопроводительном письме! Присоединяйтесь! У нас действительно круто!",dou,2020-11-12,Инженер по работе с данными — Data engineer@Edifecs,,,"{""Required skills"": [""\u0422\u044b \u0441\u0442\u0430\u043d\u0435\u0448\u044c \u043d\u0430\u0448\u0438\u043c \u043b\u0443\u0447\u0448\u0438\u043c \u043a\u0430\u043d\u0434\u0438\u0434\u0430\u0442\u043e\u043c, \u0435\u0441\u043b\u0438:\u0418\u043c\u0435\u0435\u0448\u044c \u0432\u044b\u0441\u0448\u0435\u0435 \u0442\u0435\u0445\u043d\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u0435;\u041f\u043e\u043d\u0438\u043c\u0430\u0435\u0448\u044c, \u0447\u0442\u043e \u0442\u0430\u043a\u043e\u0435 IT, \u043a\u0430\u043a \u0443\u0441\u0442\u0440\u043e\u0435\u043d \u043f\u0440\u043e\u0446\u0435\u0441\u0441 \u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u043f\u0440\u043e\u0434\u0443\u043a\u0442\u0430, \u043a\u0442\u043e \u0432 \u043d\u0435\u0433\u043e \u0432\u043e\u0432\u043b\u0435\u0447\u0435\u043d\u041f\u043e\u043d\u0438\u043c\u0430\u0435\u0448\u044c \u043e\u0441\u043d\u043e\u0432\u043d\u044b\u0435 \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u044b \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u044f \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438"", ""XML, JSON, CSV, PFF etc.\u0417\u043d\u0430\u0435\u0448\u044c \u043e\u0441\u043d\u043e\u0432\u044b \u043f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u043d\u0430 \u043b\u044e\u0431\u043e\u043c \u044f\u0437\u044b\u043a\u0435 (JS, Java, Groovy etc.)\u0418\u043c\u0435\u0435\u0448\u044c \u0443\u0440\u043e\u0432\u0435\u043d\u044c \u0430\u043d\u0433\u043b\u0438\u0439\u0441\u043a\u043e\u0433\u043e \u044f\u0437\u044b\u043a\u0430, \u0434\u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u044b\u0439 \u0434\u043b\u044f \u0447\u0442\u0435\u043d\u0438\u044f \u0442\u0435\u0445\u043d\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u043b\u0438\u0442\u0435\u0440\u0430\u0442\u0443\u0440\u044b, \u0441\u043e\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u044f \u043e\u043f\u0438\u0441\u0430\u043d\u0438\u0439 \u0438 \u0432\u0435\u0434\u0435\u043d\u0438\u044f \u043f\u0435\u0440\u0435\u043f\u0438\u0441\u043a\u0438 \u0441 \u043a\u043e\u043b\u043b\u0435\u0433\u0430\u043c\u0438;\u0418\u043c\u0435\u0435\u0448\u044c \u0445\u043e\u0440\u043e\u0448\u0438\u0435 \u0430\u043d\u0430\u043b\u0438\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u0441\u043f\u043e\u0441\u043e\u0431\u043d\u043e\u0441\u0442\u0438 \u0438 \u0441\u0438\u0441\u0442\u0435\u043c\u043d\u044b\u0439 \u043f\u043e\u0434\u0445\u043e\u0434 \u043a \u0440\u0435\u0448\u0435\u043d\u0438\u044e \u0437\u0430\u0434\u0430\u0447, \u0443\u043c\u0435\u043d\u0438\u0435 \u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c \u0441 \u0431\u043e\u043b\u044c\u0448\u0438\u043c\u0438 \u043e\u0431\u044a\u0435\u043c\u0430\u043c\u0438 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438, \u0441\u043f\u043e\u0441\u043e\u0431\u043d\u043e\u0441\u0442\u044c \u043f\u0440\u0435\u0434\u043b\u0430\u0433\u0430\u0442\u044c \u0440\u0435\u0448\u0435\u043d\u0438\u044f, \u0430 \u043d\u0435 \u043e\u043f\u0438\u0441\u044b\u0432\u0430\u0442\u044c \u043f\u0440\u043e\u0431\u043b\u0435\u043c\u044b.""], ""We offer"": [""\u0421\u0442\u0430\u0432 \u0447\u0430\u0441\u0442\u044c\u044e \u043d\u0430\u0448\u0435\u0439 \u043a\u043e\u043c\u0430\u043d\u0434\u044b, \u0442\u044b \u043f\u043e\u043b\u0443\u0447\u0438\u0448\u044c:\u0435\u0436\u0435\u043c\u0435\u0441\u044f\u0447\u043d\u0443\u044e \u043e\u043f\u043b\u0430\u0442\u0443 \u0432 \u0434\u043e\u043b\u043b\u0430\u0440\u0430\u0445 \u0421\u0428\u0410 \u0441 \u0435\u0436\u0435\u0433\u043e\u0434\u043d\u044b\u043c \u043f\u0435\u0440\u0435\u0441\u043c\u043e\u0442\u0440\u043e\u043c\u043e\u0444\u0438\u0446\u0438\u0430\u043b\u044c\u043d\u044b\u0435 \u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u044f \u043d\u0430 \u043a\u043e\u043d\u0442\u0440\u0430\u043a\u0442\u043d\u043e\u0439 \u043e\u0441\u043d\u043e\u0432\u0435 (\u0418\u041f)\u043f\u043e\u0441\u0442\u043e\u044f\u043d\u043d\u044b\u0439 \u043f\u0440\u043e\u0435\u043a\u0442 \u0441 \u043e\u0434\u043d\u0438\u043c \u0437\u0430\u043a\u0430\u0437\u0447\u0438\u043a\u043e\u043c\u043e\u043f\u044b\u0442 \u0441 \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0432\u043e\u0441\u0442\u0440\u0435\u0431\u043e\u0432\u0430\u043d\u043d\u044b\u043c\u0438 \u0438 \u0441\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u043c\u0438 \u0438\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442\u0430\u043c\u0438 \u0438 \u0442\u0435\u0445\u043d\u043e\u043b\u043e\u0433\u0438\u044f\u043c\u0438\u0433\u0438\u0431\u043a\u0438\u0439 \u0433\u0440\u0430\u0444\u0438\u043a \u0432\u043a\u043b\u044e\u0447\u0430\u044f \u043a\u043e\u043c\u043f\u0435\u043d\u0441\u0430\u0446\u0438\u044e \u0431\u043e\u043b\u044c\u043d\u0438\u0447\u043d\u043e\u0433\u043e \u0438 \u043e\u0442\u043f\u0443\u0441\u043a\u0430\u0438\u043d\u0434\u0438\u0432\u0438\u0434\u0443\u0430\u043b\u044c\u043d\u044b\u0439 \u043f\u043e\u0434\u0445\u043e\u0434 \u043a \u043f\u0440\u043e\u0444\u0435\u0441\u0441\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e\u043c\u0443 \u0440\u043e\u0441\u0442\u0443 \u0438 \u0440\u0430\u0437\u0432\u0438\u0442\u0438\u044e \u0432\u043d\u0443\u0442\u0440\u0438 \u043a\u043e\u043c\u043f\u0430\u043d\u0438\u0438""], ""Responsibilities"": [""\u0412 \u0442\u0432\u043e\u0438 \u0437\u0430\u0434\u0430\u0447\u0438 \u0431\u0443\u0434\u0435\u0442 \u0432\u0445\u043e\u0434\u0438\u0442\u044c:\u0410\u043d\u0430\u043b\u0438\u0437 \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043e\u0432 \u0438 \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u044f \u0434\u0430\u043d\u043d\u044b\u0445 \u043d\u0430 \u0430\u043d\u0433\u043b\u0438\u0439\u0441\u043a\u043e\u043c \u044f\u0437\u044b\u043a\u0435\u0421\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440 \u0434\u0430\u043d\u043d\u044b\u0445, \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u044b\u0445 \u0434\u043b\u044f \u0431\u0438\u0437\u043d\u0435\u0441 \u043f\u0440\u043e\u0446\u0435\u0441\u0441\u043e\u0432/\u0431\u0438\u0437\u043d\u0435\u0441 \u0437\u0430\u0434\u0430\u0447\u0421\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u0441\u0438\u0441\u0442\u0435\u043c \u0442\u0440\u0430\u043d\u0441\u043b\u044f\u0446\u0438\u0438 (\u043a\u043e\u043d\u0432\u0435\u0440\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f) \u0438\u0437 \u043e\u0434\u043d\u0438\u0445 \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440 \u0432 \u0434\u0440\u0443\u0433\u0438\u0435, \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f \u043c\u043e\u0434\u0443\u043b\u044c MapBuilder \u043f\u0440\u043e\u0434\u0443\u043a\u0442\u0430 SpecBuilder \u043a\u043e\u043c\u043f\u0430\u043d\u0438\u0438 Edifecs. \u0423 \u043f\u0440\u043e\u0434\u0443\u043a\u0442\u0430 \u0435\u0441\u0442\u044c \u043f\u043e\u043b\u043d\u043e\u0446\u0435\u043d\u043d\u044b\u0439 UI, \u043f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u043d\u0435 \u0442\u0440\u0435\u0431\u0443\u0435\u0442\u0441\u044f. \u041f\u0440\u043e\u0434\u0443\u043a\u0442 \u0441\u043e\u0434\u0435\u0440\u0436\u0438\u0442 \u043e\u0433\u0440\u043e\u043c\u043d\u043e\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u0445 \u043a\u043e\u043c\u043f\u043e\u043d\u0435\u043d\u0442\u043e\u0432, \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u044e\u0449\u0438\u0445 \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u0442\u044c \u0434\u0430\u043d\u043d\u044b\u0435 \u0438\u0437 \u043e\u0434\u043d\u043e\u0433\u043e \u0432\u0438\u0434\u0430 \u0432 \u0434\u0440\u0443\u0433\u043e\u0439. \u0412 \u0440\u0435\u0434\u043a\u0438\u0445 \u0441\u043b\u0443\u0447\u0430\u044f\u0445, \u043a\u043e\u0433\u0434\u0430 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e\u0433\u043e \u043a\u043e\u043c\u043f\u043e\u043d\u0435\u043d\u0442\u0430 \u043d\u0435\u0442, \u043b\u043e\u0433\u0438\u043a\u0443 \u0435\u0433\u043e \u0440\u0430\u0431\u043e\u0442\u044b \u043d\u0443\u0436\u043d\u043e \u043d\u0430\u043f\u0438\u0441\u0430\u0442\u044c \u043d\u0430 JavaScript;\u0421\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438 \u0434\u043b\u044f \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u044b\u0445 \u0441\u0438\u0441\u0442\u0435\u043c \u0442\u0440\u0430\u043d\u0441\u043b\u044f\u0446\u0438\u0438 \u043d\u0430 \u0430\u043d\u0433\u043b\u0438\u0439\u0441\u043a\u043e\u043c \u044f\u0437\u044b\u043a\u0435 \u0432 \u043f\u043e\u043d\u044f\u0442\u043d\u043e\u0439 \u0434\u043b\u044f \u043c\u0435\u043d\u0435\u0434\u0436\u043c\u0435\u043d\u0442\u0430 \u0438 \u043a\u043b\u0438\u0435\u043d\u0442\u043e\u0432 \u0444\u043e\u0440\u043c\u0435\u0422\u0435\u0441\u043d\u043e\u0435 \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0435 \u0441 \u0434\u0435\u043f\u0430\u0440\u0442\u0430\u043c\u0435\u043d\u0442\u0430\u043c\u0438 \u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u0438 \u0442\u0435\u0441\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f: \u0440\u0430\u0437\u044a\u044f\u0441\u043d\u0435\u043d\u0438\u0435 \u0431\u0438\u0437\u043d\u0435\u0441 \u0441\u043c\u044b\u0441\u043b\u0430 \u0440\u0430\u0437\u0440\u0430\u0431\u0430\u0442\u044b\u0432\u0430\u0435\u043c\u044b\u0445 \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440 \u0438 \u0441\u0438\u0441\u0442\u0435\u043c \u0442\u0440\u0430\u043d\u0441\u043b\u044f\u0446\u0438\u0438\u0423\u0441\u0442\u0440\u0430\u043d\u0435\u043d\u0438\u0435 \u043f\u0440\u043e\u0431\u043b\u0435\u043c \u043a\u043b\u0438\u0435\u043d\u0442\u043e\u0432, \u0441\u0432\u044f\u0437\u0430\u043d\u043d\u044b\u0445 \u0441 \u043e\u0448\u0438\u0431\u043a\u0430\u043c\u0438 \u0442\u0440\u0430\u043d\u0441\u043b\u044f\u0446\u0438\u0438 \u0434\u0430\u043d\u043d\u044b\u0445.""], ""Project description"": [""\u0412 \u0441\u0432\u044f\u0437\u0438 \u0441 \u0440\u0430\u0441\u0448\u0438\u0440\u0435\u043d\u0438\u0435\u043c \u043f\u0443\u043b\u0430 \u0438\u043d\u0442\u0435\u0440\u0435\u0441\u043d\u044b\u0445 \u0438 \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0445 \u0437\u0430\u0434\u0430\u0447 \u0432 \u043a\u0440\u0443\u043f\u043d\u0443\u044e \u0430\u043c\u0435\u0440\u0438\u043a\u0430\u043d\u0441\u043a\u0443\u044e \u043a\u043e\u043c\u043f\u0430\u043d\u0438\u044e Edifecs (www.edifecs.com) \u0440\u0430\u0437\u044b\u0441\u043a\u0438\u0432\u0430\u0435\u043c \u0418\u043d\u0436\u0435\u043d\u0435\u0440 \u043f\u043e \u0440\u0430\u0431\u043e\u0442\u0435 \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438"", ""Data engineer, \u0441\u043f\u043e\u0441\u043e\u0431\u043d\u043e\u0433\u043e \u043f\u043e\u0441\u0432\u044f\u0442\u0438\u0442\u044c \u0441\u0435\u0431\u044f \u043b\u044e\u0431\u0438\u043c\u043e\u0439 \u0440\u0430\u0431\u043e\u0442\u0435 \u0438 \u0441\u0442\u0430\u0442\u044c \u043f\u0440\u0435\u0434\u0430\u043d\u043d\u044b\u043c \u0447\u043b\u0435\u043d\u043e\u043c \u043d\u0430\u0448\u0435\u0439 \u0434\u0440\u0443\u0436\u043d\u043e\u0439 \u043a\u043e\u043c\u0430\u043d\u0434\u044b, \u0440\u0430\u0437 \u0438 \u043d\u0430\u0432\u0441\u0435\u0433\u0434\u0430 :)""]}"
Jabil Software Services,https://jobs.dou.ua/companies/kuatro-technologies/,Middle/Senior Data Engineer,https://jobs.dou.ua/companies/kuatro-technologies/vacancies/133921/," Kharkiv, remote",03 November 2020,,"Required skills Job requirements:∙ Minimum of 3 or more years working as Data Engineer on Advanced Analytics and Data Provisioning projects∙ Business Requirements Analysis and technical design∙ Communication with key business users, Data Scientists, IT Tech leads.∙ Hands-on experience with BIG data processing, clean, transform and prepare data.∙ Hands-on experience working with Data Lakes (Hadoop, AWS S3, Azure ADLS)∙ Expertise on joining datasets and creation of de-normalized views∙ Experience with Data Preparation and Features Engineering as part of Machine Learning or Advance Analytics project∙ Experience using Code/Version management tools As a plus Will be a plus:∙ Some basic knowledge of Supply chain and Manufacturing process domain∙ Understanding of Cloud Solutions and Architecture We offer We offer:∙ competitive compensation∙ excellent benefit package∙ flexible schedule∙ sports activities support∙ health insurance support∙ corporate English∙ free lunches∙ opportunities to grow professionally∙ well-equipped workplace∙ convenient office location (Botanichniy Sad metro station) Responsibilities Experience and knowledge:∙ Deep knowledge of AWS Data Services∙ Experienced writing AWS Glue, Spark code for data processing and preparation∙ Experienced working with Glue Endpoints. Step functions, Lambda∙ Experienced in SageMaker notebooks using Python and PySpark to create features and write to S3 & AuroraDB∙ Experienced in IoT Analytics using AWS Kinesis data streams or Kafka streaming∙ Automate with CI/CD pipelines by Modularizing / generalizing with variables∙ Experienced working in any of the popular ETL tools∙ Experienced or Working knowledge of Talend∙ Working knowledge of Snowflake∙ Working knowledge of Hadoop toolset like HDFS, Hive, Impala∙ Working knowledge of Linux Shell Scripting Project description Data Engineer will be responsible for the design, development and implementation of BIG data & IoT Analytics solutions within Jabil’s Enterprise Information Platform. A successful candidate will be able to create, evaluate and implement plans and design proposals for high-impact analytics solutions and their use involving leading-edge cloud technologies and methods considering key factors such as their long-term effectiveness (service delivery and cost), practicality, technical limitations and criticality. Must be a team player who is both compassionate for learning and firm on standards to effectively deliver reporting solutions.",dou,2020-11-12,Middle/Senior Data Engineer@Jabil Software Services,,,"{""Required skills"": [""Job requirements:\u2219 Minimum of 3 or more years working as Data Engineer on Advanced Analytics and Data Provisioning projects\u2219 Business Requirements Analysis and technical design\u2219 Communication with key business users, Data Scientists, IT Tech leads.\u2219 Hands-on experience with BIG data processing, clean, transform and prepare data.\u2219 Hands-on experience working with Data Lakes (Hadoop, AWS S3, Azure ADLS)\u2219 Expertise on joining datasets and creation of de-normalized views\u2219 Experience with Data Preparation and Features Engineering as part of Machine Learning or Advance Analytics project\u2219 Experience using Code/Version management tools""], ""As a plus"": [""Will be a plus:\u2219 Some basic knowledge of Supply chain and Manufacturing process domain\u2219 Understanding of Cloud Solutions and Architecture""], ""We offer"": [""We offer:\u2219 competitive compensation\u2219 excellent benefit package\u2219 flexible schedule\u2219 sports activities support\u2219 health insurance support\u2219 corporate English\u2219 free lunches\u2219 opportunities to grow professionally\u2219 well-equipped workplace\u2219 convenient office location (Botanichniy Sad metro station)""], ""Responsibilities"": [""Experience and knowledge:\u2219 Deep knowledge of AWS Data Services\u2219 Experienced writing AWS Glue, Spark code for data processing and preparation\u2219 Experienced working with Glue Endpoints. Step functions, Lambda\u2219 Experienced in SageMaker notebooks using Python and PySpark to create features and write to S3 & AuroraDB\u2219 Experienced in IoT Analytics using AWS Kinesis data streams or Kafka streaming\u2219 Automate with CI/CD pipelines by Modularizing / generalizing with variables\u2219 Experienced working in any of the popular ETL tools\u2219 Experienced or Working knowledge of Talend\u2219 Working knowledge of Snowflake\u2219 Working knowledge of Hadoop toolset like HDFS, Hive, Impala\u2219 Working knowledge of Linux Shell Scripting""], ""Project description"": [""Data Engineer will be responsible for the design, development and implementation of BIG data & IoT Analytics solutions within Jabil\u2019s Enterprise Information Platform. A successful candidate will be able to create, evaluate and implement plans and design proposals for high-impact analytics solutions and their use involving leading-edge cloud technologies and methods considering key factors such as their long-term effectiveness (service delivery and cost), practicality, technical limitations and criticality. Must be a team player who is both compassionate for learning and firm on standards to effectively deliver reporting solutions.""]}"
Qencode,https://jobs.dou.ua/companies/qencode/,Data Engineer,https://jobs.dou.ua/companies/qencode/vacancies/137464/," Kyiv, remote",03 November 2020,,"Required skills ​​• 2+ years experience in the data warehouse space.• опыт работы с хранилищами данных от 2 лет; • 2+ years experience in custom ETL design, implementation and maintenance.• опыт проектирования, разработки и поддержания процедур извлечения, трансформации и загрузки данных от 2 лет; • 2+ years experience working with either a MapReduce or an MPP system.• опыт работы с моделью MapReduce или архитектурой MPP от 2 лет; • 2+ years experience with object-oriented programming languages.• опыт работы с объектно-ориентированными языками программирования от 2 лет; • 2+ years experience with schema design and dimensional data modeling.• опыт работы в области проектирования схем управления данными и размерного моделирования от 2 лет; • 2+ years experience in writing SQL statements.• опыт работы с операторами SQL от 2 лет; • Experience analyzing data to identify deliverables, gaps and inconsistencies.• опыт анализа данных с целью определения результатов, пробелов и несоответствий; • Experience managing and communicating data warehouse plans to internal clients.• опыт управления и планирования хранилищ данных для внутренних клиентов; • BS/BA in Technical Field, Computer Science or Mathematics.• высшее образование в области технических наук, информационных технологий или математики; • Knowledge in Python or Java• знание Python или Java. We offer • Work from home, now and always (fully remote job)• работу из дома, сейчас и всегда (полностью удаленная работа); • Company sponsored educational programs from the third-party of your choosing• оплачиваемые работодателем образовательные программы на ваш выбор; • Growing company, serving the in the high growth video sector• работу в растущей компании в динамично развивающейся сфере видеоданных. Responsibilities • Manage data warehouse plans for Qencode’s suite of products• управление планами хранилищ данных для линейки продуктов Qencode; • Interface with the executive team and engineers to understand data needs.• взаимодействие с руководством и инженерами с целью понимания требований к данным; • Build data expertise and own data quality.• развитие экспертных знаний в области данных и обеспечение качества данных; • Design, build and launch new data models in production.• проектирование, создание и запуск новых моделей данных; • Design, build and launch new data extraction, transformation and loading processes.• проектирование, создание и запуск новых процедур извлечения, трансформации и загрузки данных; • Support existing processes running across production.• поддержка функционирования действующих производственных процессов; • Define and manage SLA for all data sets across Qencode.• определение и управление уровнем качества предоставляемых услуг в отношении всех наборов данных в Qencode; • Work with data infrastructure to triage infra issues and drive to resolution.• работа с инфраструктурой данных с целью сортировки проблем инфраструктуры и их дальнейшего решения. Project description Do you like working with large amounts of data? Do you like developing data models that will affect user experience, company decisions, and growth? If so, we want to speak with you! About Qencode We are a team of passionate engineers and managers working to redefine the future of cloud video technology. With internet bandwidth and costs at the top of business priorities, our unique solution optimizes video files, making them 70% smaller, improving load times and saving our customers lot’s of money on Storage and CDN costs. Our customers include some of the largest players in online media, entertainment, sports, news, broadcasting, over-the-top apps (OTT) and online video streaming services (as well as other video websites and applications). Our robust API offers full flexibility for any workflow, with hundreds of customization parameters, allowing the largest video companies in the world to build powerful video workflows to fit their needs in this quickly evolving market. Current changes in the market and economy means our services are more needed than ever before. If you are interested in helping business save money in this difficult time, while improving the video experience of their customers, please reach out to us! ---------------------------------------- Вам нравится работать с большими объемам данных? Вам нравится разрабатывать модели данных, которые расширяют опыт пользователя, влияют на принимаемые решения и ведут к росту компании? Если «да», то мы хотим встретится в вами! О компании Qencode Мы — команда увлеченных инженеров и менеджеров, стремящихся переосмыслить будущее облачной видеотехнологии. Скорость и стоимость интернет-соединения приобретают первостепенное значение для бизнеса. Учитывая это обстоятельство, мы разработали решение, с помощью которого можно оптимизировать видео файлы, уменьшив их размер на 70 процентов, увеличить скорость загрузки и сэкономить деньги наших клиентов, сократив расходы на хранение и сети доставки данных. Наши клиенты — крупнейшие интернет-издания, развлекательные, спортивные, новостные и вещательные компании, компании, работающие с приложениями по технологии OTT и оказывающие услуги по передаче потокового видео (а также другие компании, на сайтах которых используется видеоданные). При помощи нашего надежного прикладного интерфейса обеспечивается гибкость любого производственного процесса и возможность настройки большого количества параметров. Благодаря этой функции крупнейшие компании, работающие с видеоданными, могут выстроить функциональные процессы, соответствующие требованиям быстрорастущего рынка.Текущие изменения в экономике приводят к тому, что спрос на наши услуги высок как никогда. Свяжитесь с нами, если вы хотите улучшить качество оказываемых услуг по обработке видеоданных и сэкономить в это сложное время!",dou,2020-11-12,Data Engineer@Qencode,,,"{""Required skills"": [""\u200b\u200b"", ""2+ years experience in the data warehouse space."", ""\u043e\u043f\u044b\u0442 \u0440\u0430\u0431\u043e\u0442\u044b \u0441 \u0445\u0440\u0430\u043d\u0438\u043b\u0438\u0449\u0430\u043c\u0438 \u0434\u0430\u043d\u043d\u044b\u0445 \u043e\u0442 2 \u043b\u0435\u0442;""], ""We offer"": [""2+ years experience in custom ETL design, implementation and maintenance."", ""\u043e\u043f\u044b\u0442 \u043f\u0440\u043e\u0435\u043a\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f, \u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u0438 \u043f\u043e\u0434\u0434\u0435\u0440\u0436\u0430\u043d\u0438\u044f \u043f\u0440\u043e\u0446\u0435\u0434\u0443\u0440 \u0438\u0437\u0432\u043b\u0435\u0447\u0435\u043d\u0438\u044f, \u0442\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438 \u0438 \u0437\u0430\u0433\u0440\u0443\u0437\u043a\u0438 \u0434\u0430\u043d\u043d\u044b\u0445 \u043e\u0442 2 \u043b\u0435\u0442;""], ""Responsibilities"": [""2+ years experience working with either a MapReduce or an MPP system."", ""\u043e\u043f\u044b\u0442 \u0440\u0430\u0431\u043e\u0442\u044b \u0441 \u043c\u043e\u0434\u0435\u043b\u044c\u044e MapReduce \u0438\u043b\u0438 \u0430\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u043e\u0439 MPP \u043e\u0442 2 \u043b\u0435\u0442;""], ""Project description"": [""2+ years experience with object-oriented programming languages."", ""\u043e\u043f\u044b\u0442 \u0440\u0430\u0431\u043e\u0442\u044b \u0441 \u043e\u0431\u044a\u0435\u043a\u0442\u043d\u043e-\u043e\u0440\u0438\u0435\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u043c\u0438 \u044f\u0437\u044b\u043a\u0430\u043c\u0438 \u043f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u043e\u0442 2 \u043b\u0435\u0442;""]}"
Data Science UA,https://jobs.dou.ua/companies/data-science-ua/,Python Developer (Location — Dnipro or remote),https://jobs.dou.ua/companies/data-science-ua/vacancies/137449/, remote,03 November 2020,,"Required skills — 2+ years of commercial programming with Python3— Experience with SQL, ETL— Understanding of DWH architecture— Understanding of system design— MS SSIS / Apache Airflow, asyncio, monitoring of machine learning models We offer — Official employment— Vacation period for 24 calendar days— Medical insurance — up to 60%— Ability to work in the office and remotely— Full time job: Mon — Fri from 9:00 to 18:00— Opportunity for advanced training. The company pays for the courses (80% of the cost) Responsibilities — Work closely with Data Scientists and BI engineers— Development of ETL / ELT processes— Design and monitoring of databases — Work with internal customers— Testing of ClickHouse Cluster— ML Ops tasks Project description Our partner has tens of thousands of orders every day and a place among the leaders of the Ukrainian drogerie online. Without exaggeration, the strongest e-commerce team is behind this project. The team that delivered 680% + growth in a year and 50,000 new items on the site that are not on store shelves. We’re looking for a skilled Python Data Engineer with good understanding of the business and product needs to create new age of Ukrainian e-commerce. We’re using new and modern technologies and not afraid to experiment, take responsibility or defend our technical decisions.By joining our team, you can growth combining Python development, Data Science and DevOps skills.",dou,2020-11-12,Python Developer (Location — Dnipro or remote)@Data Science UA,,,"{""Required skills"": [""2+ years of commercial programming with Python3"", ""Experience with SQL, ETL"", ""Understanding of DWH architecture"", ""Understanding of system design"", ""MS SSIS / Apache Airflow, asyncio, monitoring of machine learning models""], ""We offer"": [""Official employment"", ""Vacation period for 24 calendar days"", ""Medical insurance"", ""up to 60%"", ""Ability to work in the office and remotely"", ""Full time job: Mon"", ""Fri from 9:00 to 18:00"", ""Opportunity for advanced training. The company pays for the courses (80% of the cost)""], ""Responsibilities"": [""Work closely with Data Scientists and BI engineers"", ""Development of ETL / ELT processes"", ""Design and monitoring of databases"", ""Work with internal customers"", ""Testing of ClickHouse Cluster"", ""ML Ops tasks""], ""Project description"": [""Our partner has tens of thousands of orders every day and a place among the leaders of the Ukrainian drogerie online. Without exaggeration, the strongest e-commerce team is behind this project. The team that delivered 680% + growth in a year and 50,000 new items on the site that are not on store shelves.""]}"
Turnitin,https://jobs.dou.ua/companies/unicheck/,Senior Data Scientist (NLP),https://jobs.dou.ua/companies/unicheck/vacancies/121275/, Kyiv,03 November 2020,,"Required skills — Previous experience in Software Engineering.— 5+ years of experience in NLP and 3+ with Python.— Good practical and theoretical knowledge of Machine Learning and Deep Learning.— Understanding of classic text processing and embeddings.— Understanding of unsupervised learning algorithms and Machine Learning metrics.— Experience with scikit-learn and any of Keras / Tensorflow / PyTorch.— Knowledge of SQL and experience with relational or NoSQL databases.— Experience with Git, Linux.— Good communication and teamwork skills.— Good written and spoken English. We offer — Professional growth opportunity.— Generous holiday allowance.— Free English classes.— Young and friendly team.— Close-knit family atmosphere.— Flexible schedule.— Comfortable and modern office. Responsibilities — Lead research projects and specific research areas in the company.— Propose new ideas and directions of research, improvements, technical decisions and best practices.— Fully cover (develop, maintain and monitor) the entire lifecycle of created models.— Propose and realize new ideas to benefit our customers and the company.— Create production-ready solutions, dockerize them, launch MVPs, help DevOps Team to deploy them.— Share knowledge, ideas and new approaches with team members.— Stay up to date with the latest findings in applied data science.",dou,2020-11-12,Senior Data Scientist (NLP)@Turnitin,,,"{""Required skills"": [""Previous experience in Software Engineering."", ""5+ years of experience in NLP and 3+ with Python."", ""Good practical and theoretical knowledge of Machine Learning and Deep Learning."", ""Understanding of classic text processing and embeddings."", ""Understanding of unsupervised learning algorithms and Machine Learning metrics."", ""Experience with scikit-learn and any of Keras / Tensorflow / PyTorch."", ""Knowledge of SQL and experience with relational or NoSQL databases."", ""Experience with Git, Linux."", ""Good communication and teamwork skills."", ""Good written and spoken English.""], ""We offer"": [""Professional growth opportunity."", ""Generous holiday allowance."", ""Free English classes."", ""Young and friendly team."", ""Close-knit family atmosphere."", ""Flexible schedule."", ""Comfortable and modern office.""], ""Responsibilities"": [""Lead research projects and specific research areas in the company."", ""Propose new ideas and directions of research, improvements, technical decisions and best practices."", ""Fully cover (develop, maintain and monitor) the entire lifecycle of created models."", ""Propose and realize new ideas to benefit our customers and the company."", ""Create production-ready solutions, dockerize them, launch MVPs, help DevOps Team to deploy them."", ""Share knowledge, ideas and new approaches with team members."", ""Stay up to date with the latest findings in applied data science.""]}"
8allocate,https://jobs.dou.ua/companies/8allocate/,Data Engineer,https://jobs.dou.ua/companies/8allocate/vacancies/137423/," Kyiv, remote",03 November 2020,,"Required skills You will fit if you:— Commercial experience developing Spark Jobs using Scala— Commercial experience using Java and Scala (Python nice to have)— Experience in data processing using traditional and distributed systems (Hadoop, Spark, AWS — S3)— Experience designing data models and data warehouses.— Experience in SQL, NoSQL database management systems (PostgreSQL and Cassandra)— Commercial experience using messaging technologies (RabbitMQ, Kafka)— Experience using orchestration software (Chef, Puppet, Ansible, Salt)— Confident with building complex ETL workflows (Luigi, Airflow)— Good knowledge working cloud technologies (AWS)— Good knowledge using monitoring software (ELK stack)— Motivated problem-solving skills, ability to bring ideas forward and adapt solutions to complex challenges We offer Why choose us?— “Family and Friends”. We are no longer a start-up, but still, have a family atmosphere in our supportive and spirited team, who are all working together on the same goal.— “Just break down all barriers and find a better way”. Every day you’ll meet with interesting and challenging (international) projects that are covering industries from commercial aviation to fintech (different technologies, different products).— “Hungry for learning”. You will get a lot of chances for career advancement and the development of new skills, opportunities for mentorship, or learning from more experienced colleagues. Benefits from 8allocate:— Corporate events, holidays and team buildings for your joy.— Training and development: we have a huge library (about 500 books!) and a budget for your professional development.— People-oriented management without bureaucracy.— Paid vacation and sick leaves.— Relocation program: if you are from another city and want to move to Kyiv, we will be happy to help you. Responsibilities Main responsibilities and activities:— Build services/features/libraries that serve as a definitive examples for new engineers and makes major contributions to library code or core services— Design low risk Spark process and write effective complex Spark jobs (data processes, aggregations, pipeline)— Design low risk APIs and write complex asynchronous, highly parallel low latency APIs and processesWork as part of Agile team to maintain, improve, monitor Adthena’s data collection processes using Java and Scala— Write high quality, extensible and testable code by applying good engineering practices (TDD, SOLID) using.— Understand and apply modern technologies, data structures and design patterns to solve real problems efficiently— Support TA and Data Science team to help deliver and productionise their backlog/prototypes— Take ownership and pride in the products we build and always make sure they are of the highest standard— Be empathetic towards team members and customers Project description 8allocate is a global provider of end-to-end custom software development solutions to companies all over the globe, from North America to the EU to Israel to Australia. Headquartered in Estonia, we run offshore R&D centers in Kyiv and Lviv. Our team is 50% remote and distributed. We specialize in flexible interaction exclusively with international clients (we cover industries from commercial aviation to fintech) thanks to a multinational support group of experts and management. Currently, we are looking for a Data Engineer for an Adtech-related project. About the project: a market leader in developing Competitive Intelligence for AdTech Search.Application teams develop unparalleled technologies that help our clients understand their paid and organic search landscape and improve campaign performance. As a Data Engineer, you will be working across our entire stack, so a real passion to drive the product and technology forward is something that we value. Your responsibilities will include helping with a vision for the future architecture of this complex data system, adding innovative ideas that use the latest cutting edge technology. You will work closely with Web and Data Science teams to deliver user-centric solutions to our customers and become an expert in developing high quality technical solutions. The stack of a project: — Languages: Java, Scala, JavaScript (React, Backbone), SQL and scripting using Bash and Python— Frameworks: DropWizard, React, Akka and Play Framework (Scala)— Databases: PostgreSQL, AWS(S3), Redshift, Redis, MongoDB, Cassandra— Technologies: RabbitMQ (messaging), Quartz scheduling, Docker and Kubernetes, Maven— CI/CD: TeamCity, Jenkins— Source Control: Git (GitHub)— Other Tools: IntelliJ IDEA, Jira, Grafana",dou,2020-11-12,Data Engineer@8allocate,,,"{""Required skills"": [""You will fit if you:"", ""Commercial experience developing Spark Jobs using Scala"", ""Commercial experience using Java and Scala (Python nice to have)"", ""Experience in data processing using traditional and distributed systems (Hadoop, Spark, AWS"", ""S3)"", ""Experience designing data models and data warehouses."", ""Experience in SQL, NoSQL database management systems (PostgreSQL and Cassandra)"", ""Commercial experience using messaging technologies (RabbitMQ, Kafka)"", ""Experience using orchestration software (Chef, Puppet, Ansible, Salt)"", ""Confident with building complex ETL workflows (Luigi, Airflow)"", ""Good knowledge working cloud technologies (AWS)"", ""Good knowledge using monitoring software (ELK stack)"", ""Motivated problem-solving skills, ability to bring ideas forward and adapt solutions to complex challenges""], ""We offer"": [""Why choose us?"", ""\u201cFamily and Friends\u201d. We are no longer a start-up, but still, have a family atmosphere in our supportive and spirited team, who are all working together on the same goal."", ""\u201cJust break down all barriers and find a better way\u201d. Every day you\u2019ll meet with interesting and challenging (international) projects that are covering industries from commercial aviation to fintech (different technologies, different products)."", ""\u201cHungry for learning\u201d. You will get a lot of chances for career advancement and the development of new skills, opportunities for mentorship, or learning from more experienced colleagues.""], ""Responsibilities"": [""Benefits from 8allocate:"", ""Corporate events, holidays and team buildings for your joy."", ""Training and development: we have a huge library (about 500 books!) and a budget for your professional development."", ""People-oriented management without bureaucracy."", ""Paid vacation and sick leaves."", ""Relocation program: if you are from another city and want to move to Kyiv, we will be happy to help you.""], ""Project description"": [""Main responsibilities and activities:"", ""Build services/features/libraries that serve as a definitive examples for new engineers and makes major contributions to library code or core services"", ""Design low risk Spark process and write effective complex Spark jobs (data processes, aggregations, pipeline)"", ""Design low risk APIs and write complex asynchronous, highly parallel low latency APIs and processesWork as part of Agile team to maintain, improve, monitor Adthena\u2019s data collection processes using Java and Scala"", ""Write high quality, extensible and testable code by applying good engineering practices (TDD, SOLID) using."", ""Understand and apply modern technologies, data structures and design patterns to solve real problems efficiently"", ""Support TA and Data Science team to help deliver and productionise their backlog/prototypes"", ""Take ownership and pride in the products we build and always make sure they are of the highest standard"", ""Be empathetic towards team members and customers""]}"
Kyivstar,https://jobs.dou.ua/companies/kyivstar/,Middle Data Scientist,https://jobs.dou.ua/companies/kyivstar/vacancies/119233/, Kyiv,03 November 2020,,"Required skills — 1+ year experience with R / Python (statistical and ML packages)— Understanding theoretical concepts of statistics/probability, data mining, machine learning— Advanced knowledge of SQL— Experience in solving business problems with analytical models— Able to build meaningful visualizations of results As a plus — Applied research experience in the domain of data management, visualization and analytics, incl. affinity to the industrial research way of working. — Experience with Data Sciences within Telecom. — Familiar with Hortonworks or Cloudera distributions— Knowledge and hands-on experience with one or more of the following: Spark/Pytorch/TensorFlow— Kaggle challenges We offer — A unique experience of working for the largest and most beloved mobile operator in Ukraine — Real opportunity to ship digital products to millions of customers; — A competitive salary; Annual bonus Paid sick leave and vacation Financial aid in different life situations Possibility to work remotely at regular intervals Flexible working hours — Medical and life insurance; — Great possibilities for professional development and career growth; — Friendly & Collaborative Environment. Responsibilities — Preprocess data for analysis— Exploratory data analysis— Generate insights from data— Develop pipelines with machine learning and artificial intelligence models and algorithms— Prepare development and deploy documentation for data engineers— Explain modeling approaches and discuss results / business impacts with non-technical staff Project description Within the digital transformation strategy, Kyivstar is investing resources into the development of a data management platform (DMP) based on BigData technologies, the Hadoop ecosystem and cloud computing platforms. One of the areas of development is the elaborationand implementation of products for internal and external customers using big data and machine learning technologies. So we are looking for a team of DataScientists — experts in the field of machine learning and analytics with practical skills in MLpipelines development using OpenSource tools.",dou,2020-11-12,Middle Data Scientist@Kyivstar,,,"{""Required skills"": [""1+ year experience with R / Python (statistical and ML packages)"", ""Understanding theoretical concepts of statistics/probability, data mining, machine learning"", ""Advanced knowledge of SQL"", ""Experience in solving business problems with analytical models"", ""Able to build meaningful visualizations of results""], ""As a plus"": [""Applied research experience in the domain of data management, visualization and analytics, incl. affinity to the industrial research way of working."", ""Experience with Data Sciences within Telecom."", ""Familiar with Hortonworks or Cloudera distributions"", ""Knowledge and hands-on experience with one or more of the following: Spark/Pytorch/TensorFlow"", ""Kaggle challenges""], ""We offer"": [""A unique experience of working for the largest and most beloved mobile operator in Ukraine"", ""Real opportunity to ship digital products to millions of customers;"", ""A competitive salary; Annual bonus Paid sick leave and vacation Financial aid in different life situations Possibility to work remotely at regular intervals Flexible working hours"", ""Medical and life insurance;"", ""Great possibilities for professional development and career growth;"", ""Friendly & Collaborative Environment.""], ""Responsibilities"": [""Preprocess data for analysis"", ""Exploratory data analysis"", ""Generate insights from data"", ""Develop pipelines with machine learning and artificial intelligence models and algorithms"", ""Prepare development and deploy documentation for data engineers"", ""Explain modeling approaches and discuss results / business impacts with non-technical staff""], ""Project description"": [""Within the digital transformation strategy, Kyivstar is investing resources into the development of a data management platform (DMP) based on BigData technologies, the Hadoop ecosystem and cloud computing platforms. One of the areas of development is the elaborationand implementation of products for internal and external customers using big data and machine learning technologies. So we are looking for a team of DataScientists"", ""experts in the field of machine learning and analytics with practical skills in MLpipelines development using OpenSource tools.""]}"
Kyivstar,https://jobs.dou.ua/companies/kyivstar/,BigData ETL Engineer,https://jobs.dou.ua/companies/kyivstar/vacancies/116530/, Kyiv,03 November 2020,,"Required skills — 3+ years of programming experience or relevant ETL experience— Data formats knowledge and the differences between them— Solid experience with Hadoop stack— Experience with RDBMS and/or NoSQL — Experience with Kafka— Understanding of processes design and development for data modeling, processing and analysis— Experience with Java and/or Scala and/or python— Knowledge of version control system: git or bitbucket— Background in test driven development, automated testing and other software engineering best practices (e.g., performance, security, BDD, etc.)— Docker/Kubernetes paradigm understanding We offer - A unique experience of working the most customers beloved and largest mobile operator in Ukraine- Real opportunity to ship digital products to millions of customers- A competitive salary- Remote day per week- Great possibilities for professional development and career growth- Medical insurance- Life insurance- Friendly & Collaborative Environment Responsibilities — Developing ETL flows based on Hadoop/BigData stack technology: Apache Nifi, Hive, Spark, Kafka — Troubleshooting and performance optimization for data processing flows, data models Project description Kyivstar is looking for a BigData Engineer to develop a platform for collection, processing and storing petabytes data. Built with open source technologies, the systems handle billions of records daily, generating terabytes of data.",dou,2020-11-12,BigData ETL Engineer@Kyivstar,,,"{""Required skills"": [""3+ years of programming experience or relevant ETL experience"", ""Data formats knowledge and the differences between them"", ""Solid experience with Hadoop stack"", ""Experience with RDBMS and/or NoSQL"", ""Experience with Kafka"", ""Understanding of processes design and development for data modeling, processing and analysis"", ""Experience with Java and/or Scala and/or python"", ""Knowledge of version control system: git or bitbucket"", ""Background in test driven development, automated testing and other software engineering best practices (e.g., performance, security, BDD, etc.)"", ""Docker/Kubernetes paradigm understanding""], ""We offer"": [""- A unique experience of working the most customers beloved and largest mobile operator in Ukraine- Real opportunity to ship digital products to millions of customers- A competitive salary- Remote day per week- Great possibilities for professional development and career growth- Medical insurance- Life insurance- Friendly & Collaborative Environment""], ""Responsibilities"": [""Developing ETL flows based on Hadoop/BigData stack technology: Apache Nifi, Hive, Spark, Kafka"", ""Troubleshooting and performance optimization for data processing flows, data models""], ""Project description"": [""Kyivstar is looking for a BigData Engineer to develop a platform for collection, processing and storing petabytes data. Built with open source technologies, the systems handle billions of records daily, generating terabytes of data.""]}"
Valid-X,https://jobs.dou.ua/companies/validbook/,Data Engineer,https://jobs.dou.ua/companies/validbook/vacancies/137403/," Lviv, remote",03 November 2020,,"Required skills — Spark— Python— Databricks As a plus — Airflow— Kafka— AWS We offer — Competitive salary— Dynamic environment of a fast growing company— Bonuses— Health insurance Responsibilities — Create and maintain optimal data pipeline architecture,— Assemble large, complex data sets that meet functional / non-functional business requirements.— Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. Project description Develop, modernize and optimize a back-end for a portfolio of online games that are currently being used by 100K+ users. You will work with our data & AI team to make these games safe, secure and fair.",dou,2020-11-12,Data Engineer@Valid-X,,,"{""Required skills"": [""Spark"", ""Python"", ""Databricks""], ""As a plus"": [""Airflow"", ""Kafka"", ""AWS""], ""We offer"": [""Competitive salary"", ""Dynamic environment of a fast growing company"", ""Bonuses"", ""Health insurance""], ""Responsibilities"": [""Create and maintain optimal data pipeline architecture,"", ""Assemble large, complex data sets that meet functional / non-functional business requirements."", ""Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.""], ""Project description"": [""Develop, modernize and optimize a back-end for a portfolio of online games that are currently being used by 100K+ users. You will work with our data & AI team to make these games safe, secure and fair.""]}"
Stairway Soft,https://jobs.dou.ua/companies/stairwaysoft/,Data Scientist (ML),https://jobs.dou.ua/companies/stairwaysoft/vacancies/137385/, Odesa,03 November 2020,,"Required skills ♦ about 3 years of experience in Data Science and Machine Learning♦ models\algorithms development on Python3♦ you can solve problems End-to-End — including research,planning,data mining, extraction, normalization and ♦ implementation of different algorithms and models♦ experience in production working algorithms. ♦ Independent and result oriented person As a plus ♦ an academic degree in Science\Technology (computer science, Mathematics, engineering) We offer ♦ competitive salary♦ comfortable office right in the city centre♦ 20 days of paid vacation♦ 14 days of paid sick leave♦ corporate events and team buildings♦ perfect working conditions and dynamic, young and friendly team to work with Responsibilities ♦ research and experiment with different machine learning algorithms ♦ data exploration and its quality evaluation♦ proposing new ideas Project description It is full-time position, long-term collaboration.Looking in Odessa city, but if you want you can work remotely. About the project:♦ The project has growed a lot during past year. That is why we need Data Scientist to contribute to the project even more. The project is stable and long-term.♦ We can tell you more during the technical interview ;) Team of the project:♦ Our team is every developer`s dream: We are friendly and motivated; every developer brings his and her own valuable input — thanks to their past experience using various technologies.♦ Our strong team consists of: 1 Senior backend Team Lead, 3 Middle PHP Developers, 1 Senior AQA. If you love working independently — we might be perfect for each other! Interview process:♦ Phone interview with our Recruiter — 15 minutes♦ Technical interview in the office/ remotely for 60 minutes♦ Interview with our client for 30 minutes (in English or Russian)Feedback on every stage — within 3 working days. You can contact Kateryna to apply for position or ask any questions:Skype: e.d.burovaE-mail: kateryna.burova@stairwaysoft.com / hr@stairwaysoft.com",dou,2020-11-12,Data Scientist (ML)@Stairway Soft,,,"{""Required skills"": [""\u2666 about 3 years of experience in Data Science and Machine Learning\u2666 models\\algorithms development on Python3\u2666 you can solve problems End-to-End"", ""including research,planning,data mining, extraction, normalization and \u2666 implementation of different algorithms and models\u2666 experience in production working algorithms. \u2666 Independent and result oriented person""], ""As a plus"": [""\u2666 an academic degree in Science\\Technology (computer science, Mathematics, engineering)""], ""We offer"": [""\u2666 competitive salary\u2666 comfortable office right in the city centre\u2666 20 days of paid vacation\u2666 14 days of paid sick leave\u2666 corporate events and team buildings\u2666 perfect working conditions and dynamic, young and friendly team to work with""], ""Responsibilities"": [""\u2666 research and experiment with different machine learning algorithms \u2666 data exploration and its quality evaluation\u2666 proposing new ideas""], ""Project description"": [""It is full-time position, long-term collaboration.Looking in Odessa city, but if you want you can work remotely.""]}"
Data Virtuality GmbH,https://jobs.dou.ua/companies/data-virtuality-gmbh/,SQL Support Engineer — remote. fulltime. m/f/d,https://jobs.dou.ua/companies/data-virtuality-gmbh/vacancies/137358/, remote,03 November 2020,,"Required skills SMART. SQL-NATIVE. MULTI-TALENTED...what you should bring from your professional side — Understanding of database technology— Experience with Cloud Platforms (AWS, Azure)— Experience with container platforms— Working experience with Linux— Good SQL skills— Ability to debug the source code and to reproduce customer issues on a local environment— Basic understanding of Business intelligence use cases— Minimum 2-3 years experience in software support/consulting or comparable function INNOVATIVE. AMBITIOUS. INTERNATIONAL...what you should bring from your personal side. — Strong communication skills paired with empathy— Fluent in spoken and written English— Self-organized working method— The highest levels of motivation, responsibility, and ambition to proactively support the growth of our company— Analytical and number-based approach— Problem-solving skills— Inquiring mindset As a plus NICE-TO-HAVE— Sales Experience— University Degree We offer BENEFITS:— Payments in EURO— Work from everywhere— Potential relocation to Germany Responsibilities DIVERSIFIED, CHALLENGING, NEVER BORING! TROUBLESHOOTING — You are the primary and secondary technical contact for our customers. You are finding the best solutions for upcoming issues either by only answering small questions or by supporting the operation of our software solutions and server. And you are going one step further: by sharing product feedback you will contribute to our product development andmake it better every day. SAAS MONITORING — You use the monitoring systems and tools to proactively monitor, identify, and process any incidents by communicating to the customer or fixing the problem. SETUP AND OPERATIONS — You have the responsibility to set up, configure, and use cloud management and monitoring tools, as well as managing the cloud environments (backup, etc.). Additionally, you will support the operation of our software and servers and performing software updates and upgrades on Windows and Linux customer machines remote via RDP and SSH. SQL-NATIVE — You have a good understanding of SQL engine operation and are able to write, understand and debug the SQL code of our API connectors, modify code of the connectors on the fly and install updates to the customers. In cases, when heavy SQL reworking is required, you reach out to our SQL department with these tasks.",dou,2020-11-12,SQL Support Engineer — remote. fulltime. m/f/d@Data Virtuality GmbH,,,"{""Required skills"": [""SMART. SQL-NATIVE. MULTI-TALENTED...what you should bring from your professional side""], ""As a plus"": [""Understanding of database technology"", ""Experience with Cloud Platforms (AWS, Azure)"", ""Experience with container platforms"", ""Working experience with Linux"", ""Good SQL skills"", ""Ability to debug the source code and to reproduce customer issues on a local environment"", ""Basic understanding of Business intelligence use cases"", ""Minimum 2-3 years experience in software support/consulting or comparable function""], ""We offer"": [""INNOVATIVE. AMBITIOUS. INTERNATIONAL...what you should bring from your personal side.""], ""Responsibilities"": [""Strong communication skills paired with empathy"", ""Fluent in spoken and written English"", ""Self-organized working method"", ""The highest levels of motivation, responsibility, and ambition to proactively support the growth of our company"", ""Analytical and number-based approach"", ""Problem-solving skills"", ""Inquiring mindset""]}"
Data Virtuality GmbH,https://jobs.dou.ua/companies/data-virtuality-gmbh/,Customer Operations Engineer with SQL expertise — remote. fulltime. m/f/d,https://jobs.dou.ua/companies/data-virtuality-gmbh/vacancies/137356/, remote,03 November 2020,,"Required skills DIVERSIFIED, CHALLENGING, NEVER BORING! TROUBLESHOOTING — You are the primary and secondary technical contact for our customers and answer customer requests in in-product Intercom. You are finding the best solutions for upcoming issues either by only answering small questions or by sup- porting the operation of our software solutions and server. You are providing first level support for EU time zone customers and to US customers outside US working hours. SAAS MONITORING — You use the monitoring systems and tools to proactively monitor, identify and process any incidents by communicating to customer or fixing the problem. SETUP AND OPERATIONS — You have the responsibility to set up, configure and use cloud management / monitoring tools, as well as managing the cloud environments (backup etc.). Additionally you will support the operation of our software and servers and performing software updates and upgrades on Windowsand Linux customer machines remote via RDP and SSH. SMART. SQL-NATIVE. MULTI-TALENTED. ...what you should bring from your professional side Understanding of database technologyExperience with Cloud Platforms (AWS, Azure) & container platformsWorking experience with LinuxGood SQL skillsAbility to debug the source code and to reproduce customer issues on a local environmentMinimum 2-3 years experience in software support/con- sulting or comparable functionBasic understanding of Business intelligence use cases INNOVATIVE. AMBITIOUS. INTERNATIONAL ...what you should bring from your personal side. Strong communication skills paired with empathyFluent in spoken and written EnglishSelf-organized working methodThe highest levels of motivation, responsibility and ambi- tion to proactively support the growth of our companyAnalytical and number-based approach • Problem solving skillsInquiring mindset BENEFITS: Payments in EUROWork from everywherePotential relocation to Germany THAT’S US — DATA VIRTUALITY Our mission is to enable businesses to leverage the full potential of their data by providing a single source of truth platform to connect and manage all data. Our focus is centered on our customers. To us, this is not just a saying but the motto which guides our everyday life! We want our customers to be able to manage their data in as carefree of a manner as possible, as quickly as possible. For this goal, Data Virtuality faithfully delivers first-class service every time. As a plus NICE-TO-HAVE— Sales Experience— University Degree We offer BENEFITS:— Payments in EURO— Work from everywhere— Potential relocation to Germany Responsibilities DIVERSIFIED, CHALLENGING, NEVER BORING! TROUBLESHOOTING — You are the primary and secondary technical contact for our customers. You are finding the best solutions for upcoming issues either by only answering small questions or by supporting the operation of our software solutions and server. And you are going one step further: by sharing product feedback you will contribute to our product development andmake it better every day. SAAS MONITORING — You use the monitoring systems and tools to proactively monitor, identify, and process any incidents by communicating to the customer or fixing the problem. SETUP AND OPERATIONS — You have the responsibility to set up, configure, and use cloud management and monitoring tools, as well as managing the cloud environments (backup, etc.). Additionally, you will support the operation of our software and servers and performing software updates and upgrades on Windows and Linux customer machines remote via RDP and SSH. SQL-NATIVE — You have a good understanding of SQL engine operation and are able to write, understand and debug the SQL code of our API connectors, modify code of the connectors on the fly and install updates to the customers. In cases, when heavy SQL reworking is required, you reach out to our SQL department with these tasks.",dou,2020-11-12,Customer Operations Engineer with SQL expertise — remote. fulltime. m/f/d@Data Virtuality GmbH,,,"{""Required skills"": [""DIVERSIFIED, CHALLENGING, NEVER BORING!""], ""As a plus"": [""TROUBLESHOOTING"", ""You are the primary and secondary technical contact for our customers and answer customer requests in in-product Intercom. You are finding the best solutions for upcoming issues either by only answering small questions or by sup- porting the operation of our software solutions and server. You are providing first level support for EU time zone customers and to US customers outside US working hours.""], ""We offer"": [""SAAS MONITORING"", ""You use the monitoring systems and tools to proactively monitor, identify and process any incidents by communicating to customer or fixing the problem.""], ""Responsibilities"": [""SETUP AND OPERATIONS"", ""You have the responsibility to set up, configure and use cloud management / monitoring tools, as well as managing the cloud environments (backup etc.). Additionally you will support the operation of our software and servers and performing software updates and upgrades on Windowsand Linux customer machines remote via RDP and SSH.""]}"
"Svitla Systems, Inc.",https://jobs.dou.ua/companies/svitla-systems-inc/,Senior Data Engineer,https://jobs.dou.ua/companies/svitla-systems-inc/vacancies/137339/," Kyiv, Kharkiv, Lviv, remote",03 November 2020,,"Required skills 5+ years of experience as a Data Engineer;Bachelor’s degree in Computer Science, Statistics, Informatics, Information Systems, or another quantitative field;Experience with data tools: Spark/Scala; Advanced working SQL knowledge and experience working with relational and non-relational databases, query authoring (SQL) as well as working familiarity with a variety of databases, including Snowflake and MariaDB AX;Experience with AWS cloud services: EC2, EMR, RDS, and Redshift;Experience with stream-processing systems: Storm, Spark-Streaming, etc;Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc;Experience building and optimizing data pipelines, architectures, and data sets;Experience in supporting build processes of data transformation, data structures, metadata, dependencies, and workload management;Strong analytics skills related to working with unstructured datasets;A successful history of manipulating, processing, and extracting value from large disconnected datasets;Experience performing root cause analysis on internal and external data, processing answer specific business questions, and identify opportunities for improvement;Working knowledge of message queuing, stream processing, and highly scalable data stores;Strong communication project management and organizational skills;Experience supporting and working with cross-functional teams in a dynamic environment. We offer Competitive compensation plan that takes skills and experience into consideration.Annual performance appraisals.Possibility to choose your workspace either remote or combination of your home and one of our development offices.Flexible working hours and adjustable work/life balance. Projects that use advanced, cutting-edge technologies.Vacation time, sick-leaves, national holidays, family supplementary days off.Comprehensive medical insurance including dental services, massages, and sports activities.Support for a healthy lifestyle, compensation of running events.Maternity leave policy.A personal loan budget is available for long-term personnel.Partial compensation of conferences, courses, and English classes.Free meetups, webinars, and conferences organized by Svitla.Birthday presents for personnel and New Year gifts for children.Fun summer and winter corporate parties and memorable anniversary presents. Responsibilities Create and maintain optimal data pipeline architecture;Assemble large, complex data sets that meet functional/non-functional business requirements;Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc;Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and ‘big data’ technologies;Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics;Work with stakeholders including the Executive, Product, Development, Data, and Design teams to assist with data-related technical issues and support their data infrastructure needs;Keep SpendHQ data separate and secure across boundaries through different regions;Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader;Work with data and analytics experts to strive for greater functionality in our data systems. Project description Our client develops a spend analysis solution that provides rapid, accurate, and detailed visibility into enterprise spend data. This full-service SaaS (Software-as-a-Service) spend analytics solution delivers actionable insight for sourcing and procurement professionals.",dou,2020-11-12,"Senior Data Engineer@Svitla Systems, Inc.",,,"{""Required skills"": [""5+ years of experience as a Data Engineer;Bachelor\u2019s degree in Computer Science, Statistics, Informatics, Information Systems, or another quantitative field;Experience with data tools: Spark/Scala; Advanced working SQL knowledge and experience working with relational and non-relational databases, query authoring (SQL) as well as working familiarity with a variety of databases, including Snowflake and MariaDB AX;Experience with AWS cloud services: EC2, EMR, RDS, and Redshift;Experience with stream-processing systems: Storm, Spark-Streaming, etc;Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc;Experience building and optimizing data pipelines, architectures, and data sets;Experience in supporting build processes of data transformation, data structures, metadata, dependencies, and workload management;Strong analytics skills related to working with unstructured datasets;A successful history of manipulating, processing, and extracting value from large disconnected datasets;Experience performing root cause analysis on internal and external data, processing answer specific business questions, and identify opportunities for improvement;Working knowledge of message queuing, stream processing, and highly scalable data stores;Strong communication project management and organizational skills;Experience supporting and working with cross-functional teams in a dynamic environment.""], ""We offer"": [""Competitive compensation plan that takes skills and experience into consideration.Annual performance appraisals.Possibility to choose your workspace either remote or combination of your home and one of our development offices.Flexible working hours and adjustable work/life balance. Projects that use advanced, cutting-edge technologies.Vacation time, sick-leaves, national holidays, family supplementary days off.Comprehensive medical insurance including dental services, massages, and sports activities.Support for a healthy lifestyle, compensation of running events.Maternity leave policy.A personal loan budget is available for long-term personnel.Partial compensation of conferences, courses, and English classes.Free meetups, webinars, and conferences organized by Svitla.Birthday presents for personnel and New Year gifts for children.Fun summer and winter corporate parties and memorable anniversary presents.""], ""Responsibilities"": [""Create and maintain optimal data pipeline architecture;Assemble large, complex data sets that meet functional/non-functional business requirements;Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc;Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and \u2018big data\u2019 technologies;Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics;Work with stakeholders including the Executive, Product, Development, Data, and Design teams to assist with data-related technical issues and support their data infrastructure needs;Keep SpendHQ data separate and secure across boundaries through different regions;Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader;Work with data and analytics experts to strive for greater functionality in our data systems.""], ""Project description"": [""Our client develops a spend analysis solution that provides rapid, accurate, and detailed visibility into enterprise spend data. This full-service SaaS (Software-as-a-Service) spend analytics solution delivers actionable insight for sourcing and procurement professionals.""]}"
"Svitla Systems, Inc.",https://jobs.dou.ua/companies/svitla-systems-inc/,Senior Data & Automation Developer,https://jobs.dou.ua/companies/svitla-systems-inc/vacancies/137337/," Kyiv, Kharkiv, Lviv, remote",03 November 2020,,"Required skills Excellent knowledge of Python and Python ETL;Medium/Advanced knowledge in PostgreSQL;Ability to write queries with aggregations, joins, window functions;Ability to write queries with transformations such as string functions, date functions using regex;Work with functions/procedures;Ability to understand EXPLAIN queries to calculate weightage of queries and improve performance;Upper intermediate English is a must; As a plus Proven experience with Git;Experience working with AWS Aurora Postgres and Redshift. We offer Competitive compensation plan that takes skills and experience into consideration.Annual performance appraisals.Possibility to choose your workspace either remote or combination of your home and one of our development offices. Flexible working hours and adjustable work/life balance.Projects that use advanced, cutting-edge technologies.Competitive bonuses for a personal recommendation of new employees.Vacation time, sick-leaves, national holidays, family supplementary days off.Comprehensive medical insurance including dental services, massages, and sports activities.Support for a healthy lifestyle, compensation of running events.Maternity leave policy.A personal loan budget is available for long-term personnel.Partial compensation of conferences, courses, and English classes.Free meetups, webinars, and conferences organized by Svitla.Birthday presents for personnel and New Year gifts for children.Fun summer and winter corporate parties and memorable anniversary presents. Responsibilities To build and support Python based backend service;To write queries and work with functions/procedures;To write the test automation scripts in Python (E2E, integration and load tests);To debug the application. Project description Our client is a leading healthcare data activation platform company focused on delivering more efficient and effective healthcare through the use of pioneering analytics and transparent, clean, and accurate data. Their aim is to simplify complex data from all points of care, streamline the information, and help organizations make powerful decisions and realize strategic goals based on key insights and predictions from their data. Their products have been deployed across more than 500 locations with over 10,000 providers leveraging it at institutions, governmental organizations, and several corporate enterprises such as Mercy ACO, StratiFi Health, Catalyst Health Network, Osler Health Network, and PHIX HIE. Our client is based in San Francisco with offices around the United States and Asia.",dou,2020-11-12,"Senior Data & Automation Developer@Svitla Systems, Inc.",,,"{""Required skills"": [""Excellent knowledge of Python and Python ETL;Medium/Advanced knowledge in PostgreSQL;Ability to write queries with aggregations, joins, window functions;Ability to write queries with transformations such as string functions, date functions using regex;Work with functions/procedures;Ability to understand EXPLAIN queries to calculate weightage of queries and improve performance;Upper intermediate English is a must;""], ""As a plus"": [""Proven experience with Git;Experience working with AWS Aurora Postgres and Redshift.""], ""We offer"": [""Competitive compensation plan that takes skills and experience into consideration.Annual performance appraisals.Possibility to choose your workspace either remote or combination of your home and one of our development offices. Flexible working hours and adjustable work/life balance.Projects that use advanced, cutting-edge technologies.Competitive bonuses for a personal recommendation of new employees.Vacation time, sick-leaves, national holidays, family supplementary days off.Comprehensive medical insurance including dental services, massages, and sports activities.Support for a healthy lifestyle, compensation of running events.Maternity leave policy.A personal loan budget is available for long-term personnel.Partial compensation of conferences, courses, and English classes.Free meetups, webinars, and conferences organized by Svitla.Birthday presents for personnel and New Year gifts for children.Fun summer and winter corporate parties and memorable anniversary presents.""], ""Responsibilities"": [""To build and support Python based backend service;To write queries and work with functions/procedures;To write the test automation scripts in Python (E2E, integration and load tests);To debug the application.""], ""Project description"": [""Our client is a leading healthcare data activation platform company focused on delivering more efficient and effective healthcare through the use of pioneering analytics and transparent, clean, and accurate data. Their aim is to simplify complex data from all points of care, streamline the information, and help organizations make powerful decisions and realize strategic goals based on key insights and predictions from their data. Their products have been deployed across more than 500 locations with over 10,000 providers leveraging it at institutions, governmental organizations, and several corporate enterprises such as Mercy ACO, StratiFi Health, Catalyst Health Network, Osler Health Network, and PHIX HIE. Our client is based in San Francisco with offices around the United States and Asia.""]}"
Data Science UA,https://jobs.dou.ua/companies/data-science-ua/,"Senior Data Scientist, Team Leader",https://jobs.dou.ua/companies/data-science-ua/vacancies/137334/," Kyiv, remote",03 November 2020,,"Required skills — 5+ years of experience as a data scientist focused on working with financial data and risk modeling— Good understanding of the machine learning algorithms and their real-world advantages/drawbacks.— Experience in deploying models to the production— Good knowledge of SQL— Advanced knowledge of Python’s data manipulation libraries (pandas, numpy, scipy etc.)— Experience with deep learning frameworks (tensorflow, pytorch)— Experience with time-series— At least Upper Intermediate level of English Responsibilities — Planning and scoping efforts to deliver on the Machine learning-driven product requirements— Selecting and building features to be used in the models— Mining the data from the main database— Augmenting available data with the data from other sources— Building software collaboration with the engineering team to exchange the data— Identifying and building appropriate models— After February, managing the team of data scientist and machine learning engineers— Managing collaboration with the Product and Engineering teams Project description Our patner is ambuilding a cutting-edge accounts receivable management software that uses automation and machine learning to help finance teams become revenue heroes.They’re a rapidly growing company at the forefront of back-office automation, AI, and machine learning and also one of the leaders of the industry in terms of user experience and consumerization of enterprise software.",dou,2020-11-12,"Senior Data Scientist, Team Leader@Data Science UA",,,"{""Required skills"": [""5+ years of experience as a data scientist focused on working with financial data and risk modeling"", ""Good understanding of the machine learning algorithms and their real-world advantages/drawbacks."", ""Experience in deploying models to the production"", ""Good knowledge of SQL"", ""Advanced knowledge of Python\u2019s data manipulation libraries (pandas, numpy, scipy etc.)"", ""Experience with deep learning frameworks (tensorflow, pytorch)"", ""Experience with time-series"", ""At least Upper Intermediate level of English""], ""Responsibilities"": [""Planning and scoping efforts to deliver on the Machine learning-driven product requirements"", ""Selecting and building features to be used in the models"", ""Mining the data from the main database"", ""Augmenting available data with the data from other sources"", ""Building software collaboration with the engineering team to exchange the data"", ""Identifying and building appropriate models"", ""After February, managing the team of data scientist and machine learning engineers"", ""Managing collaboration with the Product and Engineering teams""], ""Project description"": [""Our patner is ambuilding a cutting-edge accounts receivable management software that uses automation and machine learning to help finance teams become revenue heroes.They\u2019re a rapidly growing company at the forefront of back-office automation, AI, and machine learning and also one of the leaders of the industry in terms of user experience and consumerization of enterprise software.""]}"
eTeam,https://jobs.dou.ua/companies/eteam/,Senior Data Engineer for Xendit,https://jobs.dou.ua/companies/eteam/vacancies/133785/," Kyiv, remote",03 November 2020,,"Required skills — Experience in at least two of the following: Go, Python, Node.js (Typescript, Javascript)— Experience in SQL and NoSQL databases (Postgres, MongoDB)— Experience in AWS EKS / Kubernetes, AWS Elastic Beanstalk, AWS EC2, and Docker as infrastructure platform technology— Experience in implementing unit, integration, end-to-end, and API tests— Experience in refactoring— Experience in CI/CD technology— Experience in analyzing large datasets (SQL, Spark, Python, R, etc.)— Familiarity with Scrum as a development methodology— Have a great sense of end-to-end ownership of the products and tools you build— Upper-intermediate written and spoken English. We offer — Grow professionally with subsidized certifications, courses, and conferences— Improve your English with conversation clubs and direct client communication— Enjoy our extra perks: unlimited vacation, flexible schedule, bonuses for important life events, fresh food & snacks in the office— Let loose with fun parties, BBQs, online activities and off-sites. Responsibilities — Maintain and improve our microservices that support our fraud detection systems to ensure high reliability and security— Building new data features that can be used in real-time to support our machine learning algorithms as well as fraud rules— Improving our fraud detection system to be able to process complex fraud rules involving different parameters which will provide merchants more flexibility in tuning their fraud detection performance— Collaborating with tech lead, project manager, and engineers to deliver feature based on considerations around system performance, availability, reliability, and scalability Project description On behalf of Xendit, eTeam is looking for a Senior Data Engineer for our team in Kyiv on a full-time basis. Xendit is an Indonesian fintech company that processes payments, runs marketplaces, disburses payroll and loans, detects fraud and helps other businesses grow exponentially. It serves these companies by providing a suite of world-class APIs, eCommerce platform integrations, and easy to use applications for individual entrepreneurs, SMBs, and enterprises alike. Xendit’s mission is to make payments simple, secure and easy for everyone. It’s one of the fastest-growing companies in South East Asia that processes millions of transactions monthly, growing 8% month on month for the last 4 years. They are trusted and backed by some of the largest VCs in the world, who invested in Facebook, Slack, Twitch and Grab, and are alumni of the prestigious YCombinator (S15).",dou,2020-11-12,Senior Data Engineer for Xendit@eTeam,,,"{""Required skills"": [""Experience in at least two of the following: Go, Python, Node.js (Typescript, Javascript)"", ""Experience in SQL and NoSQL databases (Postgres, MongoDB)"", ""Experience in AWS EKS / Kubernetes, AWS Elastic Beanstalk, AWS EC2, and Docker as infrastructure platform technology"", ""Experience in implementing unit, integration, end-to-end, and API tests"", ""Experience in refactoring"", ""Experience in CI/CD technology"", ""Experience in analyzing large datasets (SQL, Spark, Python, R, etc.)"", ""Familiarity with Scrum as a development methodology"", ""Have a great sense of end-to-end ownership of the products and tools you build"", ""Upper-intermediate written and spoken English.""], ""We offer"": [""Grow professionally with subsidized certifications, courses, and conferences"", ""Improve your English with conversation clubs and direct client communication"", ""Enjoy our extra perks: unlimited vacation, flexible schedule, bonuses for important life events, fresh food & snacks in the office"", ""Let loose with fun parties, BBQs, online activities and off-sites.""], ""Responsibilities"": [""Maintain and improve our microservices that support our fraud detection systems to ensure high reliability and security"", ""Building new data features that can be used in real-time to support our machine learning algorithms as well as fraud rules"", ""Improving our fraud detection system to be able to process complex fraud rules involving different parameters which will provide merchants more flexibility in tuning their fraud detection performance"", ""Collaborating with tech lead, project manager, and engineers to deliver feature based on considerations around system performance, availability, reliability, and scalability""], ""Project description"": [""On behalf of Xendit, eTeam is looking for a Senior Data Engineer for our team in Kyiv on a full-time basis.""]}"
KitRUM,https://jobs.dou.ua/companies/kitrum/,Senior Big Data Engineer (Scala/Spark),https://jobs.dou.ua/companies/kitrum/vacancies/124906/," Kyiv, Kharkiv, Lviv, Dnipro, Uzhhorod, remote",03 November 2020,,"Required skills — 5+ years of professional experience— Solid experience with Scala and Spark. — Experience with AWS— Great coding skills and software development experience — Level of English: Upper-Intermediate— Watched all seasons of “Rick and Morty” We offer — High compensation according to your technical skills— Long-term projects (12m+) with great Customers— 5-day working week, 8-hour working day, flexible schedule— Democratic management style & friendly environment— WFH option (Possibility to work from home)— Annual Paid vacation — 15 b/days + unpaid vacation— Paid sick leaves — 6 b/days per year— Ukrainian official holidays— Corporate Perks (external training, English courses, corporate events/team buildings)— Cozy office in the center of the city— Coffee, cookies and other goodies— Professional and personal growth Responsibilities — Perform tasks related to data migration— Work under a close supervision of Lead Big Data Engineer and help other engineers to execute the compute migration. Project description Client is an American e-book and audiobook subscription service that includes one million titles. Platform hosts 60 million documents on its open publishing platform.The platform allows: — anyone to share his/her ideas with the world;— access to audio books;— access to world’s composers who publish their music;— incorporates articles from private publishers and world magazines;— allows access to exclusive content. Сompany description KitRUM is a one-stop custom software development company headquartered in sunny Florida with development centers in Ukraine and Poland.With a pool of 300+ top-notch engineering resources, we help CxOs of VC-backed startups and fast-growing tech companies in the US, EU and Australia to custom build software engineering teams packed with top-tier talent. Why us? We realize that one of the most crucial things for developers — adequate client and fascinating projects. So we qualify our clients to make sure that they:— have an idea that they believe will make the world a better place;— think long-term and looking for a trusted-tech partner;— want to rely on and avoid micromanaging;— are not f$%$ing jackasses :-)Follow our team on Instagram to know more about our daily life :)We adore making new friends on the board!",dou,2020-11-12,Senior Big Data Engineer (Scala/Spark)@KitRUM,,,"{""Required skills"": [""5+ years of professional experience"", ""Solid experience with Scala and Spark."", ""Experience with AWS"", ""Great coding skills and software development experience"", ""Level of English: Upper-Intermediate"", ""Watched all seasons of \u201cRick and Morty\u201d""], ""We offer"": [""High compensation according to your technical skills"", ""Long-term projects (12m+) with great Customers"", ""5-day working week, 8-hour working day, flexible schedule"", ""Democratic management style & friendly environment"", ""WFH option (Possibility to work from home)"", ""Annual Paid vacation"", ""15 b/days + unpaid vacation"", ""Paid sick leaves"", ""6 b/days per year"", ""Ukrainian official holidays"", ""Corporate Perks (external training, English courses, corporate events/team buildings)"", ""Cozy office in the center of the city"", ""Coffee, cookies and other goodies"", ""Professional and personal growth""], ""Responsibilities"": [""Perform tasks related to data migration"", ""Work under a close supervision of Lead Big Data Engineer and help other engineers to execute the compute migration.""], ""Project description"": [""Client is an American e-book and audiobook subscription service that includes one million titles. Platform hosts 60 million documents on its open publishing platform.The platform allows:"", ""anyone to share his/her ideas with the world;"", ""access to audio books;"", ""access to world\u2019s composers who publish their music;"", ""incorporates articles from private publishers and world magazines;"", ""allows access to exclusive content.""]}"
EPAM,https://jobs.dou.ua/companies/epam-systems/,Senior Software Engineer (Big Data) [Lviv],https://jobs.dou.ua/companies/epam-systems/vacancies/137258/, Lviv,02 November 2020,,"Required skills • Commercial experience in Big Data 2+ years• Overall experience in IT area — 3+ years• Java/Scala/Python — advanced level• AWS (EMR, Lambda, S3) — 1+ years• Spark (core, sql) — 1+ years• Hands on Hive, Kafka — 1+ years• Linux/Unix OS — 2+ years• English level is upper intermediate We offer • Competitive compensation depending on experience and skills• Individual career path• Social package — medical insurance, sports• Social package — medical insurance, sports• Sick leave and regular vacation• Unlimited access to Linkedin learning solutions• English classes with certified English teachers• Flexible work schedule Project description Striving for excellence is in our DNA. Since 1993, we have been helping the world’s leading companies imagine, design, engineer, and deliver software and digital experiences that change the world. We are more than just specialists, we are experts. EPAM is committed to providing our global team of 36,700+ EPAMers with inspiring careers from day one. EPAMers lead with passion and honesty and think creatively. Our people are the source of our success and we value collaboration, try to always understand our customers’ business, and strive for the highest standards of excellence. In today’s new market conditions, we continue to support operations for hundreds of clients around the world remotely, with the vast majority of our teams working from home. No matter where you are located, you’ll join a dedicated, diverse community that will help you discover your fullest potential. Our customer is a Swiss-based global company that produces agrochemicals and seeds. As a biotechnology company, it conducts genomic research. It was formed in 2000. As of 2009 it ranked third in seeds and biotechnology sales. Sales in 2015 were approximately US$13.4 billion, over half of which were in emerging markets.",dou,2020-11-12,Senior Software Engineer (Big Data) [Lviv]@EPAM,,,"{""Required skills"": [""Commercial experience in Big Data 2+ years"", ""Overall experience in IT area"", ""3+ years"", ""Java/Scala/Python"", ""advanced level"", ""AWS (EMR, Lambda, S3)"", ""1+ years"", ""Spark (core, sql)"", ""1+ years"", ""Hands on Hive, Kafka"", ""1+ years"", ""Linux/Unix OS"", ""2+ years"", ""English level is upper intermediate""], ""We offer"": [""Competitive compensation depending on experience and skills"", ""Individual career path"", ""Social package"", ""medical insurance, sports"", ""Social package"", ""medical insurance, sports"", ""Sick leave and regular vacation"", ""Unlimited access to Linkedin learning solutions"", ""English classes with certified English teachers"", ""Flexible work schedule""], ""Project description"": [""Striving for excellence is in our DNA. Since 1993, we have been helping the world\u2019s leading companies imagine, design, engineer, and deliver software and digital experiences that change the world. We are more than just specialists, we are experts.""]}"
Squro,https://jobs.dou.ua/companies/squro/,BI Data Analyst,https://jobs.dou.ua/companies/squro/vacancies/122908/, Kyiv,02 November 2020,,"Required skills — 2+ years of experience with BI analytical tools (Tableau / Qlik / Power BI, etc.);— Strong knowledge of SQL;— Strong knowledge of MS Excel;— Strong analytical and data visualization skills;— Knowledge of approaches to data analysis and visualization;— Understanding of the principles of building data warehouses;— Ability to work with large amounts of data;— English — Intermediate+ (on the level of reading and writing technical documentation). As a plus — Mathematical statistics;— Python / R / Scala / Java;— Building machine learning models;— Linux systems;— Spark, Kafka;— Non-relational databases. We offer — Interesting and challenging tasks with our own product;— Friendly working environment;— Flexible schedule;— Possibility of personal and professional growth;— Compensation of lunch, gym and language courses, and professional conferences;— Cozy office in the city center (1 minute from Pecherskaya metro station);— Regular corporate events and other company benefits;— Competitive salary. Responsibilities — Participation in the implementation of BI systems;— Working with analytical systems;— Building dashboards;— Setting up a monitoring / data analysis system;— Making predictions with ML;— Enabling data-driven business decisions;— Definition of anomalies;— Drawing up requirements for engineers;— Formation of documentation. Project description We are an international IT FinTech product company in the B2C segment. We provide services to our clients worldwide, including countries of South-East Asia, Latin America, Europe, and Africa. We are currently looking for a BI Data Analyst due to the team expansion.",dou,2020-11-12,BI Data Analyst@Squro,,,"{""Required skills"": [""2+ years of experience with BI analytical tools (Tableau / Qlik / Power BI, etc.);"", ""Strong knowledge of SQL;"", ""Strong knowledge of MS Excel;"", ""Strong analytical and data visualization skills;"", ""Knowledge of approaches to data analysis and visualization;"", ""Understanding of the principles of building data warehouses;"", ""Ability to work with large amounts of data;"", ""English"", ""Intermediate+ (on the level of reading and writing technical documentation).""], ""As a plus"": [""Mathematical statistics;"", ""Python / R / Scala / Java;"", ""Building machine learning models;"", ""Linux systems;"", ""Spark, Kafka;"", ""Non-relational databases.""], ""We offer"": [""Interesting and challenging tasks with our own product;"", ""Friendly working environment;"", ""Flexible schedule;"", ""Possibility of personal and professional growth;"", ""Compensation of lunch, gym and language courses, and professional conferences;"", ""Cozy office in the city center (1 minute from Pecherskaya metro station);"", ""Regular corporate events and other company benefits;"", ""Competitive salary.""], ""Responsibilities"": [""Participation in the implementation of BI systems;"", ""Working with analytical systems;"", ""Building dashboards;"", ""Setting up a monitoring / data analysis system;"", ""Making predictions with ML;"", ""Enabling data-driven business decisions;"", ""Definition of anomalies;"", ""Drawing up requirements for engineers;"", ""Formation of documentation.""], ""Project description"": [""We are an international IT FinTech product company in the B2C segment. We provide services to our clients worldwide, including countries of South-East Asia, Latin America, Europe, and Africa. We are currently looking for a BI Data Analyst due to the team expansion.""]}"
Snap Inc.,https://jobs.dou.ua/companies/snap/,"Machine Learning, CV",https://jobs.dou.ua/companies/snap/vacancies/127214/, Kyiv,02 November 2020,,"Snap Inc. is a camera company. We believe that reinventing the camera represents our greatest opportunity to improve the way people live and communicate. Our products empower people to express themselves, live in the moment, learn about the world, and have fun together.Snapchat is the camera used by millions of people every day to Snap with family, watch Stories from friends, see events from around the world, and explore expertly curated content from top publishers. In short, we are a passionate team working hard to build the best platform in the world for communication and storytelling. We’re looking for a Software Engineer, Machine Learning to join Snap Inc! Working from Kyiv office, you will collaborate with researchers, engineers, and designers and develop machine learning frameworks to create exciting products and breakthrough interactive experiences for millions of Snapchatters around the world. What you’ll do:— Develop and deploy production-quality machine learning frameworks for mobile devices— Explore and implement challenging state-of-the-art algorithms to move the needle in the machine learning area— Evaluate the technical tradeoffs of every decision— Perform code reviews and ensure exceptional code quality— Work closely with other Snap teams to explore and prototype new product features— Iterate quickly without compromising quality Minimum qualifications:— 3+ years of software engineering experience— Bachelor’s degree in a technical field such as computer science or equivalent experience Preferred qualifications:— Experience working with machine learning frameworks such as PyTorch, TensorFlow, Caffe2 or related frameworks— Experience developing and optimizing real-time software for mobile applications— M.S. degree in computer science or related field— Strong understanding of machine learning approaches and algorithms— Expertise in any of the following fields: segmentation, object detection, classification, deep learning model optimizations (e.g. pruning, quantization, distillation)— Ability to proactively learn new concepts and apply them at work",dou,2020-11-12,"Machine Learning, CV@Snap Inc.",,,{}
Weigandt Consulting,https://jobs.dou.ua/companies/weigandt-consulting/,"Big data engineer (Python, Hadoop)",https://jobs.dou.ua/companies/weigandt-consulting/vacancies/137173/," Saint Petersburg (Russia), remote",02 November 2020,,"Required skills — Three or more years of experience working in the R&D or Data Engineering field;— Good knowledge of data discovery, modelling, retrieval, and profiling;— Practical experience with Hadoop and Big Data technologies ( Spark / PySpark), Spark library;— Proficiency with core Python tools for data manipulation, analysis, visualization, machine learning;— Critical thinking and problem-solving skills;— Ability to translate a business objective to a data science problem, then drill down to well-defined tasks;— Experience working as a team member, general knowledge of collaboration and version control tools;— Upper-intermediate English. As a plus — General knowledge of systems architecture, data governance, and data management;— Experience deploying data science solutions to production systems;— Passion for learning and innovating new methodologies in the intersection of applied ML/Engineering fields. We offer — Excellent compensation (to be discussed on the results of the interviews);— Working with an international team on exciting and challenging projects;— Regular business trips to Europe;— Remote work or Relocation to St. Petersburg Responsibilities — Design and delivery Big Data solutions for our clients;— Leveraging data to solve strategic, tactical, structured and unstructured business problems;— Developing, learning, testing and validating predictive models;— Supporting go-to-client professionals during client discussions with technical expertise. Project description The project aim is the development (CI/CD) of the intelligent automated markdown tool. You will have a chance to take part in optimization of elasticity curves for one of the top ten world retail groups, implement price-reducing logic to different pricing strategies, experiment with the real big data on Hadoop stack, improve current code, and data engineering processes collaborating with a high-level professional team.",dou,2020-11-12,"Big data engineer (Python, Hadoop)@Weigandt Consulting",,,"{""Required skills"": [""Three or more years of experience working in the R&D or Data Engineering field;"", ""Good knowledge of data discovery, modelling, retrieval, and profiling;"", ""Practical experience with Hadoop and Big Data technologies ( Spark / PySpark), Spark library;"", ""Proficiency with core Python tools for data manipulation, analysis, visualization, machine learning;"", ""Critical thinking and problem-solving skills;"", ""Ability to translate a business objective to a data science problem, then drill down to well-defined tasks;"", ""Experience working as a team member, general knowledge of collaboration and version control tools;"", ""Upper-intermediate English.""], ""As a plus"": [""General knowledge of systems architecture, data governance, and data management;"", ""Experience deploying data science solutions to production systems;"", ""Passion for learning and innovating new methodologies in the intersection of applied ML/Engineering fields.""], ""We offer"": [""Excellent compensation (to be discussed on the results of the interviews);"", ""Working with an international team on exciting and challenging projects;"", ""Regular business trips to Europe;"", ""Remote work or Relocation to St. Petersburg""], ""Responsibilities"": [""Design and delivery Big Data solutions for our clients;"", ""Leveraging data to solve strategic, tactical, structured and unstructured business problems;"", ""Developing, learning, testing and validating predictive models;"", ""Supporting go-to-client professionals during client discussions with technical expertise.""], ""Project description"": [""The project aim is the development (CI/CD) of the intelligent automated markdown tool. You will have a chance to take part in optimization of elasticity curves for one of the top ten world retail groups, implement price-reducing logic to different pricing strategies, experiment with the real big data on Hadoop stack, improve current code, and data engineering processes collaborating with a high-level professional team.""]}"
Genesis,https://jobs.dou.ua/companies/genesis-technology-partners/,Middle Data Analyst,https://jobs.dou.ua/companies/genesis-technology-partners/vacancies/123666/, Kyiv,02 November 2020,,"Genesis is one of the largest IT companies in Ukraine with more than 1500 people in 9 countries, who create products for 200 million users monthly. We are the most high-loaded company in the country and one of the largest partner of Facebook, Google, Snapchat and Apple in the CEE region. Our team is one of the best high-tech teams in Eastern Europe. Genesis was recognised by DOU.UA as the Best IT Employer in Ukraine in the category 800 — 1500 employees over the last years. We received very high ratings across all major evaluation categories such as professional growth, compensation, working conditions, communication with management and colleagues, etc. So, what You’ll Be Doing:• Metrics’ monitoring work:• daily all metrics’ check;• finding the causes of metrics’ changes.• Monitoring the integrity and quality of data, which is written to analytics systems.• Customer’s management, ideas for growth:• monitoring of performance indicators;• building reports for clients;• ideas for A / B testing. What We’ll Expect from You:• You have more than a 2 years of working experience in the product company as an analyst;• You are able to quickly analyze data and make decisions (with a deadline);• Logical thinking + mathematics’ skills;• High level of SQL;• Experience with BI systems (Tableau will be an advantage). Genesis is a unique place for the development and growth with:• Expertise in the development of high-loaded products in international markets;• Unique opportunities for learning including various training and seminars within the company, access to a valuable and extensive book library, English courses and participation in key IT industry events worldwide;• Perfect working conditions: an excellent office in a 5 minutes’ walk from Taras Shevchenko metro station, free food and drinks in the office, medical insurance. The company has professional coaches in football, basketball and running and supports various sports initiatives (swim races, marathons, etc).",dou,2020-11-12,Middle Data Analyst@Genesis,,,{}
Genesis,https://jobs.dou.ua/companies/genesis-technology-partners/,Middle/Senior Data Analyst,https://jobs.dou.ua/companies/genesis-technology-partners/vacancies/121242/, Kyiv,02 November 2020,,"Genesis is one of the largest IT companies in Ukraine with more than 1500 people in 9 countries, who create products for 200 million users monthly. We are the most high-loaded company in the country and one of the largest partner of Facebook, Google, Snapchat and Apple in the CEE region. Our team is one of the best high-tech teams in Eastern Europe. Genesis was recognised by DOU.UA as the Best IT Employer in Ukraine in the category 800 — 1500 employees over the last years. We received very high ratings across all major evaluation categories such as professional growth, compensation, working conditions, communication with management and colleagues, etc. What We’ll Expect From You:• 1-3 years of experience doing quantitative product/marketing analysis;• 1-3 years of initiating and driving projects to completion with minimal guidance;• Highly skilled in data visualization tools such as Tableau;• Experience using product analytics tools like Grafana, Google Analytics;• Proficiency in using SQL;• Proficiency in using Python. Who You Are:• You have a passion for creating and supporting new great products;• You are highly motivated and hard-working as well as curious and creative at problem-solving;• You have strong verbal and written communication skills;• You thrive on collaboration, working side by side with people of all backgrounds and disciplines;• Analytical mindset; ability to structure information and dive deeper when it is needed. What You’ll Do:• Passion for digital advertising;• Be organised and self-motivated;• Strong analytical skills and ability to make data-driven decisions;• Entrepreneurial Mindset;• English — Upper Intermediate or higher. Genesis is a unique place for the development and growth with:• Expertise in the development of high-loaded products in international markets;• Unique opportunities for learning including various training and seminars within the company, access to a valuable and extensive book library, English courses and participation in key IT industry events worldwide;• Perfect working conditions: an excellent office in a 5 minutes’ walk from Taras Shevchenko metro station, free food and drinks in the office, medical insurance. The company has professional coaches in football, basketball and running and supports various sports initiatives (swim races, marathons, etc).",dou,2020-11-12,Middle/Senior Data Analyst@Genesis,,,{}
SoftServe,https://jobs.dou.ua/companies/softserve/,"Big Data Architect (Azure), ID 57772",https://jobs.dou.ua/companies/softserve/vacancies/137165/," Kyiv, Kharkiv, Lviv, Dnipro, remote",02 November 2020,,"WE ARE The largest Big Data & AI Center of Excellence in Eastern Europe. Our expertise in Machine Learning, AI, Big Data, IoT, Cloud technologies is recognized by Google and Amazon.We are transforming the way thousands of global organizations do business by creating the most innovative solutions using proven architecture design methodologies, innovation frameworks and experience design approaches. Our group successfully delivered 50+ discovery & consulting projects and 75+ implementation projects in 2019.Together with the SEI, Carnegie Mellon University, we invented Smart Decisions game, in order to promote architecture design methodologies, intended to facilitate the design of complex architectures.Please read more at smartdecisionsgame.com YOU ARE A seasoned Data Architect with hands-on experience in building large-scale Data Platforms and decision-support systems, with a proven record of success in delivering Big Data & Analytics solutions for different businesses, having an excellent understanding of distributed computing, architectural tactics, and patterns; demonstrating strong interpersonal and presentation skills. With extensive experience in • Designing and implementing enterprise-scale solutions in the AWS, GCP, or Azure clouds• Hadoop stack, NoSQL, MPP technologies, processing frameworks, or other Big Data technology areas• One of the following programming languages: Java, Scala, Python, or strong SQL• Building large-scale Data Platforms and decision-support systems, covering at least one of the following areas: data ingestion, consolidation, streaming, batching, Data Lakes, Data Warehousing, or analytics platforms• Designing sustainable architectures, performing trade-off analysis of different architecture tactics and patterns, and applying proven architecture design approaches and methodologies• Driving projects roll-outs from requirements gathering to go-live YOU WANT TO • Design and implement sustainable architectures, create innovative Big Data and AI solutions• Evaluate cutting-edge Big Data technologies, implement PoCs and MVPs, utilize rapid prototyping techniques to accelerate time-to-market for our Clients• Deliver business value, leveraging available data assets, Big Data technologies, ML and AI• Adopt new technologies and methodologies, define and promote best practices on your projects• Demonstrate leadership and motivate project team• Perform trade-off analysis of different architecture tactics and patterns, provide justification of architectural decisions• Manage risk and make decision regarding timelines and deliverables, other commitments associated with project scope TOGETHER WE WILL • Engage with new customers and build relationships with different business stakeholders, work closely with Sales team• Become a trusted advisor and solution provider for different organizations• Implement scalable Data Platforms and decision-support systems• Support your technical and personal growth — we have a dedicated career plan for all roles in our company• Take part in internal and external events where you can build and promote your personal brandYour benefits • Assimilate best practices from experts, working in the team of top-notch Architects• Work with the world-leading companies• Being a part of Center of Excellence, you will have a chance to address different business and technology challenges, drive multiple projects and initiatives• Attend and speak at international events",dou,2020-11-12,"Big Data Architect (Azure), ID 57772@SoftServe",,,{}
Daxx,https://jobs.dou.ua/companies/daxx-group/,Senior Big Data (Java) Developer for AgileLab,https://jobs.dou.ua/companies/daxx-group/vacancies/133166/," Kyiv, Kharkiv, Lviv, Dnipro",02 November 2020,,"Required skills — Excellent knowledge of Java— Expert in JDBC connectors— AWS EMR (Elastic MapReduce)— Understanding of the CI/CD concept— Presto and Drill deep knowledge— Distributed Computing experience— Master Degree (Computer Science or Computer Science Engineering)— Upper-Intermediate English language (written and spoken) As a plus — Experience with Atlassian products We offer — Direct cooperation with the customer— Dedicated HR/ Client Manager— Regular performance reviews— Competitive salary, medical insurance, 20 working vacation days— Regular corporate events, team buildings, etc. Responsibilities Currently, the AgileLab team developing software for a big energetic company. We are looking for a Big Data Developer who will develop a Drill plugin together with two teammates. This plugin will be used to connect to microservices, AWS S3, 3rd party databases, data catalogs and etc. Project description AgileLab is an Italian R&D company focused on the development of Big Data and AI Applications. They operate in different business sectors and work with well-known big companies on the Italian market (e.g. Unicredit, Brembo). AgileLab provides essentially BigData services or platforms, IoT environments, analytical platforms, etc. They are very specialized in BigData and machine learning. AgileLab develops a high- and low-level architecture of the solution, appoint a Team Leader and build teams according to the client’s goals from scratch.",dou,2020-11-12,Senior Big Data (Java) Developer for AgileLab@Daxx,,,"{""Required skills"": [""Excellent knowledge of Java"", ""Expert in JDBC connectors"", ""AWS EMR (Elastic MapReduce)"", ""Understanding of the CI/CD concept"", ""Presto and Drill deep knowledge"", ""Distributed Computing experience"", ""Master Degree (Computer Science or Computer Science Engineering)"", ""Upper-Intermediate English language (written and spoken)""], ""As a plus"": [""Experience with Atlassian products""], ""We offer"": [""Direct cooperation with the customer"", ""Dedicated HR/ Client Manager"", ""Regular performance reviews"", ""Competitive salary, medical insurance, 20 working vacation days"", ""Regular corporate events, team buildings, etc.""], ""Responsibilities"": [""Currently, the AgileLab team developing software for a big energetic company. We are looking for a Big Data Developer who will develop a Drill plugin together with two teammates. This plugin will be used to connect to microservices, AWS S3, 3rd party databases, data catalogs and etc.""], ""Project description"": [""AgileLab is an Italian R&D company focused on the development of Big Data and AI Applications. They operate in different business sectors and work with well-known big companies on the Italian market (e.g. Unicredit, Brembo).""]}"
ELITEX,https://jobs.dou.ua/companies/elitex/,Happy Data Engineer,https://jobs.dou.ua/companies/elitex/vacancies/137161/," Lviv, remote",02 November 2020,,"Required skills — A self-starting, curious data professional with 3+ years of experience— Expert in SQL, adept in Python— Proficient in Google Cloud Platform— Experienced working on relational databases (eg. PostgreSQL, Redshift)— A strong communicator and team player— Familiar with ETL frameworks (eg. Airflow, Stitch) and/or tools (eg. Jitterbit, dbt)— Not afraid to roll up your sleeves and tackle challenging data problems As a plus — Looker (which I’m guessing is why you’re here!)— High-growth startups We offer — Modern working place (27″ iMac or MacBook or other preferred hardware);— Cozy office in the very center of the city (near Opera house);— Flexible working hours;— Possibility of partly remote work;— Regular performance reviews;— 20 working days of paid vacation annually;— Additional vacation days for special lifetime events;— 10 working days of paid sick-leaves;— Medical insurance;— Maternity leave policy;— Free English classes;— Compensation of educational and training programs;— Team buildings and corporate events. Responsibilities — Build and maintain data pipelines between 1st- and 3rd-party products and the data warehouse used for company-wide analytics— Collaborate with all teams across the company to ingest data and apply appropriate business logic— Administer and optimize Bigquery databases, data model— Work on a globally distributed data team in New York, Lagos and Nairobi— Be a member of an agile data team making a large impact on project strategy Project description The client is an engineering-as-a-service business that helps companies build remote teams quickly and cost-effectively. 1,000+ software engineers working as full-time, embedded members of development teams at over 200 leading tech companies. The company is headquartered in New York, has a globally distributed leadership team, and is backed by investors including Spark Capital, Generation Investment Management, Serena Ventures, Chan Zuckerberg Education, and Omidyar Network.",dou,2020-11-12,Happy Data Engineer@ELITEX,,,"{""Required skills"": [""A self-starting, curious data professional with 3+ years of experience"", ""Expert in SQL, adept in Python"", ""Proficient in Google Cloud Platform"", ""Experienced working on relational databases (eg. PostgreSQL, Redshift)"", ""A strong communicator and team player"", ""Familiar with ETL frameworks (eg. Airflow, Stitch) and/or tools (eg. Jitterbit, dbt)"", ""Not afraid to roll up your sleeves and tackle challenging data problems""], ""As a plus"": [""Looker (which I\u2019m guessing is why you\u2019re here!)"", ""High-growth startups""], ""We offer"": [""Modern working place (27\u2033 iMac or MacBook or other preferred hardware);"", ""Cozy office in the very center of the city (near Opera house);"", ""Flexible working hours;"", ""Possibility of partly remote work;"", ""Regular performance reviews;"", ""20 working days of paid vacation annually;"", ""Additional vacation days for special lifetime events;"", ""10 working days of paid sick-leaves;"", ""Medical insurance;"", ""Maternity leave policy;"", ""Free English classes;"", ""Compensation of educational and training programs;"", ""Team buildings and corporate events.""], ""Responsibilities"": [""Build and maintain data pipelines between 1st- and 3rd-party products and the data warehouse used for company-wide analytics"", ""Collaborate with all teams across the company to ingest data and apply appropriate business logic"", ""Administer and optimize Bigquery databases, data model"", ""Work on a globally distributed data team in New York, Lagos and Nairobi"", ""Be a member of an agile data team making a large impact on project strategy""], ""Project description"": [""The client is an engineering-as-a-service business that helps companies build remote teams quickly and cost-effectively. 1,000+ software engineers working as full-time, embedded members of development teams at over 200 leading tech companies. The company is headquartered in New York, has a globally distributed leadership team, and is backed by investors including Spark Capital, Generation Investment Management, Serena Ventures, Chan Zuckerberg Education, and Omidyar Network.""]}"
Tilting Point,https://jobs.dou.ua/companies/tilting-point/,Data Engineer,https://jobs.dou.ua/companies/tilting-point/vacancies/106765/, Kyiv,02 November 2020,,"Required skills ● Minimum bachelor’s in Computer Science● 2+ years of experience as a Data Engineer working on big data analytics● Excellent knowledge of Python and SQL● Good knowledge of Scala and Spark● Good knowledge of Apache Airflow● You are fluent in English (written and oral), additional languages a plus● Experience with integration of data from multiple data sources and APIs● Knowledge of various ETL techniques and frameworks● Ability to collaborate with colleagues across different disciplines/locations As a plus ● Have experience/exposure to mobile User Acquisition and Marketing● Worked in the gaming industry and knowledge game specific data● Comfortable with AWS cloud (S3, EC2, Redshift, EKS(Kubernetes))● Knowledge of existing third party analytics visualization software (Amplitude, Looker, Tableau) We offer ● Startup environment — where each individual makes a large impact● Ability to own technical direction of various products and systems● Work with great people on great games that reach millions of people each month Responsibilities Responsibilities include, but are not limited to the following, on an as-needed basis:● Work with data analysts, game producers, and customer service to help define technology needs for the logging, processing, storage and presentation of data in a manner that delivers business value● Design, implement, maintain tools and reports for User Acquisition and LiveOps teams from start to finish● Work with various internal teams to satisfy analytical needs by building both large systematic reports and small custom pieces● Build data expertise and own data quality for a variety of products Project description Tilting Point is looking for a Data Engineer to join our team in Kyiv. As part of the team, you will work on collecting, storing, processing, and analyzing huge sets of game data. The primary focus will be in owning a complete project and developing tools and systems for internal stakeholders for improving User Acquisition and LiveOps. Opportunity to combine data engineering, full stack development and data science. You will interface with various internal teams to understand their data needs and provide high quality data and insights which drive business value.",dou,2020-11-12,Data Engineer@Tilting Point,,,"{""Required skills"": [""\u25cf Minimum bachelor\u2019s in Computer Science\u25cf 2+ years of experience as a Data Engineer working on big data analytics\u25cf Excellent knowledge of Python and SQL\u25cf Good knowledge of Scala and Spark\u25cf Good knowledge of Apache Airflow\u25cf You are fluent in English (written and oral), additional languages a plus\u25cf Experience with integration of data from multiple data sources and APIs\u25cf Knowledge of various ETL techniques and frameworks\u25cf Ability to collaborate with colleagues across different disciplines/locations""], ""As a plus"": [""\u25cf Have experience/exposure to mobile User Acquisition and Marketing\u25cf Worked in the gaming industry and knowledge game specific data\u25cf Comfortable with AWS cloud (S3, EC2, Redshift, EKS(Kubernetes))\u25cf Knowledge of existing third party analytics visualization software (Amplitude, Looker, Tableau)""], ""We offer"": [""\u25cf Startup environment"", ""where each individual makes a large impact\u25cf Ability to own technical direction of various products and systems\u25cf Work with great people on great games that reach millions of people each month""], ""Responsibilities"": [""Responsibilities include, but are not limited to the following, on an as-needed basis:\u25cf Work with data analysts, game producers, and customer service to help define technology needs for the logging, processing, storage and presentation of data in a manner that delivers business value\u25cf Design, implement, maintain tools and reports for User Acquisition and LiveOps teams from start to finish\u25cf Work with various internal teams to satisfy analytical needs by building both large systematic reports and small custom pieces\u25cf Build data expertise and own data quality for a variety of products""], ""Project description"": [""Tilting Point is looking for a Data Engineer to join our team in Kyiv. As part of the team, you will work on collecting, storing, processing, and analyzing huge sets of game data. The primary focus will be in owning a complete project and developing tools and systems for internal stakeholders for improving User Acquisition and LiveOps. Opportunity to combine data engineering, full stack development and data science. You will interface with various internal teams to understand their data needs and provide high quality data and insights which drive business value.""]}"
XOVI GmbH,https://jobs.dou.ua/companies/xovi-gmbh/,Data Engineer,https://jobs.dou.ua/companies/xovi-gmbh/vacancies/130176/, Odesa,02 November 2020,,"Required skills * Hadoop Ecosystem* MapReduce (Hadoop or Spark)* SQL / Apache Hive As a plus * Java* Python* Accumulo* Oozie* Cloudera Manager* Ansible / Puppet We offer + немецкая компания+ кооперативные трениниги & Slack (600+ сотрудников в разных странах)+ достойный уровень оплаты (официальное трудоустройство)+ оплачиваемый отпуск+ работа в офисе (40 часовая неделя)+ перспектива карьерного роста+ командировки в Германию (по желанию) Responsibilities У тебя есть опыт работы с Hadoop и ты хотел бы далее развиваться в этом направлении? Присоединяйся к XOVI и тем самым к WebPros. Project description «We Simplify SEO» — это дивиз XOVI GmbH. Наш SaaS продукт помогает SEO специалистам быть более успешными в интернете с 2009 года и является лидером на немецком рынке. Использование наших данных и программного обеспечения дает нашим клиентам значительное конкурентное преимущество в поисковой оптимизации и маркетинге в целом. C 2017 года мы являемся частью корпорации WebPros, в которую входят такие компании как cPanel, Plesk, WHMOS & SolusVM.",dou,2020-11-12,Data Engineer@XOVI GmbH,,,"{""Required skills"": [""* Hadoop Ecosystem* MapReduce (Hadoop or Spark)* SQL / Apache Hive""], ""As a plus"": [""* Java* Python* Accumulo* Oozie* Cloudera Manager* Ansible / Puppet""], ""We offer"": [""+ \u043d\u0435\u043c\u0435\u0446\u043a\u0430\u044f \u043a\u043e\u043c\u043f\u0430\u043d\u0438\u044f+ \u043a\u043e\u043e\u043f\u0435\u0440\u0430\u0442\u0438\u0432\u043d\u044b\u0435 \u0442\u0440\u0435\u043d\u0438\u043d\u0438\u0433\u0438 & Slack (600+ \u0441\u043e\u0442\u0440\u0443\u0434\u043d\u0438\u043a\u043e\u0432 \u0432 \u0440\u0430\u0437\u043d\u044b\u0445 \u0441\u0442\u0440\u0430\u043d\u0430\u0445)+ \u0434\u043e\u0441\u0442\u043e\u0439\u043d\u044b\u0439 \u0443\u0440\u043e\u0432\u0435\u043d\u044c \u043e\u043f\u043b\u0430\u0442\u044b (\u043e\u0444\u0438\u0446\u0438\u0430\u043b\u044c\u043d\u043e\u0435 \u0442\u0440\u0443\u0434\u043e\u0443\u0441\u0442\u0440\u043e\u0439\u0441\u0442\u0432\u043e)+ \u043e\u043f\u043b\u0430\u0447\u0438\u0432\u0430\u0435\u043c\u044b\u0439 \u043e\u0442\u043f\u0443\u0441\u043a+ \u0440\u0430\u0431\u043e\u0442\u0430 \u0432 \u043e\u0444\u0438\u0441\u0435 (40 \u0447\u0430\u0441\u043e\u0432\u0430\u044f \u043d\u0435\u0434\u0435\u043b\u044f)+ \u043f\u0435\u0440\u0441\u043f\u0435\u043a\u0442\u0438\u0432\u0430 \u043a\u0430\u0440\u044c\u0435\u0440\u043d\u043e\u0433\u043e \u0440\u043e\u0441\u0442\u0430+ \u043a\u043e\u043c\u0430\u043d\u0434\u0438\u0440\u043e\u0432\u043a\u0438 \u0432 \u0413\u0435\u0440\u043c\u0430\u043d\u0438\u044e (\u043f\u043e \u0436\u0435\u043b\u0430\u043d\u0438\u044e)""], ""Responsibilities"": [""\u0423 \u0442\u0435\u0431\u044f \u0435\u0441\u0442\u044c \u043e\u043f\u044b\u0442 \u0440\u0430\u0431\u043e\u0442\u044b \u0441 Hadoop \u0438 \u0442\u044b \u0445\u043e\u0442\u0435\u043b \u0431\u044b \u0434\u0430\u043b\u0435\u0435 \u0440\u0430\u0437\u0432\u0438\u0432\u0430\u0442\u044c\u0441\u044f \u0432 \u044d\u0442\u043e\u043c \u043d\u0430\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u0438? \u041f\u0440\u0438\u0441\u043e\u0435\u0434\u0438\u043d\u044f\u0439\u0441\u044f \u043a XOVI \u0438 \u0442\u0435\u043c \u0441\u0430\u043c\u044b\u043c \u043a WebPros.""], ""Project description"": [""\u00abWe Simplify SEO\u00bb"", ""\u044d\u0442\u043e \u0434\u0438\u0432\u0438\u0437 XOVI GmbH. \u041d\u0430\u0448 SaaS \u043f\u0440\u043e\u0434\u0443\u043a\u0442 \u043f\u043e\u043c\u043e\u0433\u0430\u0435\u0442 SEO \u0441\u043f\u0435\u0446\u0438\u0430\u043b\u0438\u0441\u0442\u0430\u043c \u0431\u044b\u0442\u044c \u0431\u043e\u043b\u0435\u0435 \u0443\u0441\u043f\u0435\u0448\u043d\u044b\u043c\u0438 \u0432 \u0438\u043d\u0442\u0435\u0440\u043d\u0435\u0442\u0435 \u0441 2009 \u0433\u043e\u0434\u0430 \u0438 \u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u043b\u0438\u0434\u0435\u0440\u043e\u043c \u043d\u0430 \u043d\u0435\u043c\u0435\u0446\u043a\u043e\u043c \u0440\u044b\u043d\u043a\u0435.""]}"
lifecell,https://jobs.dou.ua/companies/lifecell/,Big Data Engineer,https://jobs.dou.ua/companies/lifecell/vacancies/67462/," Kyiv, remote",02 November 2020,,"Required skills — 3+ years of experience as a Data Engineer or Database Developer.— Experience with RDBMS’s like Oracle or MySQL, experience in SQL and PL/SQL development and performance tuning.— Knowledge of Data warehouse concepts and experience in ETL development. — Proficiency in one of Python, Java or Scala.— Experience with Big Data technologies such as Hadoop, Spark, Hive, Kafka, HBase, Sqoop and other tools.— Experience in implementation of Machine Learning pipelines using Spark ML or Scikit-learn would be advantage.— Experience in Telecom domain would be advantage.— Good analytical and problem solving skills. Responsibilities 1. Work with analysts and business stakeholders to clarify their requirements.2. Develop ETL processes, implementing new or extending existing data marts.3. Design and develop data processing pipelines in Hadoop data platform, for both batch and streaming modes.4. Participate in implementation and integration of new data sources.5. Develop API for integration of external systems with data platforms.6. Perform root-cause analysis of data pipelines issues, bug fixing and performance tuning.7. Drive data quality and manage SLA’s for the data platforms. Project description At this position you will work with data about customers activity in one of the largest telecom operators on the market. You will develop ETL processes and data pipelines in data platforms: Data Warehouse, BI and reporting systems and Hadoop Data Platform. You will participate in development and integration of data driven products and services for internal and B2B clients.",dou,2020-11-12,Big Data Engineer@lifecell,,,"{""Required skills"": [""3+ years of experience as a Data Engineer or Database Developer."", ""Experience with RDBMS\u2019s like Oracle or MySQL, experience in SQL and PL/SQL development and performance tuning."", ""Knowledge of Data warehouse concepts and experience in ETL development."", ""Proficiency in one of Python, Java or Scala."", ""Experience with Big Data technologies such as Hadoop, Spark, Hive, Kafka, HBase, Sqoop and other tools."", ""Experience in implementation of Machine Learning pipelines using Spark ML or Scikit-learn would be advantage."", ""Experience in Telecom domain would be advantage."", ""Good analytical and problem solving skills.""], ""Responsibilities"": [""1. Work with analysts and business stakeholders to clarify their requirements.2. Develop ETL processes, implementing new or extending existing data marts.3. Design and develop data processing pipelines in Hadoop data platform, for both batch and streaming modes.4. Participate in implementation and integration of new data sources.5. Develop API for integration of external systems with data platforms.6. Perform root-cause analysis of data pipelines issues, bug fixing and performance tuning.7. Drive data quality and manage SLA\u2019s for the data platforms.""], ""Project description"": [""At this position you will work with data about customers activity in one of the largest telecom operators on the market. You will develop ETL processes and data pipelines in data platforms: Data Warehouse, BI and reporting systems and Hadoop Data Platform. You will participate in development and integration of data driven products and services for internal and B2B clients.""]}"
Rakuten,https://jobs.dou.ua/companies/rakuten/,Data Scientist,https://jobs.dou.ua/companies/rakuten/vacancies/129697/," Odesa, remote",02 November 2020,,"Rakuten Americas is looking for a talented Data Scientist to join our amazing Machine Learning team in Ukraine. Rakuten Americas is a global leader in internet services, empowering individuals, communities, businesses and society. Our 20+ businesses span e-commerce, digital content, communications and data analytics, bringing the joy of discovery to millions of members around the world. We’ve brought together a unique set of internet services that are changing the way retailers and marketers do business. With businesses such as Rakuten Marketing, Rakuten Rewards and Rakuten Intelligence, we can offer marketers the platform and data intelligence to reach consumers at the moment they want to make a purchase. Our Data Science group consists of ~40 Data Scientists and Engineers, part of them based in Ukraine. We apply SOTA and classical machine learning techniques to Rakuten data to improve the product, internal processes and personalization for users, which in turn improves profitability of Rakuten business units. Our products are composed of many microservices. We own the products we deliver and maintain them. We follow DevOps approach when developers are responsible for code delivery with help of existing tools created by a dedicated Infra team. As a team is distributed, you can work remotely or from our Odessa/Kiev offices. We cover english classes, health insurance, sport activities, certifications/trainings and participation in conferences. •BS or MS in a technical field •2+ years of experience in machine learning, the experience in at least one of next domains will be a plus: NLP, Recommendation Systems, Predictive Analytics, Knowledge graphs •Expertise in Python and SQL •Strong CS fundamentals, such as algorithms and data structures •Experience with cloud computing stacks such as Amazon Web Services preferred •Excellent written and verbal communication skills •Enthusiasm for working hard and having fun in a dynamic environment •Prepare the required datasets for modeling purposes •Evaluate the models and perform investigative error analyses •Report and provide feedback on the findings of analyses to team members •Research and experiment with different machine learning algorithms and techniques to solve business problems •Work with engineers to make sure the engines scale well on high volumes of data",dou,2020-11-12,Data Scientist@Rakuten,,,{}
Windsor.ai,https://jobs.dou.ua/companies/windsor-ai/,"Senior python developer, analytics and data SAAS company",https://jobs.dou.ua/companies/windsor-ai/vacancies/121825/," Kyiv, remote",01 November 2020,$2500–4000,"Required skills Python, R, Bash, Linux, Postgres, Pandas We are looking for an experienced engineer to join our team. We use docker and bitbucket pipelines for continuous integration.Interest in joining a startup and contribute to its growth and building a good product. As a plus Analytics and visualisation skills, data-science.Scientific and curious mindset.Good programming practices, writing tests etc.Docker experience We offer Great team and the opportunity to be part of a startup that is growing.This is a company where you can influence and have an impact.We have a very flat structure. Responsibilities -optimise data-architectures and pipelines-Improve algorithms, both R and python-Develop AI models for advertising Project description We pull data from all big marketing platforms like Google Analytics, Adwords, Bing, Adobe etc.. We join all this data and do attribution modelling and provide recommendatations on how to optimise the marketing. Big part of what we do is to big good data pipelines. We are looking for skilled and talented engineers who want to be part of a fast-growing start-up for the long term.",dou,2020-11-12,"Senior python developer, analytics and data SAAS company@Windsor.ai",,,"{""Required skills"": [""Python, R, Bash, Linux, Postgres, Pandas""], ""As a plus"": [""We are looking for an experienced engineer to join our team. We use docker and bitbucket pipelines for continuous integration.Interest in joining a startup and contribute to its growth and building a good product.""], ""We offer"": [""Analytics and visualisation skills, data-science.Scientific and curious mindset.Good programming practices, writing tests etc.Docker experience""], ""Responsibilities"": [""Great team and the opportunity to be part of a startup that is growing.This is a company where you can influence and have an impact.We have a very flat structure.""], ""Project description"": [""-optimise data-architectures and pipelines-Improve algorithms, both R and python-Develop AI models for advertising""]}"
Windsor.ai,https://jobs.dou.ua/companies/windsor-ai/,"Data Engineer/Scientist, data analytics SAAS company",https://jobs.dou.ua/companies/windsor-ai/vacancies/75673/," Kyiv, remote",01 November 2020,$2500–3500,"Required skills Python, R, Bash, Linux, Postgres, Pandas We are looking for an experienced engineer to join our team. We use docker and bitbucket pipelines for continuous integration.Interest in joining a startup and contribute to its growth and building a good product. As a plus Analytics and visualisation skills, data-science.Scientific and curious mindset.Good programming practices, writing tests etc.Docker experience We offer Great team and the opportunity to be part of one of the fastest growing european Analytics data companies. Opportunity to travel and meet clients if desired. Responsibilities -optimise data-architectures and pipelines-Improve algorithms, both R and python-Develop AI models for advertising Project description We pull data from many different marketing platforms. We join all this data and do attribution modelling. Then we propose better budget allocations to the clients. Some of the Worlds biggest advertisers use our platform. We are looking for skilled and talented engineers who want to be part of a fast-growing start-up for the long term.",dou,2020-11-12,"Data Engineer/Scientist, data analytics SAAS company@Windsor.ai",,,"{""Required skills"": [""Python, R, Bash, Linux, Postgres, Pandas""], ""As a plus"": [""We are looking for an experienced engineer to join our team. We use docker and bitbucket pipelines for continuous integration.Interest in joining a startup and contribute to its growth and building a good product.""], ""We offer"": [""Analytics and visualisation skills, data-science.Scientific and curious mindset.Good programming practices, writing tests etc.Docker experience""], ""Responsibilities"": [""Great team and the opportunity to be part of one of the fastest growing european Analytics data companies.""], ""Project description"": [""Opportunity to travel and meet clients if desired.""]}"
DataRobot,https://jobs.dou.ua/companies/datarobot/,"DevOps Engineer, Data Management",https://jobs.dou.ua/companies/datarobot/vacancies/125367/," Kyiv, Lviv",30 October 2020,,"Required skills A passion for automating everythingA passion for collaborating and tearing down communication silos3+ years of experience in DevOps focused on Big Data environments3+ years of experience scripting in Bash, Java, Scala, Python or similar3+ years of experience with Linux (Ubuntu, RedHat or similar)Experience with Docker and/or container orchestration (Docker, Kubernetes, Mesos, or similar)Experience in configuring and setting up Kubernetes and Hadoop clustersExperience in setting up Kerberos authentication and impersonationExperience with Hadoop ecosystem/Hadoop stack (Hadoop, Spark, Hive, etc)Experience with Kubernetes ecosystem/Kubernetes stack (Kubeflow, EKS)Good coding skills. In the interview process, you will be evaluated on your performance in a number of coding and design scenarios — be prepared to think!Good writing and communication skills As a plus Experience with launching/managing computing resources in AWS, Azure or similarExperience with CI/CD tools (Jenkins, Team City, Ansible, etcd, Terraform or similar)Familiar with DevOps methodologiesFamiliarity with both Cloud Deployment and On-Premise Release WorkflowsApplication-level metrics familiarity (ELK stack, Instana, Grafana, Prometheus, Tracing)Have experience creating automated build pipelines Responsibilities As a DevOps Engineer, you will build out a Data Prep & ETL system inside DataRobot utilizing distributed frameworks such as Spark, Hadoop, and Kubernetes. You will work on availability, security, scalability, and resiliency of solutions focused on data processing. You will also be responsible for the design and integration of pipelines for all kinds of automated testing. Ideally, a candidate can bring new ideas from concept to implementation, write quality, testable code, and participate in design/development discussions. We value engineers who are familiar with DevOps tools and practices, who do not believe that any problem is too hard, and who are willing and eager to chase problems down no matter where they lead. Project description The Global Data Domain at DataRobot helps make AI incredibly easy to build by surfacing and connecting users to all enterprise data, and enabling data prep at modeling and prediction time. In this role, you’ll help enable our users to easily interact with the data required to train models and generate predictions. Additionally, you’ll help design solutions that foster collaboration amongst distributed teams of engineers, data scientists, and beyond.",dou,2020-11-12,"DevOps Engineer, Data Management@DataRobot",,,"{""Required skills"": [""A passion for automating everythingA passion for collaborating and tearing down communication silos3+ years of experience in DevOps focused on Big Data environments3+ years of experience scripting in Bash, Java, Scala, Python or similar3+ years of experience with Linux (Ubuntu, RedHat or similar)Experience with Docker and/or container orchestration (Docker, Kubernetes, Mesos, or similar)Experience in configuring and setting up Kubernetes and Hadoop clustersExperience in setting up Kerberos authentication and impersonationExperience with Hadoop ecosystem/Hadoop stack (Hadoop, Spark, Hive, etc)Experience with Kubernetes ecosystem/Kubernetes stack (Kubeflow, EKS)Good coding skills. In the interview process, you will be evaluated on your performance in a number of coding and design scenarios"", ""be prepared to think!Good writing and communication skills""], ""As a plus"": [""Experience with launching/managing computing resources in AWS, Azure or similarExperience with CI/CD tools (Jenkins, Team City, Ansible, etcd, Terraform or similar)Familiar with DevOps methodologiesFamiliarity with both Cloud Deployment and On-Premise Release WorkflowsApplication-level metrics familiarity (ELK stack, Instana, Grafana, Prometheus, Tracing)Have experience creating automated build pipelines""], ""Responsibilities"": [""As a DevOps Engineer, you will build out a Data Prep & ETL system inside DataRobot utilizing distributed frameworks such as Spark, Hadoop, and Kubernetes. You will work on availability, security, scalability, and resiliency of solutions focused on data processing. You will also be responsible for the design and integration of pipelines for all kinds of automated testing.""], ""Project description"": [""Ideally, a candidate can bring new ideas from concept to implementation, write quality, testable code, and participate in design/development discussions. We value engineers who are familiar with DevOps tools and practices, who do not believe that any problem is too hard, and who are willing and eager to chase problems down no matter where they lead.""]}"
DataRobot,https://jobs.dou.ua/companies/datarobot/,Customer Facing Data Scientist,https://jobs.dou.ua/companies/datarobot/vacancies/133351/," Kyiv, Kharkiv, Lviv, Odesa, Khmelnytskyi",30 October 2020,,"Required skills 4-5+ years of real-world business experience in a data science roleEnglish speaking a mustHands-on experience building and implementing predictive models using machine learning algorithmsStrong customer interaction experienceStrong project management skillsExcellent organizational, communication, writing and interpersonal skillsFamiliarity with a variety of technical tools for the manipulation of datasetsFluency with scripting (Python / R)15-40% travel As a plus Familiarity with consultative sales process in the analytics marketplaceFamiliarity with Hadoop and related Big Data technologiesFamiliarity with data prep tools like Alteryx or TrifactaExperience dealing with complex customer organizationsDeep experience with specific industries (e.g. banking, healthcare, insurance) or specific problem types (e.g. time-series, optimization)Experience in MLOps — model deployment, monitoring, validation and operationalisation We offer Customer Facing Data Scientists (CFDSs) are critical to making our customers successful. An ideal CFDS candidate should have strong fundamentals of applied data science in business setting, and should enjoy communicating and evangelizing data science solutions to business stakeholders. You will be based in one of our Ukraine offices and primarily interact with clients and the team on videoconference, but you will also sometimes visit clients on-site. Responsibilities Product:Representing the DataRobot product from a technical standpoint to customers — including demonstrations, conducting proof-of-concept trials, helping clients evaluate success criteria, and training usersProviding the customer’s point of view to DataRobot’s Product team, informing the direction of future product feature development. Data Science:Enabling customers to solve complex data science problems using DataRobot — including problem framing, data preparation, model building, model deployment, model management, and output consumptionIn some cases, executing data science workflows for customersProviding data science knowledge and expertise as a trusted advisor to the client. Project management:Conducting and managing data science projects with the customer’s vision of success in mindCollaborating with Sales, Field Engineers, and the rest of the DataRobot team to identify the best possible resources to move forward customer’s projects. LeadershipBuilding a long-term trusted relationship with the customer so that the customers can be led towards successUnderstanding and empathizing with customers’ pain points of building AI solutionsQualifying opportunities where DataRobot can be a suitable fit and thus making DataRobot more efficientPresenting DataRobot in industry conferences as well as creating powerful technical content for marketing purposes. Project description On a day-to-day basis, CFDS works side-by-side with the Sales, Account Management, and Field Engineering teams to help our customers achieve their goals with DataRobot. Internally, CFDS act as the voice of customer to the Product, Engineering, and Marketing teams.",dou,2020-11-12,Customer Facing Data Scientist@DataRobot,,,"{""Required skills"": [""4-5+ years of real-world business experience in a data science roleEnglish speaking a mustHands-on experience building and implementing predictive models using machine learning algorithmsStrong customer interaction experienceStrong project management skillsExcellent organizational, communication, writing and interpersonal skillsFamiliarity with a variety of technical tools for the manipulation of datasetsFluency with scripting (Python / R)15-40% travel""], ""As a plus"": [""Familiarity with consultative sales process in the analytics marketplaceFamiliarity with Hadoop and related Big Data technologiesFamiliarity with data prep tools like Alteryx or TrifactaExperience dealing with complex customer organizationsDeep experience with specific industries (e.g. banking, healthcare, insurance) or specific problem types (e.g. time-series, optimization)Experience in MLOps"", ""model deployment, monitoring, validation and operationalisation""], ""We offer"": [""Customer Facing Data Scientists (CFDSs) are critical to making our customers successful. An ideal CFDS candidate should have strong fundamentals of applied data science in business setting, and should enjoy communicating and evangelizing data science solutions to business stakeholders.""], ""Responsibilities"": [""You will be based in one of our Ukraine offices and primarily interact with clients and the team on videoconference, but you will also sometimes visit clients on-site.""], ""Project description"": [""Product:Representing the DataRobot product from a technical standpoint to customers"", ""including demonstrations, conducting proof-of-concept trials, helping clients evaluate success criteria, and training usersProviding the customer\u2019s point of view to DataRobot\u2019s Product team, informing the direction of future product feature development.""]}"
Data Science UA,https://jobs.dou.ua/companies/data-science-ua/,IT Researcher,https://jobs.dou.ua/companies/data-science-ua/vacancies/87168/," Kyiv, remote",30 October 2020,,"Required skills — 1-2 years of working experience on similar role;— Familiar with modern recruitment tools, techniques and best practices;— Good spoken and written English;— Strong teamwork (it’s very essential for us!);— Analytical mind, be initiative and independent;— Be very tolerant, and like having fun:) As a plus — Experience in searching specialists for Data science related positions. We offer — Warm and friendly working environment;— Competitive compensation;— Fully-equipped perfect office space located in the city center (“Creative Quarter” co-working);— Opportunities to participate in Professional forums and conferences;— Flexible schedule and the ability to work remotely. Responsibilities — Search for wide range of IT professionals (engineers, testers, developers, and architects)— Search for talents using several channels— Screening, sourcing and analyzing local IT market, considering new trended technologies— Create a wide Talent pool and updating CV search database— Formatting CVs— Communicate with candidates on a daily basis— Working as a part of Recruiting Team— Provide statistic reports on a weekly and monthly basis. Project description Data Science UA is growing fast and we are looking for talented IT Researcher to join our team!",dou,2020-11-12,IT Researcher@Data Science UA,,,"{""Required skills"": [""1-2 years of working experience on similar role;"", ""Familiar with modern recruitment tools, techniques and best practices;"", ""Good spoken and written English;"", ""Strong teamwork (it\u2019s very essential for us!);"", ""Analytical mind, be initiative and independent;"", ""Be very tolerant, and like having fun:)""], ""As a plus"": [""Experience in searching specialists for Data science related positions.""], ""We offer"": [""Warm and friendly working environment;"", ""Competitive compensation;"", ""Fully-equipped perfect office space located in the city center (\u201cCreative Quarter\u201d co-working);"", ""Opportunities to participate in Professional forums and conferences;"", ""Flexible schedule and the ability to work remotely.""], ""Responsibilities"": [""Search for wide range of IT professionals (engineers, testers, developers, and architects)"", ""Search for talents using several channels"", ""Screening, sourcing and analyzing local IT market, considering new trended technologies"", ""Create a wide Talent pool and updating CV search database"", ""Formatting CVs"", ""Communicate with candidates on a daily basis"", ""Working as a part of Recruiting Team"", ""Provide statistic reports on a weekly and monthly basis.""], ""Project description"": [""Data Science UA is growing fast and we are looking for talented IT Researcher to join our team!""]}"
Maxitech Software,https://jobs.dou.ua/companies/maxitech/,Data Analyst for BI project,https://jobs.dou.ua/companies/maxitech/vacancies/129122/, Kyiv,30 October 2020,,"Required skills — Excellent knowledge of SQL (creation of views, stored procedures, functions, indexes, and relational database management in general)— Understanding the concepts of data profiling and main data quality checks— Understanding of Business Intelligence projects (data integration, data warehousing and visualization)— Basic knowledge of programming and scripting language (preferably Python) to deliver test automation for data consistency and data integrity checks— At least Upper-intermediate English level. As a plus — Strong financial and business analysis skills— Experience with a whole BI process verification (ETL, DWH, Data Marts, Reporting)— Hands-on knowledge of Python to deliver data quality test automation. We offer — Unique working environment where you communicate and work directly with client— Being a part of the positive and fun team— Cutting-edge technology stack— Team of strong IT professionals working in a dynamic startup environment— Big amount of knowledge-sharing practices and sessions— Centrally located office near Poshtova Square metro station with great view from our terrace— Competitive salary, reasonable and fair working conditions, flexible schedule— Medical insurance after 3 months cooperation— English classes— Lanches provided by Maxitech— Corporative events. Responsibilities — Ensure data completeness, consistency and accuracy while performing data checks— Perform data quality validation using SQL for the next data sources: Microsoft SQL Server, MySQL databases— Perform data profiling and regression testing— Perform data integrity checks for REST API data sources and Data Warehouse (DWH)— Validation of the financial metrics calculation using Power BI and Microsoft Excel— Make sure that current Power BI reports updated with new business metrics— Bug reporting via TFS and help with data quality issues communication and troubleshooting— Create and maintain test documentation— Conduct experiments and test with BI Data Model to find gaps and issues— Perform validation of full-cycle Business Intelligence solution— Participate and contribute to CI/CD processes in alignment with DevOps Team — Cooperate with Business Analysts team to assist with Power BI report implementation.— Suggest ideas to improve data model, reporting, visualization. Project description Maxitech is a R&D office of large international holding, building a portfolio of products linked to each other with the business goals they are addressing. Our project is long term and has stable financing. There are existing solutions for Marketing&Sales departments which are already in production and have active phase of new functions development as well as start-up products. Maxitech R&D team’s target is developing software solutions maximizing the performance of the core business. Current team size is 200+ employees. We are looking for experts ready to bring their knowledge and contribute to building the new, best-in-the class, software product.Our company headquarter is in Israel. We also have offices in 8 locations worldwide. R&D team is fully concentrated in Kyiv.We don’t do outstaff or outsource, we don’t have a “client” and don’t sell any service. We make qualitative software products to enable the business become more efficient and successful.",dou,2020-11-12,Data Analyst for BI project@Maxitech Software,,,"{""Required skills"": [""Excellent knowledge of SQL (creation of views, stored procedures, functions, indexes, and relational database management in general)"", ""Understanding the concepts of data profiling and main data quality checks"", ""Understanding of Business Intelligence projects (data integration, data warehousing and visualization)"", ""Basic knowledge of programming and scripting language (preferably Python) to deliver test automation for data consistency and data integrity checks"", ""At least Upper-intermediate English level.""], ""As a plus"": [""Strong financial and business analysis skills"", ""Experience with a whole BI process verification (ETL, DWH, Data Marts, Reporting)"", ""Hands-on knowledge of Python to deliver data quality test automation.""], ""We offer"": [""Unique working environment where you communicate and work directly with client"", ""Being a part of the positive and fun team"", ""Cutting-edge technology stack"", ""Team of strong IT professionals working in a dynamic startup environment"", ""Big amount of knowledge-sharing practices and sessions"", ""Centrally located office near Poshtova Square metro station with great view from our terrace"", ""Competitive salary, reasonable and fair working conditions, flexible schedule"", ""Medical insurance after 3 months cooperation"", ""English classes"", ""Lanches provided by Maxitech"", ""Corporative events.""], ""Responsibilities"": [""Ensure data completeness, consistency and accuracy while performing data checks"", ""Perform data quality validation using SQL for the next data sources: Microsoft SQL Server, MySQL databases"", ""Perform data profiling and regression testing"", ""Perform data integrity checks for REST API data sources and Data Warehouse (DWH)"", ""Validation of the financial metrics calculation using Power BI and Microsoft Excel"", ""Make sure that current Power BI reports updated with new business metrics"", ""Bug reporting via TFS and help with data quality issues communication and troubleshooting"", ""Create and maintain test documentation"", ""Conduct experiments and test with BI Data Model to find gaps and issues"", ""Perform validation of full-cycle Business Intelligence solution"", ""Participate and contribute to CI/CD processes in alignment with DevOps Team"", ""Cooperate with Business Analysts team to assist with Power BI report implementation."", ""Suggest ideas to improve data model, reporting, visualization.""], ""Project description"": [""Maxitech is a R&D office of large international holding, building a portfolio of products linked to each other with the business goals they are addressing. Our project is long term and has stable financing. There are existing solutions for Marketing&Sales departments which are already in production and have active phase of new functions development as well as start-up products.""]}"
CASAFARI - real estate data,https://jobs.dou.ua/companies/casafari-real-estate-data/,"Data Collection, Quality Analysis Intern",https://jobs.dou.ua/companies/casafari-real-estate-data/vacancies/136951/?from=first-job," Kharkiv, remote",30 October 2020,$400–600,"Required skills Requirements: Understanding of CSS, HTML, regular expressionsThe desire to learn and growStrong quantitative analytical skillsIntermediate + English skills As a plus Nice to have:Ability to query with SQLBasic Python We offer What we offer: Mentorship;Great datasets;Fast-growing products and business;Clear added business value;Full-time employment option;Upon full-time employment — employee stock option / equity;Achievement-based performance evaluation;Growth focused mindset; Responsibilities What should be done: Data exploration and preparationData collection and integrationData quality monitoring and analysis Project description Are you ready for a real challenge? CASAFARI is young but growing fast, which means you’ll get to help shape our future and revolutionise the real estate industry. We’re looking for a passionate, proactive and pragmatic Data Collection and Quality Analysis Intern to join our team of 20+ nationalities. You’ll grow with us as we strive to fulfill our mission and vision: to bring transparency to the real estate market with the cleanest and most complete real estate database in the world. Our end products are: a metasearch like Trivago or Skyscanner, a daily market data feed like Bloomberg Terminal, and we are currently building property market analytics with the most complete database updated in real time. Problem, vision, and mission: Casafari solves the chaos on the real estate market by bringing transparency with the cleanest and most complete real estate database in the world. What does our team look like?3 offices: Lisbon, Budapest, Kharkiv40+ teammates20+ nationalities27 y.o. is the average age1 serial company unicorn dog At CASAFARI we value collaboration, curiosity, ownership, meritocracy, diversity, and inclusiveness even more than hard skills. A desire to learn and an eagerness to change the world are what make a difference to us. We’re always looking for people to drive the company towards our vision and who are looking to evolve and grow with us. CASAFARI strives to create an inclusive environment where we learn from each other, work together as a team, support each other and share ideas freely. There are no limits to your imagination. Instead, we encourage you to think outside the box — to be bold and venturing. ‘Fail fast, learn fast’ is our motto.",dou,2020-11-12,"Data Collection, Quality Analysis Intern@CASAFARI - real estate data",,,"{""Required skills"": [""Requirements:""], ""As a plus"": [""Understanding of CSS, HTML, regular expressionsThe desire to learn and growStrong quantitative analytical skillsIntermediate + English skills""], ""We offer"": [""Nice to have:Ability to query with SQLBasic Python""], ""Responsibilities"": [""What we offer:""], ""Project description"": [""Mentorship;Great datasets;Fast-growing products and business;Clear added business value;Full-time employment option;Upon full-time employment"", ""employee stock option / equity;Achievement-based performance evaluation;Growth focused mindset;""]}"
Grid Dynamics,https://jobs.dou.ua/companies/grid-dynamics/,Big Data Engineer with Scala and Spark,https://jobs.dou.ua/companies/grid-dynamics/vacancies/129968/," Kyiv, Kharkiv, Lviv",30 October 2020,,"Our customer is one of the world’s largest technology companies based in Silicon Valley with operations all over the world. On this project, we are working with bleeding-edge big data technologies to develop a high-performance data analytics platform, which handles petabytes of data. We are looking for an enthusiastic and technology-proficient Big Data Developer, who is eager to participate in design and implementation of a top-notch big data solution that will be deployed at a massive scale. Responsibilities:— Participate in the design and development of a big data analytics application— Design, support and continuously enhance the project code base, continuous integration pipeline, etc.— Write complex ETL processes and frameworks for analytics and data management— Implement large-scale near real-time streaming data processing pipelines— Work with a team of industry experts on cutting-edge big data technologies to develop solutions for deployment at massive scale Requirements:— Strong knowledge of Scala— In-depth knowledge of Hadoop and Spark, experience with data mining and stream processing technologies (Kafka, Spark Streaming, Akka Streams)— Understanding of the best practices in data quality and quality engineering— Experience with version control systems, Git in particular — Ability to quickly learn new tools and technologies Will be a plus:— Knowledge of Unix-based operating systems (bash/ssh/ps/grep etc.)— Experience with JVM build systems (SBT, Maven, Gradle) We offer:— Opportunity to work on bleeding-edge projects— Work with a highly motivated and dedicated team— Competitive salary— Flexible schedule— Benefits program— Social package — medical insurance, sports— Corporate social events— Professional development opportunities— Opportunity for long business trips to the US and possibility for relocation About us:Grid Dynamics is the engineering services company known for transformative, mission-critical cloud solutions for retail, finance and technology sectors. We architected some of the busiest e-commerce services on the Internet and have never had an outage during the peak season. Founded in 2006 and headquartered in San Ramon, California with offices throughout the US and Eastern Europe, we focus on big data analytics, scalable omnichannel services, DevOps, and cloud enablement.",dou,2020-11-12,Big Data Engineer with Scala and Spark@Grid Dynamics,,,{}
MEGOGO,https://jobs.dou.ua/companies/megogonet-/,Data Engineer,https://jobs.dou.ua/companies/megogonet-/vacancies/79414/, Kyiv,29 October 2020,,"Наш идеальный кандидат:Ищем Senior Data Engineer, которого не пугает работа с сотнями миллионов событий, и ответственность за создание инструментов и процессов обработки данных для поддержки создания моделей машинного обучения и проведение экспериментов, которые будут определять развитие одного из крупнейших ведеосервисов СНГ. Мы идем по пути data driven и у вас есть возможности реализовать свои навыки и умения в нашей команде. Что мы ждем:5+ лет опыта проектирования, разработки, тестирования, развёртывания и поддержки Data Lake/Data Warehouse решений.Экспертные знания Python в контексте создания ETL data pipelines.Экспертные знания PySpark: преобразования, агрегации, оконные функции, написание и оптимизация UDF.Экспертные знания SQL/NoSQL баз данных: MySQL, PostgreSQL, MongoDB.Опыт практической эксплуатации Big Data стека: Hadoop, Hive, Kafka, Spark, Cassandra.Опыт создания комплексных аналитических систем.Умение находить и быстрые решения для экспериментов и стабильные production-ready в зависимости от обстоятельств. Будет плюсом:Опыт работы с AWS (EC2, EMR, ECS, Kinesis, S3).Опыт программирования на Java/Scala.Опыт работы с аналитическими базами данных: Vertica, Exasol, Teradata, Redshift, BigQuery, Druid, Clickhouse.Навыки разработки в Docker/Kubernetes окружении.Незабытые университетские знания линейки, матана и статистики.Опыт создания продуктов с использованием machine learning. Обязанности:Создание и валидация идей для построения data driven в компании.Разработка архитектуры и выбор технических решений.Разработка и поддержка корпоративного DWH.Разработка ETL data pipelines: загрузка из различных источников, очистка, преобразование, обогащение внешними данными, нормализация/денормализация, сохранение с учётом нюансов и ограничений, покрытие тестами, мониторинг, эксплуатация. Что мы предлагаем:— Работу в стабильной компании — мы более восьми лет на медиарынке.— Возможность поучаствовать в создании видеосервиса будущего.— Конкурентный уровень заработной платы.— Крутые корпоративы. Например, всей командой на Atlas Weekend:)— Отношения, построенные на доверии.— Активный отдых: играй в футбол или настольный теннис. У нас есть своя футбольная команда, а наш тренер по пинг-понгу всему научит, он всегда в офисе. Выбирай, что тебе по душе, и участвуй в соревнованиях! Если очень хочется, даже есть возможность исполнить свою детскую мечту, если хотелось стать спортивным комментатором.— Всегда свежие фрукты в офисе и собственный бариста.— Возможности для развития.— Бесплатные уроки английского языка в офисе. Чувствуешь, что твои навыки соответствуют требованиям? Хочешь стать частью нашей крутой команды? Скорее отправляй свое резюме! Ответив на вакансию и отправив свое резюме в Компанию (ООО «МЕГОГО»), зарегистрированную и действующую в соответствии с законодательством Украины, регистрационный номер 38347009, адрес местонахождения: Украина, г. Киев, ул. Новоконстантиновская, 18 В (далее «Компания»), вы подтверждаете и соглашаетесь с тем, что Компания обрабатывает ваши личные данные, представленные в вашем резюме, в соответствии с Законом Украины «О защите персональных данных» и правилами GDPR.",dou,2020-11-12,Data Engineer@MEGOGO,,,{}
EVOPLAY,https://jobs.dou.ua/companies/evoplay/,Data scientist,https://jobs.dou.ua/companies/evoplay/vacancies/128293/, Kyiv,29 October 2020,,"Required skills Сильные аналитические навыки и навыки интерпретации данных.Опыт работы с Python for DS (Pandas, NumPy, bs4, Selenium, Sklearn, SciPy, Keras).Визуализация данных (plotly, matplotlib etc...).Опыт работы с сырыми данными, понимание критериев очистки, data mining.Практический опыт и хорошее понимание различных алгоритмов машинного обучения (log reg, linear regression, neural net, time series analysis).Хорошее понимание глубинных основ статистических методов.Знания теории вероятности и высшей математики на высоком уровне.Понимание точности, интерпретируемости, статистической важности результатов,и дальнейшего использования результатов модели. As a plus Образование в области математики, статистики, computer science или смежных технических дисциплин.Опыт разработки рабочих моделей для предсказания результатов спортивных дисциплин (football, basketball, csgo, dota2, fortnite etc).Работа с базами данных: MongoDB, PostgreSQL.Знание языка Golang. We offer Офис в парке м. ШулявскаяМолодая дружная компания, минимум бюрократии.Гибкий график (начало работы с 8 до 11, 8 ч/день).Соц.пакет, 100% оплата отпусков и больничных.Бесплатная парковка.Бесплатные уроки английского языка со своим учителем (3 часа в неделю).Кухня в офисе, со всеми сопутствующими элементами. Responsibilities Сбор, обработка и предварительный анализ данных.Формулирование и проверка статистических гипотез, построение моделей, как на основе классических статистических методов так и ML алгоритмов.Улучшение уже готовых математических моделей.Тюнинг и оптимизация, внедрение и дальнейшая поддержка в продакшене математических моделей. Project description Платформа построена на базе микросервисов, сейчас это около сотни различных сервисов. Обслуживает платформа около двух десятков высоконагруженных мировых известных порталов. Используем современные облачные технологии в продакшен, контейнеризация.",dou,2020-11-12,Data scientist@EVOPLAY,,,"{""Required skills"": [""\u0421\u0438\u043b\u044c\u043d\u044b\u0435 \u0430\u043d\u0430\u043b\u0438\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u043d\u0430\u0432\u044b\u043a\u0438 \u0438 \u043d\u0430\u0432\u044b\u043a\u0438 \u0438\u043d\u0442\u0435\u0440\u043f\u0440\u0435\u0442\u0430\u0446\u0438\u0438 \u0434\u0430\u043d\u043d\u044b\u0445.\u041e\u043f\u044b\u0442 \u0440\u0430\u0431\u043e\u0442\u044b \u0441 Python for DS (Pandas, NumPy, bs4, Selenium, Sklearn, SciPy, Keras).\u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0434\u0430\u043d\u043d\u044b\u0445 (plotly, matplotlib etc...).\u041e\u043f\u044b\u0442 \u0440\u0430\u0431\u043e\u0442\u044b \u0441 \u0441\u044b\u0440\u044b\u043c\u0438 \u0434\u0430\u043d\u043d\u044b\u043c\u0438, \u043f\u043e\u043d\u0438\u043c\u0430\u043d\u0438\u0435 \u043a\u0440\u0438\u0442\u0435\u0440\u0438\u0435\u0432 \u043e\u0447\u0438\u0441\u0442\u043a\u0438, data mining.\u041f\u0440\u0430\u043a\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u043e\u043f\u044b\u0442 \u0438 \u0445\u043e\u0440\u043e\u0448\u0435\u0435 \u043f\u043e\u043d\u0438\u043c\u0430\u043d\u0438\u0435 \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u0445 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u043e\u0432 \u043c\u0430\u0448\u0438\u043d\u043d\u043e\u0433\u043e \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f (log reg, linear regression, neural net, time series analysis).\u0425\u043e\u0440\u043e\u0448\u0435\u0435 \u043f\u043e\u043d\u0438\u043c\u0430\u043d\u0438\u0435 \u0433\u043b\u0443\u0431\u0438\u043d\u043d\u044b\u0445 \u043e\u0441\u043d\u043e\u0432 \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u043c\u0435\u0442\u043e\u0434\u043e\u0432.\u0417\u043d\u0430\u043d\u0438\u044f \u0442\u0435\u043e\u0440\u0438\u0438 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0438 \u0438 \u0432\u044b\u0441\u0448\u0435\u0439 \u043c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u043a\u0438 \u043d\u0430 \u0432\u044b\u0441\u043e\u043a\u043e\u043c \u0443\u0440\u043e\u0432\u043d\u0435.\u041f\u043e\u043d\u0438\u043c\u0430\u043d\u0438\u0435 \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u0438, \u0438\u043d\u0442\u0435\u0440\u043f\u0440\u0435\u0442\u0438\u0440\u0443\u0435\u043c\u043e\u0441\u0442\u0438, \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432,\u0438 \u0434\u0430\u043b\u044c\u043d\u0435\u0439\u0448\u0435\u0433\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u044f \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432 \u043c\u043e\u0434\u0435\u043b\u0438.""], ""As a plus"": [""\u041e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u0435 \u0432 \u043e\u0431\u043b\u0430\u0441\u0442\u0438 \u043c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u043a\u0438, \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u043a\u0438, computer science \u0438\u043b\u0438 \u0441\u043c\u0435\u0436\u043d\u044b\u0445 \u0442\u0435\u0445\u043d\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u0434\u0438\u0441\u0446\u0438\u043f\u043b\u0438\u043d.\u041e\u043f\u044b\u0442 \u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u0440\u0430\u0431\u043e\u0447\u0438\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u0434\u043b\u044f \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432 \u0441\u043f\u043e\u0440\u0442\u0438\u0432\u043d\u044b\u0445 \u0434\u0438\u0441\u0446\u0438\u043f\u043b\u0438\u043d (football, basketball, csgo, dota2, fortnite etc).\u0420\u0430\u0431\u043e\u0442\u0430 \u0441 \u0431\u0430\u0437\u0430\u043c\u0438 \u0434\u0430\u043d\u043d\u044b\u0445: MongoDB, PostgreSQL.\u0417\u043d\u0430\u043d\u0438\u0435 \u044f\u0437\u044b\u043a\u0430 Golang.""], ""We offer"": [""\u041e\u0444\u0438\u0441 \u0432 \u043f\u0430\u0440\u043a\u0435 \u043c. \u0428\u0443\u043b\u044f\u0432\u0441\u043a\u0430\u044f\u041c\u043e\u043b\u043e\u0434\u0430\u044f \u0434\u0440\u0443\u0436\u043d\u0430\u044f \u043a\u043e\u043c\u043f\u0430\u043d\u0438\u044f, \u043c\u0438\u043d\u0438\u043c\u0443\u043c \u0431\u044e\u0440\u043e\u043a\u0440\u0430\u0442\u0438\u0438.\u0413\u0438\u0431\u043a\u0438\u0439 \u0433\u0440\u0430\u0444\u0438\u043a (\u043d\u0430\u0447\u0430\u043b\u043e \u0440\u0430\u0431\u043e\u0442\u044b \u0441 8 \u0434\u043e 11, 8 \u0447/\u0434\u0435\u043d\u044c).\u0421\u043e\u0446.\u043f\u0430\u043a\u0435\u0442, 100% \u043e\u043f\u043b\u0430\u0442\u0430 \u043e\u0442\u043f\u0443\u0441\u043a\u043e\u0432 \u0438 \u0431\u043e\u043b\u044c\u043d\u0438\u0447\u043d\u044b\u0445.\u0411\u0435\u0441\u043f\u043b\u0430\u0442\u043d\u0430\u044f \u043f\u0430\u0440\u043a\u043e\u0432\u043a\u0430.\u0411\u0435\u0441\u043f\u043b\u0430\u0442\u043d\u044b\u0435 \u0443\u0440\u043e\u043a\u0438 \u0430\u043d\u0433\u043b\u0438\u0439\u0441\u043a\u043e\u0433\u043e \u044f\u0437\u044b\u043a\u0430 \u0441\u043e \u0441\u0432\u043e\u0438\u043c \u0443\u0447\u0438\u0442\u0435\u043b\u0435\u043c (3 \u0447\u0430\u0441\u0430 \u0432 \u043d\u0435\u0434\u0435\u043b\u044e).\u041a\u0443\u0445\u043d\u044f \u0432 \u043e\u0444\u0438\u0441\u0435, \u0441\u043e \u0432\u0441\u0435\u043c\u0438 \u0441\u043e\u043f\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u043c\u0438 \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u0430\u043c\u0438.""], ""Responsibilities"": [""\u0421\u0431\u043e\u0440, \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0438 \u043f\u0440\u0435\u0434\u0432\u0430\u0440\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0439 \u0430\u043d\u0430\u043b\u0438\u0437 \u0434\u0430\u043d\u043d\u044b\u0445.\u0424\u043e\u0440\u043c\u0443\u043b\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u0438 \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0430 \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u0433\u0438\u043f\u043e\u0442\u0435\u0437, \u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0435\u0439, \u043a\u0430\u043a \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u043a\u043b\u0430\u0441\u0441\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u043c\u0435\u0442\u043e\u0434\u043e\u0432 \u0442\u0430\u043a \u0438 ML \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u043e\u0432.\u0423\u043b\u0443\u0447\u0448\u0435\u043d\u0438\u0435 \u0443\u0436\u0435 \u0433\u043e\u0442\u043e\u0432\u044b\u0445 \u043c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439.\u0422\u044e\u043d\u0438\u043d\u0433 \u0438 \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u044f, \u0432\u043d\u0435\u0434\u0440\u0435\u043d\u0438\u0435 \u0438 \u0434\u0430\u043b\u044c\u043d\u0435\u0439\u0448\u0430\u044f \u043f\u043e\u0434\u0434\u0435\u0440\u0436\u043a\u0430 \u0432 \u043f\u0440\u043e\u0434\u0430\u043a\u0448\u0435\u043d\u0435 \u043c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439.""], ""Project description"": [""\u041f\u043b\u0430\u0442\u0444\u043e\u0440\u043c\u0430 \u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0430 \u043d\u0430 \u0431\u0430\u0437\u0435 \u043c\u0438\u043a\u0440\u043e\u0441\u0435\u0440\u0432\u0438\u0441\u043e\u0432, \u0441\u0435\u0439\u0447\u0430\u0441 \u044d\u0442\u043e \u043e\u043a\u043e\u043b\u043e \u0441\u043e\u0442\u043d\u0438 \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u0445 \u0441\u0435\u0440\u0432\u0438\u0441\u043e\u0432. \u041e\u0431\u0441\u043b\u0443\u0436\u0438\u0432\u0430\u0435\u0442 \u043f\u043b\u0430\u0442\u0444\u043e\u0440\u043c\u0430 \u043e\u043a\u043e\u043b\u043e \u0434\u0432\u0443\u0445 \u0434\u0435\u0441\u044f\u0442\u043a\u043e\u0432 \u0432\u044b\u0441\u043e\u043a\u043e\u043d\u0430\u0433\u0440\u0443\u0436\u0435\u043d\u043d\u044b\u0445 \u043c\u0438\u0440\u043e\u0432\u044b\u0445 \u0438\u0437\u0432\u0435\u0441\u0442\u043d\u044b\u0445 \u043f\u043e\u0440\u0442\u0430\u043b\u043e\u0432. \u0418\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u0441\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435 \u043e\u0431\u043b\u0430\u0447\u043d\u044b\u0435 \u0442\u0435\u0445\u043d\u043e\u043b\u043e\u0433\u0438\u0438 \u0432 \u043f\u0440\u043e\u0434\u0430\u043a\u0448\u0435\u043d, \u043a\u043e\u043d\u0442\u0435\u0439\u043d\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u044f.""]}"
MGID,https://jobs.dou.ua/companies/mgid/,Lead Data Scientist,https://jobs.dou.ua/companies/mgid/vacancies/126350/, Kyiv,29 October 2020,,"MGID was founded in 2008 and is one of the leading companies in native advertising. We enable our media partners to monetize their audience and help brands to promote their services and goods effectively. MGID offers a range of integrated solutions covering the promotion process every step of the way; we offer services ranging from planning out the marketing strategy to its thoughtful implementation and optimization. Our clients include major international brands like Renault, Domino’s, airbnb, PizzaHut, Qatar Airlines, and many others, including media organizations and web agencies. MGID is:— One of the largest MarTech-companies in the Ukrainian market;— A proprietary Highload service that delivers 185 billion advertisements to 850 million unique users in more than 60 languages;— The winner of multiple AdTech awards for innovation and product quality;— A workforce of 500+ employees operating from offices in the US, Europe and Asia;— A passion for cutting-edge technologies and a seamless vertical structure that allows the regional teams to exchange skills and development practices. Your main duties:— Develop new models for different recommender systems within our product (classification, clusterization etc.);— Act as a bridge between the business and technical stakeholders in development of new ML models and data solutions;— Identify new opportunities to leverage data science to different parts of the MGID product;— Research, patterns identification and formulating insights; — Identify proper evaluation metrics;— Lead the existing data science team. You fit these requirements:— 5+ years of overall technical experience;— 3+ years of experience in Machine Learning/Data Science;— Experience with high-load systems;— Exposure to advertising/adtech domain;— Hands on experience with ML concepts: regression and classification, clustering, feature selection, feature engineering, curse of dimensionality, bias-variance tradeoff, neural networks, SVMs, etc.;— Understanding of true model validation, p-value and equivalent. What we offer:— A career in one of the most ambitious, high-potential IT companies in Ukraine;— A new comfortable office near the city center;— A key role in our quest to take over the global digital advertising market;— A diverse work experience with extensive business trips;— A ‘Premium’-level health insurance package.",dou,2020-11-12,Lead Data Scientist@MGID,,,{}
SoftServe,https://jobs.dou.ua/companies/softserve/,"Lead Big Data Architect (AWS), ID 57771",https://jobs.dou.ua/companies/softserve/vacancies/136780/," Kyiv, Kharkiv, Lviv, Dnipro, Warsaw (Poland), Wroclaw (Poland), remote",29 October 2020,,"WE ARE The largest Big Data & AI Center of Excellence in Eastern Europe. Our expertise in Machine Learning, AI, Big Data, IoT, Cloud technologies is recognized by Google and Amazon.We are transforming the way thousands of global organizations do business by creating the most innovative solutions using proven architecture design methodologies, innovation frameworks, and experience design approaches. Our group successfully delivered 50+ discovery & consulting projects and 75+ implementation projects in 2019. Together with the SEI, Carnegie Mellon University, we invented Smart Decisions game, in order to promote architecture design methodologies, intended to facilitate the design of complex architectures.Please read more at smartdecisionsgame.com YOU ARE A seasoned Data Architect with hands-on experience in building large-scale Data Platforms and decision-support systems, with a proven record of success in delivering Big Data & Analytics solutions for different businesses, having an excellent understanding of distributed computing, architectural tactics and patterns; demonstrating strong interpersonal and presentation skills. With extensive experience in • Designing and implementing enterprise-scale solutions in the AWS, GCP, or Azure clouds • Hadoop stack, NoSQL, MPP technologies, processing frameworks, or other Big Data technology areas • One of the following programming languages: Java, Scala, Python, or strong SQL • Building large-scale Data Platforms and decision-support systems, covering at least one of the following areas: data ingestion, consolidation, streaming, batching, Data Lakes, Data Warehousing or analytics platforms • Designing sustainable architectures, performing trade-off analysis of different architecture tactics and patterns and applying proven architecture design approaches and methodologies • Driving projects roll-outs from requirements gathering to go-live YOU WANT TO • Design and implement sustainable architectures, create innovative Big Data and AI solutions • Evaluate cutting-edge Big Data technologies, implement PoCs and MVPs, utilize rapid prototyping techniques to accelerate time-to-market for our Clients • Deliver business value, leveraging available data assets, Big Data technologies, ML and AI • Adopt new technologies and methodologies, define and promote best practices on your projects • Demonstrate leadership and motivate project team • Perform trade-off analysis of different architecture tactics and patterns, provide justification of architectural decisions • Manage risk and make decision regarding timelines and deliverables, other commitments associated with project scope TOGETHER WE WILL • Engage with new customers and build relationships with different business stakeholders, work closely with Sales team • Become a trusted advisor and solution provider for different organizations • Implement scalable Data Platforms and decision-support systems • Support your technical and personal growth — we have a dedicated career plan for all roles in our company • Take part in internal and external events where you can build and promote your personal brand Your benefits • Assimilate best practices from experts, working in the team of top-notch Architects • Work with the world-leading companies • Being a part of Center of Excellence, you will have a chance to address different business and technology challenges, drive multiple projects and initiatives • Attend and speak at international events",dou,2020-11-12,"Lead Big Data Architect (AWS), ID 57771@SoftServe",,,{}
Zoolatech,https://jobs.dou.ua/companies/zoolatech/,Regular Data Engineer,https://jobs.dou.ua/companies/zoolatech/vacancies/136770/, Kyiv,29 October 2020,,"Required skills Must have:— Classical Data Warehouse, Data structures, Data marked, Data profiling— 3+ years of experience writing advanced SQL— Experience with one of Java, Python or Scala— Experience with ETL As a plus — Cloud Computing Experience (e.g AWS, Azure)— Experience with Kafka, Presto, Hive, Impala or similar SQL based engine for Big Data— Experience with Hadoop— Experience with Spark We offer TeamOur team is built on excellent working experience, open-mindedness, and togetherness. We are open to discussion, helpful and ready to share our expertise with the newcomers. Education and DevelopmentWe encourage our employees to participate in workshops, conferences, and meetups. We organize our own, and we partly compensate the cost of external events and English courses for our employees. Social PackageWe offer fully-paid sick leaves, medical insurance, gym compensation, and a weekly catch up with a therapist in our office. OfficeWe have a modern and cozy office near the city center with a free bus from Palats Ukraine metro station. Technical EventsWe are organizing ZoolaTalks and Developers’ Days, where are meetups for employees, where experienced engineers share their cases, solutions, and ideas. We go deeper into knowledge-sharing as it helps to build strong teams with deep expertise. Career opportunitiesZoolatech has grown twice in the last year, and there are a lot of opportunities for the employees’ career development. We encourage our colleagues to evolve and to grow along in line with the company. Responsibilities • Write clean, maintainable code, perform peer code-reviews• Be responsible for arch design, writing code, testing, deployment and support of new application• Partner with team on finding new ways to solve technical and business problems• Eager to learn new languages (Python, Golang)• Production support activities Project description Nordstrom Data Technology is pivotal to the Nordstrom customer experience. As a Data Engineer you will own the design and build the next generation of the Nordstrom Analytics Platform and deliver on the strategic vision of Customer focused features and services to enhance “The Nordstrom Customer Service Excellence”.A Data Engineer is a key part of Nordstrom Technology team that applies scientific, mathematical and social principles to design, build, and maintain technology products, devices, systems and solutions.The ideal candidate has experience in data transformation and modeling. The team is working on producing events to Kafka as part of Nordstrom Analytics Platform. Technologies used on the project: Java, Spark, Kafka, Hadoop cluster, AWS, TeraData, Tableau",dou,2020-11-12,Regular Data Engineer@Zoolatech,,,"{""Required skills"": [""Must have:"", ""Classical Data Warehouse, Data structures, Data marked, Data profiling"", ""3+ years of experience writing advanced SQL"", ""Experience with one of Java, Python or Scala"", ""Experience with ETL""], ""As a plus"": [""Cloud Computing Experience (e.g AWS, Azure)"", ""Experience with Kafka, Presto, Hive, Impala or similar SQL based engine for Big Data"", ""Experience with Hadoop"", ""Experience with Spark""], ""We offer"": [""TeamOur team is built on excellent working experience, open-mindedness, and togetherness. We are open to discussion, helpful and ready to share our expertise with the newcomers.""], ""Responsibilities"": [""Education and DevelopmentWe encourage our employees to participate in workshops, conferences, and meetups. We organize our own, and we partly compensate the cost of external events and English courses for our employees.""], ""Project description"": [""Social PackageWe offer fully-paid sick leaves, medical insurance, gym compensation, and a weekly catch up with a therapist in our office.""]}"
Zoolatech,https://jobs.dou.ua/companies/zoolatech/,Senior Bigdata Engineer,https://jobs.dou.ua/companies/zoolatech/vacancies/136767/, Kyiv,29 October 2020,,"Required skills Must have:— Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets— One or more of the following data processing technologies: Hadoop Map Reduce, Spark, Kafka Streams, Flink, Storm, Apache Beam— Good object-oriented design and development skills— Some Experience with Kafka, Presto, Hive, Impala or similar SQL based engine for Big Data— Some experience with at least one of Java, Scala, Python, JavaScript, or other modern language stack As a plus 1. Cloud Experience2. Kubernetes Experience3. Understanding of automation and orchestration platforms such as Airflow4. Understanding of various data formats such as JSON, Parquet, Avro, ORC5. Good SQL writing skills6. Good DW experience We offer TeamOur team is built on excellent working experience, open-mindedness, and togetherness. We are open to discussion, helpful and ready to share our expertise with the newcomers. Education and DevelopmentWe encourage our employees to participate in workshops, conferences, and meetups. We organize our own, and we partly compensate the cost of external events and English courses for our employees. Social PackageWe offer fully-paid sick leaves, medical insurance, gym compensation, and a weekly catch up with a therapist in our office. OfficeWe have a modern and cozy office near the city center with a free bus from Palats Ukraine metro station. Technical EventsWe are organizing ZoolaTalks and Developers’ Days, where are meetups for employees, where experienced engineers share their cases, solutions, and ideas. We go deeper into knowledge-sharing as it helps to build strong teams with deep expertise. Career opportunitiesZoolatech has grown twice in the last year, and there are a lot of opportunities for the employees’ career development. We encourage our colleagues to evolve and to grow along in line with the company. Responsibilities A day in the life of an Engineer on the team...· Design, develop, test, and debug large scale complex platform using big data technologies· Create high-performance Nordstrom data analytics platform· Possesses deep proficiency in engineering best practices· Support the development and evolution of Biddata platform and data warehouse· Partner with other Technology teams, Program, and Product on building a best-in-class suite of tools and reporting mechanisms to bring the most salient, insightful data more directly into key business functions· Take part in the production support Project description NAP (Nordstrom Analytical Platform) is an ecosystem, which integrates business events from the various source system, schematize them into object model and persist them in a query-able profile data structure.We are responsible for developing and building Analytical and Reporting solutions, Provide data to internal applications and external vendors, and partner with the Business and Data Science teams to improve business performance.",dou,2020-11-12,Senior Bigdata Engineer@Zoolatech,,,"{""Required skills"": [""Must have:"", ""Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets"", ""One or more of the following data processing technologies: Hadoop Map Reduce, Spark, Kafka Streams, Flink, Storm, Apache Beam"", ""Good object-oriented design and development skills"", ""Some Experience with Kafka, Presto, Hive, Impala or similar SQL based engine for Big Data"", ""Some experience with at least one of Java, Scala, Python, JavaScript, or other modern language stack""], ""As a plus"": [""1. Cloud Experience2. Kubernetes Experience3. Understanding of automation and orchestration platforms such as Airflow4. Understanding of various data formats such as JSON, Parquet, Avro, ORC5. Good SQL writing skills6. Good DW experience""], ""We offer"": [""TeamOur team is built on excellent working experience, open-mindedness, and togetherness. We are open to discussion, helpful and ready to share our expertise with the newcomers.""], ""Responsibilities"": [""Education and DevelopmentWe encourage our employees to participate in workshops, conferences, and meetups. We organize our own, and we partly compensate the cost of external events and English courses for our employees.""], ""Project description"": [""Social PackageWe offer fully-paid sick leaves, medical insurance, gym compensation, and a weekly catch up with a therapist in our office.""]}"
Very Good Security,https://jobs.dou.ua/companies/very-good-group/,Data Engineer (Python),https://jobs.dou.ua/companies/very-good-group/vacancies/127646/," Kyiv, Lviv, remote",29 October 2020,,"Required skills — 5+ years of software development experience- ideally at a product company.— 3+ years of experience architecting, building and supporting scalable as well as fault-tolerant batch, real-time and/or near-real-time data pipelines. — 3+ years of experience working with big data ecosystem tools such as Kafka, Protobuf/Thrift, and Spark/Flink/Storm 2.0.— 3+ years experience with data-flow programming tools such as Apache NiFi, Apache Beam, etc.— Strong knowledge of data modeling experience with both relational and NoSQL databases— Hands-on experience with data warehouses, preferably AWS Redshift— Expert knowledge of SQL and Python. — Knowledge and practical experience with Docker, Terraform/CloudFormation, and the AWS stack: EC2, Kinesis, Lambda, etc.— Ability to work independently to deliver well-designed, high-quality, and testable code on time.— English — upper-intermediate/advanced As a plus — Java or Golang experience— Experience working with CI/CD tools (such as CircleCI, Jenkins, etc.).— Understanding and hands-on experience with Kubernetes— Open source projects on GitHub We offer — Silicon Valley Experience;— 3 weeks of paid vacation and 2 weeks of days off+sick leaves;— Hackers’ days;— Corporate retreats;— Paid lunches and parking;— Covering professional learning: conferences, trainings, and other events;— Sports activities compensation;— English Speaking Club with native speakers;— Medical insurance;— VGS stock options. Responsibilities — Create and maintain optimal data pipeline architecture.— Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS technologies.— Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics.— Maintain a high culture of code — rigorous testing and automation. Improve test coverage of code that you do not own.— Be proactive and innovative. We rely on your feedback to build up the product expertise.— Work with stakeholders including the Executive, Product, Data, and Design teams to assist with data-related technical issues and support their data infrastructure needs. Project description At Very Good Security (“VGS”) we are on a mission to protect the world’s sensitive data — and we’d love to have you along for this journey. VGS was founded by highly successful repeat entrepreneurs and is backed by world-class investors like Goldman Sachs, Andreessen Horowitz, and Visa. We are building an amazing global team spread across four cities. As a young and growing company, we are laser-focused on delighting our customers and hiring talented and entrepreneurial-minded individuals. We’re looking for a Senior Data Engineer with an equal flair for creative problem solving, new technologies enthusiasm, and desire to contribute to product development.",dou,2020-11-12,Data Engineer (Python)@Very Good Security,,,"{""Required skills"": [""5+ years of software development experience- ideally at a product company."", ""3+ years of experience architecting, building and supporting scalable as well as fault-tolerant batch, real-time and/or near-real-time data pipelines."", ""3+ years of experience working with big data ecosystem tools such as Kafka, Protobuf/Thrift, and Spark/Flink/Storm 2.0."", ""3+ years experience with data-flow programming tools such as Apache NiFi, Apache Beam, etc."", ""Strong knowledge of data modeling experience with both relational and NoSQL databases"", ""Hands-on experience with data warehouses, preferably AWS Redshift"", ""Expert knowledge of SQL and Python."", ""Knowledge and practical experience with Docker, Terraform/CloudFormation, and the AWS stack: EC2, Kinesis, Lambda, etc."", ""Ability to work independently to deliver well-designed, high-quality, and testable code on time."", ""English"", ""upper-intermediate/advanced""], ""As a plus"": [""Java or Golang experience"", ""Experience working with CI/CD tools (such as CircleCI, Jenkins, etc.)."", ""Understanding and hands-on experience with Kubernetes"", ""Open source projects on GitHub""], ""We offer"": [""Silicon Valley Experience;"", ""3 weeks of paid vacation and 2 weeks of days off+sick leaves;"", ""Hackers\u2019 days;"", ""Corporate retreats;"", ""Paid lunches and parking;"", ""Covering professional learning: conferences, trainings, and other events;"", ""Sports activities compensation;"", ""English Speaking Club with native speakers;"", ""Medical insurance;"", ""VGS stock options.""], ""Responsibilities"": [""Create and maintain optimal data pipeline architecture."", ""Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS technologies."", ""Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics."", ""Maintain a high culture of code"", ""rigorous testing and automation. Improve test coverage of code that you do not own."", ""Be proactive and innovative. We rely on your feedback to build up the product expertise."", ""Work with stakeholders including the Executive, Product, Data, and Design teams to assist with data-related technical issues and support their data infrastructure needs.""], ""Project description"": [""At Very Good Security (\u201cVGS\u201d) we are on a mission to protect the world\u2019s sensitive data"", ""and we\u2019d love to have you along for this journey. VGS was founded by highly successful repeat entrepreneurs and is backed by world-class investors like Goldman Sachs, Andreessen Horowitz, and Visa. We are building an amazing global team spread across four cities. As a young and growing company, we are laser-focused on delighting our customers and hiring talented and entrepreneurial-minded individuals.""]}"
Data Science UA,https://jobs.dou.ua/companies/data-science-ua/,Senior Data Analyst,https://jobs.dou.ua/companies/data-science-ua/vacancies/128481/, Kyiv,29 October 2020,,"Required skills ● Marketing / sales analytics ● Good knowledge of A/B, A/A tests. ● SQL, Excel, R / Python, Tableau / Power BI / etc. and also you: ● Proactive (not afraid to be initiative and suggest new non-standard solutions) ● You work for results● Ambitious (you want to be the best at everything you do) ● Think positively (“this is not a problem — this is an opportunity”) ● Open to new challenges We offer ● Flexible schedule.● Professional development.● Compensation for sports activities (there is a corporate football team).● Medical insurance policy for all Elite level employees.● Access to literature, trainings, seminars.● Participation in key events in the IT industry (both in Ukraine and abroad).● Working in a cool team. Responsibilities Do you like to analyze and draw conclusions? You also know how to answer these questions:● How to assess the quality of traffic in the first hours of a transactional premium business (the yield curve is stretched over years, the% of conversion to paysite is low)?● How fast can you make a decision to disable ads / raise budgets?● Should an A / B test be assessed on old users with different lifespan / content consumption rates?● How to identify the target user by their behavioral characteristics? Project description Our partner is an IT product company in the Social Discovery , develop products that are used by millions of people around the world.Their main values are people and team. They are constantly on the lookout for result-oriented professionals who will directly impact the product and business. Every year the team achieves its goals and goes on vacation. Their team already visited the resort of Slovakia, Egypt, Austria.In September 2019, an internal startup was launched and now they need a person to strengthen the cross-functional senior team (6 FTE), following a data-driven approach in product development",dou,2020-11-12,Senior Data Analyst@Data Science UA,,,"{""Required skills"": [""\u25cf Marketing / sales analytics \u25cf Good knowledge of A/B, A/A tests. \u25cf SQL, Excel, R / Python, Tableau / Power BI / etc. and also you: \u25cf Proactive (not afraid to be initiative and suggest new non-standard solutions) \u25cf You work for results\u25cf Ambitious (you want to be the best at everything you do) \u25cf Think positively (\u201cthis is not a problem"", ""this is an opportunity\u201d) \u25cf Open to new challenges""], ""We offer"": [""\u25cf Flexible schedule.\u25cf Professional development.\u25cf Compensation for sports activities (there is a corporate football team).\u25cf Medical insurance policy for all Elite level employees.\u25cf Access to literature, trainings, seminars.\u25cf Participation in key events in the IT industry (both in Ukraine and abroad).\u25cf Working in a cool team.""], ""Responsibilities"": [""Do you like to analyze and draw conclusions? You also know how to answer these questions:\u25cf How to assess the quality of traffic in the first hours of a transactional premium business (the yield curve is stretched over years, the% of conversion to paysite is low)?\u25cf How fast can you make a decision to disable ads / raise budgets?\u25cf Should an A / B test be assessed on old users with different lifespan / content consumption rates?\u25cf How to identify the target user by their behavioral characteristics?""], ""Project description"": [""Our partner is an IT product company in the Social Discovery , develop products that are used by millions of people around the world.Their main values are people and team. They are constantly on the lookout for result-oriented professionals who will directly impact the product and business. Every year the team achieves its goals and goes on vacation. Their team already visited the resort of Slovakia, Egypt, Austria.In September 2019, an internal startup was launched and now they need a person to strengthen the cross-functional senior team (6 FTE), following a data-driven approach in product development""]}"
Metamova,https://jobs.dou.ua/companies/metamova/,NLP Engineer,https://jobs.dou.ua/companies/metamova/vacancies/132884/, Lviv,29 October 2020,,"Required skills — 1+ years experience with Python, including engineering experience (API solutions etc.)— familiarity with dynamic programming— experience with NLTK, possibly Spacy or StanfordNLP— understanding of linguistics— understanding of statistical and machine learning methods— advanced English As a plus We want to see your code / how you think. An active github / medium is a plus! Experience with NER systems, co-reference resolution, other information extraction tasksExperience with neural networks, particularly in transfer learningExperience working in an international team developing a client-facing application We offer An interesting project with space to innovateA friendly, supportive teamFlexible work hours / work placeGood compensation Responsibilities Developing NLP algorithmsEvaluating, debugging and maintaining a NER systemFine-tuning and evaluating classifiersOther information extraction tasks Project description Metamova is looking for an NLP engineer to help with different NLP projects. In particular, right now we need help expanding, improving and maintaining a NER system for a fintech client. Advanced English is important, as well as having a very good grasp on linguistics.",dou,2020-11-12,NLP Engineer@Metamova,,,"{""Required skills"": [""1+ years experience with Python, including engineering experience (API solutions etc.)"", ""familiarity with dynamic programming"", ""experience with NLTK, possibly Spacy or StanfordNLP"", ""understanding of linguistics"", ""understanding of statistical and machine learning methods"", ""advanced English""], ""As a plus"": [""We want to see your code / how you think. An active github / medium is a plus!""], ""We offer"": [""Experience with NER systems, co-reference resolution, other information extraction tasksExperience with neural networks, particularly in transfer learningExperience working in an international team developing a client-facing application""], ""Responsibilities"": [""An interesting project with space to innovateA friendly, supportive teamFlexible work hours / work placeGood compensation""], ""Project description"": [""Developing NLP algorithmsEvaluating, debugging and maintaining a NER systemFine-tuning and evaluating classifiersOther information extraction tasks""]}"
Data Science UA,https://jobs.dou.ua/companies/data-science-ua/,DevOps Engineer (with Python),https://jobs.dou.ua/companies/data-science-ua/vacancies/132695/, Kyiv,28 October 2020,,"Required skills What you need to be successful:• Professional experience building applications in Python.• Experience with AWS.• Experience with current DevOps technologies and practices — Kubernetes, Ansible, Prometheus.• Experience setting up logging, monitoring and metrics for applications.• Experience with CI/CD frameworks, e.g. Circle CI, BuildKite, Jenkins, TeamCity.• Good communications skills, ability to work collaboratively in a team environment, and enjoyment of learning from and teaching other team new skills. Would be plus (any of them):• Experience with modern build systems, e.g. Bazel, Maven, Gradle.• Experience with JVM. We offer • Flexible working — We give you the option to Work from Home or better still you can work from our office for 2 weeks in total per year.• Sharing culture — If you have learned something new, we welcome you to share it with the team through a short presentation.• Diversity — You will be working in global diverse teams with intelligent and like-minded individuals.• Days off — You will get 20 days holiday per year plus 2 bonus days off including your birthday and all 11 of the Ukraine public holidays off. Responsibilities What you will be doing:• Improve existing tooling, evaluate and developing new ones.• Large focus on building scalable solutions that will support the growth of the business.• Maximum uptime to both our internal and external customers.• Evaluation of AWS-provided tools and services.• Improve and maintain current CI/CD framework.• Participate in application and infrastructure planning, testing and development.• Constantly improve our monitoring and performance of services and tools. Project description We are looking for a Tools & Infrastructure Software Engineer for our partner company. This guy will work as a part of a team of multi-disciplined engineers, responsible for enabling the efficient, well-organised and robust flow of data through their systems, all the way from capture through to processing and delivery.",dou,2020-11-12,DevOps Engineer (with Python)@Data Science UA,,,"{""Required skills"": [""What you need to be successful:"", ""Professional experience building applications in Python."", ""Experience with AWS."", ""Experience with current DevOps technologies and practices"", ""Kubernetes, Ansible, Prometheus."", ""Experience setting up logging, monitoring and metrics for applications."", ""Experience with CI/CD frameworks, e.g. Circle CI, BuildKite, Jenkins, TeamCity."", ""Good communications skills, ability to work collaboratively in a team environment, and enjoyment of learning from and teaching other team new skills.""], ""We offer"": [""Would be plus (any of them):"", ""Experience with modern build systems, e.g. Bazel, Maven, Gradle."", ""Experience with JVM.""], ""Responsibilities"": [""Flexible working"", ""We give you the option to Work from Home or better still you can work from our office for 2 weeks in total per year."", ""Sharing culture"", ""If you have learned something new, we welcome you to share it with the team through a short presentation."", ""Diversity"", ""You will be working in global diverse teams with intelligent and like-minded individuals."", ""Days off"", ""You will get 20 days holiday per year plus 2 bonus days off including your birthday and all 11 of the Ukraine public holidays off.""], ""Project description"": [""What you will be doing:"", ""Improve existing tooling, evaluate and developing new ones."", ""Large focus on building scalable solutions that will support the growth of the business."", ""Maximum uptime to both our internal and external customers."", ""Evaluation of AWS-provided tools and services."", ""Improve and maintain current CI/CD framework."", ""Participate in application and infrastructure planning, testing and development."", ""Constantly improve our monitoring and performance of services and tools.""]}"
SD Solutions,https://jobs.dou.ua/companies/sd-solutions/,Data Scientist,https://jobs.dou.ua/companies/sd-solutions/vacancies/136634/," Kyiv, remote",28 October 2020,,"Required skills M.Sc. (with thesis) or Ph.D. in Computer Science, Mathematics, Physics or related fieldProgramming experience in Python/MatlabExperience of 2+ years in data science working in a production environmentStrong English and communication skills (both speech and writing)Highly motivated and positive-attituded personTeam playerExperience with agile development methodologyExcellent English verbal and written communication skillsOutstanding communication skillsPassion for data As a plus Delivery orientedKnowledge of Spark, Kubernetes, Ray, Tune is an advantageKnowledge of Sklearn, Pandas, TimeseriesExperience with JavaExperience with Signal ProcessingExperience in the field of predictive maintenanceData scienceMachine learning algorithmsData engineeringDeep learningCS algorithmsPython /MatlabSignal processing We offer Be part of a young growing company;Build great products with emerging technologies;Work with the sharp and success-oriented team;Gain experience in one of the hottest technological markets;Competitive compensation and social packages;Comfortable office in Kyiv Center;Food supply in the kitchen e.g. fruits, cookiesFlexible working schedule,21 calendar days of paid vacation Responsibilities Bringing solutions from research into productionDeveloping AutoML components for predictive modelingDeveloping methods to explain prediction models of industrial failuresDeveloping unsupervised approaches for anomaly detectionMaintain and support product development and product users Project description On behalf of Presenso, part of SKF group, we are looking for a talented, dynamic, high impact data-scientist with full motivation and energy to be part of the Data-Science department. As such, you will be a part of an international team that designing and developing AI solutions for the predictive maintenance ecosystem in SKF.industrial-ai.skf.com",dou,2020-11-12,Data Scientist@SD Solutions,,,"{""Required skills"": [""M.Sc. (with thesis) or Ph.D. in Computer Science, Mathematics, Physics or related fieldProgramming experience in Python/MatlabExperience of 2+ years in data science working in a production environmentStrong English and communication skills (both speech and writing)Highly motivated and positive-attituded personTeam playerExperience with agile development methodologyExcellent English verbal and written communication skillsOutstanding communication skillsPassion for data""], ""As a plus"": [""Delivery orientedKnowledge of Spark, Kubernetes, Ray, Tune is an advantageKnowledge of Sklearn, Pandas, TimeseriesExperience with JavaExperience with Signal ProcessingExperience in the field of predictive maintenanceData scienceMachine learning algorithmsData engineeringDeep learningCS algorithmsPython /MatlabSignal processing""], ""We offer"": [""Be part of a young growing company;Build great products with emerging technologies;Work with the sharp and success-oriented team;Gain experience in one of the hottest technological markets;Competitive compensation and social packages;Comfortable office in Kyiv Center;Food supply in the kitchen e.g. fruits, cookiesFlexible working schedule,21 calendar days of paid vacation""], ""Responsibilities"": [""Bringing solutions from research into productionDeveloping AutoML components for predictive modelingDeveloping methods to explain prediction models of industrial failuresDeveloping unsupervised approaches for anomaly detectionMaintain and support product development and product users""], ""Project description"": [""On behalf of Presenso, part of SKF group, we are looking for a talented, dynamic, high impact data-scientist with full motivation and energy to be part of the Data-Science department. As such, you will be a part of an international team that designing and developing AI solutions for the predictive maintenance ecosystem in SKF.industrial-ai.skf.com""]}"
SoftServe,https://jobs.dou.ua/companies/softserve/,"Data Scientist (Advanced Analytics), ID 57786",https://jobs.dou.ua/companies/softserve/vacancies/136608/," Kyiv, remote",28 October 2020,,"WE ARE Transforming the way thousands of global organizations do business by developing the most innovative technologies and processes in Big Data, Internet of Things (IoT), Data Science, and experience design. We are one of the best and oldest Data Science teams in Ukraine and you will get tons of experience working with the best talents in the field.We are a Data Science Center of Excellence and you will have a chance to contribute to the wide range of projects from different areas and technologies. We’re looking for you, a person who is inspired by data, analytics, AI as we do, and who wants to grow with us! YOU ARE A Data Scientist who will help us discover the information hidden in vast amounts of data and make smarter decisions to deliver even better products. Your primary focus will be to work on Data Science related problems for different business domains, implement and support predictive models, apply data mining techniques, provide analytical support in the pipeline of Machine Learning models productionalization process, communicate with the world-leading companies from our logo portfolio.You should be strongly competent in Applied Statistical Data Analysis, proficient in solving Data Mining and Machine Learning problems with application to the Predictive and Prescriptive analytics techniques, and qualified in Deep Learning techniques application. You have in-depth knowledge of all mandatory technical and logical areas of the practice, understand all technical and logical areas of the practice, and possess your own researches and publications. A candidate demonstrating such experience and abilities as • Ph.D./Master’s Degree in a quantitative field (Maths, Statistics, Computer Science, Engineering, Data Science, Operations Research, etc.)• Extensive knowledge and practical experience in Applied Statistics (Exploratory Data Analysis and Distributions Fitting, Inference on Populations, Regression Analysis, Factor Analysis, and Dimensional Reduction) and Data Mining (Clustering, Frequent Pattern Mining, Outliers Detection) • Advanced knowledge and hands-on experience in Advanced Data Analysis (Casuality, Data Analysis in Social Science, Network Analysis) and Predictive analytics (Regression Models, Time-series analysis and forecasting, Survival or duration analysis)• Strong knowledge of Machine learning applications and Deep Learning• Experience working on projects of complex scope where independent judgment is used within a broad range of defined procedures and practices• Understanding of software development company functioning• Concept of software development life cycles• Upper-Intermediate English level or higher (oral/written)Your extra power • Good understanding of Digital Signal Processing and Prescriptive Analytics (what-if analysis, Decision-tree based model, fuzzy inference systems)• Knowledge in Big Data solutions and advanced data mining tools• Expertise in Data Science Specializations (Natural Language Processing, Recommender Systems) • Experience with Machine Learning/Advanced analytics Cloud Platforms• Understanding of machine learning productionalization processes• Advanced proficiency in statistical programming software, (e.g, R, Python, SAS, Matlab, etc.) • Data Warehousing, Collection and Transformation (Databases, database systems, SQL and NoSQL)Development Tools (SVN, Git) YOU WANT TO WORK WITH • Full-stack Data Analysis, Data Mining, Predictive Analytics, Machine Learning models pipeline, and deep analysis of customer data collected. It includes making decisions regarding relevant computational tools for study, experiment, or trial research objectives• The production of clear, concise, well-organized, and error-free computer programs, statistical reporting with the appropriate technological stack• Compiling and interpreting analysis results as well as contributing to the reports delivered to the customer• The application of appropriate Data Mining and Predictive analytics methods TOGETHER WE WILL • Maintain synergy of Data Scientists, DevOps team, and ML Engineers to build infrastructure, set up processes, productize machine learning pipelines, and integrate them into existing business environments• Participate in international events• Get certifications on cutting-edge technologies• Have the ability to work with the latest modern tools and technologies on different projects• Get access to strong educational and mentorship programs• Communicate with the world-leading companies from our logos portfolio• Work as a consultant on different projects with a flexible schedule",dou,2020-11-12,"Data Scientist (Advanced Analytics), ID 57786@SoftServe",,,{}
N-iX,https://jobs.dou.ua/companies/n-ix/,Senior Data Engineer,https://jobs.dou.ua/companies/n-ix/vacancies/129333/," Kyiv, Lviv, remote",28 October 2020,,"N-iX is looking for an experienced Data Engineer who will build data applications to solve product problems. In this role, you’ll partner closely with data analysts and product team to create the technology that generates and transforms data into applications, insights and experiences for users.​Technologies stack: Python, Spark, Pandas, SQL, Git, AWS Cloud, Redshift. Responsibilities:• Build and rewrite existing data pipelines using Python/Go to improve efficiency and latency• Develop and automate ETL pipelines• Design data models for optimal storage and retrieval, and optimize the data architecture to meet critical product and business requirements• Improve data quality through anomaly detection by building and working with internal tools to measure data and automatically detect changes• Data Modeling and improving our existing data models for analytics Requirements:• 4+ years of relevant industry experience• Technologies: Python, Spark, Pandas, SQL, AWS Cloud, Redshift, Docker, Kubernetes, Spinnaker• Working knowledge of relational databases and data warehouses.• Bachelor’s and/or Master’s degree, preferably in CS, or equivalent experience• Demonstrated ability to analyze data sets to identify gaps and inconsistencies, provide data insights, and advance effective product solutions• Experience designing and deploying production systems with reliable monitoring and logging practices• Excellent communication skills, both written and verbal We offer:• Flexible working hours• A competitive salary and good compensation package• Possibility of partial remote work• Best hardware• A masseur and a corporate doctor• Healthcare & sport benefits• An inspiring and comfy office Professional growth:• Challenging tasks and innovative projects• Meetups and events for professional development• An individual development plan• Mentorship program Fun:• Corporate events and outstanding parties• Exciting team buildings• Memorable anniversary presents• A fun zone where you can play video games, foosball, ping pong, and more.",dou,2020-11-12,Senior Data Engineer@N-iX,,,{}
GeoGuard,https://jobs.dou.ua/companies/geoguard/,Live Service Data Analyst,https://jobs.dou.ua/companies/geoguard/vacancies/136586/, Kharkiv,28 October 2020,,"Required skills We are looking for a Live Service Data Analyst to join our team in Kharkiv! Our Live Service Data Analyst plays an important role in making sure that system updates and configuration changes are functioning as expected when deployed to Production Environments. You should be able to analyze data and gather actionable conclusions. You are comfortable and able to work under pressure and tight timelines. You enjoy working in small collaborative teams and have excellent communication and interpersonal skills. You have experience working in short, agile, iterative development cycles and have a great passion for working in fast-paced development process environments. Your skills & qualifications:— Education or training in a relevant field— Have worked in an environment monitoring Production Systems in real-time within an Incident Management Framework.— Not less than 2-3 years of experience in the data analysis— Advanced Windows or MacOS knowledge— Knowledge of Python (scikit-learn, Pandas, NumPy), MySQL, MongoDB— Knowledge of ELK— Ability to work productively with minimal direction— Good problem-solving skills— Analytical and creative thinker— Strong attention to detail— Advanced written and spoken technical English, enough to discuss issues with teams remotely We offer — Motivating compensation: high competitive salary, bonus system, corporate language classes— Personal development plan for each employee— The chance to work with a motivated and talented international team— Medical insurance— Modern office space in the center of Kharkiv, near subway station “Naukova”— We’re always stocked up on snacks and coffee and have free lunches monthly— Frequent team building events Responsibilities — Have experience reviewing live systems and flagging issues to appropriate teams via an Incident Management Framework— Design and perform manual data analysis on live production data for detection of data anomalies— Design and script reports based on requirements (often ad hoc)— Monitor releases and development to foresee any issues that may arise in future— Stay informed of configuration, deployment, and database changes to ensure analytics tools are always working as expected— Ensure future browser and operating system releases will not interfere with fraud detection solutions— Data analysis and visualisation; review production statistics and forecast potential problem areas— Organize data in an easily understandable format using graphs and pivot tables— Collaborate with teams to report issues facing production systems, particularly Incident Management, Release Management and NoC teams. Project description Since 2011, GeoGuard has been providing geolocation, anti-fraud and compliance solutions for many of the world’s largest entertainment and iGaming companies, state lotteries, streaming video broadcasters, media rightsholders and financial institutions. As a multi-award winning geolocation technology provider, GeoGuard has quickly become the markets’ trusted solution for location compliance, accuracy and reliability.",dou,2020-11-12,Live Service Data Analyst@GeoGuard,,,"{""Required skills"": [""We are looking for a Live Service Data Analyst to join our team in Kharkiv!""], ""We offer"": [""Our Live Service Data Analyst plays an important role in making sure that system updates and configuration changes are functioning as expected when deployed to Production Environments.""], ""Responsibilities"": [""You should be able to analyze data and gather actionable conclusions. You are comfortable and able to work under pressure and tight timelines. You enjoy working in small collaborative teams and have excellent communication and interpersonal skills. You have experience working in short, agile, iterative development cycles and have a great passion for working in fast-paced development process environments.""], ""Project description"": [""Your skills & qualifications:"", ""Education or training in a relevant field"", ""Have worked in an environment monitoring Production Systems in real-time within an Incident Management Framework."", ""Not less than 2-3 years of experience in the data analysis"", ""Advanced Windows or MacOS knowledge"", ""Knowledge of Python (scikit-learn, Pandas, NumPy), MySQL, MongoDB"", ""Knowledge of ELK"", ""Ability to work productively with minimal direction"", ""Good problem-solving skills"", ""Analytical and creative thinker"", ""Strong attention to detail"", ""Advanced written and spoken technical English, enough to discuss issues with teams remotely""]}"
Luxoft,https://jobs.dou.ua/companies/luxoft/,Senior GoLang engineer with Big Data,https://jobs.dou.ua/companies/luxoft/vacancies/78180/," Kyiv, remote",28 October 2020,,"Required skills MUSTGolangHadoopSpark SHOULDKafka or any other messaging systemDockerAWSHDFSHivePythonJavaShell scriptYarnApache RangerEMRDynamo DBKubernetesDevOps skillsTerraform As a plus Apache AtlasApache KnoxOktaGitLab CI/CDDisaster RecoveryNew RelicSplunkPagerduty We offer • An interesting job in one of the largest IT companies• Challenging & professional work environment within an international team• Competitive salary and opportunities for professional development• Various in-house trainings & webinars• Internal programs for professional development — mobility within the company’s projects• Wellness & Sport Center Discounts• Health Management Program• Sport Competitions, Social Activities and Community Projects• Relocation Package Responsibilities -implement new features-bugfix existing features-refactor existing code-improve test coverage-build and ship product artifacts to dev/prod cluster Project description One of the largest retail worldwide company that located in USA needs a professional, effective and result-oriented team to design, develop and support complex enterprise Big Data solutions. The team supports Product data for the Company Analytical Platform. The main goal is to organize all data related tasks across the domain: allocation, transferring, processing and storage. Also, a candidate must be ready for periodical 12 hours of on-call support.",dou,2020-11-12,Senior GoLang engineer with Big Data@Luxoft,,,"{""Required skills"": [""MUSTGolangHadoopSpark""], ""As a plus"": [""SHOULDKafka or any other messaging systemDockerAWSHDFSHivePythonJavaShell scriptYarnApache RangerEMRDynamo DBKubernetesDevOps skillsTerraform""], ""We offer"": [""Apache AtlasApache KnoxOktaGitLab CI/CDDisaster RecoveryNew RelicSplunkPagerduty""], ""Responsibilities"": [""An interesting job in one of the largest IT companies"", ""Challenging & professional work environment within an international team"", ""Competitive salary and opportunities for professional development"", ""Various in-house trainings & webinars"", ""Internal programs for professional development"", ""mobility within the company\u2019s projects"", ""Wellness & Sport Center Discounts"", ""Health Management Program"", ""Sport Competitions, Social Activities and Community Projects"", ""Relocation Package""], ""Project description"": [""-implement new features-bugfix existing features-refactor existing code-improve test coverage-build and ship product artifacts to dev/prod cluster""]}"
Luxoft,https://jobs.dou.ua/companies/luxoft/,Java Regular data engineer,https://jobs.dou.ua/companies/luxoft/vacancies/62775/," Kyiv, Dnipro",28 October 2020,,"Required skills — Experience with Java— Experience with Spark— Experience with Kafka or any Message broker (Rabbit etc)— Knowledge of design patterns Languages:English: B2 Upper Intermediate As a plus — Cloud Computing Experience (e.g AWS, Azure)— Experience with Hadoop— Experience with Scala We offer • An exciting and challenging job in a dynamic team• An opportunity to be innovative and to learn• High salary and attractive compensation package Responsibilities — Design, development and maintenance of Java server side components,— Work closely with technical leads, analysts and developers to design and implement best practice cross project solutions within a structured development process. Project description Our client is a chain of luxury department stores in the USA. The projects are in Supply Chain/Product/Customer domain. You would need to develop enterprise-grade microservices, that serve employee and company business needs. Company has its mature fine-grained engineering standards and complete list of detailed requirements applied to implemented solutions. Any implemented software should have full set of autotests, like unit, integration, system tests out of the box, that’s why TDD methodologies are used all over the way. Both RPC and messaging communication models are heavily utilized in the solutions. Only modern technology stack is used in implemented software. Kafka, Avro, Spark and AWS like DynamoDB, S3, SQS, etc are widely used across the projects.Customer have fully-fledged CI/CD processes incorporated as well as Kubernetes-over-AWS cluster for container orchestration in production environment. In your daily activity you would have to collaborate with USA engineers and analytics, to share the domain knowledge and best engineering practices, as well as collect functional and non-functional requirements. Since the customer is located in the USA (West coast), so the work schedule is shifted into evening hours (~11-20 is preferable schedule). The team is located in Kyiv and Dnipro.",dou,2020-11-12,Java Regular data engineer@Luxoft,,,"{""Required skills"": [""Experience with Java"", ""Experience with Spark"", ""Experience with Kafka or any Message broker (Rabbit etc)"", ""Knowledge of design patterns""], ""As a plus"": [""Languages:English: B2 Upper Intermediate""], ""We offer"": [""Cloud Computing Experience (e.g AWS, Azure)"", ""Experience with Hadoop"", ""Experience with Scala""], ""Responsibilities"": [""An exciting and challenging job in a dynamic team"", ""An opportunity to be innovative and to learn"", ""High salary and attractive compensation package""], ""Project description"": [""Design, development and maintenance of Java server side components,"", ""Work closely with technical leads, analysts and developers to design and implement best practice cross project solutions within a structured development process.""]}"
SoftServe,https://jobs.dou.ua/companies/softserve/,"Senior Business Analyst for Big Data project (Agriculture), ID 57821",https://jobs.dou.ua/companies/softserve/vacancies/136546/," Kyiv, remote",28 October 2020,,"WE ARE We are a team of SoftServe engineers and Architects working with one of the biggest manufacturers of industrial vehicles in the world. Our team is operating in the area of Big Data processing and building a new platform for real-time data streaming. In one direction, we are running big data processing of different types of data from IoT devices, in the other direction we are building a platform for real-time data streaming.By becoming a part of this project, you will be able to work on one of the directions and get the opportunity to learn and grow to have your proficiency recognized being a member of a great team. Project technologies: Java, Python/Go, .Net, Flink, Geomesa, Geoserver, Cassandra, Accumulo, Kafka, Kubernetes, Azure DevOps, Docker, Azure cloud Teams: Architect, TL+engineering tea, ATQC, PM, BA Methodology: Scrum, 2-week sprints YOU ARE A highly motivated Business Analyst willing to work with Big Data and IoT technologies.You possess the following experience • At least 3-year-expertise in business analysis• Working on purely BE projects• Requirements gathering and elicitation, scope and release planning• BigData or IoT expertise would be your advantage•Background working by Agile/Scrum methodologies and practices• Upper-Intermediate English level YOU WANT TO WORK WITH • Use cases gathering• Reqs gathering and prioritization with client• Developing user/business flows• Scope planning and estimation with dev team• Business Features and User stories detailed elaboration for tech and test team• Releases planning, and road maps building• Aligning scope plans with other teams, dealing with dependencies TOGETHER WE WILL • Work in a friendly atmosphere with a proficient team:)• Enjoy the ability to offer and implement our own solutions• Enable the possibility to work remotely• Participate in conferences, training sessions, seminars• Have access to robust educational and mentorship programs• Get certifications on cutting-edge technologies• Share a package of benefits (medical insurance/ additional (paid) vacation, anniversary gifts/ corporate events) and foreign language classes",dou,2020-11-12,"Senior Business Analyst for Big Data project (Agriculture), ID 57821@SoftServe",,,{}
"MODUS, DTEK LLC",https://jobs.dou.ua/companies/dtek-llc/,Big Data Engineer,https://jobs.dou.ua/companies/dtek-llc/vacancies/95955/, Kyiv,28 October 2020,,"Required skills — Опыт работы в роли инженера данных не менее 2-х лет— Практические навыки проектирования и реализации систем сбора и обработки данных— Понимание основных этапов обработки и анализа данных, инструментов и методов, применяемых в этой области— Понимание принципов распределенных вычислений— Способность осуществлять исследование данных (data exploration) и взаимодействовать с экспертами предметной области с целью достижения бизнес-целей— Глубокое знание SQL и NoSQL, опыт работы с различными СУБД (Oracle / MySQL / PostgreSQL / Cassandra и пр.)— Практический опыт работы с элементами Hadoop экосистемыApache Spark: Core, SQL— Streaming / Stream Processing— Практический опыт работы с Cloud (AWS, Azure и т.п.)— Java и/или Scala и/или Python As a plus — Apache Spark: Streaming, GraphX— Apache Ignite— Apache Flink— Apache (Confluent) Kafka / Apache Pulsar— NoSQL (Cassandra / Hbase / Kudu / Impala)— Indexing engines (Elastic Search / Solr)— Apache Hive (Tez), Pig— AWS: S3, EMR, EC2, Lambda, Kinesis/Kinesis Firehose, IAM, Athena, Glue, DynamoDB, Redshift, Aurora— Azure: Blob storage, HDInsight, VM, Functions, Event Hubs etc.— Alluxio— Gradle We offer — Официальное оформление— Конкурентный уровень з/п— Бонус по итогам года работы в компании— Работа в ультрасовременном офисе (UNIT.City)— Возможность применять самые современные технологии— Возможность профессионального роста— Медицинскую страховку по истечении трехмесячного срока Responsibilities — Разработка конвейеров данных (data pipelines) с целью обеспечения сбора, обработки и структурирования данных для последующего анализа и использования в системе— Разработка оптимальной структуры хранения, передачи, предоставления данных по конкретному продукту (подсистеме)— Разработка и внедрение эффективных механизмов и способов взаимодействия по данным между командами аналитики и ИТ инфраструктуры ДТЭК— Взаимодействие с аналитиками данных, владельцем продукта для определения требований к выгрузке, конвертации и представлению данных в рамках конвейеров данных— Взаимодействие с архитектором и руководителем группы для обеспечения качества разрабатываемых технических решений обработки данных— Участие в создании POC с нуля — проектирование, разработка и прототипирование— Участие во всех циклах разработки — дизайн, кодирование, тестирование и релиз— Проведение код ревью с целью обеспечения качества разработки— Применение концепции непрерывной интеграции и доставки (CI/CD)— Оптимизация конвейеров данных и усовершенствование архитектуры с точки зрения эффективности функционирования как отдельных элементов, так и системы в целом Project description ДТЭК начал программу цифровой трансформации бизнеса, которая охватит все основные производственные и административные процессы в компании. Для реализации цифровых проектов ДТЭК создал выделенное подразделение — Центр цифровой трансформации. Его сотрудниками станут 70 IT- и digital-специалистов, которых компания намерена привлечь на протяжении 2019 года. Центр будет базироваться в новом кампусе инновационного парка UNIT.City.В рамках реализации данной стратегии компания набирает штат инженеров данных (data engineers) в команду для создания и внедрения внутренней единой аналитической платформы на основе технологий Big Data, экосистемы Hadoop и платформ облачных вычислений. В качестве инженеров данных мы ищем специалистов в области разработки программных решений для данных с практическими навыками работы с Hadoop, Spark, NoSQL и экосистемой Big Data с открытым исходным кодом. Нам нужны увлеченные инженеры для реализации и внедрения конвейеров данных, включающих в себя аналитические модели.",dou,2020-11-12,"Big Data Engineer@MODUS, DTEK LLC",,,"{""Required skills"": [""\u041e\u043f\u044b\u0442 \u0440\u0430\u0431\u043e\u0442\u044b \u0432 \u0440\u043e\u043b\u0438 \u0438\u043d\u0436\u0435\u043d\u0435\u0440\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 \u043d\u0435 \u043c\u0435\u043d\u0435\u0435 2-\u0445 \u043b\u0435\u0442"", ""\u041f\u0440\u0430\u043a\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u043d\u0430\u0432\u044b\u043a\u0438 \u043f\u0440\u043e\u0435\u043a\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u0438 \u0440\u0435\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 \u0441\u0438\u0441\u0442\u0435\u043c \u0441\u0431\u043e\u0440\u0430 \u0438 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u0434\u0430\u043d\u043d\u044b\u0445"", ""\u041f\u043e\u043d\u0438\u043c\u0430\u043d\u0438\u0435 \u043e\u0441\u043d\u043e\u0432\u043d\u044b\u0445 \u044d\u0442\u0430\u043f\u043e\u0432 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u0438 \u0430\u043d\u0430\u043b\u0438\u0437\u0430 \u0434\u0430\u043d\u043d\u044b\u0445, \u0438\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442\u043e\u0432 \u0438 \u043c\u0435\u0442\u043e\u0434\u043e\u0432, \u043f\u0440\u0438\u043c\u0435\u043d\u044f\u0435\u043c\u044b\u0445 \u0432 \u044d\u0442\u043e\u0439 \u043e\u0431\u043b\u0430\u0441\u0442\u0438"", ""\u041f\u043e\u043d\u0438\u043c\u0430\u043d\u0438\u0435 \u043f\u0440\u0438\u043d\u0446\u0438\u043f\u043e\u0432 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u043d\u044b\u0445 \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u0439"", ""\u0421\u043f\u043e\u0441\u043e\u0431\u043d\u043e\u0441\u0442\u044c \u043e\u0441\u0443\u0449\u0435\u0441\u0442\u0432\u043b\u044f\u0442\u044c \u0438\u0441\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u043d\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0445 (data exploration) \u0438 \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u043e\u0432\u0430\u0442\u044c \u0441 \u044d\u043a\u0441\u043f\u0435\u0440\u0442\u0430\u043c\u0438 \u043f\u0440\u0435\u0434\u043c\u0435\u0442\u043d\u043e\u0439 \u043e\u0431\u043b\u0430\u0441\u0442\u0438 \u0441 \u0446\u0435\u043b\u044c\u044e \u0434\u043e\u0441\u0442\u0438\u0436\u0435\u043d\u0438\u044f \u0431\u0438\u0437\u043d\u0435\u0441-\u0446\u0435\u043b\u0435\u0439"", ""\u0413\u043b\u0443\u0431\u043e\u043a\u043e\u0435 \u0437\u043d\u0430\u043d\u0438\u0435 SQL \u0438 NoSQL, \u043e\u043f\u044b\u0442 \u0440\u0430\u0431\u043e\u0442\u044b \u0441 \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u043c\u0438 \u0421\u0423\u0411\u0414 (Oracle / MySQL / PostgreSQL / Cassandra \u0438 \u043f\u0440.)"", ""\u041f\u0440\u0430\u043a\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u043e\u043f\u044b\u0442 \u0440\u0430\u0431\u043e\u0442\u044b \u0441 \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u0430\u043c\u0438 Hadoop \u044d\u043a\u043e\u0441\u0438\u0441\u0442\u0435\u043c\u044bApache Spark: Core, SQL"", ""Streaming / Stream Processing"", ""\u041f\u0440\u0430\u043a\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u043e\u043f\u044b\u0442 \u0440\u0430\u0431\u043e\u0442\u044b \u0441 Cloud (AWS, Azure \u0438 \u0442.\u043f.)"", ""Java \u0438/\u0438\u043b\u0438 Scala \u0438/\u0438\u043b\u0438 Python""], ""As a plus"": [""Apache Spark: Streaming, GraphX"", ""Apache Ignite"", ""Apache Flink"", ""Apache (Confluent) Kafka / Apache Pulsar"", ""NoSQL (Cassandra / Hbase / Kudu / Impala)"", ""Indexing engines (Elastic Search / Solr)"", ""Apache Hive (Tez), Pig"", ""AWS: S3, EMR, EC2, Lambda, Kinesis/Kinesis Firehose, IAM, Athena, Glue, DynamoDB, Redshift, Aurora"", ""Azure: Blob storage, HDInsight, VM, Functions, Event Hubs etc."", ""Alluxio"", ""Gradle""], ""We offer"": [""\u041e\u0444\u0438\u0446\u0438\u0430\u043b\u044c\u043d\u043e\u0435 \u043e\u0444\u043e\u0440\u043c\u043b\u0435\u043d\u0438\u0435"", ""\u041a\u043e\u043d\u043a\u0443\u0440\u0435\u043d\u0442\u043d\u044b\u0439 \u0443\u0440\u043e\u0432\u0435\u043d\u044c \u0437/\u043f"", ""\u0411\u043e\u043d\u0443\u0441 \u043f\u043e \u0438\u0442\u043e\u0433\u0430\u043c \u0433\u043e\u0434\u0430 \u0440\u0430\u0431\u043e\u0442\u044b \u0432 \u043a\u043e\u043c\u043f\u0430\u043d\u0438\u0438"", ""\u0420\u0430\u0431\u043e\u0442\u0430 \u0432 \u0443\u043b\u044c\u0442\u0440\u0430\u0441\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u043c \u043e\u0444\u0438\u0441\u0435 (UNIT.City)"", ""\u0412\u043e\u0437\u043c\u043e\u0436\u043d\u043e\u0441\u0442\u044c \u043f\u0440\u0438\u043c\u0435\u043d\u044f\u0442\u044c \u0441\u0430\u043c\u044b\u0435 \u0441\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435 \u0442\u0435\u0445\u043d\u043e\u043b\u043e\u0433\u0438\u0438"", ""\u0412\u043e\u0437\u043c\u043e\u0436\u043d\u043e\u0441\u0442\u044c \u043f\u0440\u043e\u0444\u0435\u0441\u0441\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e\u0433\u043e \u0440\u043e\u0441\u0442\u0430"", ""\u041c\u0435\u0434\u0438\u0446\u0438\u043d\u0441\u043a\u0443\u044e \u0441\u0442\u0440\u0430\u0445\u043e\u0432\u043a\u0443 \u043f\u043e \u0438\u0441\u0442\u0435\u0447\u0435\u043d\u0438\u0438 \u0442\u0440\u0435\u0445\u043c\u0435\u0441\u044f\u0447\u043d\u043e\u0433\u043e \u0441\u0440\u043e\u043a\u0430""], ""Responsibilities"": [""\u0420\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u043a\u043e\u043d\u0432\u0435\u0439\u0435\u0440\u043e\u0432 \u0434\u0430\u043d\u043d\u044b\u0445 (data pipelines) \u0441 \u0446\u0435\u043b\u044c\u044e \u043e\u0431\u0435\u0441\u043f\u0435\u0447\u0435\u043d\u0438\u044f \u0441\u0431\u043e\u0440\u0430, \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u0438 \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u0434\u0430\u043d\u043d\u044b\u0445 \u0434\u043b\u044f \u043f\u043e\u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0435\u0433\u043e \u0430\u043d\u0430\u043b\u0438\u0437\u0430 \u0438 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u044f \u0432 \u0441\u0438\u0441\u0442\u0435\u043c\u0435"", ""\u0420\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u043e\u043f\u0442\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0439 \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u044b \u0445\u0440\u0430\u043d\u0435\u043d\u0438\u044f, \u043f\u0435\u0440\u0435\u0434\u0430\u0447\u0438, \u043f\u0440\u0435\u0434\u043e\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u044f \u0434\u0430\u043d\u043d\u044b\u0445 \u043f\u043e \u043a\u043e\u043d\u043a\u0440\u0435\u0442\u043d\u043e\u043c\u0443 \u043f\u0440\u043e\u0434\u0443\u043a\u0442\u0443 (\u043f\u043e\u0434\u0441\u0438\u0441\u0442\u0435\u043c\u0435)"", ""\u0420\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0438 \u0432\u043d\u0435\u0434\u0440\u0435\u043d\u0438\u0435 \u044d\u0444\u0444\u0435\u043a\u0442\u0438\u0432\u043d\u044b\u0445 \u043c\u0435\u0445\u0430\u043d\u0438\u0437\u043c\u043e\u0432 \u0438 \u0441\u043f\u043e\u0441\u043e\u0431\u043e\u0432 \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u044f \u043f\u043e \u0434\u0430\u043d\u043d\u044b\u043c \u043c\u0435\u0436\u0434\u0443 \u043a\u043e\u043c\u0430\u043d\u0434\u0430\u043c\u0438 \u0430\u043d\u0430\u043b\u0438\u0442\u0438\u043a\u0438 \u0438 \u0418\u0422 \u0438\u043d\u0444\u0440\u0430\u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u044b \u0414\u0422\u042d\u041a"", ""\u0412\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0435 \u0441 \u0430\u043d\u0430\u043b\u0438\u0442\u0438\u043a\u0430\u043c\u0438 \u0434\u0430\u043d\u043d\u044b\u0445, \u0432\u043b\u0430\u0434\u0435\u043b\u044c\u0446\u0435\u043c \u043f\u0440\u043e\u0434\u0443\u043a\u0442\u0430 \u0434\u043b\u044f \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u0442\u0440\u0435\u0431\u043e\u0432\u0430\u043d\u0438\u0439 \u043a \u0432\u044b\u0433\u0440\u0443\u0437\u043a\u0435, \u043a\u043e\u043d\u0432\u0435\u0440\u0442\u0430\u0446\u0438\u0438 \u0438 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u044e \u0434\u0430\u043d\u043d\u044b\u0445 \u0432 \u0440\u0430\u043c\u043a\u0430\u0445 \u043a\u043e\u043d\u0432\u0435\u0439\u0435\u0440\u043e\u0432 \u0434\u0430\u043d\u043d\u044b\u0445"", ""\u0412\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0435 \u0441 \u0430\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u043e\u0440\u043e\u043c \u0438 \u0440\u0443\u043a\u043e\u0432\u043e\u0434\u0438\u0442\u0435\u043b\u0435\u043c \u0433\u0440\u0443\u043f\u043f\u044b \u0434\u043b\u044f \u043e\u0431\u0435\u0441\u043f\u0435\u0447\u0435\u043d\u0438\u044f \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430 \u0440\u0430\u0437\u0440\u0430\u0431\u0430\u0442\u044b\u0432\u0430\u0435\u043c\u044b\u0445 \u0442\u0435\u0445\u043d\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u0440\u0435\u0448\u0435\u043d\u0438\u0439 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u0434\u0430\u043d\u043d\u044b\u0445"", ""\u0423\u0447\u0430\u0441\u0442\u0438\u0435 \u0432 \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u0438 POC \u0441 \u043d\u0443\u043b\u044f"", ""\u043f\u0440\u043e\u0435\u043a\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435, \u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0438 \u043f\u0440\u043e\u0442\u043e\u0442\u0438\u043f\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435"", ""\u0423\u0447\u0430\u0441\u0442\u0438\u0435 \u0432\u043e \u0432\u0441\u0435\u0445 \u0446\u0438\u043a\u043b\u0430\u0445 \u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u043a\u0438"", ""\u0434\u0438\u0437\u0430\u0439\u043d, \u043a\u043e\u0434\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435, \u0442\u0435\u0441\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u0438 \u0440\u0435\u043b\u0438\u0437"", ""\u041f\u0440\u043e\u0432\u0435\u0434\u0435\u043d\u0438\u0435 \u043a\u043e\u0434 \u0440\u0435\u0432\u044c\u044e \u0441 \u0446\u0435\u043b\u044c\u044e \u043e\u0431\u0435\u0441\u043f\u0435\u0447\u0435\u043d\u0438\u044f \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430 \u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u043a\u0438"", ""\u041f\u0440\u0438\u043c\u0435\u043d\u0435\u043d\u0438\u0435 \u043a\u043e\u043d\u0446\u0435\u043f\u0446\u0438\u0438 \u043d\u0435\u043f\u0440\u0435\u0440\u044b\u0432\u043d\u043e\u0439 \u0438\u043d\u0442\u0435\u0433\u0440\u0430\u0446\u0438\u0438 \u0438 \u0434\u043e\u0441\u0442\u0430\u0432\u043a\u0438 (CI/CD)"", ""\u041e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u044f \u043a\u043e\u043d\u0432\u0435\u0439\u0435\u0440\u043e\u0432 \u0434\u0430\u043d\u043d\u044b\u0445 \u0438 \u0443\u0441\u043e\u0432\u0435\u0440\u0448\u0435\u043d\u0441\u0442\u0432\u043e\u0432\u0430\u043d\u0438\u0435 \u0430\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u044b \u0441 \u0442\u043e\u0447\u043a\u0438 \u0437\u0440\u0435\u043d\u0438\u044f \u044d\u0444\u0444\u0435\u043a\u0442\u0438\u0432\u043d\u043e\u0441\u0442\u0438 \u0444\u0443\u043d\u043a\u0446\u0438\u043e\u043d\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u043a\u0430\u043a \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u044b\u0445 \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u043e\u0432, \u0442\u0430\u043a \u0438 \u0441\u0438\u0441\u0442\u0435\u043c\u044b \u0432 \u0446\u0435\u043b\u043e\u043c""], ""Project description"": [""\u0414\u0422\u042d\u041a \u043d\u0430\u0447\u0430\u043b \u043f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u0443 \u0446\u0438\u0444\u0440\u043e\u0432\u043e\u0439 \u0442\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438 \u0431\u0438\u0437\u043d\u0435\u0441\u0430, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u043e\u0445\u0432\u0430\u0442\u0438\u0442 \u0432\u0441\u0435 \u043e\u0441\u043d\u043e\u0432\u043d\u044b\u0435 \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u0435 \u0438 \u0430\u0434\u043c\u0438\u043d\u0438\u0441\u0442\u0440\u0430\u0442\u0438\u0432\u043d\u044b\u0435 \u043f\u0440\u043e\u0446\u0435\u0441\u0441\u044b \u0432 \u043a\u043e\u043c\u043f\u0430\u043d\u0438\u0438. \u0414\u043b\u044f \u0440\u0435\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 \u0446\u0438\u0444\u0440\u043e\u0432\u044b\u0445 \u043f\u0440\u043e\u0435\u043a\u0442\u043e\u0432 \u0414\u0422\u042d\u041a \u0441\u043e\u0437\u0434\u0430\u043b \u0432\u044b\u0434\u0435\u043b\u0435\u043d\u043d\u043e\u0435 \u043f\u043e\u0434\u0440\u0430\u0437\u0434\u0435\u043b\u0435\u043d\u0438\u0435"", ""\u0426\u0435\u043d\u0442\u0440 \u0446\u0438\u0444\u0440\u043e\u0432\u043e\u0439 \u0442\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438. \u0415\u0433\u043e \u0441\u043e\u0442\u0440\u0443\u0434\u043d\u0438\u043a\u0430\u043c\u0438 \u0441\u0442\u0430\u043d\u0443\u0442 70 IT- \u0438 digital-\u0441\u043f\u0435\u0446\u0438\u0430\u043b\u0438\u0441\u0442\u043e\u0432, \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u043a\u043e\u043c\u043f\u0430\u043d\u0438\u044f \u043d\u0430\u043c\u0435\u0440\u0435\u043d\u0430 \u043f\u0440\u0438\u0432\u043b\u0435\u0447\u044c \u043d\u0430 \u043f\u0440\u043e\u0442\u044f\u0436\u0435\u043d\u0438\u0438 2019 \u0433\u043e\u0434\u0430. \u0426\u0435\u043d\u0442\u0440 \u0431\u0443\u0434\u0435\u0442 \u0431\u0430\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c\u0441\u044f \u0432 \u043d\u043e\u0432\u043e\u043c \u043a\u0430\u043c\u043f\u0443\u0441\u0435 \u0438\u043d\u043d\u043e\u0432\u0430\u0446\u0438\u043e\u043d\u043d\u043e\u0433\u043e \u043f\u0430\u0440\u043a\u0430 UNIT.City.\u0412 \u0440\u0430\u043c\u043a\u0430\u0445 \u0440\u0435\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 \u0434\u0430\u043d\u043d\u043e\u0439 \u0441\u0442\u0440\u0430\u0442\u0435\u0433\u0438\u0438 \u043a\u043e\u043c\u043f\u0430\u043d\u0438\u044f \u043d\u0430\u0431\u0438\u0440\u0430\u0435\u0442 \u0448\u0442\u0430\u0442 \u0438\u043d\u0436\u0435\u043d\u0435\u0440\u043e\u0432 \u0434\u0430\u043d\u043d\u044b\u0445 (data engineers) \u0432 \u043a\u043e\u043c\u0430\u043d\u0434\u0443 \u0434\u043b\u044f \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u044f \u0438 \u0432\u043d\u0435\u0434\u0440\u0435\u043d\u0438\u044f \u0432\u043d\u0443\u0442\u0440\u0435\u043d\u043d\u0435\u0439 \u0435\u0434\u0438\u043d\u043e\u0439 \u0430\u043d\u0430\u043b\u0438\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u043f\u043b\u0430\u0442\u0444\u043e\u0440\u043c\u044b \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u0442\u0435\u0445\u043d\u043e\u043b\u043e\u0433\u0438\u0439 Big Data, \u044d\u043a\u043e\u0441\u0438\u0441\u0442\u0435\u043c\u044b Hadoop \u0438 \u043f\u043b\u0430\u0442\u0444\u043e\u0440\u043c \u043e\u0431\u043b\u0430\u0447\u043d\u044b\u0445 \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u0439.""]}"
Parimatch Tech,https://jobs.dou.ua/companies/parimatch-tech/,Junior Data Analyst,https://jobs.dou.ua/companies/parimatch-tech/vacancies/136510/, Kyiv,27 October 2020,,"Required skills We are highly successful Company with great ambitions. We operate on a very competitive market so every day we are looking for opportunities to be better. To be faster. Even faster. Never stand aside and never afraid to try. Having a lot of own ideas we are very open for fresh ones. Equally important, we have resources to bring these into motion. As a plus — Develop and maintain ad-hoc and scheduled operational reports that assist management;— Provide ad-hoc data support on complex subjects in an expedited manner;— Participate in analytical investigations aimed at addressing specific business situations using data with an emphasis on identifying and interpreting trends. — Experience with PostgreSQL/SQL;— Technical English;— Excellent communication skills;— A proactive approach to problem-solving. — Experience with Tableau;— Experience with NoSQL;— Experience with big query. — Medical insurance/Sport compensation;— Sport club participation (football, running, basketball or swimming clubs);— 100% paid sick leaves;— 20 working days of paid vacation. — Competitive salary and сonstant encouragement for your efforts and contribution;— Bonuses according to company’s policy;— Welfare (financial support in critical situations);— Gifts for significant life events (marriage, childbirth). — Individual annual training budget with an opportunity to visit paid conferences, training sessions, workshops, etc.;— Free corporate library;— Opportunity to visit our non-stop internal meetups: open talks, IT Pump, etc. as a participant or a speaker and exchange knowledge;— A world-class team of T-shaped skilled professionals that share knowledge and support each other. — Corporate parties and events (Pub Quiz, Carquest, bowling championships, etc.);— PM Foundation activities (social responsibility events);— Weekly events aimed at culture, arts, soft skills development.",dou,2020-11-12,Junior Data Analyst@Parimatch Tech,,,"{""Required skills"": [""We are highly successful Company with great ambitions. We operate on a very competitive market so every day we are looking for opportunities to be better. To be faster. Even faster. Never stand aside and never afraid to try. Having a lot of own ideas we are very open for fresh ones. Equally important, we have resources to bring these into motion.""], ""As a plus"": [""Develop and maintain ad-hoc and scheduled operational reports that assist management;"", ""Provide ad-hoc data support on complex subjects in an expedited manner;"", ""Participate in analytical investigations aimed at addressing specific business situations using data with an emphasis on identifying and interpreting trends.""]}"
Parimatch Tech,https://jobs.dou.ua/companies/parimatch-tech/,Head of Analytics,https://jobs.dou.ua/companies/parimatch-tech/vacancies/136509/, Kyiv,27 October 2020,,"We are highly successful Company with great ambitions. We operate on a very competitive market so every day we are looking for opportunities to be better. To be faster. Even faster. Never stand aside and never afraid to try. Having a lot of own ideas we are very open for fresh ones. Equally important, we have resources to bring these into motion. We are looking for an Analytics Manager to organize our analytics function and manage our team of analysts. You will implement tools and strategies to translate raw data into valuable business insights. In this role, we expect you to have strong logical reasoning skills and business intelligence. The ability to communicate effectively is essential. If you also have solid industry experience, we’d like to meet you. Your goal will be to help our business use data to drive high performance and quality. — Develop strategies for effective data analysis and reporting; — Define company-wide metrics and relevant data sources;— Select, configure and implement analytics solutions;— Lead and develop a team of 3 data analysts;— Oversee all analytics operations to correct discrepancies and ensure quality;— Extract reports from multiple sources;— Build systems to transform raw data into actionable business insights;— Apply industry knowledge to interpret data and improve performance;— Keep abreast of industry news and trends. — Proven experience as an Analytics Manager or BI; — Solid experience in data analysis and reporting;— Industry experience is a plus Background in market research and project management; — Superb communication skills; — Analytical skills and strong organizational abilities; — Attention to detail; — Problem-solving aptitude; — BSc/BA in Computer Science, Statistics, Data Management or a related field. — Medical insurance/Sport compensation;— Sport club participation (football, running, basketball or swimming clubs);— 100% paid sick leaves;— 20 working days of paid vacation. — Competitive salary and сonstant encouragement for your efforts and contribution;— Bonuses according to company’s policy;— Welfare (financial support in critical situations);— Gifts for significant life events (marriage, childbirth). — Individual annual training budget with an opportunity to visit paid conferences, training sessions, workshops, etc.;— Free corporate library;— Opportunity to visit our non-stop internal meetups: open talks, IT Pump, etc. as a participant or a speaker and exchange knowledge;— A world-class team of T-shaped skilled professionals that share knowledge and support each other. — Corporate parties and events (Pub Quiz, Carquest, bowling championships, etc.);— PM Foundation activities (social responsibility events);— Weekly events aimed at culture, arts, soft skills development.",dou,2020-11-12,Head of Analytics@Parimatch Tech,,,{}
Parimatch Tech,https://jobs.dou.ua/companies/parimatch-tech/,Middle Manual QA Engineer for Data Platform,https://jobs.dou.ua/companies/parimatch-tech/vacancies/136508/, Kyiv,27 October 2020,,"We are highly successful Company with great ambitions. We operate on a very competitive market so every day we are looking for opportunities to be better. To be faster. Even faster. Never stand aside and never afraid to try. Having a lot of own ideas we are very open for fresh ones. Equally important, we have resources to bring these into motion. Verification of data transformations, migrations, alignments, reports using SQL;Analyze and document features of the product and business requirements;Write and maintain test cases and test scenarios;Execute exploratory, functional, acceptance, regression testing;Communicate with other departments and team members on a daily basis;Participate in meetings, make demo and knowledge sharing sessions;Estimate current activities and report results to the team;Root causes analysis of failed test and report issues/take corrective actions;Be an active member of the team, suggesting improvements of the processes to increase quality of the product and performance of the team;Participate in the creation of new automated tests (In the future). 2+ years of manual testing experience;Knowledge of desktop and web testing (DB side);Strong experience with ~SQL databases (queries);Experience and good knowledge of DWH (Data Warehouse) and ETL concepts;Experience with REST API testing;Gathering and analyzing logs (e.g. Kibana);Experience with processing technologies for data flow (ActiveMQ, rabbitMQ, Kafka, Flink, SparkStreaming, etc.)An ability to create and support tests documentation, execute test runs;Knowledge of testing theory, approaches, etc;Good problem-solving abilities and strong passion to learn new things;Understanding of Agile methodologies and engineering practices;Good analytical and communication skill. Experience in sports betting;Programming skills with any OOP languages;Experience in test automation and knowledge of test automation frameworks;Knowledge of continuous integration/delivery (TeamCity, Jenkins, etc.) and source control systems (e.g GIT);Good understanding of HTTP protocol and Client-Server architecture;Experience in non-functional tests such as Performance, Security, and Accessibility. — Medical insurance/Sport compensation;— Sport club participation (football, running, basketball or swimming clubs);— 100% paid sick leaves;— 20 working days of paid vacation. — Competitive salary and сonstant encouragement for your efforts and contribution;— Bonuses according to company’s policy;— Welfare (financial support in critical situations);— Gifts for significant life events (marriage, childbirth). — Individual annual training budget with an opportunity to visit paid conferences, training sessions, workshops, etc.;— Free corporate library;— Opportunity to visit our non-stop internal meetups: open talks, IT Pump, etc. as a participant or a speaker and exchange knowledge;— A world-class team of T-shaped skilled professionals that share knowledge and support each other. — Corporate parties and events (Pub Quiz, Carquest, bowling championships, etc.);— PM Foundation activities (social responsibility events);— Weekly events aimed at culture, arts, soft skills development.",dou,2020-11-12,Middle Manual QA Engineer for Data Platform@Parimatch Tech,,,{}
Smartiway,https://jobs.dou.ua/companies/smartiway/,Data Scientist / Risk analyst,https://jobs.dou.ua/companies/smartiway/vacancies/133035/, Kyiv,27 October 2020,$1000–2000,"Required skills 1+ года опыта работы в качестве скормейкера или на аналогичной позицииМатематическая подготовка, знание статистики, теории вероятности и смежных областейЗнание методов статистического анализа данных и построения математических моделейЗнание SQL на уровне написания сложных запросовОпыт работы с R/PythonПрактический опыт применения методов машинного обучения (регрессия, деревья решений, нейронные сети, SVM и т.д.)Способность четко формулировать свои мысли и наглядно демонстрировать результаты своей работыАналитический склад ума и быстрая обучаемостьМногозадачность As a plus 3+года опыта работы в данной сфереОпыт программирования (C/C++/C#/Java/PHP)Опыт работы с автоматизированными системами отчетностиПонимание принципов кредитования и кредитного скорингаОпыт работы в сфере разработки программного обеспеченияОпыт работы с «большими» и неструктурированными данными We offer Высокоинтенсивная «прокачка» и повышение квалификации в развивающейся компанииСовременные технологии стекаПолностью автоматизированная платформа для принятия решений и обработки с запатентованной системой рейтингования на основе машинного обучения и алгоритмов ИИScrum-based framework (Agile, Kanban, Scrumban)Медицинская страховкаФитнес залЕжемесячные командные вечеринки с барбекю и активным отдыхомПятничный покер (пиво, пицца, скотч / виски)Кофе машина, закуски, фрукты Responsibilities Разработка/оптимизация/анализ работы системы принятия решенийПостроение скоринговых моделей разной направленностиРазработка отчетностиОптимизация и автоматизации повседневных задач в компании (маркетинг, коллекшен, поддержка клиентов) средствами машинного обучения и анализа данныхТочечный анализ и исследованияНеобычные, но интересные и не однообразные задачи гарантированы. Project description О проектеПроект, который мы стараемся сделать для пользователя максимально удобным и выгодным, благодаря технологичности, 100% автоматизации и мгновенности принятия решений, самой низкой % ставкой по рынку, с очень гибкими и лояльными условиями в момент просрочки. Наш продукт первый в сфере кредитования создан mobile only, целью которого является воспитание социальной ответственности общества. Основная задача в разработке и поддержке продукта — simple & smart.Бэк и фронт части системы разрабатываются одной командой, что гарантирует бесшовную интеграцию и широчайшие возможности по накоплению и использованию данных, полученных в процессе использования приложения, а также позволит, при желании, расширить профессиональные навыки в смежных областях.",dou,2020-11-12,Data Scientist / Risk analyst@Smartiway,,,"{""Required skills"": [""1+ \u0433\u043e\u0434\u0430 \u043e\u043f\u044b\u0442\u0430 \u0440\u0430\u0431\u043e\u0442\u044b \u0432 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u0441\u043a\u043e\u0440\u043c\u0435\u0439\u043a\u0435\u0440\u0430 \u0438\u043b\u0438 \u043d\u0430 \u0430\u043d\u0430\u043b\u043e\u0433\u0438\u0447\u043d\u043e\u0439 \u043f\u043e\u0437\u0438\u0446\u0438\u0438\u041c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u043f\u043e\u0434\u0433\u043e\u0442\u043e\u0432\u043a\u0430, \u0437\u043d\u0430\u043d\u0438\u0435 \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u043a\u0438, \u0442\u0435\u043e\u0440\u0438\u0438 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0438 \u0438 \u0441\u043c\u0435\u0436\u043d\u044b\u0445 \u043e\u0431\u043b\u0430\u0441\u0442\u0435\u0439\u0417\u043d\u0430\u043d\u0438\u0435 \u043c\u0435\u0442\u043e\u0434\u043e\u0432 \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0433\u043e \u0430\u043d\u0430\u043b\u0438\u0437\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 \u0438 \u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u044f \u043c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439\u0417\u043d\u0430\u043d\u0438\u0435 SQL \u043d\u0430 \u0443\u0440\u043e\u0432\u043d\u0435 \u043d\u0430\u043f\u0438\u0441\u0430\u043d\u0438\u044f \u0441\u043b\u043e\u0436\u043d\u044b\u0445 \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u0432\u041e\u043f\u044b\u0442 \u0440\u0430\u0431\u043e\u0442\u044b \u0441 R/Python\u041f\u0440\u0430\u043a\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u043e\u043f\u044b\u0442 \u043f\u0440\u0438\u043c\u0435\u043d\u0435\u043d\u0438\u044f \u043c\u0435\u0442\u043e\u0434\u043e\u0432 \u043c\u0430\u0448\u0438\u043d\u043d\u043e\u0433\u043e \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f (\u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044f, \u0434\u0435\u0440\u0435\u0432\u044c\u044f \u0440\u0435\u0448\u0435\u043d\u0438\u0439, \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u044b\u0435 \u0441\u0435\u0442\u0438, SVM \u0438 \u0442.\u0434.)\u0421\u043f\u043e\u0441\u043e\u0431\u043d\u043e\u0441\u0442\u044c \u0447\u0435\u0442\u043a\u043e \u0444\u043e\u0440\u043c\u0443\u043b\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0441\u0432\u043e\u0438 \u043c\u044b\u0441\u043b\u0438 \u0438 \u043d\u0430\u0433\u043b\u044f\u0434\u043d\u043e \u0434\u0435\u043c\u043e\u043d\u0441\u0442\u0440\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b \u0441\u0432\u043e\u0435\u0439 \u0440\u0430\u0431\u043e\u0442\u044b\u0410\u043d\u0430\u043b\u0438\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u0441\u043a\u043b\u0430\u0434 \u0443\u043c\u0430 \u0438 \u0431\u044b\u0441\u0442\u0440\u0430\u044f \u043e\u0431\u0443\u0447\u0430\u0435\u043c\u043e\u0441\u0442\u044c\u041c\u043d\u043e\u0433\u043e\u0437\u0430\u0434\u0430\u0447\u043d\u043e\u0441\u0442\u044c""], ""As a plus"": [""3+\u0433\u043e\u0434\u0430 \u043e\u043f\u044b\u0442\u0430 \u0440\u0430\u0431\u043e\u0442\u044b \u0432 \u0434\u0430\u043d\u043d\u043e\u0439 \u0441\u0444\u0435\u0440\u0435\u041e\u043f\u044b\u0442 \u043f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f (C/C++/C#/Java/PHP)\u041e\u043f\u044b\u0442 \u0440\u0430\u0431\u043e\u0442\u044b \u0441 \u0430\u0432\u0442\u043e\u043c\u0430\u0442\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u043c\u0438 \u0441\u0438\u0441\u0442\u0435\u043c\u0430\u043c\u0438 \u043e\u0442\u0447\u0435\u0442\u043d\u043e\u0441\u0442\u0438\u041f\u043e\u043d\u0438\u043c\u0430\u043d\u0438\u0435 \u043f\u0440\u0438\u043d\u0446\u0438\u043f\u043e\u0432 \u043a\u0440\u0435\u0434\u0438\u0442\u043e\u0432\u0430\u043d\u0438\u044f \u0438 \u043a\u0440\u0435\u0434\u0438\u0442\u043d\u043e\u0433\u043e \u0441\u043a\u043e\u0440\u0438\u043d\u0433\u0430\u041e\u043f\u044b\u0442 \u0440\u0430\u0431\u043e\u0442\u044b \u0432 \u0441\u0444\u0435\u0440\u0435 \u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u043f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u043d\u043e\u0433\u043e \u043e\u0431\u0435\u0441\u043f\u0435\u0447\u0435\u043d\u0438\u044f\u041e\u043f\u044b\u0442 \u0440\u0430\u0431\u043e\u0442\u044b \u0441 \u00ab\u0431\u043e\u043b\u044c\u0448\u0438\u043c\u0438\u00bb \u0438 \u043d\u0435\u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u043c\u0438 \u0434\u0430\u043d\u043d\u044b\u043c\u0438""], ""We offer"": [""\u0412\u044b\u0441\u043e\u043a\u043e\u0438\u043d\u0442\u0435\u043d\u0441\u0438\u0432\u043d\u0430\u044f \u00ab\u043f\u0440\u043e\u043a\u0430\u0447\u043a\u0430\u00bb \u0438 \u043f\u043e\u0432\u044b\u0448\u0435\u043d\u0438\u0435 \u043a\u0432\u0430\u043b\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438 \u0432 \u0440\u0430\u0437\u0432\u0438\u0432\u0430\u044e\u0449\u0435\u0439\u0441\u044f \u043a\u043e\u043c\u043f\u0430\u043d\u0438\u0438\u0421\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435 \u0442\u0435\u0445\u043d\u043e\u043b\u043e\u0433\u0438\u0438 \u0441\u0442\u0435\u043a\u0430\u041f\u043e\u043b\u043d\u043e\u0441\u0442\u044c\u044e \u0430\u0432\u0442\u043e\u043c\u0430\u0442\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u0430\u044f \u043f\u043b\u0430\u0442\u0444\u043e\u0440\u043c\u0430 \u0434\u043b\u044f \u043f\u0440\u0438\u043d\u044f\u0442\u0438\u044f \u0440\u0435\u0448\u0435\u043d\u0438\u0439 \u0438 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u0441 \u0437\u0430\u043f\u0430\u0442\u0435\u043d\u0442\u043e\u0432\u0430\u043d\u043d\u043e\u0439 \u0441\u0438\u0441\u0442\u0435\u043c\u043e\u0439 \u0440\u0435\u0439\u0442\u0438\u043d\u0433\u043e\u0432\u0430\u043d\u0438\u044f \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u043c\u0430\u0448\u0438\u043d\u043d\u043e\u0433\u043e \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u0438 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u043e\u0432 \u0418\u0418Scrum-based framework (Agile, Kanban, Scrumban)\u041c\u0435\u0434\u0438\u0446\u0438\u043d\u0441\u043a\u0430\u044f \u0441\u0442\u0440\u0430\u0445\u043e\u0432\u043a\u0430\u0424\u0438\u0442\u043d\u0435\u0441 \u0437\u0430\u043b\u0415\u0436\u0435\u043c\u0435\u0441\u044f\u0447\u043d\u044b\u0435 \u043a\u043e\u043c\u0430\u043d\u0434\u043d\u044b\u0435 \u0432\u0435\u0447\u0435\u0440\u0438\u043d\u043a\u0438 \u0441 \u0431\u0430\u0440\u0431\u0435\u043a\u044e \u0438 \u0430\u043a\u0442\u0438\u0432\u043d\u044b\u043c \u043e\u0442\u0434\u044b\u0445\u043e\u043c\u041f\u044f\u0442\u043d\u0438\u0447\u043d\u044b\u0439 \u043f\u043e\u043a\u0435\u0440 (\u043f\u0438\u0432\u043e, \u043f\u0438\u0446\u0446\u0430, \u0441\u043a\u043e\u0442\u0447 / \u0432\u0438\u0441\u043a\u0438)\u041a\u043e\u0444\u0435 \u043c\u0430\u0448\u0438\u043d\u0430, \u0437\u0430\u043a\u0443\u0441\u043a\u0438, \u0444\u0440\u0443\u043a\u0442\u044b""], ""Responsibilities"": [""\u0420\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u043a\u0430/\u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u044f/\u0430\u043d\u0430\u043b\u0438\u0437 \u0440\u0430\u0431\u043e\u0442\u044b \u0441\u0438\u0441\u0442\u0435\u043c\u044b \u043f\u0440\u0438\u043d\u044f\u0442\u0438\u044f \u0440\u0435\u0448\u0435\u043d\u0438\u0439\u041f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u0435 \u0441\u043a\u043e\u0440\u0438\u043d\u0433\u043e\u0432\u044b\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u0440\u0430\u0437\u043d\u043e\u0439 \u043d\u0430\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u043d\u043e\u0441\u0442\u0438\u0420\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u043e\u0442\u0447\u0435\u0442\u043d\u043e\u0441\u0442\u0438\u041e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u044f \u0438 \u0430\u0432\u0442\u043e\u043c\u0430\u0442\u0438\u0437\u0430\u0446\u0438\u0438 \u043f\u043e\u0432\u0441\u0435\u0434\u043d\u0435\u0432\u043d\u044b\u0445 \u0437\u0430\u0434\u0430\u0447 \u0432 \u043a\u043e\u043c\u043f\u0430\u043d\u0438\u0438 (\u043c\u0430\u0440\u043a\u0435\u0442\u0438\u043d\u0433, \u043a\u043e\u043b\u043b\u0435\u043a\u0448\u0435\u043d, \u043f\u043e\u0434\u0434\u0435\u0440\u0436\u043a\u0430 \u043a\u043b\u0438\u0435\u043d\u0442\u043e\u0432) \u0441\u0440\u0435\u0434\u0441\u0442\u0432\u0430\u043c\u0438 \u043c\u0430\u0448\u0438\u043d\u043d\u043e\u0433\u043e \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u0438 \u0430\u043d\u0430\u043b\u0438\u0437\u0430 \u0434\u0430\u043d\u043d\u044b\u0445\u0422\u043e\u0447\u0435\u0447\u043d\u044b\u0439 \u0430\u043d\u0430\u043b\u0438\u0437 \u0438 \u0438\u0441\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u043d\u0438\u044f\u041d\u0435\u043e\u0431\u044b\u0447\u043d\u044b\u0435, \u043d\u043e \u0438\u043d\u0442\u0435\u0440\u0435\u0441\u043d\u044b\u0435 \u0438 \u043d\u0435 \u043e\u0434\u043d\u043e\u043e\u0431\u0440\u0430\u0437\u043d\u044b\u0435 \u0437\u0430\u0434\u0430\u0447\u0438 \u0433\u0430\u0440\u0430\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u044b.""], ""Project description"": [""\u041e \u043f\u0440\u043e\u0435\u043a\u0442\u0435\u041f\u0440\u043e\u0435\u043a\u0442, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u043c\u044b \u0441\u0442\u0430\u0440\u0430\u0435\u043c\u0441\u044f \u0441\u0434\u0435\u043b\u0430\u0442\u044c \u0434\u043b\u044f \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u043e \u0443\u0434\u043e\u0431\u043d\u044b\u043c \u0438 \u0432\u044b\u0433\u043e\u0434\u043d\u044b\u043c, \u0431\u043b\u0430\u0433\u043e\u0434\u0430\u0440\u044f \u0442\u0435\u0445\u043d\u043e\u043b\u043e\u0433\u0438\u0447\u043d\u043e\u0441\u0442\u0438, 100% \u0430\u0432\u0442\u043e\u043c\u0430\u0442\u0438\u0437\u0430\u0446\u0438\u0438 \u0438 \u043c\u0433\u043d\u043e\u0432\u0435\u043d\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u043d\u044f\u0442\u0438\u044f \u0440\u0435\u0448\u0435\u043d\u0438\u0439, \u0441\u0430\u043c\u043e\u0439 \u043d\u0438\u0437\u043a\u043e\u0439 % \u0441\u0442\u0430\u0432\u043a\u043e\u0439 \u043f\u043e \u0440\u044b\u043d\u043a\u0443, \u0441 \u043e\u0447\u0435\u043d\u044c \u0433\u0438\u0431\u043a\u0438\u043c\u0438 \u0438 \u043b\u043e\u044f\u043b\u044c\u043d\u044b\u043c\u0438 \u0443\u0441\u043b\u043e\u0432\u0438\u044f\u043c\u0438 \u0432 \u043c\u043e\u043c\u0435\u043d\u0442 \u043f\u0440\u043e\u0441\u0440\u043e\u0447\u043a\u0438. \u041d\u0430\u0448 \u043f\u0440\u043e\u0434\u0443\u043a\u0442 \u043f\u0435\u0440\u0432\u044b\u0439 \u0432 \u0441\u0444\u0435\u0440\u0435 \u043a\u0440\u0435\u0434\u0438\u0442\u043e\u0432\u0430\u043d\u0438\u044f \u0441\u043e\u0437\u0434\u0430\u043d mobile only, \u0446\u0435\u043b\u044c\u044e \u043a\u043e\u0442\u043e\u0440\u043e\u0433\u043e \u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u0432\u043e\u0441\u043f\u0438\u0442\u0430\u043d\u0438\u0435 \u0441\u043e\u0446\u0438\u0430\u043b\u044c\u043d\u043e\u0439 \u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u0441\u0442\u0438 \u043e\u0431\u0449\u0435\u0441\u0442\u0432\u0430. \u041e\u0441\u043d\u043e\u0432\u043d\u0430\u044f \u0437\u0430\u0434\u0430\u0447\u0430 \u0432 \u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u043a\u0435 \u0438 \u043f\u043e\u0434\u0434\u0435\u0440\u0436\u043a\u0435 \u043f\u0440\u043e\u0434\u0443\u043a\u0442\u0430"", ""simple & smart.\u0411\u044d\u043a \u0438 \u0444\u0440\u043e\u043d\u0442 \u0447\u0430\u0441\u0442\u0438 \u0441\u0438\u0441\u0442\u0435\u043c\u044b \u0440\u0430\u0437\u0440\u0430\u0431\u0430\u0442\u044b\u0432\u0430\u044e\u0442\u0441\u044f \u043e\u0434\u043d\u043e\u0439 \u043a\u043e\u043c\u0430\u043d\u0434\u043e\u0439, \u0447\u0442\u043e \u0433\u0430\u0440\u0430\u043d\u0442\u0438\u0440\u0443\u0435\u0442 \u0431\u0435\u0441\u0448\u043e\u0432\u043d\u0443\u044e \u0438\u043d\u0442\u0435\u0433\u0440\u0430\u0446\u0438\u044e \u0438 \u0448\u0438\u0440\u043e\u0447\u0430\u0439\u0448\u0438\u0435 \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e\u0441\u0442\u0438 \u043f\u043e \u043d\u0430\u043a\u043e\u043f\u043b\u0435\u043d\u0438\u044e \u0438 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u044e \u0434\u0430\u043d\u043d\u044b\u0445, \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u044b\u0445 \u0432 \u043f\u0440\u043e\u0446\u0435\u0441\u0441\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u044f \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u044f, \u0430 \u0442\u0430\u043a\u0436\u0435 \u043f\u043e\u0437\u0432\u043e\u043b\u0438\u0442, \u043f\u0440\u0438 \u0436\u0435\u043b\u0430\u043d\u0438\u0438, \u0440\u0430\u0441\u0448\u0438\u0440\u0438\u0442\u044c \u043f\u0440\u043e\u0444\u0435\u0441\u0441\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u044b\u0435 \u043d\u0430\u0432\u044b\u043a\u0438 \u0432 \u0441\u043c\u0435\u0436\u043d\u044b\u0445 \u043e\u0431\u043b\u0430\u0441\u0442\u044f\u0445.""]}"
Parimatch Tech,https://jobs.dou.ua/companies/parimatch-tech/,Data Security Manager,https://jobs.dou.ua/companies/parimatch-tech/vacancies/136485/, Kyiv,27 October 2020,,"We are highly successful Company with great ambitions. We operate on a very competitive market so every day we are looking for opportunities to be better. To be faster. Even faster. Never stand aside and never afraid to try. Having a lot of own ideas we are very open for fresh ones. Equally important, we have resources to bring these into motion. — Create and manage effective data privacy and data security solutions for global technology company;— Work to align advanced technologies and Privacy by Design principles from the first stages of development and ensure that the data use meets established regulatory compliance needs;— Responsible for assisting with the management of the data privacy, data protection, data usability, performance and the integrity of the privacy solution;— Perform regular privacy assessments of operational processes, identifying, and mitigating risks across the company through effective tools, training and guidance. — 5+ years experience in privacy/data protection as Data privacy officer/engineer, Information security manager;— Strong knowledge of the major privacy frameworks and evolving legislation, security laws, rules and regulations worldwide, as well as industry leading-practices and standards;— Understanding of the IT and/or software engineering processes and tools;— Understanding of best practices in data lifecycle management, security and data protection and privacy;— Knowledge of various EDR and DLP, DRM, cloud technologies;— Strong experience at privacy by design methodologies;— CDPSE/CISM/CIPP/CIPT certifications will be an advantage. — Medical insurance/Sport compensation;— Sport club participation (football, running, basketball or swimming clubs);— 100% paid sick leaves;— 20 working days of paid vacation. — Competitive salary and сonstant encouragement for your efforts and contribution;— Bonuses according to company’s policy;— Welfare (financial support in critical situations);— Gifts for significant life events (marriage, childbirth). — Individual annual training budget with an opportunity to visit paid conferences, training sessions, workshops, etc.;— Free corporate library;— Opportunity to visit our non-stop internal meetups: open talks, IT Pump, etc. as a participant or a speaker and exchange knowledge;— A world-class team of T-shaped skilled professionals that share knowledge and support each other. — Corporate parties and events (Pub Quiz, Carquest, bowling championships, etc.);— PM Foundation activities (social responsibility events);— Weekly events aimed at culture, arts, soft skills development.",dou,2020-11-12,Data Security Manager@Parimatch Tech,,,{}
Ciklum,https://jobs.dou.ua/companies/ciklum/,Middle Database Developer for Derivco,https://jobs.dou.ua/companies/ciklum/vacancies/136484/, Kyiv,27 October 2020,,"On behalf of Derivco, Ciklum is looking for a Middle Database Developer to join Kyiv team on a full-time basis. Our client is Yield business unit within Derivco. They work hard and of course play hard, but most importantly Derivco don’t take themselves too seriously. Their core business values are passion, ownership and care. Our customer believes in own people and shows it! We are always looking for new technology to take advantage of and are always keen to embrace change! We embrace innovation in everything we do (and hand out some pretty cool prizes for it as well). We’re convinced that we employ some of the smartest people in the business, so if you like being challenged and feel that you could contribute, we look forward to hearing from you! Derivco Durban, South Africawww.youtube.com/watch?v=8_uQgB0_hpsDerivco Hong Kongwww.youtube.com/watch?v=0kywWpYy58QOur Family Day at Derivco Durbanwww.youtube.com/watch?v=M2UCbBhI-Vg",dou,2020-11-12,Middle Database Developer for Derivco@Ciklum,,,{}
"CyberVision, Inc.",https://jobs.dou.ua/companies/cybervision/,Java/Hadoop Application L3 Support Engineer,https://jobs.dou.ua/companies/cybervision/vacancies/131884/, Kyiv,27 October 2020,,"Required skills * Strong Linux system administrator skills (3+ Yrs)* Working knowledge of Hadoop components *Knowledge of file system and Linux Os internals latency, throughput, availability, consistency, security, etc.*Ability to communicate technical concepts clearly and effectively.* Strong troubleshooting and debugging skills, with a passion for problem-solving and investigation* Ability and willingness to learn new technologies*Good is written and verbal communication skills in English As a plus *Shell scripting (Shell, Awk) ability to trace (read/understand) Java/ C++*SQL/ NoSQL Databases knowledge*familiar with different security technologies ( Kerberos, SSL, etc.)*understanding of Java/ JVM concepts is a plus*prior technical support experience is a plus Project description One of the biggest companies in the Hadoop world provides industry’s only converged data platform that integrates the power of Hadoop and Spark with global event streaming, real-time database capabilities, and enterprise storage. We are looking for talented engineers to help support Platform functionality on customers’ side and make it the platform of choice for operational and analytic big data use-cases.",dou,2020-11-12,"Java/Hadoop Application L3 Support Engineer@CyberVision, Inc.",,,"{""Required skills"": [""* Strong Linux system administrator skills (3+ Yrs)* Working knowledge of Hadoop components *Knowledge of file system and Linux Os internals latency, throughput, availability, consistency, security, etc.*Ability to communicate technical concepts clearly and effectively.* Strong troubleshooting and debugging skills, with a passion for problem-solving and investigation* Ability and willingness to learn new technologies*Good is written and verbal communication skills in English""], ""As a plus"": [""*Shell scripting (Shell, Awk) ability to trace (read/understand) Java/ C++*SQL/ NoSQL Databases knowledge*familiar with different security technologies ( Kerberos, SSL, etc.)*understanding of Java/ JVM concepts is a plus*prior technical support experience is a plus""], ""Project description"": [""One of the biggest companies in the Hadoop world provides industry\u2019s only converged data platform that integrates the power of Hadoop and Spark with global event streaming, real-time database capabilities, and enterprise storage. We are looking for talented engineers to help support Platform functionality on customers\u2019 side and make it the platform of choice for operational and analytic big data use-cases.""]}"
Data Science UA,https://jobs.dou.ua/companies/data-science-ua/,"Data Scientist (AI, ML)",https://jobs.dou.ua/companies/data-science-ua/vacancies/136455/, remote,27 October 2020,,"Required skills — 3-5+ years working in data science and machine learning— Fluent English— Proficient using statistics and learning tools— Experience working in the learning field, recommendations and personalization— Content analysis, rate, lead scoring— Have experience building production-ready systems that capture and utilize large data sets in order to improve products performance— Experienced in computer science fundamentals such as object-oriented design, data structures and algorithms— Excellent communication skills. Communicate with project and product management on customer side directly on daily basis— Be ready for R&D environment. Bring new ideas and be ready to implement them. Work on POCs— Communicate closely with product team— Expertise in Python and SQL. As a plus — Java development experience— Experience in NLP area We offer — Full-time position— Work in friendly and professional team— Location: Kharkiv, Kiev (remote)— Paid sick-list— 4 weeks of paid vacation— Medical insurance— Corporate library— English classes— Corporate club (for some relax during workday) Responsibilities — Research and experiment with different machine learning algorithms and techniques to solve business problems— Data exploration and its quality evaluation— Feature engineering and data fusion— Perform statistical analysis— Train, fine-tuning and evaluation of ML models— Work with engineers to deploy ready ML models— Proposing new ideas Project description Our partner is a company providing online organizing tools to nonprofits. The tools they create allow thousands to organize and create change across the country and across the globe. Their greatest strength lies not only within the technology they produce, but also within the greatness of their people. They pride themselves on their team of talented, driven, entrepreneurial individuals and in having a workplace that values work/life balance, creativity, and innovation",dou,2020-11-12,"Data Scientist (AI, ML)@Data Science UA",,,"{""Required skills"": [""3-5+ years working in data science and machine learning"", ""Fluent English"", ""Proficient using statistics and learning tools"", ""Experience working in the learning field, recommendations and personalization"", ""Content analysis, rate, lead scoring"", ""Have experience building production-ready systems that capture and utilize large data sets in order to improve products performance"", ""Experienced in computer science fundamentals such as object-oriented design, data structures and algorithms"", ""Excellent communication skills. Communicate with project and product management on customer side directly on daily basis"", ""Be ready for R&D environment. Bring new ideas and be ready to implement them. Work on POCs"", ""Communicate closely with product team"", ""Expertise in Python and SQL.""], ""As a plus"": [""Java development experience"", ""Experience in NLP area""], ""We offer"": [""Full-time position"", ""Work in friendly and professional team"", ""Location: Kharkiv, Kiev (remote)"", ""Paid sick-list"", ""4 weeks of paid vacation"", ""Medical insurance"", ""Corporate library"", ""English classes"", ""Corporate club (for some relax during workday)""], ""Responsibilities"": [""Research and experiment with different machine learning algorithms and techniques to solve business problems"", ""Data exploration and its quality evaluation"", ""Feature engineering and data fusion"", ""Perform statistical analysis"", ""Train, fine-tuning and evaluation of ML models"", ""Work with engineers to deploy ready ML models"", ""Proposing new ideas""], ""Project description"": [""Our partner is a company providing online organizing tools to nonprofits. The tools they create allow thousands to organize and create change across the country and across the globe. Their greatest strength lies not only within the technology they produce, but also within the greatness of their people. They pride themselves on their team of talented, driven, entrepreneurial individuals and in having a workplace that values work/life balance, creativity, and innovation""]}"
Data Science UA,https://jobs.dou.ua/companies/data-science-ua/,Product Analyst,https://jobs.dou.ua/companies/data-science-ua/vacancies/136444/, Kyiv,27 October 2020,,"Required skills ● Interpreting data for management strategies● Using statistical methods to test hypotheses, speed up their testing, interpretation● Ability to implement architectural solutions of a database● Experience in ClickHouse, PostgreSQL, Tableau, Google Analytics, Firebase Analytics, R, Python● Knowledge of probability theory, forecasting, clustering We offer ● Friendly team● Cool product-based company● Growth opprotunity● Cozy office Responsibilities ● Competently set up and analyze A / B tests● Develop an analytical infrastructure and think about analytics broadly, independently solving product tasks, and not only solving narrow tasks from managers● Look for “growth points” — trying to understand how the most successful users solve their problems in the product in order to provide a convenient way to perform the same tasks for the entire mass of the audience● Analyze the retention rate and engagement rate of the audience, look for ways to improve these metrics● Look for ways to manage the user’s life cycle through “aha moment”, “habit moment”, “setup moment” Project description We are looking for a Product Analyst — Data Analyst in a product IT company",dou,2020-11-12,Product Analyst@Data Science UA,,,"{""Required skills"": [""\u25cf Interpreting data for management strategies\u25cf Using statistical methods to test hypotheses, speed up their testing, interpretation\u25cf Ability to implement architectural solutions of a database\u25cf Experience in ClickHouse, PostgreSQL, Tableau, Google Analytics, Firebase Analytics, R, Python\u25cf Knowledge of probability theory, forecasting, clustering""], ""We offer"": [""\u25cf Friendly team\u25cf Cool product-based company\u25cf Growth opprotunity\u25cf Cozy office""], ""Responsibilities"": [""\u25cf Competently set up and analyze A / B tests\u25cf Develop an analytical infrastructure and think about analytics broadly, independently solving product tasks, and not only solving narrow tasks from managers\u25cf Look for \u201cgrowth points\u201d"", ""trying to understand how the most successful users solve their problems in the product in order to provide a convenient way to perform the same tasks for the entire mass of the audience\u25cf Analyze the retention rate and engagement rate of the audience, look for ways to improve these metrics\u25cf Look for ways to manage the user\u2019s life cycle through \u201caha moment\u201d, \u201chabit moment\u201d, \u201csetup moment\u201d""], ""Project description"": [""We are looking for a Product Analyst"", ""Data Analyst in a product IT company""]}"
appflame,https://jobs.dou.ua/companies/appflame/,Middle Data Analyst,https://jobs.dou.ua/companies/appflame/vacancies/124561/, Kyiv,27 October 2020,,"appflame — is a young and fast-growing company. It launches and develops its products in social discovery / social network / dating niches. Our products operate primarily on Tier-1 markets, such as the USA, Great Britain, Canada, and Australia. The speed of project development from the first idea to a ready-to-use product, the absence of bureaucracy, result-oriented work, data-driven development, and decision making are our key advantages. We are fans of the data analysis. We like making hypotheses, coming up with new ideas and implementing them on our products. All the members of our young but already big team love what they do. Responsibilities:• Forming different hypotheses in the area of product development (with the aim of increasing the metrics of involvement and monetization);• Running split tests to verify different hypothesis;• Operational monitoring of key metrics and determining the causes of deviations from the expected values;• Providing analytical support in other areas of the project (Developers, PM, Marketing);• Proactive involvement in Product Management process. Requirements:• Proficiency in SQL (Vertica, Clickhouse);• Advanced knowledge of statistics;• Practical experience in big database management;• Basic skills of Machine Learning (theory and practice);• Ability to formulate product development hypotheses and lead a data-driven product management process;• 2+ years of experience in analytics/machine learning;• Experience BI and data visualization tools (Tableau is a plus);• Experience in data-driven Product management is a plus. What we offer:For our employees we offer the best working conditions comparable with those modern corporations can provide:💸 Competitive salary based on highest market benchmarks, regular performance, and compensation review based on results achieved.📕 Continuous growth and development opportunities: free English courses, fiction and non-fiction literature, courses / seminars / exhibitions / conferences, and everything else you may need for your personal and professional development.🚑 Full medical insurance after the probation period, as well as a corporate doctor.🏫 Comfortable, spacious offices with new repair near Taras Shevchenko and Kontraktova Ploshcha metro stations. Free parking for all staff, shower.💻 Modern working equipment, and everything else you may need for productive work.✈️ Team travels. In just two years of working together, we have already visited Cyprus, Egypt, Sri Lanka, and Spain.🏓 Games room with ping-pong, sports corner, table games and PlayStation, nap room.🏃‍♂️Office yoga, team participation in different sports events worldwide. Football, basketball, and running trainings.🍔 Free meals (breakfast and lunch), drinks/juices/snacks, fresh fruits, sweets.",dou,2020-11-12,Middle Data Analyst@appflame,,,{}
SoftServe,https://jobs.dou.ua/companies/softserve/,Senior Big Data Engineer (ID 57268),https://jobs.dou.ua/companies/softserve/vacancies/136426/," Kyiv, Kharkiv, Lviv, Dnipro",27 October 2020,,"WE ARE The largest Big Data & AI Center of Excellence in Eastern Europe, our expertise in Machine Learning, AI, Big Data, IoT, Cloud technologies recognized by Google and Amazon. We are transforming the way thousands of global organizations do business by creating the most innovative solutions using proven architecture design methodologies, innovation frameworks, and experience design approaches. Our group successfully delivered 50+ discovery & consulting projects and 75+ implementation projects in 2019. Together with the SEI, and Carnegie Mellon University, we invented the Smart Decisions game, in order to promote architecture design methodologies, intended to facilitate the design of complex architectures.Please read more at smartdecisionsgame.com YOU ARE An expert with • Excellent understanding of distributed computing technologies, approaches, and patterns• Proficiency in one of the following programming languages: Java, Scala, or Python• Cloud experience, which is a big plus point: AWS, GCP or Azure• Hands-on experience with Hadoop, NoSQL, MPP technologies, processing frameworks, or other Big Data technology areas: data ingestion, consolidation, streaming or batching• Experience in at least one of the processing and computation frameworks: Kafka Streams, Storm, Spark, Flink, Beam/DataFlow, Akka, etc.• Expertise in at least one of the RDBMS or NoSQL engines: PostgreSQL, MySQL, Cassandra, HBase, Elasticsearch, Redis, MongoDB, Impala, Kudu, etc.• A skill in implementing Data Lakes, Data Warehousing, or analytics systems, which is a big plus point YOU WANT TO WORK WITH • Creating sustainable Big Data and AI solutions• Evaluation of cutting-edge Big Data technologies, implement PoCs and MVPs• Learning new technologies and obtain certifies• Learning how to design architectures using proven methodologies by SEI, Carnegie Mellon• Becoming a top expert, or a certified Architect• Gaining deep expertise in one or multiple clouds TOGETHER WE WILL • Deliver discovery and consulting projects, such as solution design, technology assessment, and architecture evaluation together with CoE Lead Architects• Implement full-scale high-performance Data Platforms and decision-support systems• Utilize rapid prototyping techniques to accelerate the implementation of new technical solutions• Adopt cutting-edge technologies on a challenging project• Provide high-value services to different range of companies: from startups to Fortune 100• Engage new clients and work closely with the Sales team",dou,2020-11-12,Senior Big Data Engineer (ID 57268)@SoftServe,,,{}
CQG,https://jobs.dou.ua/companies/cqg/,Senior DBA & Data Architect (Cassandra),https://jobs.dou.ua/companies/cqg/vacancies/60961/, Kyiv,27 October 2020,,"Required skills • Experience with databases with high-transaction volume in a fast-paced, globally distributed highly-available (24×7) production environment is required.• Advanced understanding and development experience with NoSQL Cassandra/Datastax DB is required.• Hands-on experience in setting up a Cassandra Database in an enterprise, Design and Support data migration on a worldwide multi-datacenter cluster is required.• Experience with backup & recovery, security, monitoring, performance tuning and managing Cassandra cluster is required.• Strong understanding of complex inner workings of Cassandra, such as the gossip protocol, hinted handoffs, read repairs, Merkle trees is required.• Strong understanding and experience with JVM tuning.• Five years’ experience using Linux with scripting or python is required.• Proficiency in English written and oral communication is required. As a plus • Ansible experience is strongly desired.• Demonstrated knowledge of Server Administration, Security Administration, Network Administration and Application Development is preferred.• Excellent problem-solving skills, a self-starting attitude and good communication skills are essential• Must be able to work in a team-oriented environment and as a member of the production support team• Must demonstrate leadership skills without requiring positional authority We offer CQG provides a variety of benefits to enhance your personal and financial well-being. Employees and eligible dependents may participate in the following:• Career and professional opportunities;• Work in multicultural environment;• Completely “White” and competitive salary;• Employment in accordance with Ukrainian labor legislation;• Full medical insurance for employee and family (dental insurance is included); Responsibilities • Keep abreast of Data Base technologies and provide Designs, guidance and support forother DBA’s in order to build, deploy and support our production, stage, test,and development operations of database back end systems while focusing on monitoring,disaster recovery, security, integrity, stability, and scalability • Responsible for performance tuning and optimization of production, stage, test, anddevelopment operations of database back end systems including monitoring,disaster recovery, security, integrity, stability, and scalability • Advanced troubleshooting including diagnostics andresolution of issues • Automate day to day management functions • Communicate issues that arise from day to daymanagement functions • Design or validate designs of custom databasesystems • Responsible for code, script, documentation andother artifact generation of custom database systems • Mentor and advise on best practices for enterprisedata management to the team • Off hours support including being on call whenneeded Project description Summary:Responsible for implementation, development, management, support, upgrade and monitoring of database environments supporting CQG applications, infrastructure systems and services. Define enterprise architecture and mentor the architecture team in the development of architecture and design of multi-tier and federated back end database systems and clients for CQG applications. Lead development activities revolving around DB’s by providing mentoring, support, and decisions for database administrators, developers, and other internal customers. About us CQG, Inc. was founded more than 30 years ago in the United States. We are one of the world leaders in the delivery of market data and financial information, analysis technologies and stock trading. More than 500 people work in 15 offices around the world — from Denver and Chicago to Sydney and Singapore. The development offices are located in Denver, Moscow, Kiev, Samara, Yerevan and Zelenograd. Kiev office was opened in 2004 and now has about 30 people. Our friendly team for more than 10 years working on server solutions for stock trading, client applications for the analysis of stock quotes and many others. We’re looking for experienced motivated experts who will be interested in joining our team.",dou,2020-11-12,Senior DBA & Data Architect (Cassandra)@CQG,,,"{""Required skills"": [""Experience with databases with high-transaction volume in a fast-paced, globally distributed highly-available (24\u00d77) production environment is required."", ""Advanced understanding and development experience with NoSQL Cassandra/Datastax DB is required."", ""Hands-on experience in setting up a Cassandra Database in an enterprise, Design and Support data migration on a worldwide multi-datacenter cluster is required."", ""Experience with backup & recovery, security, monitoring, performance tuning and managing Cassandra cluster is required."", ""Strong understanding of complex inner workings of Cassandra, such as the gossip protocol, hinted handoffs, read repairs, Merkle trees is required."", ""Strong understanding and experience with JVM tuning."", ""Five years\u2019 experience using Linux with scripting or python is required."", ""Proficiency in English written and oral communication is required.""], ""As a plus"": [""Ansible experience is strongly desired."", ""Demonstrated knowledge of Server Administration, Security Administration, Network Administration and Application Development is preferred."", ""Excellent problem-solving skills, a self-starting attitude and good communication skills are essential"", ""Must be able to work in a team-oriented environment and as a member of the production support team"", ""Must demonstrate leadership skills without requiring positional authority""], ""We offer"": [""CQG provides a variety of benefits to enhance your personal and financial well-being. Employees and eligible dependents may participate in the following:"", ""Career and professional opportunities;"", ""Work in multicultural environment;"", ""Completely \u201cWhite\u201d and competitive salary;"", ""Employment in accordance with Ukrainian labor legislation;"", ""Full medical insurance for employee and family (dental insurance is included);""], ""Responsibilities"": [""Keep abreast of Data Base technologies and provide Designs, guidance and support forother DBA\u2019s in order to build, deploy and support our production, stage, test,and development operations of database back end systems while focusing on monitoring,disaster recovery, security, integrity, stability, and scalability"", ""Responsible for performance tuning and optimization of production, stage, test, anddevelopment operations of database back end systems including monitoring,disaster recovery, security, integrity, stability, and scalability"", ""Advanced troubleshooting including diagnostics andresolution of issues"", ""Automate day to day management functions"", ""Communicate issues that arise from day to daymanagement functions"", ""Design or validate designs of custom databasesystems"", ""Responsible for code, script, documentation andother artifact generation of custom database systems"", ""Mentor and advise on best practices for enterprisedata management to the team"", ""Off hours support including being on call whenneeded""], ""Project description"": [""Summary:Responsible for implementation, development, management, support, upgrade and monitoring of database environments supporting CQG applications, infrastructure systems and services. Define enterprise architecture and mentor the architecture team in the development of architecture and design of multi-tier and federated back end database systems and clients for CQG applications. Lead development activities revolving around DB\u2019s by providing mentoring, support, and decisions for database administrators, developers, and other internal customers.""]}"
Data Science UA,https://jobs.dou.ua/companies/data-science-ua/,Data Engineer,https://jobs.dou.ua/companies/data-science-ua/vacancies/136417/, Kyiv,27 October 2020,,"Required skills — 5+ years of experience in design, development, testing, deployment and support of Data Warehouse solutions — Experience with AWS (EC2, EMR, ECS, Kinesis, S3). — Knowledge and experience in creating ETL data pipelines.— Expert knowledge of databases, e.g. PostgreSQL, MongoDB. — Ability to find both quick solutions for experiments and stable production-ready, depending on the conditions As a plus — Experience in Java programming We offer — Official employment— Free education, bonus programs— Good salary— Social guarantees The company will allow you to:— Fully express yourself and your abilities— Gain invaluable experience— Realize opportunities for personal and professional growth— To make a career Responsibilities — Database design and analysis of the existing structure (PostgreSQL)— Database optimization— Development of data marts— Creation of your own Storage, integration with existing systems Project description Our partner is a company who really wants to find a Senior Data Engineer, who would like to realize his potential and who is not afraid to be the only data engineer on the team. You have opportunities to implement your skills and abilities in a team.",dou,2020-11-12,Data Engineer@Data Science UA,,,"{""Required skills"": [""5+ years of experience in design, development, testing, deployment and support of Data Warehouse solutions"", ""Experience with AWS (EC2, EMR, ECS, Kinesis, S3)."", ""Knowledge and experience in creating ETL data pipelines."", ""Expert knowledge of databases, e.g. PostgreSQL, MongoDB."", ""Ability to find both quick solutions for experiments and stable production-ready, depending on the conditions""], ""As a plus"": [""Experience in Java programming""], ""We offer"": [""Official employment"", ""Free education, bonus programs"", ""Good salary"", ""Social guarantees""], ""Responsibilities"": [""The company will allow you to:"", ""Fully express yourself and your abilities"", ""Gain invaluable experience"", ""Realize opportunities for personal and professional growth"", ""To make a career""], ""Project description"": [""Database design and analysis of the existing structure (PostgreSQL)"", ""Database optimization"", ""Development of data marts"", ""Creation of your own Storage, integration with existing systems""]}"
CHI Software,https://jobs.dou.ua/companies/chi-software/,Data Engineer,https://jobs.dou.ua/companies/chi-software/vacancies/136404/," Kharkiv, remote",27 October 2020,,"Required skills Requirements: — At least 3-4 years of commercial experience in Software Engineering / Data Engineering / Data Warehousing— Experience with big data tools: Hadoop, Spark, Kafka, etc— Confident knowledge of SQL, understanding of relational and noSQL storages— Familiarity with Elasticsearch, Logstash, and Kibana or similar technologies— Experience with AWS cloud services: EC2, EMR, RDS, Redshift— Experience with stream-processing systems: Storm, Spark-Streaming, etc.— English level intermediate or higher As a plus Will be a plus: — Experience with Scala/Java We offer With us you can: Develop your technical knowledge:— Use latest technologies— Participate in technical events and conferences (the cost is covered by the company)— Regular techtalks and professional development Improve your soft skills:— Build strong teamwork skills and become an essential part of the dynamic teams— Improve your English at classes and speaking directly with clients— Increase your productivity and communication level via Scrum, Kanban, Agile methodologies What else do we offer?— Competitive compensation and benefits— Flexible and negotiable schedule— Nice and comfortable office located near metro station— Covered rest period (20 business days)— Free English classes (we have 3 teachers in our team)— Break area with Xbox, air hockey, ping-pong and table soccer— Truly friendly atmosphere and unforgettable events— Bookcrossing— Basketball and ping-pong teams— Discounts offered by individual bonus cards (our partners are coffee shops, bars, and fitness centers)",dou,2020-11-12,Data Engineer@CHI Software,,,"{""Required skills"": [""Requirements:""], ""As a plus"": [""At least 3-4 years of commercial experience in Software Engineering / Data Engineering / Data Warehousing"", ""Experience with big data tools: Hadoop, Spark, Kafka, etc"", ""Confident knowledge of SQL, understanding of relational and noSQL storages"", ""Familiarity with Elasticsearch, Logstash, and Kibana or similar technologies"", ""Experience with AWS cloud services: EC2, EMR, RDS, Redshift"", ""Experience with stream-processing systems: Storm, Spark-Streaming, etc."", ""English level intermediate or higher""], ""We offer"": [""Will be a plus:""]}"
Upswot,https://jobs.dou.ua/companies/bnesis/,Middle Data Scientist,https://jobs.dou.ua/companies/bnesis/vacancies/136030/, remote,27 October 2020,,"Required skills Our candidate is highly motivated, result-oriented, hard-working, desire data analysis, and creation of prediction models. We are looking for an experienced data scientists with strong mathematic skills, experience in data analysis, data structuring, building models for cash flow prediction, sales values predictions for different business areas for SMB and Mid-market.Our candidate must have successfully validated models for the prediction of any economic factors. The minimum tech stack for our candidate is:* Python — advanced (experience in working with prediction libraries and libraries used for ML)* MySQL — advanced* RESTful API — basic understanding of REST principals* Proficient using statistics and learning tools.* Have experience building production-ready systems that capture and utilize large data sets in order to improve products performance* Understanding theoretical concepts of statistics/probability, data mining, machine learning* Extensive experience with data preparation for statistical or machine learning models Strong knowledge of data science principles and theory is a must.Our candidate has to know mainstreams in the data science area.The ability and desire to self-studying is a must.Experience in ML is a must. Our successful candidate is a team player, always ready to help, to support, and to teach teammates. As a plus The deep understanding of accountancy, principals of CF, PL, and general business management are nice to have. We offer We offer to become a part of a strongly motivated team.Develop a product that changes the banking culture of interaction with SMB customers.You will have many exciting tasks with the ability to create.You will be able to implement the most innovative ideas that change bank SMB-customers’ life. We offer shares in the company. Responsibilities Our candidate will be responsible for the whole data science process:* to define the business value of the task we solve* creating of the hypotрesis* data preparation for statistical or machine learning models (extensive experience is needed)* marking up the data set* the development of the model* model Validation* automation of the model’s training process Project description Upswot is the Ukrainian startup that is actively scaling in CEE, Western Europe and the US. Upswot provides an on-prem or private cloud, a white-labeled platform designed to motivate SMB to share the data from their business accounts. Upswot automatically integrates, structures, and analyses authorized data from data sources including Accounting, CRM, WEB analytics, eWallets, ERP, and eCommerce systems SMBs already use. Based on the insights of the data from these systems, Upswot helps Lenders & Carriers cross-sell more effectively to both new and current clients with highly personalized and relevant offers. Besides, our technology saves a lot of time for SME by automating reporting to the Lender or Insurer about the results of the operational activities. We allow SMBs to combine all their business systems in a single screen to get insights into their business, sales, and operations. We are working with several dozens of Lenders in 4 countries. upswot.com www.facebook.com/upswot",dou,2020-11-12,Middle Data Scientist@Upswot,,,"{""Required skills"": [""Our candidate is highly motivated, result-oriented, hard-working, desire data analysis, and creation of prediction models.""], ""As a plus"": [""We are looking for an experienced data scientists with strong mathematic skills, experience in data analysis, data structuring, building models for cash flow prediction, sales values predictions for different business areas for SMB and Mid-market.Our candidate must have successfully validated models for the prediction of any economic factors.""], ""We offer"": [""The minimum tech stack for our candidate is:* Python"", ""advanced (experience in working with prediction libraries and libraries used for ML)* MySQL"", ""advanced* RESTful API"", ""basic understanding of REST principals* Proficient using statistics and learning tools.* Have experience building production-ready systems that capture and utilize large data sets in order to improve products performance* Understanding theoretical concepts of statistics/probability, data mining, machine learning* Extensive experience with data preparation for statistical or machine learning models""], ""Responsibilities"": [""Strong knowledge of data science principles and theory is a must.Our candidate has to know mainstreams in the data science area.The ability and desire to self-studying is a must.Experience in ML is a must.""], ""Project description"": [""Our successful candidate is a team player, always ready to help, to support, and to teach teammates.""]}"
Dataforest,https://jobs.dou.ua/companies/dataforest/,Data Scientist,https://jobs.dou.ua/companies/dataforest/vacancies/76386/, Kyiv,26 October 2020,,"Required skills — 1 year+ commercial experience with Python and Data Science techniques.— Experience with Machine learning, and data manipulation. — Experience with Data gathering, Transformation, Cleaning and Visualization.— Some Experience with NLP , computer vision, clustering and forecasting. — Experience with data analysis. As a plus — Experience with NoSQL / elasticsearch— Exposure to back-end frameworks like Flask.— Experience (at least knowledge) with agile development methodologies. We offer • Interesting projects with new technologies ;• Salary fixed in USD;• Flexible work schedule ; Responsibilities • Develop NLP models , • Data analysis and data manipulation,• Computer vision project,• Develop forecasting models.• Develop micro-services.• Develop services for integration with external APIs. Project description Dataforest— we are experts committed to creating bespoke data-driven solutions for the variety of corporate and private clients over the world.",dou,2020-11-12,Data Scientist@Dataforest,,,"{""Required skills"": [""1 year+ commercial experience with Python and Data Science techniques."", ""Experience with Machine learning, and data manipulation."", ""Experience with Data gathering, Transformation, Cleaning and Visualization."", ""Some Experience with NLP , computer vision, clustering and forecasting."", ""Experience with data analysis.""], ""As a plus"": [""Experience with NoSQL / elasticsearch"", ""Exposure to back-end frameworks like Flask."", ""Experience (at least knowledge) with agile development methodologies.""], ""We offer"": [""Interesting projects with new technologies ;"", ""Salary fixed in USD;"", ""Flexible work schedule ;""], ""Responsibilities"": [""Develop NLP models ,"", ""Data analysis and data manipulation,"", ""Computer vision project,"", ""Develop forecasting models."", ""Develop micro-services."", ""Develop services for integration with external APIs.""], ""Project description"": [""Dataforest"", ""we are experts committed to creating bespoke data-driven solutions for the variety of corporate and private clients over the world.""]}"
Proxify,https://jobs.dou.ua/companies/proxify/,Data Engineer,https://jobs.dou.ua/companies/proxify/vacancies/133082/, remote,26 October 2020,,"Proxify is a Sweden IT company, experiencing intense growth. We match remote IT professionals with IT companies in Sweden and abroad. The difference with us is that we like to make sure that the remote workers we present are the very best in their field. We like to make sure we get it the right first time, every time! We are growing fast and currently looking for a Data Engineer to join our team. What you’ll do:You as a Data Engineer in the team will try to make sense out of a vast number of data sources that have been ingested as part of the Discovery phase. Most likely you will use a graph to find patterns and identify high-value opportunities for enterprises. Given that this will be a greenfield proof of concept product, you will have the opportunity to choose your preferred stack and have a pivotal role in shaping the company’s’ future. Who are you:— You are curious, open-minded and enjoy teamwork in a fast-changing environment; — You are interested or have experience with business strategy and exploratory analyses.— has 4+ years of working experience as a Data Engineer; — has 3+ years of experience with Node.js;— 0,5+ years with Neo4J;— Upper-intermediate (or higher) English level. As a plus:— You have studied Econometrics, Macroeconomics, Statistics, Engineering, or any other relevant field. What we offer: 💻100% remote work (work from where you want);💪We pay for overtime (over 8 hours);🏄🏻‍♂️Business trips to Sweden at company expense (if and when necessary);👌🏻The ability to change the project to another one;💵Competitive salary/hour with a potential bonus scheme;🧘🏻‍♂️Very flexible working schedule;🚀Opportunities for professional development and personal growth;🐕Pet-friendly office, if you need to work from the office, you can come with your little friend.",dou,2020-11-12,Data Engineer@Proxify,,,{}
Data Science UA,https://jobs.dou.ua/companies/data-science-ua/,DBA developer,https://jobs.dou.ua/companies/data-science-ua/vacancies/132734/, Kyiv,26 October 2020,,"Required skills • Knowledge of the modern stack of technologies for working with data• Experience with relational databases (PostgreSQL, MongoDB, MS SQL Server and others).• Experience with non-relational databases and tools for collecting them (MongoDB, Kafka, etc.).• Experience in database design, taking into account the requirements for data normalization, speed of work, differentiation of access rights• Experience in installing, configuring and administering PostgreSQL and MS SQL Server DBMS.• Excellent knowledge of SQL, including dialects for PostgreSQL and MS SQL Server• Knowledge of data types in the DBMS (PostgreSQL, MS SQL Server) and the ability to assess the technical requirements for database hosting (sizing)• Knowledge of the mechanisms for implementing indexing in a DBMS (PostgreSQL, MS SQL Server) As a plus • Experience with Amazon Web Services• Experience in setting up Tableau Server, experience in setting up dashboards in Tableau.• Experience in Bloomberg, Thompson Reuters.• Basic knowledge of Python We offer • Labor contract • Work schedule from 9 a.m to 6 p.m• Vacation, sick leave — provided• Salary is paid in local currency (UAH). Responsibilities • Design of information flows, databases, collection tools for the implementation of analytical solutions.• Configuring DBMS PostgreSQL, MS SQL Server and others in data center hosting and Amazon Web Services.• Administration of DBMS and other tools for working with data.• Participation in the development of technical specifications, technical projects for analytical solutions in terms of DBMS and other tools for working with data.• Participation in the implementation of an IT project to develop an analytical solution in trading.• Research market information available on subscriptions from market agencies, including Bloomberg, Thompson Reuters, Argus and others Project description The company is the largest energy holding in Ukraine. We are looking for a Data Engineer in the Analytics Department.",dou,2020-11-12,DBA developer@Data Science UA,,,"{""Required skills"": [""Knowledge of the modern stack of technologies for working with data"", ""Experience with relational databases (PostgreSQL, MongoDB, MS SQL Server and others)."", ""Experience with non-relational databases and tools for collecting them (MongoDB, Kafka, etc.)."", ""Experience in database design, taking into account the requirements for data normalization, speed of work, differentiation of access rights"", ""Experience in installing, configuring and administering PostgreSQL and MS SQL Server DBMS."", ""Excellent knowledge of SQL, including dialects for PostgreSQL and MS SQL Server"", ""Knowledge of data types in the DBMS (PostgreSQL, MS SQL Server) and the ability to assess the technical requirements for database hosting (sizing)"", ""Knowledge of the mechanisms for implementing indexing in a DBMS (PostgreSQL, MS SQL Server)""], ""As a plus"": [""Experience with Amazon Web Services"", ""Experience in setting up Tableau Server, experience in setting up dashboards in Tableau."", ""Experience in Bloomberg, Thompson Reuters."", ""Basic knowledge of Python""], ""We offer"": [""Labor contract"", ""Work schedule from 9 a.m to 6 p.m"", ""Vacation, sick leave"", ""provided"", ""Salary is paid in local currency (UAH).""], ""Responsibilities"": [""Design of information flows, databases, collection tools for the implementation of analytical solutions."", ""Configuring DBMS PostgreSQL, MS SQL Server and others in data center hosting and Amazon Web Services."", ""Administration of DBMS and other tools for working with data."", ""Participation in the development of technical specifications, technical projects for analytical solutions in terms of DBMS and other tools for working with data."", ""Participation in the implementation of an IT project to develop an analytical solution in trading."", ""Research market information available on subscriptions from market agencies, including Bloomberg, Thompson Reuters, Argus and others""], ""Project description"": [""The company is the largest energy holding in Ukraine. We are looking for a Data Engineer in the Analytics Department.""]}"
Proxet,https://jobs.dou.ua/companies/proxet/,Lead Machine Learning Engineer for Picsart,https://jobs.dou.ua/companies/proxet/vacancies/136312/, remote,26 October 2020,,"Required skills — 5+ years of Python engineering and experience in research and development of machine learning and deep learning technologies for real-world products— Experience leading data science teams— Experience of deploying and maintaining machine learning models in production— Experience working with information retrieval projects, working with learned ranking for information retrieval systems is a must— Experience writing, maintainable, testable, production-grade Python code— Understanding of different machine learning and deep learning algorithm families and their tradeoffs, especially ones related to computer vision, natural language processing, and information retrieval— Experience with machine learning and deep learning libraries and frameworks (Scikit-learn, XGBoost, TensorFlow, Keras, PyTorch, etc.)— Good knowledge of ML/DL theoretical underpinnings— Team-player with strong interpersonal skills As a plus — Experience working with Elasticsearch/Solr— Experience with large-scale machine learning (100GB+ datasets)— Experience in design and development of data processing pipelines (Spark, Airflow, etc.)— Competitive machine learning experience (e.g. Kaggle) could be a plus We offer — Challenging work in an international professional environment— The long-standing team as this is for a long term project— Competitive salary— Flexible work-from-home & remote work policy— Mastering English language with a native speaker— 40-hours working week with flexible working hours— PE accounting and support— 20 paid vacation days per year— 14 paid sick leaves per year— Collaborative friendly team environment— Cozy fully equipped office space in the city center Responsibilities — Developing high-load information retrieval system based on machine learning-enriched algorithms— Owning the full Data Science life-cycle from conception to prototyping, testing, deploying, and measuring its overall business value— Providing technical leadership and strategic initiatives for your team Project description PicsArt is an all-in-one photo and video editing app for making your social content pop. The platform allows users to take and edit pictures and videos, draw with layers, and share the images on PicsArt’s and other social networks. We make it super easy to step up your photo-editing game, create amazing images and videos, and share them with friends.It’s the one app that lets you be truly creative with its almost limitless features ― and it’s starting a movement to help the people “go beyond the filter” and make awesome pictures.A consistent top 20 downloaded app with more than 150 million monthly active users, PicsArt spans the globe and is available in over 30 languages.PicsArt is your mobile creative playground. Remix images with us and discover an entire world of content and tools to help you tell your unique story.picsart.com/explore",dou,2020-11-12,Lead Machine Learning Engineer for Picsart@Proxet,,,"{""Required skills"": [""5+ years of Python engineering and experience in research and development of machine learning and deep learning technologies for real-world products"", ""Experience leading data science teams"", ""Experience of deploying and maintaining machine learning models in production"", ""Experience working with information retrieval projects, working with learned ranking for information retrieval systems is a must"", ""Experience writing, maintainable, testable, production-grade Python code"", ""Understanding of different machine learning and deep learning algorithm families and their tradeoffs, especially ones related to computer vision, natural language processing, and information retrieval"", ""Experience with machine learning and deep learning libraries and frameworks (Scikit-learn, XGBoost, TensorFlow, Keras, PyTorch, etc.)"", ""Good knowledge of ML/DL theoretical underpinnings"", ""Team-player with strong interpersonal skills""], ""As a plus"": [""Experience working with Elasticsearch/Solr"", ""Experience with large-scale machine learning (100GB+ datasets)"", ""Experience in design and development of data processing pipelines (Spark, Airflow, etc.)"", ""Competitive machine learning experience (e.g. Kaggle) could be a plus""], ""We offer"": [""Challenging work in an international professional environment"", ""The long-standing team as this is for a long term project"", ""Competitive salary"", ""Flexible work-from-home & remote work policy"", ""Mastering English language with a native speaker"", ""40-hours working week with flexible working hours"", ""PE accounting and support"", ""20 paid vacation days per year"", ""14 paid sick leaves per year"", ""Collaborative friendly team environment"", ""Cozy fully equipped office space in the city center""], ""Responsibilities"": [""Developing high-load information retrieval system based on machine learning-enriched algorithms"", ""Owning the full Data Science life-cycle from conception to prototyping, testing, deploying, and measuring its overall business value"", ""Providing technical leadership and strategic initiatives for your team""], ""Project description"": [""PicsArt is an all-in-one photo and video editing app for making your social content pop. The platform allows users to take and edit pictures and videos, draw with layers, and share the images on PicsArt\u2019s and other social networks.""]}"
Quantum,https://jobs.dou.ua/companies/quantum/,Data Science Engineer,https://jobs.dou.ua/companies/quantum/vacancies/61209/, Kharkiv,26 October 2020,,"Required skills — At least 2 years of commercial experience in DS;— Strong knowledge of linear algebra, calculus, statistics and probability theory;— Knowledge and experience with algorithms and data structures;— Experience with Machine Learning libraries (NumPy, SciPy, Pandas, ScikitLearn ,etc);— Experience with at least one of Deep Learning frameworks (Tensorflow, Keras, PyTorch, etc);— Experience with SQL;— Strong knowledge of OOP;— Intermediate English. As a plus — Participation in Kaggle competitions;— Knowledge of modern Neural Networks architectures (DNN, CNN, LSTM, etc);— Experience in classical Computer Vision algorithms;— Experience in Natural Language Processing;— Experience with production ML/DL frameworks (OpenVino, TensorRT, etc.);— Docker practical experience;— Basic understanding of Big Data concepts;— Experience with Cloud Computing Platforms (AWS, GCloud, Azure). We offer — Exchange of experience, professional development;— A strong team, a healthy atmosphere;— Flexible working time;— 20 days paid vacation;— Paid sick leave;— 8-hour working day and 5-day working week;— English lessons and massage service in the office (partially paid by the company);— Opportunity to take part in conferences, meetups etc. (fully or partially paid by the company);— Regular company events. Responsibilities — Data Analysis and Preparation;— Development of Deep Learning / Machine Learning / Computer Vision solutions;— You will be working on full-cycle data science projects. Your tasks will include data preparation, developing ML models and deploying them to production. Sometimes, this will require the ability to implement methods from scientific papers and apply them to new domains. Project description We are opening a position for a Data Science Engineer to expand our team.",dou,2020-11-12,Data Science Engineer@Quantum,,,"{""Required skills"": [""At least 2 years of commercial experience in DS;"", ""Strong knowledge of linear algebra, calculus, statistics and probability theory;"", ""Knowledge and experience with algorithms and data structures;"", ""Experience with Machine Learning libraries (NumPy, SciPy, Pandas, ScikitLearn ,etc);"", ""Experience with at least one of Deep Learning frameworks (Tensorflow, Keras, PyTorch, etc);"", ""Experience with SQL;"", ""Strong knowledge of OOP;"", ""Intermediate English.""], ""As a plus"": [""Participation in Kaggle competitions;"", ""Knowledge of modern Neural Networks architectures (DNN, CNN, LSTM, etc);"", ""Experience in classical Computer Vision algorithms;"", ""Experience in Natural Language Processing;"", ""Experience with production ML/DL frameworks (OpenVino, TensorRT, etc.);"", ""Docker practical experience;"", ""Basic understanding of Big Data concepts;"", ""Experience with Cloud Computing Platforms (AWS, GCloud, Azure).""], ""We offer"": [""Exchange of experience, professional development;"", ""A strong team, a healthy atmosphere;"", ""Flexible working time;"", ""20 days paid vacation;"", ""Paid sick leave;"", ""8-hour working day and 5-day working week;"", ""English lessons and massage service in the office (partially paid by the company);"", ""Opportunity to take part in conferences, meetups etc. (fully or partially paid by the company);"", ""Regular company events.""], ""Responsibilities"": [""Data Analysis and Preparation;"", ""Development of Deep Learning / Machine Learning / Computer Vision solutions;"", ""You will be working on full-cycle data science projects. Your tasks will include data preparation, developing ML models and deploying them to production. Sometimes, this will require the ability to implement methods from scientific papers and apply them to new domains.""], ""Project description"": [""We are opening a position for a Data Science Engineer to expand our team.""]}"
Ciklum,https://jobs.dou.ua/companies/ciklum/,Data Analyst for SimilarWeb,https://jobs.dou.ua/companies/ciklum/vacancies/136303/, Kyiv,26 October 2020,,"On behalf of SimilarWeb, Ciklum is looking for a talented and dedicated analyst with research/business mindsets to join our Data Solutions team as an Analyst. As part of this role you will help us scale and stabilize our new rocketing Digital Insights (DI) reporting products at SimilarWeb. The company was founded in 2009 and has raised $40 million dollars in funding led by Naspers and Lord David Alliance. SimilarWeb has global offices and representatives in the U.K., Dubai, Israel, Russia, Brazil, and Japan. SimilarWeb’s wealth of data combined with intuitive discovery tools uncover the pulse of the internet, that’s why SimilarWeb insights are often used in leading publications including Bloomberg Businessweek, Wall St. Journal, TechCrunch and the New York Times. SimilarWeb is used every day to help thousands of businesses, publishers, marketers and analysts to benchmark performance and find successful marketing strategies driving more traffic and opportunities. Companies that use SimilarWeb include Travelocity, Google, Taboola, KPMG, eBay and UBS. SimilarWeb is the pioneer of market intelligence and the standard for understanding the digital world. SimilarWeb provides granular insights about any website or app across all industries in every region enabling the world’s biggest brands like Airbnb, Walmart, HSBC, eBay and adidas to create the strategies necessary to win market share. SimilarWeb has been named one of Inc.com 15 Israeli Startups to Watch in 2017 and one of Business Insider’s Top 50 Enterprise Startups to Bet Your Career On. But if we want to continue helping the world’s biggest brands reimagine what is possible online, we need talented and driven individuals who are looking for the chance to make a real and tangible impact. Does this sound like you? What makes the SimilarWeb R&D team awesome? We love data! At SimilarWeb you have the opportunity to work with it on a petabyte scale! As part of the team you will support everything from our multi million client side panel across all platforms through our massive ingest pipeline (Big data starts with ingesting even bigger data. Our Node.js, Kafka, Java, Python, Go pipeline is impressive!) We are solving some really complex problems with web server scale, algorithms on the client side, research of different OS’s and APIs, managing multi-million users products and more. Work with cutting edge technology — we often beta test the tech that other people will only discover next year! We want you to get better and help you set goals and conquer them. We Want You!We are looking for a talented and dedicated analyst with research/business mindsets to join our Data Solutions team as an Analyst. As part of this role you will help us scale and stabilize our new rocketing Digital Insights (DI) reporting products at SimilarWeb. We believe that building a great product and a great company starts with finding amazing people and helping them grow and develop professionally and personally. At SimilarWeb, you’ll be surrounded by the most talented professionals and you’ll work across departments gaining skills and driving impact. Our Values:• Data Driven: We believe data driven decisions are the key to success and we are evangelists for the power of digital insights• Passionate: We care deeply about what we do, are accountable for our actions and are committed to helping each other• Excellence: It’s in our nature; and we work hard to make a difference• Winning Together: We know what it takes to build a great company and a great product and it begins with valuing each and every member of our team",dou,2020-11-12,Data Analyst for SimilarWeb@Ciklum,,,{}
8allocate,https://jobs.dou.ua/companies/8allocate/,Data Scientist,https://jobs.dou.ua/companies/8allocate/vacancies/136301/," Kyiv, remote",26 October 2020,,"Required skills You will fit if you:— Have an experience with PyTorch and related application frameworks. — Have an experience in computational advertising related tasks, such as ads response (click or conversion) prediction, bid-landscape forecasting, supply-demand forecasting, or advertising metrics and measurement;— Are able to present complicated data insights and strategic recommendations within a fast-paced Agile environment;— Have a commercial hands-on data science experience developing statistical, machine learning and deep learning methods, such as search queries classification, named entity recognition and domain knowledge graphs, optimization, etc.— Have a strong understanding of recent exciting advances in NLP such as pre-trained language models, contextualized word embeddings, attention and novel neural network architectures;— Are confident writing production-quality code preferably in Python and SQL;— Have exceptional analytical, quantitative, problem-solving, critical thinking and communication skills;— Passion for Data Science, actively seeking out opportunities for learning and development.— English Advanced As a plus As a plus:— Good knowledge using 3rd party data (e.g. Google Analytics, Google Adwords, Omniture Analytics, Adobe Campaign etc.);— Experience with high-performance cloud computing services (e.g. Amazon AWS); We offer Why choose us?— “Family and Friends”. We are no longer a start-up, but still, have a family atmosphere in our supportive and spirited team, who are all working together on the same goal.— “Just break down all barriers and find a better way”. Every day you’ll meet with interesting and challenging (international) projects that are covering industries from commercial aviation to fintech (different technologies, different products).— “Hungry for learning”. You will get a lot of chances for career advancement and the development of new skills, opportunities for mentorship, or learning from more experienced colleagues. Benefits from 8allocate:— Corporate events, holidays and team buildings for your joy.— Training and development: we have a huge library (about 500 books!) and a budget for your professional development.— People-oriented management without bureaucracy.— Paid vacation and sick leaves.— Relocation program: if you are from another city and want to move to Kyiv, we will be happy to help you. Responsibilities Main responsibilities and activities:— Rapidly design, prototype and experiment with possible machine learning and deep learning algorithms to solve real-world problems in a highly ambiguous environment of search advertising;— Maintain and enhance existing project’s ML and AI models including resolution of client issues related to any model performance;— Collaborate with Product and Engineering teams to integrate successful solutions into large-scale, highly complex production systems;— Facilitate thought leadership and best practices on data science across the business to continually improve the existed approaches and values;— Effectively communicate data findings and helps team/business make data-driven decisions;— Take ownership and pride in the products we build and always make sure they are of the highest standard;— Be empathetic towards team members and customers. Project description 8allocate is a global provider of end-to-end custom software development solutions to companies all over the globe, from North America to the EU to Israel to Australia. Headquartered in Estonia, we run offshore R&D centers in Kyiv and Lviv. Our team is 50% remote and distributed. We specialize in flexible interaction exclusively with international clients (we cover industries from commercial aviation to fintech) thanks to a multinational support group of experts and management. Currently, we are looking for a Data Scientist for an Adtech-related project. About the project: a market leader in developing Competitive Intelligence for AdTech Search.Application teams develop unparalleled technologies that help our clients understand their paid and organic search landscape and improve campaign performance. Joining the project’s award-winning Data Science team will give you the chance to work with the latest Machine Learning techniques using cutting-edge frameworks such as Neural nets with PyTorch, NLP using Zero-shot learning and BERT, or Optimisation models that run on huge datasets with billions of data points. The stack of a project: Python, R, SQLDS, Pytorch, Tensorflow, Fastai, Flair, BigGraph, Optuna, Horovod, ONNX, Flask, Shiny, Flexdashboard, Plotly, Highchart, Bokeh, Altair, PostgreSQL, AWS(S3), Redshift, Redis, MongoDB, Cassandra, Luigi, RabbitMQ (messaging), Quartz scheduling, Docker and Kubernetes, Maven, TeamCity, Jenkins, GitHub, Pycharm, Sublime Text, Netron, Jira, Miro.",dou,2020-11-12,Data Scientist@8allocate,,,"{""Required skills"": [""You will fit if you:"", ""Have an experience with PyTorch and related application frameworks."", ""Have an experience in computational advertising related tasks, such as ads response (click or conversion) prediction, bid-landscape forecasting, supply-demand forecasting, or advertising metrics and measurement;"", ""Are able to present complicated data insights and strategic recommendations within a fast-paced Agile environment;"", ""Have a commercial hands-on data science experience developing statistical, machine learning and deep learning methods, such as search queries classification, named entity recognition and domain knowledge graphs, optimization, etc."", ""Have a strong understanding of recent exciting advances in NLP such as pre-trained language models, contextualized word embeddings, attention and novel neural network architectures;"", ""Are confident writing production-quality code preferably in Python and SQL;"", ""Have exceptional analytical, quantitative, problem-solving, critical thinking and communication skills;"", ""Passion for Data Science, actively seeking out opportunities for learning and development."", ""English Advanced""], ""As a plus"": [""As a plus:"", ""Good knowledge using 3rd party data (e.g. Google Analytics, Google Adwords, Omniture Analytics, Adobe Campaign etc.);"", ""Experience with high-performance cloud computing services (e.g. Amazon AWS);""], ""We offer"": [""Why choose us?"", ""\u201cFamily and Friends\u201d. We are no longer a start-up, but still, have a family atmosphere in our supportive and spirited team, who are all working together on the same goal."", ""\u201cJust break down all barriers and find a better way\u201d. Every day you\u2019ll meet with interesting and challenging (international) projects that are covering industries from commercial aviation to fintech (different technologies, different products)."", ""\u201cHungry for learning\u201d. You will get a lot of chances for career advancement and the development of new skills, opportunities for mentorship, or learning from more experienced colleagues.""], ""Responsibilities"": [""Benefits from 8allocate:"", ""Corporate events, holidays and team buildings for your joy."", ""Training and development: we have a huge library (about 500 books!) and a budget for your professional development."", ""People-oriented management without bureaucracy."", ""Paid vacation and sick leaves."", ""Relocation program: if you are from another city and want to move to Kyiv, we will be happy to help you.""], ""Project description"": [""Main responsibilities and activities:"", ""Rapidly design, prototype and experiment with possible machine learning and deep learning algorithms to solve real-world problems in a highly ambiguous environment of search advertising;"", ""Maintain and enhance existing project\u2019s ML and AI models including resolution of client issues related to any model performance;"", ""Collaborate with Product and Engineering teams to integrate successful solutions into large-scale, highly complex production systems;"", ""Facilitate thought leadership and best practices on data science across the business to continually improve the existed approaches and values;"", ""Effectively communicate data findings and helps team/business make data-driven decisions;"", ""Take ownership and pride in the products we build and always make sure they are of the highest standard;"", ""Be empathetic towards team members and customers.""]}"
CHI Software,https://jobs.dou.ua/companies/chi-software/,Computer Vision Engineer,https://jobs.dou.ua/companies/chi-software/vacancies/131419/, Kharkiv,26 October 2020,,"Required skills Requirements:— Strong knowledge in computer vision fundamentals i.e. OpticalFlow, HOG, feature detection algorithms, Hough Transform, Haar Cascades, Homography, Morphology, Denoising/Deblurring, and image processing algorithms— Strong Python knowledge— Strong practical experience with DL/CV frameworks like OpenCV, PyTorch, MXNet, Tensorflow, or Keras— Experience with some of the well-known neural networks architectures such as Yolo, MobileNet, U-Net, R-CNN-based architectures— Understanding state-of-the-art CV approaches for problems like object detection/tracking, video analysis, semantic segmentation, pose estimation, optical character recognition— Upper-intermediate level of English mandatory As a plus Would be a plus:—Experience with R, C++—Experience with following modern neural network architectures: LSTM and other RNN-based, Transformers(BERT, etc.).—Familiarity with time-series predictive/anomaly detection analyses, natural language processing, signal processing—Understanding SOTA approaches for machine learning problems like unsupervised / semi-supervised learning.—Experience with the following DL frameworks: DLib, Darknet, Theano.—Awareness of CRISP-DM process model—Experience with continuous integration and release management tools, preferably within the AWS platform.—Hands-on Experience with the common architecture of MLOps system by the means of Hadoop, Docker, Kubernetes, cloud services and experience with managing production ML lifecycle We offer With us you can: Develop your technical knowledge:— Use latest technologies— Participate in technical events and conferences (the cost is covered by the company)— Regular techtalks and professional development Improve your soft skills:— Build strong teamwork skills and become an essential part of the dynamic teams— Improve your English at classes and speaking directly with clients— Increase your productivity and communication level via Scrum, Kanban, Agile methodologies What else do we offer?— Competitive compensation and benefits— Flexible and negotiable schedule— Nice and comfortable office— Covered rest period (20 business days)— Free English classes (we have 3 teachers in our team)— Truly friendly atmosphere and unforgettable events— Discounts offered by individual bonus cards (our partners are coffee shops, bars, and fitness centers)",dou,2020-11-12,Computer Vision Engineer@CHI Software,,,"{""Required skills"": [""Requirements:"", ""Strong knowledge in computer vision fundamentals i.e. OpticalFlow, HOG, feature detection algorithms, Hough Transform, Haar Cascades, Homography, Morphology, Denoising/Deblurring, and image processing algorithms"", ""Strong Python knowledge"", ""Strong practical experience with DL/CV frameworks like OpenCV, PyTorch, MXNet, Tensorflow, or Keras"", ""Experience with some of the well-known neural networks architectures such as Yolo, MobileNet, U-Net, R-CNN-based architectures"", ""Understanding state-of-the-art CV approaches for problems like object detection/tracking, video analysis, semantic segmentation, pose estimation, optical character recognition"", ""Upper-intermediate level of English mandatory""], ""As a plus"": [""Would be a plus:"", ""Experience with R, C++"", ""Experience with following modern neural network architectures: LSTM and other RNN-based, Transformers(BERT, etc.)."", ""Familiarity with time-series predictive/anomaly detection analyses, natural language processing, signal processing"", ""Understanding SOTA approaches for machine learning problems like unsupervised / semi-supervised learning."", ""Experience with the following DL frameworks: DLib, Darknet, Theano."", ""Awareness of CRISP-DM process model"", ""Experience with continuous integration and release management tools, preferably within the AWS platform."", ""Hands-on Experience with the common architecture of MLOps system by the means of Hadoop, Docker, Kubernetes, cloud services and experience with managing production ML lifecycle""], ""We offer"": [""With us you can:""]}"
Data Science UA,https://jobs.dou.ua/companies/data-science-ua/,Senior Front-end (React) Engineer,https://jobs.dou.ua/companies/data-science-ua/vacancies/136259/," Kyiv, remote",26 October 2020,,"Required skills Looking only candidates of the senior level, who have completed 2-3 large projects, in two different industries ● Senior-level experience working with React and React Hooks● Great knowledge of Vanilla JS and TypeScript● Great knowledge of CSS preprocessor (LESS/SASS), CSS3 and HTML5● Experience with Jest/Mocha/Jasmine● Experience working with REST API● Experience working with sockets (socket.io)● Understanding of latest ECMAScript specifications● Good knowledge of GIT● Fluent English As a plus ● Experience working with Figma is a plus● Experience in a CSS-in-JS, Styled Components, Styled System is a plus● Experience in a Symfony is a plus● Experience working with Docker is a plus● Experience working with Twig or any other PHP templating engine We offer — Competitive salary and employee stock options (the exact format will be adjusted by country)— Referral and sign-on bonus program— Flexible working hours— Flexible vacation agreed upon during the hiring process— Top-notch tech equipment to work withWellness benefits— Great startup atmosphere, team spirit, and team events Responsibilities — Develop UI/UX for new Lokalise web app using the specifications and designs— Deliver world-class user experience and user interface while executing understandable, maintainable and well-tested code— UI/UX improvement and refactoring of existing Lokalise web app— Seek a clear understanding of technical requirements through interpreting user stories and UX documents— Work closely with Product, UX/UI, and the Engineering team to execute business deliverables— Remediate bugs throughout the development process— Cover own code with unit tests— Generate technical and support procedure documentationGain in-depth product knowledge and become familiar with the software localization process over the globe Project description Our partner is a localization and translation management platform for agile teams. Embrace automation, workflow transparency, and fast project delivery",dou,2020-11-12,Senior Front-end (React) Engineer@Data Science UA,,,"{""Required skills"": [""Looking only candidates of the senior level, who have completed 2-3 large projects, in two different industries""], ""As a plus"": [""\u25cf Senior-level experience working with React and React Hooks\u25cf Great knowledge of Vanilla JS and TypeScript\u25cf Great knowledge of CSS preprocessor (LESS/SASS), CSS3 and HTML5\u25cf Experience with Jest/Mocha/Jasmine\u25cf Experience working with REST API\u25cf Experience working with sockets (socket.io)\u25cf Understanding of latest ECMAScript specifications\u25cf Good knowledge of GIT\u25cf Fluent English""], ""We offer"": [""\u25cf Experience working with Figma is a plus\u25cf Experience in a CSS-in-JS, Styled Components, Styled System is a plus\u25cf Experience in a Symfony is a plus\u25cf Experience working with Docker is a plus\u25cf Experience working with Twig or any other PHP templating engine""], ""Responsibilities"": [""Competitive salary and employee stock options (the exact format will be adjusted by country)"", ""Referral and sign-on bonus program"", ""Flexible working hours"", ""Flexible vacation agreed upon during the hiring process"", ""Top-notch tech equipment to work withWellness benefits"", ""Great startup atmosphere, team spirit, and team events""], ""Project description"": [""Develop UI/UX for new Lokalise web app using the specifications and designs"", ""Deliver world-class user experience and user interface while executing understandable, maintainable and well-tested code"", ""UI/UX improvement and refactoring of existing Lokalise web app"", ""Seek a clear understanding of technical requirements through interpreting user stories and UX documents"", ""Work closely with Product, UX/UI, and the Engineering team to execute business deliverables"", ""Remediate bugs throughout the development process"", ""Cover own code with unit tests"", ""Generate technical and support procedure documentationGain in-depth product knowledge and become familiar with the software localization process over the globe""]}"
Data Science UA,https://jobs.dou.ua/companies/data-science-ua/,Senior PHP Backend Developer,https://jobs.dou.ua/companies/data-science-ua/vacancies/136252/," Kyiv, remote",26 October 2020,,"Required skills Looking only candidates of the senior level, who have completed 2-3 large projects, in two different industries. — Senior-level knowledge of PHP and Symfony (or similar) framework— Good knowledge of DB architectural patterns and their usage in MySQL— Good knowledge of GIT— Experience with LESS/SASS, CSS3 and HTML— Experience working with Docker— Experience in evaluation of performance and memory-consumption of your code— Knowledge of Redis, Mongo, ElasticSearch, Kubernetes is a plus— Experience working with React.js Single-Page-Applications is a plus— Fluent Englis We offer — Competitive salary and employee stock options (the exact format will be adjusted by country)— Referral and sign-on bonus program— Flexible working hours— Flexible vacation agreed upon during the hiring process— Top-notch tech equipment to work withWellness benefits— Great startup atmosphere, team spirit, and team events Responsibilities — Deliver world-class customer experience while executing understandable, maintainable, and well-tested code— Seek a clear understanding of technical requirements through interpreting user stories and UX documents— Work closely with Product, UX/UI, and the Engineering team to execute business deliverables— Remediate bugs throughout the development process— Generate technical and support procedure documentation— Gain in-depth product knowledge and become familiar with the software localization process over the globe Project description Our partner is a localization and translation management platform for agile teams. Embrace automation, workflow transparency, and fast project delivery",dou,2020-11-12,Senior PHP Backend Developer@Data Science UA,,,"{""Required skills"": [""Looking only candidates of the senior level, who have completed 2-3 large projects, in two different industries.""], ""We offer"": [""Senior-level knowledge of PHP and Symfony (or similar) framework"", ""Good knowledge of DB architectural patterns and their usage in MySQL"", ""Good knowledge of GIT"", ""Experience with LESS/SASS, CSS3 and HTML"", ""Experience working with Docker"", ""Experience in evaluation of performance and memory-consumption of your code"", ""Knowledge of Redis, Mongo, ElasticSearch, Kubernetes is a plus"", ""Experience working with React.js Single-Page-Applications is a plus"", ""Fluent Englis""], ""Responsibilities"": [""Competitive salary and employee stock options (the exact format will be adjusted by country)"", ""Referral and sign-on bonus program"", ""Flexible working hours"", ""Flexible vacation agreed upon during the hiring process"", ""Top-notch tech equipment to work withWellness benefits"", ""Great startup atmosphere, team spirit, and team events""], ""Project description"": [""Deliver world-class customer experience while executing understandable, maintainable, and well-tested code"", ""Seek a clear understanding of technical requirements through interpreting user stories and UX documents"", ""Work closely with Product, UX/UI, and the Engineering team to execute business deliverables"", ""Remediate bugs throughout the development process"", ""Generate technical and support procedure documentation"", ""Gain in-depth product knowledge and become familiar with the software localization process over the globe""]}"
Proxet,https://jobs.dou.ua/companies/proxet/,Senior Data Engineer for Mindyra,https://jobs.dou.ua/companies/proxet/vacancies/136232/, remote,26 October 2020,,"Required skills — A clear understanding of ETL and data curation— Strong experience and understanding of relational databases (including internals) (e.g. PostgreSQL, MySQL, etc.)— Strong experience with AWS— Strong expertise in Python, SQL— Experience with AirFlow is a big plus— Experience with continuous integration, test automation, and deployment— Understanding of integration with BI tools— Advanced / fluent written and spoken English We offer — Challenging work in an international professional environment— Mastering English language with a native speaker— 40-hour working week with flexible working hours— Flexible work-from-home policy— Competitive salary— PE accounting and support— 20 paid vacation days per year— 14 paid sick leaves per year— Medical insurance— Annual 250$ deposit for attending external events (conferences, workshops, etc.)— Long-term employment and real opportunities to change roles and projects within the company— Yoga classes, workout corner— Collaborative friendly team environment— Cozy fully equipped office space in the city center Responsibilities — Create a data platform architecture— Build ETL pipelines— Model databases for a use by data analysts and data scientists— Working alongside teams within the business or the management team to establish business needs Project description mindyra.com is a behavioral health technology and data analytics company that was developed with one thought in mind: create a streamlined system that can be used to effortlessly, accurately, and systematically move through the phases of mental healthcare; from diagnostic assessment through tracking treatment progress. We are developing the next generation of digital health platform that automates & streamlines workflows for providers; uses AI to more precisely diagnose and coordinate care for individuals affected by emotional and substance abuse disorders. We are building a secure, HIPAA/HITECH compliant platform that includes all of the tools necessary to streamline the mental healthcare process both for patients and providers. At the core of the system is focus on measurement and data collection.",dou,2020-11-12,Senior Data Engineer for Mindyra@Proxet,,,"{""Required skills"": [""A clear understanding of ETL and data curation"", ""Strong experience and understanding of relational databases (including internals) (e.g. PostgreSQL, MySQL, etc.)"", ""Strong experience with AWS"", ""Strong expertise in Python, SQL"", ""Experience with AirFlow is a big plus"", ""Experience with continuous integration, test automation, and deployment"", ""Understanding of integration with BI tools"", ""Advanced / fluent written and spoken English""], ""We offer"": [""Challenging work in an international professional environment"", ""Mastering English language with a native speaker"", ""40-hour working week with flexible working hours"", ""Flexible work-from-home policy"", ""Competitive salary"", ""PE accounting and support"", ""20 paid vacation days per year"", ""14 paid sick leaves per year"", ""Medical insurance"", ""Annual 250$ deposit for attending external events (conferences, workshops, etc.)"", ""Long-term employment and real opportunities to change roles and projects within the company"", ""Yoga classes, workout corner"", ""Collaborative friendly team environment"", ""Cozy fully equipped office space in the city center""], ""Responsibilities"": [""Create a data platform architecture"", ""Build ETL pipelines"", ""Model databases for a use by data analysts and data scientists"", ""Working alongside teams within the business or the management team to establish business needs""], ""Project description"": [""mindyra.com is a behavioral health technology and data analytics company that was developed with one thought in mind: create a streamlined system that can be used to effortlessly, accurately, and systematically move through the phases of mental healthcare; from diagnostic assessment through tracking treatment progress. We are developing the next generation of digital health platform that automates & streamlines workflows for providers; uses AI to more precisely diagnose and coordinate care for individuals affected by emotional and substance abuse disorders. We are building a secure, HIPAA/HITECH compliant platform that includes all of the tools necessary to streamline the mental healthcare process both for patients and providers. At the core of the system is focus on measurement and data collection.""]}"
NIX Solutions,https://jobs.dou.ua/companies/nix-solutions-ltd/,Business Data Analyst,https://jobs.dou.ua/companies/nix-solutions-ltd/vacancies/77330/, Kharkiv,26 October 2020,,"Required skills • высшее образование в области информационных технологий, экономики, математики или смежной;• знание английского языка на уровне Intermediate и выше;• навыки общения и ведения переговоров с внешними организациями и внутренними клиентами;• отличные навыки тайм-менеджмента, способность многозадачности и быстрому обучению;• знание SQL;• понимание принципов реляционных БД; • понимание принципов построения хранилищ данных (Data Warehouse) и разработки ETL-процессов;• опыт использования BI инструментов (Power BI, Tableau, Looker);• опыт использования MS Excel для аналитики данных;• знание подходов анализа и визуализации данных;• опыт управления требованиями и написания технической спецификации. As a plus • базовые знания Python или R;• знание инструментов управления требованиями, в частности Jira и Confluence;• опыт использования систем управления версиями (Github/Gitlab)• опыт работы с инструментарием для аналитики пользователей: Google Analytics, GTM, AdWords, Facebook Insights и т. д.• понимание бизнес-аналитических метрик, знание статистических методов и их применение в бизнес-задачах: воронка конверсии, LTV / CAC, ROI, когортный анализ, анализ моментов a-ha, K-средних, деревья решений и т. д. We offer • удобный офис в центре Харькова (м. Университет) и легендарные корпоративы;• оплачиваемое участие в самых масштабных отраслевых конференциях мира;• отличные возможности и перспективы профессионального роста в компании с 25-летней историей;• достойный уровень вознаграждения;• премии за высокие результаты работы;• возможность постоянного обучения, курсы внутри компании, участие в конференциях от компании, курсы английского;• медицинская и спортивная программа, юридическая поддержка. Responsibilities • выявление бизнес-потребности и цели;• исследование, анализ и обработка больших объемов данных, для превращения их в полезные инсайты;• создание автоматизированных дашбордов и отчетов для повышения осведомленности и понимания ключевых показателей с использованием различных инструментов BI;• выполнение аналитических ad hoc запросов;• разработка процессов подготовки данных и моделей данных для анализа и построения отчетов;• общение с заказчиками напрямую на английском. Project description Приглашаем в команду человека с аналитическим складом ума для работы в дружном отделе аналитики. Здесь ты сможешь участвовать в нахождении инсайтов, взаимодействие с конечными пользователями, бизнес анализе новых запросов, а также проявить свои лидерские качества. Мы ищем в команду человека, готового участвовать в формировании направления развития продуктов или информационных систем, принимать важные аналитические решения.",dou,2020-11-12,Business Data Analyst@NIX Solutions,,,"{""Required skills"": [""\u0432\u044b\u0441\u0448\u0435\u0435 \u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u0435 \u0432 \u043e\u0431\u043b\u0430\u0441\u0442\u0438 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u043e\u043d\u043d\u044b\u0445 \u0442\u0435\u0445\u043d\u043e\u043b\u043e\u0433\u0438\u0439, \u044d\u043a\u043e\u043d\u043e\u043c\u0438\u043a\u0438, \u043c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u043a\u0438 \u0438\u043b\u0438 \u0441\u043c\u0435\u0436\u043d\u043e\u0439;"", ""\u0437\u043d\u0430\u043d\u0438\u0435 \u0430\u043d\u0433\u043b\u0438\u0439\u0441\u043a\u043e\u0433\u043e \u044f\u0437\u044b\u043a\u0430 \u043d\u0430 \u0443\u0440\u043e\u0432\u043d\u0435 Intermediate \u0438 \u0432\u044b\u0448\u0435;"", ""\u043d\u0430\u0432\u044b\u043a\u0438 \u043e\u0431\u0449\u0435\u043d\u0438\u044f \u0438 \u0432\u0435\u0434\u0435\u043d\u0438\u044f \u043f\u0435\u0440\u0435\u0433\u043e\u0432\u043e\u0440\u043e\u0432 \u0441 \u0432\u043d\u0435\u0448\u043d\u0438\u043c\u0438 \u043e\u0440\u0433\u0430\u043d\u0438\u0437\u0430\u0446\u0438\u044f\u043c\u0438 \u0438 \u0432\u043d\u0443\u0442\u0440\u0435\u043d\u043d\u0438\u043c\u0438 \u043a\u043b\u0438\u0435\u043d\u0442\u0430\u043c\u0438;"", ""\u043e\u0442\u043b\u0438\u0447\u043d\u044b\u0435 \u043d\u0430\u0432\u044b\u043a\u0438 \u0442\u0430\u0439\u043c-\u043c\u0435\u043d\u0435\u0434\u0436\u043c\u0435\u043d\u0442\u0430, \u0441\u043f\u043e\u0441\u043e\u0431\u043d\u043e\u0441\u0442\u044c \u043c\u043d\u043e\u0433\u043e\u0437\u0430\u0434\u0430\u0447\u043d\u043e\u0441\u0442\u0438 \u0438 \u0431\u044b\u0441\u0442\u0440\u043e\u043c\u0443 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044e;"", ""\u0437\u043d\u0430\u043d\u0438\u0435 SQL;"", ""\u043f\u043e\u043d\u0438\u043c\u0430\u043d\u0438\u0435 \u043f\u0440\u0438\u043d\u0446\u0438\u043f\u043e\u0432 \u0440\u0435\u043b\u044f\u0446\u0438\u043e\u043d\u043d\u044b\u0445 \u0411\u0414;"", ""\u043f\u043e\u043d\u0438\u043c\u0430\u043d\u0438\u0435 \u043f\u0440\u0438\u043d\u0446\u0438\u043f\u043e\u0432 \u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u044f \u0445\u0440\u0430\u043d\u0438\u043b\u0438\u0449 \u0434\u0430\u043d\u043d\u044b\u0445 (Data Warehouse) \u0438 \u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u043a\u0438 ETL-\u043f\u0440\u043e\u0446\u0435\u0441\u0441\u043e\u0432;"", ""\u043e\u043f\u044b\u0442 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u044f BI \u0438\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442\u043e\u0432 (Power BI, Tableau, Looker);"", ""\u043e\u043f\u044b\u0442 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u044f MS Excel \u0434\u043b\u044f \u0430\u043d\u0430\u043b\u0438\u0442\u0438\u043a\u0438 \u0434\u0430\u043d\u043d\u044b\u0445;"", ""\u0437\u043d\u0430\u043d\u0438\u0435 \u043f\u043e\u0434\u0445\u043e\u0434\u043e\u0432 \u0430\u043d\u0430\u043b\u0438\u0437\u0430 \u0438 \u0432\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 \u0434\u0430\u043d\u043d\u044b\u0445;"", ""\u043e\u043f\u044b\u0442 \u0443\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u044f \u0442\u0440\u0435\u0431\u043e\u0432\u0430\u043d\u0438\u044f\u043c\u0438 \u0438 \u043d\u0430\u043f\u0438\u0441\u0430\u043d\u0438\u044f \u0442\u0435\u0445\u043d\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u0441\u043f\u0435\u0446\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438.""], ""As a plus"": [""\u0431\u0430\u0437\u043e\u0432\u044b\u0435 \u0437\u043d\u0430\u043d\u0438\u044f Python \u0438\u043b\u0438 R;"", ""\u0437\u043d\u0430\u043d\u0438\u0435 \u0438\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442\u043e\u0432 \u0443\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u044f \u0442\u0440\u0435\u0431\u043e\u0432\u0430\u043d\u0438\u044f\u043c\u0438, \u0432 \u0447\u0430\u0441\u0442\u043d\u043e\u0441\u0442\u0438 Jira \u0438 Confluence;"", ""\u043e\u043f\u044b\u0442 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u044f \u0441\u0438\u0441\u0442\u0435\u043c \u0443\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u044f \u0432\u0435\u0440\u0441\u0438\u044f\u043c\u0438 (Github/Gitlab)"", ""\u043e\u043f\u044b\u0442 \u0440\u0430\u0431\u043e\u0442\u044b \u0441 \u0438\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442\u0430\u0440\u0438\u0435\u043c \u0434\u043b\u044f \u0430\u043d\u0430\u043b\u0438\u0442\u0438\u043a\u0438 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439: Google Analytics, GTM, AdWords, Facebook Insights \u0438 \u0442. \u0434."", ""\u043f\u043e\u043d\u0438\u043c\u0430\u043d\u0438\u0435 \u0431\u0438\u0437\u043d\u0435\u0441-\u0430\u043d\u0430\u043b\u0438\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u043c\u0435\u0442\u0440\u0438\u043a, \u0437\u043d\u0430\u043d\u0438\u0435 \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u043c\u0435\u0442\u043e\u0434\u043e\u0432 \u0438 \u0438\u0445 \u043f\u0440\u0438\u043c\u0435\u043d\u0435\u043d\u0438\u0435 \u0432 \u0431\u0438\u0437\u043d\u0435\u0441-\u0437\u0430\u0434\u0430\u0447\u0430\u0445: \u0432\u043e\u0440\u043e\u043d\u043a\u0430 \u043a\u043e\u043d\u0432\u0435\u0440\u0441\u0438\u0438, LTV / CAC, ROI, \u043a\u043e\u0433\u043e\u0440\u0442\u043d\u044b\u0439 \u0430\u043d\u0430\u043b\u0438\u0437, \u0430\u043d\u0430\u043b\u0438\u0437 \u043c\u043e\u043c\u0435\u043d\u0442\u043e\u0432 a-ha, K-\u0441\u0440\u0435\u0434\u043d\u0438\u0445, \u0434\u0435\u0440\u0435\u0432\u044c\u044f \u0440\u0435\u0448\u0435\u043d\u0438\u0439 \u0438 \u0442. \u0434.""], ""We offer"": [""\u0443\u0434\u043e\u0431\u043d\u044b\u0439 \u043e\u0444\u0438\u0441 \u0432 \u0446\u0435\u043d\u0442\u0440\u0435 \u0425\u0430\u0440\u044c\u043a\u043e\u0432\u0430 (\u043c. \u0423\u043d\u0438\u0432\u0435\u0440\u0441\u0438\u0442\u0435\u0442) \u0438 \u043b\u0435\u0433\u0435\u043d\u0434\u0430\u0440\u043d\u044b\u0435 \u043a\u043e\u0440\u043f\u043e\u0440\u0430\u0442\u0438\u0432\u044b;"", ""\u043e\u043f\u043b\u0430\u0447\u0438\u0432\u0430\u0435\u043c\u043e\u0435 \u0443\u0447\u0430\u0441\u0442\u0438\u0435 \u0432 \u0441\u0430\u043c\u044b\u0445 \u043c\u0430\u0441\u0448\u0442\u0430\u0431\u043d\u044b\u0445 \u043e\u0442\u0440\u0430\u0441\u043b\u0435\u0432\u044b\u0445 \u043a\u043e\u043d\u0444\u0435\u0440\u0435\u043d\u0446\u0438\u044f\u0445 \u043c\u0438\u0440\u0430;"", ""\u043e\u0442\u043b\u0438\u0447\u043d\u044b\u0435 \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e\u0441\u0442\u0438 \u0438 \u043f\u0435\u0440\u0441\u043f\u0435\u043a\u0442\u0438\u0432\u044b \u043f\u0440\u043e\u0444\u0435\u0441\u0441\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e\u0433\u043e \u0440\u043e\u0441\u0442\u0430 \u0432 \u043a\u043e\u043c\u043f\u0430\u043d\u0438\u0438 \u0441 25-\u043b\u0435\u0442\u043d\u0435\u0439 \u0438\u0441\u0442\u043e\u0440\u0438\u0435\u0439;"", ""\u0434\u043e\u0441\u0442\u043e\u0439\u043d\u044b\u0439 \u0443\u0440\u043e\u0432\u0435\u043d\u044c \u0432\u043e\u0437\u043d\u0430\u0433\u0440\u0430\u0436\u0434\u0435\u043d\u0438\u044f;"", ""\u043f\u0440\u0435\u043c\u0438\u0438 \u0437\u0430 \u0432\u044b\u0441\u043e\u043a\u0438\u0435 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b \u0440\u0430\u0431\u043e\u0442\u044b;"", ""\u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e\u0441\u0442\u044c \u043f\u043e\u0441\u0442\u043e\u044f\u043d\u043d\u043e\u0433\u043e \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f, \u043a\u0443\u0440\u0441\u044b \u0432\u043d\u0443\u0442\u0440\u0438 \u043a\u043e\u043c\u043f\u0430\u043d\u0438\u0438, \u0443\u0447\u0430\u0441\u0442\u0438\u0435 \u0432 \u043a\u043e\u043d\u0444\u0435\u0440\u0435\u043d\u0446\u0438\u044f\u0445 \u043e\u0442 \u043a\u043e\u043c\u043f\u0430\u043d\u0438\u0438, \u043a\u0443\u0440\u0441\u044b \u0430\u043d\u0433\u043b\u0438\u0439\u0441\u043a\u043e\u0433\u043e;"", ""\u043c\u0435\u0434\u0438\u0446\u0438\u043d\u0441\u043a\u0430\u044f \u0438 \u0441\u043f\u043e\u0440\u0442\u0438\u0432\u043d\u0430\u044f \u043f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u0430, \u044e\u0440\u0438\u0434\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u043f\u043e\u0434\u0434\u0435\u0440\u0436\u043a\u0430.""], ""Responsibilities"": [""\u0432\u044b\u044f\u0432\u043b\u0435\u043d\u0438\u0435 \u0431\u0438\u0437\u043d\u0435\u0441-\u043f\u043e\u0442\u0440\u0435\u0431\u043d\u043e\u0441\u0442\u0438 \u0438 \u0446\u0435\u043b\u0438;"", ""\u0438\u0441\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u043d\u0438\u0435, \u0430\u043d\u0430\u043b\u0438\u0437 \u0438 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0431\u043e\u043b\u044c\u0448\u0438\u0445 \u043e\u0431\u044a\u0435\u043c\u043e\u0432 \u0434\u0430\u043d\u043d\u044b\u0445, \u0434\u043b\u044f \u043f\u0440\u0435\u0432\u0440\u0430\u0449\u0435\u043d\u0438\u044f \u0438\u0445 \u0432 \u043f\u043e\u043b\u0435\u0437\u043d\u044b\u0435 \u0438\u043d\u0441\u0430\u0439\u0442\u044b;"", ""\u0441\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u0430\u0432\u0442\u043e\u043c\u0430\u0442\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u0434\u0430\u0448\u0431\u043e\u0440\u0434\u043e\u0432 \u0438 \u043e\u0442\u0447\u0435\u0442\u043e\u0432 \u0434\u043b\u044f \u043f\u043e\u0432\u044b\u0448\u0435\u043d\u0438\u044f \u043e\u0441\u0432\u0435\u0434\u043e\u043c\u043b\u0435\u043d\u043d\u043e\u0441\u0442\u0438 \u0438 \u043f\u043e\u043d\u0438\u043c\u0430\u043d\u0438\u044f \u043a\u043b\u044e\u0447\u0435\u0432\u044b\u0445 \u043f\u043e\u043a\u0430\u0437\u0430\u0442\u0435\u043b\u0435\u0439 \u0441 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435\u043c \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u0445 \u0438\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442\u043e\u0432 BI;"", ""\u0432\u044b\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u0435 \u0430\u043d\u0430\u043b\u0438\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0445 ad hoc \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u0432;"", ""\u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u043f\u0440\u043e\u0446\u0435\u0441\u0441\u043e\u0432 \u043f\u043e\u0434\u0433\u043e\u0442\u043e\u0432\u043a\u0438 \u0434\u0430\u043d\u043d\u044b\u0445 \u0438 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u0434\u0430\u043d\u043d\u044b\u0445 \u0434\u043b\u044f \u0430\u043d\u0430\u043b\u0438\u0437\u0430 \u0438 \u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u044f \u043e\u0442\u0447\u0435\u0442\u043e\u0432;"", ""\u043e\u0431\u0449\u0435\u043d\u0438\u0435 \u0441 \u0437\u0430\u043a\u0430\u0437\u0447\u0438\u043a\u0430\u043c\u0438 \u043d\u0430\u043f\u0440\u044f\u043c\u0443\u044e \u043d\u0430 \u0430\u043d\u0433\u043b\u0438\u0439\u0441\u043a\u043e\u043c.""], ""Project description"": [""\u041f\u0440\u0438\u0433\u043b\u0430\u0448\u0430\u0435\u043c \u0432 \u043a\u043e\u043c\u0430\u043d\u0434\u0443 \u0447\u0435\u043b\u043e\u0432\u0435\u043a\u0430 \u0441 \u0430\u043d\u0430\u043b\u0438\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u043c \u0441\u043a\u043b\u0430\u0434\u043e\u043c \u0443\u043c\u0430 \u0434\u043b\u044f \u0440\u0430\u0431\u043e\u0442\u044b \u0432 \u0434\u0440\u0443\u0436\u043d\u043e\u043c \u043e\u0442\u0434\u0435\u043b\u0435 \u0430\u043d\u0430\u043b\u0438\u0442\u0438\u043a\u0438. \u0417\u0434\u0435\u0441\u044c \u0442\u044b \u0441\u043c\u043e\u0436\u0435\u0448\u044c \u0443\u0447\u0430\u0441\u0442\u0432\u043e\u0432\u0430\u0442\u044c \u0432 \u043d\u0430\u0445\u043e\u0436\u0434\u0435\u043d\u0438\u0438 \u0438\u043d\u0441\u0430\u0439\u0442\u043e\u0432, \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0435 \u0441 \u043a\u043e\u043d\u0435\u0447\u043d\u044b\u043c\u0438 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f\u043c\u0438, \u0431\u0438\u0437\u043d\u0435\u0441 \u0430\u043d\u0430\u043b\u0438\u0437\u0435 \u043d\u043e\u0432\u044b\u0445 \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u0432, \u0430 \u0442\u0430\u043a\u0436\u0435 \u043f\u0440\u043e\u044f\u0432\u0438\u0442\u044c \u0441\u0432\u043e\u0438 \u043b\u0438\u0434\u0435\u0440\u0441\u043a\u0438\u0435 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430. \u041c\u044b \u0438\u0449\u0435\u043c \u0432 \u043a\u043e\u043c\u0430\u043d\u0434\u0443 \u0447\u0435\u043b\u043e\u0432\u0435\u043a\u0430, \u0433\u043e\u0442\u043e\u0432\u043e\u0433\u043e \u0443\u0447\u0430\u0441\u0442\u0432\u043e\u0432\u0430\u0442\u044c \u0432 \u0444\u043e\u0440\u043c\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0438 \u043d\u0430\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u044f \u0440\u0430\u0437\u0432\u0438\u0442\u0438\u044f \u043f\u0440\u043e\u0434\u0443\u043a\u0442\u043e\u0432 \u0438\u043b\u0438 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u043e\u043d\u043d\u044b\u0445 \u0441\u0438\u0441\u0442\u0435\u043c, \u043f\u0440\u0438\u043d\u0438\u043c\u0430\u0442\u044c \u0432\u0430\u0436\u043d\u044b\u0435 \u0430\u043d\u0430\u043b\u0438\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u0440\u0435\u0448\u0435\u043d\u0438\u044f.""]}"
R&D Center,https://jobs.dou.ua/companies/rnd-center/,Sensor Research Engineer,https://jobs.dou.ua/companies/rnd-center/vacancies/136223/," Kyiv, Lviv, remote",26 October 2020,,"You will join a team that researches motion localization features for several hardware products at different stages. Your everyday activities will be connected to tasks like: — development of motion detection & localization algorithms using a set of sensors; — improvement of existing algorithms; — multi-sensor fusion for better motion detection & localization; — object classification;— depth estimation;In general, you will take part in designing and researching features for top-grade security cameras and smart doorbells. — STEM degree (Computer Science/Software Engineering/Applied Mathematics/Physics/Electrical Engineering)— Strong knowledge of CS fundamentals, algorithms, and data structures— Algorithm development skills with the mathematical background— Strong C++ (11+) development skills— Solid experience with Machine Learning, Computer Vision, and of course passion to always learn more!— Good written and spoken English — Experience with Python (scikit learn, numpy, pandas, scipy)— Good understanding of physics and math or strong desire to get deeper knowledge— Experience with Digital Signal Processing, Control Theory or Robotics— Experience with Matlab — Working on the world’s most impactful security products. More than that, an opportunity to get some of those for personal usage — Competitive salary and perks— PE accounting and support— WFH and remote working mode possibility. Partial furniture compensation— Social package, including medical insurance available from the start date and sports compensation after the trial period— 18 paid vacation days per year, paid public holidays according to the Ukrainian legislation— Educational possibilities like corporate courses, knowledge hubs, and free English classes. Semiannual performance review— Free meals, fruits, and snacks when working in the office",dou,2020-11-12,Sensor Research Engineer@R&D Center,,,{}
NIX Solutions,https://jobs.dou.ua/companies/nix-solutions-ltd/,Data Engineer (Python),https://jobs.dou.ua/companies/nix-solutions-ltd/vacancies/130773/, Kharkiv,26 October 2020,,"Required skills • знание Python 3 от 1 года;• знание PySpark, Pandas, Numpy;• опыт работы по построению ETL процессов и их оптимизация с использованием PySpark;• опыт работы с реляционными (PostgreSQL, MySQL, Oracle) и нереляционными (MongoDB, Cassandra) базами данных;• знания CI/CD процессов;• умение работать в команде;• способность решать комплексные проблемы;• желание обучаться новому;• английский на уровне Intermediate. As a plus • опыт работы в machine learning;• опыт работы с Hadoop ecosystem (HDFS, Yarn);• знание основ Jenkins, NiFi, AirFlow;• опыт работы с Аgile-методологией;• понимание принципов построения хранилищ данных (Data Warehouses) на одном из клаудов (AWS, Azure, GCP);• знания принципов контейнеризации, включая Docker и Kubernetes. We offer • удобный офис в центре Харькова и легендарные корпоративы;• оплачиваемое участие в самых масштабных отраслевых конференциях мира;• отличные возможности и перспективы профессионального роста в компании с 25-летней историей;• достойный уровень вознаграждения;• премии за высокие результаты работы;• возможность обучения: курсы внутри компании, участие в конференциях от компании, курсы английского;• медицинскую и спортивную программу, бухгалтерскую поддержку. Project description ТЫ ПОЛУЧИШЬ ВОЗМОЖНОСТЬ: • приобрести колоссальный опыт по обработке, генерации и агрегации больших данных;• решать сложные и нетривиальные задачи, непосредственно влияющие на качество продукта;• работать с передовыми технологиями (такими как PySpark, Kubernetes, облачными хранилищами и др.);• координироваться с участниками проекта со всего мира;• участия в проектах компаний, входящие в рейтинг ТОП-100 компаний мира;• стремительного профессионального роста — высокий уровень код-ревью, личный план развития.",dou,2020-11-12,Data Engineer (Python)@NIX Solutions,,,"{""Required skills"": [""\u0437\u043d\u0430\u043d\u0438\u0435 Python 3 \u043e\u0442 1 \u0433\u043e\u0434\u0430;"", ""\u0437\u043d\u0430\u043d\u0438\u0435 PySpark, Pandas, Numpy;"", ""\u043e\u043f\u044b\u0442 \u0440\u0430\u0431\u043e\u0442\u044b \u043f\u043e \u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u044e ETL \u043f\u0440\u043e\u0446\u0435\u0441\u0441\u043e\u0432 \u0438 \u0438\u0445 \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u044f \u0441 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435\u043c PySpark;"", ""\u043e\u043f\u044b\u0442 \u0440\u0430\u0431\u043e\u0442\u044b \u0441 \u0440\u0435\u043b\u044f\u0446\u0438\u043e\u043d\u043d\u044b\u043c\u0438 (PostgreSQL, MySQL, Oracle) \u0438 \u043d\u0435\u0440\u0435\u043b\u044f\u0446\u0438\u043e\u043d\u043d\u044b\u043c\u0438 (MongoDB, Cassandra) \u0431\u0430\u0437\u0430\u043c\u0438 \u0434\u0430\u043d\u043d\u044b\u0445;"", ""\u0437\u043d\u0430\u043d\u0438\u044f CI/CD \u043f\u0440\u043e\u0446\u0435\u0441\u0441\u043e\u0432;"", ""\u0443\u043c\u0435\u043d\u0438\u0435 \u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c \u0432 \u043a\u043e\u043c\u0430\u043d\u0434\u0435;"", ""\u0441\u043f\u043e\u0441\u043e\u0431\u043d\u043e\u0441\u0442\u044c \u0440\u0435\u0448\u0430\u0442\u044c \u043a\u043e\u043c\u043f\u043b\u0435\u043a\u0441\u043d\u044b\u0435 \u043f\u0440\u043e\u0431\u043b\u0435\u043c\u044b;"", ""\u0436\u0435\u043b\u0430\u043d\u0438\u0435 \u043e\u0431\u0443\u0447\u0430\u0442\u044c\u0441\u044f \u043d\u043e\u0432\u043e\u043c\u0443;"", ""\u0430\u043d\u0433\u043b\u0438\u0439\u0441\u043a\u0438\u0439 \u043d\u0430 \u0443\u0440\u043e\u0432\u043d\u0435 Intermediate.""], ""As a plus"": [""\u043e\u043f\u044b\u0442 \u0440\u0430\u0431\u043e\u0442\u044b \u0432 machine learning;"", ""\u043e\u043f\u044b\u0442 \u0440\u0430\u0431\u043e\u0442\u044b \u0441 Hadoop ecosystem (HDFS, Yarn);"", ""\u0437\u043d\u0430\u043d\u0438\u0435 \u043e\u0441\u043d\u043e\u0432 Jenkins, NiFi, AirFlow;"", ""\u043e\u043f\u044b\u0442 \u0440\u0430\u0431\u043e\u0442\u044b \u0441 \u0410gile-\u043c\u0435\u0442\u043e\u0434\u043e\u043b\u043e\u0433\u0438\u0435\u0439;"", ""\u043f\u043e\u043d\u0438\u043c\u0430\u043d\u0438\u0435 \u043f\u0440\u0438\u043d\u0446\u0438\u043f\u043e\u0432 \u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u044f \u0445\u0440\u0430\u043d\u0438\u043b\u0438\u0449 \u0434\u0430\u043d\u043d\u044b\u0445 (Data Warehouses) \u043d\u0430 \u043e\u0434\u043d\u043e\u043c \u0438\u0437 \u043a\u043b\u0430\u0443\u0434\u043e\u0432 (AWS, Azure, GCP);"", ""\u0437\u043d\u0430\u043d\u0438\u044f \u043f\u0440\u0438\u043d\u0446\u0438\u043f\u043e\u0432 \u043a\u043e\u043d\u0442\u0435\u0439\u043d\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u0438, \u0432\u043a\u043b\u044e\u0447\u0430\u044f Docker \u0438 Kubernetes.""], ""We offer"": [""\u0443\u0434\u043e\u0431\u043d\u044b\u0439 \u043e\u0444\u0438\u0441 \u0432 \u0446\u0435\u043d\u0442\u0440\u0435 \u0425\u0430\u0440\u044c\u043a\u043e\u0432\u0430 \u0438 \u043b\u0435\u0433\u0435\u043d\u0434\u0430\u0440\u043d\u044b\u0435 \u043a\u043e\u0440\u043f\u043e\u0440\u0430\u0442\u0438\u0432\u044b;"", ""\u043e\u043f\u043b\u0430\u0447\u0438\u0432\u0430\u0435\u043c\u043e\u0435 \u0443\u0447\u0430\u0441\u0442\u0438\u0435 \u0432 \u0441\u0430\u043c\u044b\u0445 \u043c\u0430\u0441\u0448\u0442\u0430\u0431\u043d\u044b\u0445 \u043e\u0442\u0440\u0430\u0441\u043b\u0435\u0432\u044b\u0445 \u043a\u043e\u043d\u0444\u0435\u0440\u0435\u043d\u0446\u0438\u044f\u0445 \u043c\u0438\u0440\u0430;"", ""\u043e\u0442\u043b\u0438\u0447\u043d\u044b\u0435 \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e\u0441\u0442\u0438 \u0438 \u043f\u0435\u0440\u0441\u043f\u0435\u043a\u0442\u0438\u0432\u044b \u043f\u0440\u043e\u0444\u0435\u0441\u0441\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e\u0433\u043e \u0440\u043e\u0441\u0442\u0430 \u0432 \u043a\u043e\u043c\u043f\u0430\u043d\u0438\u0438 \u0441 25-\u043b\u0435\u0442\u043d\u0435\u0439 \u0438\u0441\u0442\u043e\u0440\u0438\u0435\u0439;"", ""\u0434\u043e\u0441\u0442\u043e\u0439\u043d\u044b\u0439 \u0443\u0440\u043e\u0432\u0435\u043d\u044c \u0432\u043e\u0437\u043d\u0430\u0433\u0440\u0430\u0436\u0434\u0435\u043d\u0438\u044f;"", ""\u043f\u0440\u0435\u043c\u0438\u0438 \u0437\u0430 \u0432\u044b\u0441\u043e\u043a\u0438\u0435 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b \u0440\u0430\u0431\u043e\u0442\u044b;"", ""\u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e\u0441\u0442\u044c \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f: \u043a\u0443\u0440\u0441\u044b \u0432\u043d\u0443\u0442\u0440\u0438 \u043a\u043e\u043c\u043f\u0430\u043d\u0438\u0438, \u0443\u0447\u0430\u0441\u0442\u0438\u0435 \u0432 \u043a\u043e\u043d\u0444\u0435\u0440\u0435\u043d\u0446\u0438\u044f\u0445 \u043e\u0442 \u043a\u043e\u043c\u043f\u0430\u043d\u0438\u0438, \u043a\u0443\u0440\u0441\u044b \u0430\u043d\u0433\u043b\u0438\u0439\u0441\u043a\u043e\u0433\u043e;"", ""\u043c\u0435\u0434\u0438\u0446\u0438\u043d\u0441\u043a\u0443\u044e \u0438 \u0441\u043f\u043e\u0440\u0442\u0438\u0432\u043d\u0443\u044e \u043f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u0443, \u0431\u0443\u0445\u0433\u0430\u043b\u0442\u0435\u0440\u0441\u043a\u0443\u044e \u043f\u043e\u0434\u0434\u0435\u0440\u0436\u043a\u0443.""], ""Project description"": [""\u0422\u042b \u041f\u041e\u041b\u0423\u0427\u0418\u0428\u042c \u0412\u041e\u0417\u041c\u041e\u0416\u041d\u041e\u0421\u0422\u042c:""]}"
Ciklum,https://jobs.dou.ua/companies/ciklum/,Lead Big Data Engineer for Ciklum Digital,https://jobs.dou.ua/companies/ciklum/vacancies/136211/, Kyiv,26 October 2020,,"On behalf of Data & Analytics, Ciklum is looking for a Lead Big Data Engineer to join the UA team on a full-time basis. You will join a highly motivated team and will be working on a modern solution for our existing client. We are looking for technology experts who want to make an impact on new business by applying best practices and taking ownership. Project description:",dou,2020-11-12,Lead Big Data Engineer for Ciklum Digital@Ciklum,,,{}
NIX Solutions,https://jobs.dou.ua/companies/nix-solutions-ltd/,Data Engineer (Microsoft stack),https://jobs.dou.ua/companies/nix-solutions-ltd/vacancies/120652/, Kharkiv,26 October 2020,,"Required skills • твердые знания T-SQL;• опыт работы с MS SQL Server и SSIS;· • оптимизации запросов к базе данных;• знание подходов и принципов построения ETL процессов;• понимание принципов построения хранилищ данных (DWH);• опыт разработки процессов подготовки данных и моделей данных для анализа и построения отчетов;• способность работать в команде;• высокие коммуникативные навыки;• знание английского на уровне Intermediate+. As a plus • доменные знания в Страховом бизнесе;• опыт в оптимизации производительности баз данных;• опыт работы с SSAS, SSRS сервисами;• опыт использования BI системам: Tableau, Power BI, Looker и др.• знание инструментов управления требованиями, в частности Jira и Confluence. We offer • крупный проект из домена страхования от одного из ведущих лидеров в отрасли;• гибкая система пересмотра и повышения оплаты труда;• зарубежные командировки;• помощь в подготовке к профессиональной сертификации MS и оплата сертификата;• оплачиваемое участие в самых масштабных отраслевых конференциях мира;• отличные возможности и перспективы профессионального роста в компании с 25-летней историей;• лучший офис в центре Харькова (м. Университет) и легендарные корпоративы;• возможность постоянного обучения, курсы внутри компании, участие в конференциях от компании, курсы английского;• медицинскую и спортивную программу, бухгалтерскую поддержку. Responsibilities • разработка и поддержка работы хранилищ данных уровня Enterprise;• создание и развитие ETL-процессов, используя MS SQL Server Integration Services;• оптимизация SQL кода, поиск/исправление ошибок; • определение рисков, улучшение схемы данных у источника и получателя;• разработка процессов подготовки данных и моделей данных для анализа и построения отчетов;• общение с заказчиками напрямую на английском. Project description Вакансия для начинающих и опытных дата инженеров, которая предполагает участие в крупном проекте в сфере страхования от ведущего лидера в своем регионе, используя Microsoft stack. В данном проекте ты сможешь присоединиться к команде специалистов и проявить свой талант и технические навыки при построении процессов подготовки, обработки, проверки и миграции данных между различными источниками.",dou,2020-11-12,Data Engineer (Microsoft stack)@NIX Solutions,,,"{""Required skills"": [""\u0442\u0432\u0435\u0440\u0434\u044b\u0435 \u0437\u043d\u0430\u043d\u0438\u044f T-SQL;"", ""\u043e\u043f\u044b\u0442 \u0440\u0430\u0431\u043e\u0442\u044b \u0441 MS SQL Server \u0438 SSIS;\u00b7"", ""\u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u0438 \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u0432 \u043a \u0431\u0430\u0437\u0435 \u0434\u0430\u043d\u043d\u044b\u0445;"", ""\u0437\u043d\u0430\u043d\u0438\u0435 \u043f\u043e\u0434\u0445\u043e\u0434\u043e\u0432 \u0438 \u043f\u0440\u0438\u043d\u0446\u0438\u043f\u043e\u0432 \u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u044f ETL \u043f\u0440\u043e\u0446\u0435\u0441\u0441\u043e\u0432;"", ""\u043f\u043e\u043d\u0438\u043c\u0430\u043d\u0438\u0435 \u043f\u0440\u0438\u043d\u0446\u0438\u043f\u043e\u0432 \u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u044f \u0445\u0440\u0430\u043d\u0438\u043b\u0438\u0449 \u0434\u0430\u043d\u043d\u044b\u0445 (DWH);"", ""\u043e\u043f\u044b\u0442 \u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u043f\u0440\u043e\u0446\u0435\u0441\u0441\u043e\u0432 \u043f\u043e\u0434\u0433\u043e\u0442\u043e\u0432\u043a\u0438 \u0434\u0430\u043d\u043d\u044b\u0445 \u0438 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u0434\u0430\u043d\u043d\u044b\u0445 \u0434\u043b\u044f \u0430\u043d\u0430\u043b\u0438\u0437\u0430 \u0438 \u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u044f \u043e\u0442\u0447\u0435\u0442\u043e\u0432;"", ""\u0441\u043f\u043e\u0441\u043e\u0431\u043d\u043e\u0441\u0442\u044c \u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c \u0432 \u043a\u043e\u043c\u0430\u043d\u0434\u0435;"", ""\u0432\u044b\u0441\u043e\u043a\u0438\u0435 \u043a\u043e\u043c\u043c\u0443\u043d\u0438\u043a\u0430\u0442\u0438\u0432\u043d\u044b\u0435 \u043d\u0430\u0432\u044b\u043a\u0438;"", ""\u0437\u043d\u0430\u043d\u0438\u0435 \u0430\u043d\u0433\u043b\u0438\u0439\u0441\u043a\u043e\u0433\u043e \u043d\u0430 \u0443\u0440\u043e\u0432\u043d\u0435 Intermediate+.""], ""As a plus"": [""\u0434\u043e\u043c\u0435\u043d\u043d\u044b\u0435 \u0437\u043d\u0430\u043d\u0438\u044f \u0432 \u0421\u0442\u0440\u0430\u0445\u043e\u0432\u043e\u043c \u0431\u0438\u0437\u043d\u0435\u0441\u0435;"", ""\u043e\u043f\u044b\u0442 \u0432 \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u0438 \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438 \u0431\u0430\u0437 \u0434\u0430\u043d\u043d\u044b\u0445;"", ""\u043e\u043f\u044b\u0442 \u0440\u0430\u0431\u043e\u0442\u044b \u0441 SSAS, SSRS \u0441\u0435\u0440\u0432\u0438\u0441\u0430\u043c\u0438;"", ""\u043e\u043f\u044b\u0442 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u044f BI \u0441\u0438\u0441\u0442\u0435\u043c\u0430\u043c: Tableau, Power BI, Looker \u0438 \u0434\u0440."", ""\u0437\u043d\u0430\u043d\u0438\u0435 \u0438\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442\u043e\u0432 \u0443\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u044f \u0442\u0440\u0435\u0431\u043e\u0432\u0430\u043d\u0438\u044f\u043c\u0438, \u0432 \u0447\u0430\u0441\u0442\u043d\u043e\u0441\u0442\u0438 Jira \u0438 Confluence.""], ""We offer"": [""\u043a\u0440\u0443\u043f\u043d\u044b\u0439 \u043f\u0440\u043e\u0435\u043a\u0442 \u0438\u0437 \u0434\u043e\u043c\u0435\u043d\u0430 \u0441\u0442\u0440\u0430\u0445\u043e\u0432\u0430\u043d\u0438\u044f \u043e\u0442 \u043e\u0434\u043d\u043e\u0433\u043e \u0438\u0437 \u0432\u0435\u0434\u0443\u0449\u0438\u0445 \u043b\u0438\u0434\u0435\u0440\u043e\u0432 \u0432 \u043e\u0442\u0440\u0430\u0441\u043b\u0438;"", ""\u0433\u0438\u0431\u043a\u0430\u044f \u0441\u0438\u0441\u0442\u0435\u043c\u0430 \u043f\u0435\u0440\u0435\u0441\u043c\u043e\u0442\u0440\u0430 \u0438 \u043f\u043e\u0432\u044b\u0448\u0435\u043d\u0438\u044f \u043e\u043f\u043b\u0430\u0442\u044b \u0442\u0440\u0443\u0434\u0430;"", ""\u0437\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u044b\u0435 \u043a\u043e\u043c\u0430\u043d\u0434\u0438\u0440\u043e\u0432\u043a\u0438;"", ""\u043f\u043e\u043c\u043e\u0449\u044c \u0432 \u043f\u043e\u0434\u0433\u043e\u0442\u043e\u0432\u043a\u0435 \u043a \u043f\u0440\u043e\u0444\u0435\u0441\u0441\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e\u0439 \u0441\u0435\u0440\u0442\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438 MS \u0438 \u043e\u043f\u043b\u0430\u0442\u0430 \u0441\u0435\u0440\u0442\u0438\u0444\u0438\u043a\u0430\u0442\u0430;"", ""\u043e\u043f\u043b\u0430\u0447\u0438\u0432\u0430\u0435\u043c\u043e\u0435 \u0443\u0447\u0430\u0441\u0442\u0438\u0435 \u0432 \u0441\u0430\u043c\u044b\u0445 \u043c\u0430\u0441\u0448\u0442\u0430\u0431\u043d\u044b\u0445 \u043e\u0442\u0440\u0430\u0441\u043b\u0435\u0432\u044b\u0445 \u043a\u043e\u043d\u0444\u0435\u0440\u0435\u043d\u0446\u0438\u044f\u0445 \u043c\u0438\u0440\u0430;"", ""\u043e\u0442\u043b\u0438\u0447\u043d\u044b\u0435 \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e\u0441\u0442\u0438 \u0438 \u043f\u0435\u0440\u0441\u043f\u0435\u043a\u0442\u0438\u0432\u044b \u043f\u0440\u043e\u0444\u0435\u0441\u0441\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e\u0433\u043e \u0440\u043e\u0441\u0442\u0430 \u0432 \u043a\u043e\u043c\u043f\u0430\u043d\u0438\u0438 \u0441 25-\u043b\u0435\u0442\u043d\u0435\u0439 \u0438\u0441\u0442\u043e\u0440\u0438\u0435\u0439;"", ""\u043b\u0443\u0447\u0448\u0438\u0439 \u043e\u0444\u0438\u0441 \u0432 \u0446\u0435\u043d\u0442\u0440\u0435 \u0425\u0430\u0440\u044c\u043a\u043e\u0432\u0430 (\u043c. \u0423\u043d\u0438\u0432\u0435\u0440\u0441\u0438\u0442\u0435\u0442) \u0438 \u043b\u0435\u0433\u0435\u043d\u0434\u0430\u0440\u043d\u044b\u0435 \u043a\u043e\u0440\u043f\u043e\u0440\u0430\u0442\u0438\u0432\u044b;"", ""\u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e\u0441\u0442\u044c \u043f\u043e\u0441\u0442\u043e\u044f\u043d\u043d\u043e\u0433\u043e \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f, \u043a\u0443\u0440\u0441\u044b \u0432\u043d\u0443\u0442\u0440\u0438 \u043a\u043e\u043c\u043f\u0430\u043d\u0438\u0438, \u0443\u0447\u0430\u0441\u0442\u0438\u0435 \u0432 \u043a\u043e\u043d\u0444\u0435\u0440\u0435\u043d\u0446\u0438\u044f\u0445 \u043e\u0442 \u043a\u043e\u043c\u043f\u0430\u043d\u0438\u0438, \u043a\u0443\u0440\u0441\u044b \u0430\u043d\u0433\u043b\u0438\u0439\u0441\u043a\u043e\u0433\u043e;"", ""\u043c\u0435\u0434\u0438\u0446\u0438\u043d\u0441\u043a\u0443\u044e \u0438 \u0441\u043f\u043e\u0440\u0442\u0438\u0432\u043d\u0443\u044e \u043f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u0443, \u0431\u0443\u0445\u0433\u0430\u043b\u0442\u0435\u0440\u0441\u043a\u0443\u044e \u043f\u043e\u0434\u0434\u0435\u0440\u0436\u043a\u0443.""], ""Responsibilities"": [""\u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0438 \u043f\u043e\u0434\u0434\u0435\u0440\u0436\u043a\u0430 \u0440\u0430\u0431\u043e\u0442\u044b \u0445\u0440\u0430\u043d\u0438\u043b\u0438\u0449 \u0434\u0430\u043d\u043d\u044b\u0445 \u0443\u0440\u043e\u0432\u043d\u044f Enterprise;"", ""\u0441\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u0438 \u0440\u0430\u0437\u0432\u0438\u0442\u0438\u0435 ETL-\u043f\u0440\u043e\u0446\u0435\u0441\u0441\u043e\u0432, \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f MS SQL Server Integration Services;"", ""\u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u044f SQL \u043a\u043e\u0434\u0430, \u043f\u043e\u0438\u0441\u043a/\u0438\u0441\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u043e\u0448\u0438\u0431\u043e\u043a;"", ""\u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0440\u0438\u0441\u043a\u043e\u0432, \u0443\u043b\u0443\u0447\u0448\u0435\u043d\u0438\u0435 \u0441\u0445\u0435\u043c\u044b \u0434\u0430\u043d\u043d\u044b\u0445 \u0443 \u0438\u0441\u0442\u043e\u0447\u043d\u0438\u043a\u0430 \u0438 \u043f\u043e\u043b\u0443\u0447\u0430\u0442\u0435\u043b\u044f;"", ""\u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u043f\u0440\u043e\u0446\u0435\u0441\u0441\u043e\u0432 \u043f\u043e\u0434\u0433\u043e\u0442\u043e\u0432\u043a\u0438 \u0434\u0430\u043d\u043d\u044b\u0445 \u0438 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u0434\u0430\u043d\u043d\u044b\u0445 \u0434\u043b\u044f \u0430\u043d\u0430\u043b\u0438\u0437\u0430 \u0438 \u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u044f \u043e\u0442\u0447\u0435\u0442\u043e\u0432;"", ""\u043e\u0431\u0449\u0435\u043d\u0438\u0435 \u0441 \u0437\u0430\u043a\u0430\u0437\u0447\u0438\u043a\u0430\u043c\u0438 \u043d\u0430\u043f\u0440\u044f\u043c\u0443\u044e \u043d\u0430 \u0430\u043d\u0433\u043b\u0438\u0439\u0441\u043a\u043e\u043c.""], ""Project description"": [""\u0412\u0430\u043a\u0430\u043d\u0441\u0438\u044f \u0434\u043b\u044f \u043d\u0430\u0447\u0438\u043d\u0430\u044e\u0449\u0438\u0445 \u0438 \u043e\u043f\u044b\u0442\u043d\u044b\u0445 \u0434\u0430\u0442\u0430 \u0438\u043d\u0436\u0435\u043d\u0435\u0440\u043e\u0432, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u043f\u0440\u0435\u0434\u043f\u043e\u043b\u0430\u0433\u0430\u0435\u0442 \u0443\u0447\u0430\u0441\u0442\u0438\u0435 \u0432 \u043a\u0440\u0443\u043f\u043d\u043e\u043c \u043f\u0440\u043e\u0435\u043a\u0442\u0435 \u0432 \u0441\u0444\u0435\u0440\u0435 \u0441\u0442\u0440\u0430\u0445\u043e\u0432\u0430\u043d\u0438\u044f \u043e\u0442 \u0432\u0435\u0434\u0443\u0449\u0435\u0433\u043e \u043b\u0438\u0434\u0435\u0440\u0430 \u0432 \u0441\u0432\u043e\u0435\u043c \u0440\u0435\u0433\u0438\u043e\u043d\u0435, \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f Microsoft stack. \u0412 \u0434\u0430\u043d\u043d\u043e\u043c \u043f\u0440\u043e\u0435\u043a\u0442\u0435 \u0442\u044b \u0441\u043c\u043e\u0436\u0435\u0448\u044c \u043f\u0440\u0438\u0441\u043e\u0435\u0434\u0438\u043d\u0438\u0442\u044c\u0441\u044f \u043a \u043a\u043e\u043c\u0430\u043d\u0434\u0435 \u0441\u043f\u0435\u0446\u0438\u0430\u043b\u0438\u0441\u0442\u043e\u0432 \u0438 \u043f\u0440\u043e\u044f\u0432\u0438\u0442\u044c \u0441\u0432\u043e\u0439 \u0442\u0430\u043b\u0430\u043d\u0442 \u0438 \u0442\u0435\u0445\u043d\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u043d\u0430\u0432\u044b\u043a\u0438 \u043f\u0440\u0438 \u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u0438 \u043f\u0440\u043e\u0446\u0435\u0441\u0441\u043e\u0432 \u043f\u043e\u0434\u0433\u043e\u0442\u043e\u0432\u043a\u0438, \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438, \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0438 \u0438 \u043c\u0438\u0433\u0440\u0430\u0446\u0438\u0438 \u0434\u0430\u043d\u043d\u044b\u0445 \u043c\u0435\u0436\u0434\u0443 \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u043c\u0438 \u0438\u0441\u0442\u043e\u0447\u043d\u0438\u043a\u0430\u043c\u0438.""]}"
SocialTech,https://jobs.dou.ua/companies/socialtech/,Senior Data Analyst,https://jobs.dou.ua/companies/socialtech/vacancies/129201/, Kyiv,26 October 2020,,"Про продуктРік тому ми запустили RnD напрямок в одній з найбільших продуктових IT-компаній України. Мета — на основі головного бізнесу компанії побудувати продукт з відмінною бізнес-моделлю.За цей час ми підтвердили ціннісну гіпотезу нового продукту: навчилися окупати десктоп версію сайту і на порозі побудови позитивної юніт-економіки на мобільній версії.Наш наступний етап — масштабування. Уже зараз входимо в топ 100 компаній України за обсягами закупівлі трафіку. До кінця наступного року увійдемо в топ 5.Тільки до кінця цього року виростемо по виручці в ~ 20x.Про командуКоманда складається з десяти фахівців Senior-рівня. Невелика кількість людей забезпечує командну гнучкість і високий фокус: кожен працює над своєю зоною відповідальності в досягненні загальних цілей.переваги:● мінімум бюрократії, максимум самостійності.● свобода дій і висока відповідальність.● глобальні рішення приймаємо разом; локальні — кожен сам. Якщо і тебе драйвить швидкість, відповідальність і прямий вплив на бізнес-рішення, тоді давай знайомитися!А ще ти знаєш відповіді на наступні запитання:● Як оцінити якість трафіку в першу добу в транзакційному преміум бізнесі (крива прибутковості розтягнута на роки,% конверсії в платника низький)?● Як швидко можна приймати рішення щодо відключення реклами / підвищення бюджетів?● Як оцінювати A/B тест на старих користувачів з різною довжиною життя / швидкістю споживання контенту?● Як визначити цільового користувача по його поведінковим характеристикам? Для цього використовуєш:● Аналітику маркетингу / продажів (воронки, AO, прогнози обсягів, і т.д.)● Добре знання A/B, A/B/n, A/A тестів і їх оцінка.● SQL, Excel, R / Python, Tableau / Power BI / etc. При цьому ти:● проактивний (не боїшся проявляти ініціативу і пропонувати нові нестандартні рішення).● працюєш на результат («бачу мету — не бачу перешкод»).● амбітний (хочеш стати найкращим у всьому, що робиш).● позитивно мислиш («це не проблема — це можливість»).● відкритий до нового (визнаєш свої помилки і несеш відповідальність за них). Спробуємо «здивувати» умовами:● Гнучкий графік.● Безкоштовні сніданки, обіди та снеки.● Професійний розвиток.● Компенсація спортивних активностей (є корпоративна команда з футболу).● Страховий медичний поліс для всіх співробітників рівня «Elite».● Корпоративний лікар.● Доступ до корисної літератури, тренінгів, семінарів.● Участь в ключових заходах IT індустрії (як в Україні, так і за кордоном).● Працюємо по ФОПу, 3 група. Бухгалтер відкриває, веде і консультує ФОП.● Робота в крутій команді. Ми особливу увагу приділяємо пошуку талантів і їх розвитку, тому всі співробітники активні і мотивовані на нестандартне рішення задач.● Новий великий і просторий офіс на Подолі (вул. Кирилівська, 40А. 5 хвилин від м.Тараса Шевченко), PlayStation 4 Pro і інші «плюшки». Тисни Apply to Position і приєднуйся до команди!Якщо знаєш гарну кандидатуру, переходь за посиланням bit.ly/33WU6Z8 та отримай приємний бонус за успішну рекомендацію.",dou,2020-11-12,Senior Data Analyst@SocialTech,,,{}
JustAnswer,https://jobs.dou.ua/companies/justanswer/,Data Analyst,https://jobs.dou.ua/companies/justanswer/vacancies/102522/, Lviv,26 October 2020,,"Required skills — Educational background in Economics/Finance/Math/Statistics/Engineering— Proficiency in analysis and business modeling using Excel — Understanding of statistics — Business thinking — Ability to coordinate multiple tasks, set and negotiate priorities— Experience with web analytics, including a good understanding of tools and processes to collect and report on all aspects of web analytics (e.g. Google Analytics, Omniture) would be a plus — Basic to mid-level proficiency in data extraction using SQL— Advanced English level (written/spoken) We offer — Time off throughout the year (paid and unpaid)— Professional development support and encouragement— Resources to help improve your overall well-being— Free membership to dive into JA product Responsibilities — Work with Product and Analytics leadership to conceive and structure analysis, and deliver highly actionable insights from “deep dives” into specific areas of our business— Conduct analysis of a tremendous amount of internal & external data to find growth & optimization opportunities for the business— Package and communicate findings and recommendations to a broad audience (including senior leadership) Project description JustAnswer is an online marketplace that is revolutionizing professional services by making fast, affordable expert help accessible to people everywhere. Hailed by The Huffington Post as “destined for glory,” our company is made of smart, hard-working people who know how to have fun and get things done. From doctors, lawyers, and vets to auto mechanics and tech support, the 10,000 experts on JustAnswer have helped over eight million people in 196 countries since 2003. Headquartered in the beautiful Presidio of San Francisco, our mission is simple and inspiring: “We help people.” JustAnswer, LLC is backed by investors Crosslink Capital, Glynn Capital, and Charles Schwab.",dou,2020-11-12,Data Analyst@JustAnswer,,,"{""Required skills"": [""Educational background in Economics/Finance/Math/Statistics/Engineering"", ""Proficiency in analysis and business modeling using Excel"", ""Understanding of statistics"", ""Business thinking"", ""Ability to coordinate multiple tasks, set and negotiate priorities"", ""Experience with web analytics, including a good understanding of tools and processes to collect and report on all aspects of web analytics (e.g. Google Analytics, Omniture) would be a plus"", ""Basic to mid-level proficiency in data extraction using SQL"", ""Advanced English level (written/spoken)""], ""We offer"": [""Time off throughout the year (paid and unpaid)"", ""Professional development support and encouragement"", ""Resources to help improve your overall well-being"", ""Free membership to dive into JA product""], ""Responsibilities"": [""Work with Product and Analytics leadership to conceive and structure analysis, and deliver highly actionable insights from \u201cdeep dives\u201d into specific areas of our business"", ""Conduct analysis of a tremendous amount of internal & external data to find growth & optimization opportunities for the business"", ""Package and communicate findings and recommendations to a broad audience (including senior leadership)""], ""Project description"": [""JustAnswer is an online marketplace that is revolutionizing professional services by making fast, affordable expert help accessible to people everywhere. Hailed by The Huffington Post as \u201cdestined for glory,\u201d our company is made of smart, hard-working people who know how to have fun and get things done. From doctors, lawyers, and vets to auto mechanics and tech support, the 10,000 experts on JustAnswer have helped over eight million people in 196 countries since 2003. Headquartered in the beautiful Presidio of San Francisco, our mission is simple and inspiring: \u201cWe help people.\u201d JustAnswer, LLC is backed by investors Crosslink Capital, Glynn Capital, and Charles Schwab.""]}"
Fastbet,https://jobs.dou.ua/companies/fastbet/,Sport Mathematician,https://jobs.dou.ua/companies/fastbet/vacancies/128875/, Kyiv,26 October 2020,,"We are the fast-growing product company and now looking for a Sport Mathematician.Our main direction is producing content for betting companies, such as broadcasts, incidents, and derivative content. We conduct tournaments in fast sports disciplines such as table tennis, FIFA, CS: GO, organize video broadcasts, and develop community among sports participants.",dou,2020-11-12,Sport Mathematician@Fastbet,,,{}
Epicentr M,https://jobs.dou.ua/companies/epicentr-m/,Lead Data Science,https://jobs.dou.ua/companies/epicentr-m/vacancies/133045/, Kyiv,26 October 2020,,"Required skills Епіцентр Маркетлпейс — це українська продуктова компанія, що створює майданчик для малого та середнього бізнесу в сегменті DIY та товарів для дому. Офлайн частина нашої компанії — це найбільша мережа торгових центрів у країні Epicetrk.ua — це більш як 3 млн. переглядів товарів та близько 20 млн. подій в день.Це терабайти даних які ми вже збираємо, та які ще потрібно почати збирати. Це офлайн та онлайн продажі одного й того самого покупця, що вчора додав у кошик мобільний телефон, а сьогодні купив в офлайні Siltek P-15 2 mm 25 (ми теж гуглили).Це місце де купують губну помаду та цемент в одному замовленні і в цьому є логіка та стійкий патерн поведінки.В нас дійсно прикольно) Що ми задумали? Зараз перед нашою командою стоїть кілька величезних задач:— Зробити крутий повнотекстовий пошук— Розробити систему рекомендацій товарів для кожного покупця. І його дружини. І кота.— Створити систему комплексної аналітики, яка почнеться в момент коли користувач міг побачити наш баннер на маршрутці в Ковелі та закінчуючи чайовими кур’єру. Хто нам треба:Зараз ми шукаємо ліда Data Science команди. Світило Tensorflow та суперсамостійного розробника, який відчуває в собі сили, а ще краще, має досвід, для побудови підрозділу Data Science.P.S.: Чесно, в нас поки що немає теоретичних задач, тільки прикладні. В кандидатах шукаємо наступне :— Досвід від 3-х років та готові проекти у портфоліо— Самостійність та вміння пояснити чому щось працює саме так, а не інакше— Високий рівень комунікаційних навичок. Ти ж лід, як ні як)— Математична база: статистика, теорія вірогідностей, гарні знання алгоритмів— Знання класичних алгоритмів ML— Гарні знання Python (C++)— PyTorch/Tensorflow (Keras)— Elastic Search. Тому що в нас на ньому пошук— Бути боженькою SQL— Вміння шукати відповіді на свої питання та швидко вчитись— Досвід роботи з WEB, коли в теле 100500 івентів в секунди— Docker— Вміння писати зрозумілі для інших людей логи своїх сервісів— Знання ETL інструментів— RabbitMQ— MongoDB— Big Query\ClickHouse\Amazon RedshiftПриходь. Буде круто ;) We offer ● Офіційне працевлаштування;● Оплату лікарняних і відпустку 24 календарних дні;● Конкурентну і вчасну заробітню плату;● Стандартний 8 годинний робочий день;● Офіс на Братиславській. Responsibilities Зараз перед нашою командою стоїть кілька величезних задач:● Зробити крутий повнотекстовий пошук● Розробити систему рекомендацій товарів для кожного покупця. І його дружини. І кота.● Створити систему комплексної аналітики, яка почнеться в момент коли користувач міг побачити наш баннер на маршрутці в Ковелі та закінчуючи чайовими кур’єру.",dou,2020-11-12,Lead Data Science@Epicentr M,,,"{""Required skills"": [""\u0415\u043f\u0456\u0446\u0435\u043d\u0442\u0440 \u041c\u0430\u0440\u043a\u0435\u0442\u043b\u043f\u0435\u0439\u0441"", ""\u0446\u0435 \u0443\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430 \u043f\u0440\u043e\u0434\u0443\u043a\u0442\u043e\u0432\u0430 \u043a\u043e\u043c\u043f\u0430\u043d\u0456\u044f, \u0449\u043e \u0441\u0442\u0432\u043e\u0440\u044e\u0454 \u043c\u0430\u0439\u0434\u0430\u043d\u0447\u0438\u043a \u0434\u043b\u044f \u043c\u0430\u043b\u043e\u0433\u043e \u0442\u0430 \u0441\u0435\u0440\u0435\u0434\u043d\u044c\u043e\u0433\u043e \u0431\u0456\u0437\u043d\u0435\u0441\u0443 \u0432 \u0441\u0435\u0433\u043c\u0435\u043d\u0442\u0456 DIY \u0442\u0430 \u0442\u043e\u0432\u0430\u0440\u0456\u0432 \u0434\u043b\u044f \u0434\u043e\u043c\u0443. \u041e\u0444\u043b\u0430\u0439\u043d \u0447\u0430\u0441\u0442\u0438\u043d\u0430 \u043d\u0430\u0448\u043e\u0457 \u043a\u043e\u043c\u043f\u0430\u043d\u0456\u0457"", ""\u0446\u0435 \u043d\u0430\u0439\u0431\u0456\u043b\u044c\u0448\u0430 \u043c\u0435\u0440\u0435\u0436\u0430 \u0442\u043e\u0440\u0433\u043e\u0432\u0438\u0445 \u0446\u0435\u043d\u0442\u0440\u0456\u0432 \u0443 \u043a\u0440\u0430\u0457\u043d\u0456 Epicetrk.ua"", ""\u0446\u0435 \u0431\u0456\u043b\u044c\u0448 \u044f\u043a 3 \u043c\u043b\u043d. \u043f\u0435\u0440\u0435\u0433\u043b\u044f\u0434\u0456\u0432 \u0442\u043e\u0432\u0430\u0440\u0456\u0432 \u0442\u0430 \u0431\u043b\u0438\u0437\u044c\u043a\u043e 20 \u043c\u043b\u043d. \u043f\u043e\u0434\u0456\u0439 \u0432 \u0434\u0435\u043d\u044c.\u0426\u0435 \u0442\u0435\u0440\u0430\u0431\u0430\u0439\u0442\u0438 \u0434\u0430\u043d\u0438\u0445 \u044f\u043a\u0456 \u043c\u0438 \u0432\u0436\u0435 \u0437\u0431\u0438\u0440\u0430\u0454\u043c\u043e, \u0442\u0430 \u044f\u043a\u0456 \u0449\u0435 \u043f\u043e\u0442\u0440\u0456\u0431\u043d\u043e \u043f\u043e\u0447\u0430\u0442\u0438 \u0437\u0431\u0438\u0440\u0430\u0442\u0438. \u0426\u0435 \u043e\u0444\u043b\u0430\u0439\u043d \u0442\u0430 \u043e\u043d\u043b\u0430\u0439\u043d \u043f\u0440\u043e\u0434\u0430\u0436\u0456 \u043e\u0434\u043d\u043e\u0433\u043e \u0439 \u0442\u043e\u0433\u043e \u0441\u0430\u043c\u043e\u0433\u043e \u043f\u043e\u043a\u0443\u043f\u0446\u044f, \u0449\u043e \u0432\u0447\u043e\u0440\u0430 \u0434\u043e\u0434\u0430\u0432 \u0443 \u043a\u043e\u0448\u0438\u043a \u043c\u043e\u0431\u0456\u043b\u044c\u043d\u0438\u0439 \u0442\u0435\u043b\u0435\u0444\u043e\u043d, \u0430 \u0441\u044c\u043e\u0433\u043e\u0434\u043d\u0456 \u043a\u0443\u043f\u0438\u0432 \u0432 \u043e\u0444\u043b\u0430\u0439\u043d\u0456 Siltek P-15 2 mm 25 (\u043c\u0438 \u0442\u0435\u0436 \u0433\u0443\u0433\u043b\u0438\u043b\u0438).\u0426\u0435 \u043c\u0456\u0441\u0446\u0435 \u0434\u0435 \u043a\u0443\u043f\u0443\u044e\u0442\u044c \u0433\u0443\u0431\u043d\u0443 \u043f\u043e\u043c\u0430\u0434\u0443 \u0442\u0430 \u0446\u0435\u043c\u0435\u043d\u0442 \u0432 \u043e\u0434\u043d\u043e\u043c\u0443 \u0437\u0430\u043c\u043e\u0432\u043b\u0435\u043d\u043d\u0456 \u0456 \u0432 \u0446\u044c\u043e\u043c\u0443 \u0454 \u043b\u043e\u0433\u0456\u043a\u0430 \u0442\u0430 \u0441\u0442\u0456\u0439\u043a\u0438\u0439 \u043f\u0430\u0442\u0435\u0440\u043d \u043f\u043e\u0432\u0435\u0434\u0456\u043d\u043a\u0438.\u0412 \u043d\u0430\u0441 \u0434\u0456\u0439\u0441\u043d\u043e \u043f\u0440\u0438\u043a\u043e\u043b\u044c\u043d\u043e) \u0429\u043e \u043c\u0438 \u0437\u0430\u0434\u0443\u043c\u0430\u043b\u0438?""], ""We offer"": [""\u0417\u0430\u0440\u0430\u0437 \u043f\u0435\u0440\u0435\u0434 \u043d\u0430\u0448\u043e\u044e \u043a\u043e\u043c\u0430\u043d\u0434\u043e\u044e \u0441\u0442\u043e\u0457\u0442\u044c \u043a\u0456\u043b\u044c\u043a\u0430 \u0432\u0435\u043b\u0438\u0447\u0435\u0437\u043d\u0438\u0445 \u0437\u0430\u0434\u0430\u0447:"", ""\u0417\u0440\u043e\u0431\u0438\u0442\u0438 \u043a\u0440\u0443\u0442\u0438\u0439 \u043f\u043e\u0432\u043d\u043e\u0442\u0435\u043a\u0441\u0442\u043e\u0432\u0438\u0439 \u043f\u043e\u0448\u0443\u043a"", ""\u0420\u043e\u0437\u0440\u043e\u0431\u0438\u0442\u0438 \u0441\u0438\u0441\u0442\u0435\u043c\u0443 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0456\u0439 \u0442\u043e\u0432\u0430\u0440\u0456\u0432 \u0434\u043b\u044f \u043a\u043e\u0436\u043d\u043e\u0433\u043e \u043f\u043e\u043a\u0443\u043f\u0446\u044f. \u0406 \u0439\u043e\u0433\u043e \u0434\u0440\u0443\u0436\u0438\u043d\u0438. \u0406 \u043a\u043e\u0442\u0430."", ""\u0421\u0442\u0432\u043e\u0440\u0438\u0442\u0438 \u0441\u0438\u0441\u0442\u0435\u043c\u0443 \u043a\u043e\u043c\u043f\u043b\u0435\u043a\u0441\u043d\u043e\u0457 \u0430\u043d\u0430\u043b\u0456\u0442\u0438\u043a\u0438, \u044f\u043a\u0430 \u043f\u043e\u0447\u043d\u0435\u0442\u044c\u0441\u044f \u0432 \u043c\u043e\u043c\u0435\u043d\u0442 \u043a\u043e\u043b\u0438 \u043a\u043e\u0440\u0438\u0441\u0442\u0443\u0432\u0430\u0447 \u043c\u0456\u0433 \u043f\u043e\u0431\u0430\u0447\u0438\u0442\u0438 \u043d\u0430\u0448 \u0431\u0430\u043d\u043d\u0435\u0440 \u043d\u0430 \u043c\u0430\u0440\u0448\u0440\u0443\u0442\u0446\u0456 \u0432 \u041a\u043e\u0432\u0435\u043b\u0456 \u0442\u0430 \u0437\u0430\u043a\u0456\u043d\u0447\u0443\u044e\u0447\u0438 \u0447\u0430\u0439\u043e\u0432\u0438\u043c\u0438 \u043a\u0443\u0440\u2019\u0454\u0440\u0443.""], ""Responsibilities"": [""\u0425\u0442\u043e \u043d\u0430\u043c \u0442\u0440\u0435\u0431\u0430:\u0417\u0430\u0440\u0430\u0437 \u043c\u0438 \u0448\u0443\u043a\u0430\u0454\u043c\u043e \u043b\u0456\u0434\u0430 Data Science \u043a\u043e\u043c\u0430\u043d\u0434\u0438. \u0421\u0432\u0456\u0442\u0438\u043b\u043e Tensorflow \u0442\u0430 \u0441\u0443\u043f\u0435\u0440\u0441\u0430\u043c\u043e\u0441\u0442\u0456\u0439\u043d\u043e\u0433\u043e \u0440\u043e\u0437\u0440\u043e\u0431\u043d\u0438\u043a\u0430, \u044f\u043a\u0438\u0439 \u0432\u0456\u0434\u0447\u0443\u0432\u0430\u0454 \u0432 \u0441\u043e\u0431\u0456 \u0441\u0438\u043b\u0438, \u0430 \u0449\u0435 \u043a\u0440\u0430\u0449\u0435, \u043c\u0430\u0454 \u0434\u043e\u0441\u0432\u0456\u0434, \u0434\u043b\u044f \u043f\u043e\u0431\u0443\u0434\u043e\u0432\u0438 \u043f\u0456\u0434\u0440\u043e\u0437\u0434\u0456\u043b\u0443 Data Science.P.S.: \u0427\u0435\u0441\u043d\u043e, \u0432 \u043d\u0430\u0441 \u043f\u043e\u043a\u0438 \u0449\u043e \u043d\u0435\u043c\u0430\u0454 \u0442\u0435\u043e\u0440\u0435\u0442\u0438\u0447\u043d\u0438\u0445 \u0437\u0430\u0434\u0430\u0447, \u0442\u0456\u043b\u044c\u043a\u0438 \u043f\u0440\u0438\u043a\u043b\u0430\u0434\u043d\u0456.""]}"
Wargaming.net,https://jobs.dou.ua/companies/wargaming/,Oracle Developer for Big Data (Relocation to Prague),https://jobs.dou.ua/companies/wargaming/vacancies/129409/," Kyiv, Prague (Czech Republic)",24 October 2020,,"We are looking for Big Data Engineer/ETL Developer for Data Alliance Services/dataWARS department. Wargaming is a well-established game development company famous for its legendary titles such as World of Tanks, World of Warships.Data Alliance Services/dataWARS department is aimed at helping projects to make smart business decisions by integrating and processing of huge amounts of data and providing efficient calculations and reports based on stored information. We operate on global level by assisting WG teams and projects all around the globe. Our team of experienced data-enthusiastic engineers wants to expand by hiring committed and creative specialist. We use cutting edge data processing technologies and improve by constant learning. Team Lead of Big Data, Development. • Write code to ingest data from various sources;• Work with game development studios to log data in a consistent and complete manner;• Write ETLs to clean/transform data into clear data models;• Optimize pipelines for performance;• Implement algorithms at scale as needed;• Implement new technologies as needed;• Have fun playing with loads of data. • 2+ years total DWH/BI experience (Oracle);• Expertise in ETL processes design and implementation, data lifecycle management;• Confident knowledge in data querying and transformation: SQL, Oracle PL/SQL;• Experience with versioning systems (SVN, git);• Ability to work with and finalize requestor’s requirements;• Communicative level of Russian language. • Experience with Hadoop ecosystem (HDFS, Impala/Hive) is a strong plus for a candidate;• Performance tuning in any DBMS. Wargaming is an award-winning online game developer and publisher headquartered in Nicosia, Cyprus. Delivering legendary games since 1998, Wargaming has grown to become one of the leaders in the free-to-play MMO gaming industry with 4500+ employees and more than 20 offices globally. Over 200 million players enjoy Wargaming’s titles across all major gaming platforms, including the massively popular World of Tanks and World of Warships. Working in our company means always having interesting challenges and gaining valuable experience while working with top-class experts.Take your place among our passionate and experienced team and bring out the best in yourself at Wargaming! Please see legal.eu.wargaming.net/...​candidate-privacy-policy for details on how Wargaming uses your personal data.",dou,2020-11-12,Oracle Developer for Big Data (Relocation to Prague)@Wargaming.net,,,{}
Revenue.ai,https://jobs.dou.ua/companies/revenue-ai/,Data Engineer,https://jobs.dou.ua/companies/revenue-ai/vacancies/136178/," Budapest (Hungary), remote",24 October 2020,$1500–2500,"Required skills Responsibilities ENGLISH: Fluent, international team, good command of written and spoken English is required.Understanding business challenges around the Revenue management spaceIdentification, organization, and ingestion of data, spread across multiple sources (operational, analytical, and reporting)Analyze and organize raw dataBuild data systems and pipelines in collaboration with stakeholders and team members to analyze user needs and develop data integration solutions.Evaluate business needs and objectives and adhere to data architecture strategy, best practices, standards, and roadmapsConduct data assessment, perform data quality checks, transform and load raw data using SQL and ETL tools. Your work will contribute to the overall implementation of product solutionsInterpret trends and patternsConduct complex data analysis and report on resultsPrepare data for prescriptive and predictive modelingBuild algorithms and prototypesTechnical capability to develop and implement data identification, ingestion and integration solution Explore ways to enhance data quality and reliabilityIdentify opportunities for data acquisitionDevelop analytical tools and programsCollaborate with data scientists and architects on several projects We offer Dynamic startup environment with international teamLearn from industry experts from Commodities / Consumer Goods / System Integrator companiesWork remotely or on-site in Poland / Hungary. Work permits and salary negotiable depending on skill level of the candidate. Responsibilities Technology Stack Big Data: Spark, Hadoop, Hive;Cloud: Azure — Storage; Compute; Networking; Identity and Security; Notebooks; Data Catalogs;Databases SQLPython Project description www.revenue.ai/data-engineer",dou,2020-11-12,Data Engineer@Revenue.ai,,,"{""Required skills"": [""Responsibilities""], ""We offer"": [""ENGLISH: Fluent, international team, good command of written and spoken English is required.Understanding business challenges around the Revenue management spaceIdentification, organization, and ingestion of data, spread across multiple sources (operational, analytical, and reporting)Analyze and organize raw dataBuild data systems and pipelines in collaboration with stakeholders and team members to analyze user needs and develop data integration solutions.Evaluate business needs and objectives and adhere to data architecture strategy, best practices, standards, and roadmapsConduct data assessment, perform data quality checks, transform and load raw data using SQL and ETL tools. Your work will contribute to the overall implementation of product solutionsInterpret trends and patternsConduct complex data analysis and report on resultsPrepare data for prescriptive and predictive modelingBuild algorithms and prototypesTechnical capability to develop and implement data identification, ingestion and integration solution Explore ways to enhance data quality and reliabilityIdentify opportunities for data acquisitionDevelop analytical tools and programsCollaborate with data scientists and architects on several projects""], ""Responsibilities"": [""Dynamic startup environment with international teamLearn from industry experts from Commodities / Consumer Goods / System Integrator companiesWork remotely or on-site in Poland / Hungary. Work permits and salary negotiable depending on skill level of the candidate.""], ""Project description"": [""Technology Stack""]}"
Lohika,https://jobs.dou.ua/companies/lohika-systems/,Senior BigData Engineer (#7866),https://jobs.dou.ua/companies/lohika-systems/vacancies/136175/," Kyiv, Lviv, Odesa",24 October 2020,,"ABOUT THE PROJECT:We are a healthcare company totally build on top of cutting-edge technologies. We care about creating a culture of diversity, openness, and transparency while engaging our intellectual curiosity, and problem-solving skills. In this role, you will be part of a team rebuilding and enhancing our platform. As an engineering team, we believe good code is easy to read and understand. We expect all of our engineers to continually teach as well as learn. We’ve got a ton of interesting problems to solve around distributed systems, data pipelines, data analytics and predictions, system reliability, security, privacy, and more. If you’re passionate about tackling hard problems while making a real difference in the world, we’d love to talk! RESPONSIBILITIES:•. Continue to evolve our data platform, data models, and analytics.• Partner with Data Scientists, ML Engineers, Analysts, and Product teams to achieve company goals.• Take part in current system audit to identify bottlenecks and more priority tasks. • Take part in requirements analysis, task planning, estimation, coding, and work distribution between the team members.• Suggest technical and functional improvements add value to the product.• Research and evaluate technical options to implement business requirements. REQUIREMENTS:• 4+ years of commercial SW development experience.• Experience developing in one or more general-purpose programming languages, including but not limited to: Java, Scala, GoLang, or Python.• Familiarity with Airflow or Glue.• Familiarity with Presto or Athena.• At least Intermediate English.• Balances getting things done with making things perfect. WOULD BE A PLUS:• BS\MS in Computer science or related field.• Familiarity with Looker.• Familiarity with Spark, Databricks, AWS services, IAM. LOHIKA BENEFITS: • Friendly and highly professional teams• Flexible working hours• Comfortable office facilities (kitchens, gyms, sports activities, yoga, lounge rooms, coffee machines etc.)• Fully paid English classes (twice per week) with own English teachers• 20 working days of annual paid vacation• Christmas holidays (1-7 January) and state holidays celebration• Paid sick leaves• Medical insurance• Incentives (marriage, childbirth)• Gym, yoga, other sports activities• Bonuses for internal training delivery",dou,2020-11-12,Senior BigData Engineer (#7866)@Lohika,,,{}
Data Science UA,https://jobs.dou.ua/companies/data-science-ua/,Senior Video Processing Software Engineer,https://jobs.dou.ua/companies/data-science-ua/vacancies/136170/, Kyiv,23 October 2020,,"Required skills — Experience with streaming video from several years— 5+ working Experience of Embedded development — Good knowledge of C, C ++, Python— Expert knowledge of Linux— Free navigation in open source libraries, from ffmpeg to OpenCV— Expert level in working with signaling and streaming protocols (RTSP) As a plus — Experience in IoT product solutions— Experience of ice team development— Good understanding of ARM architectures (v7, v8)— Experience of working with mission-critical video applications and standards— Practical experience of using and integrating with relevant open source technologiesExperience with pre-processing and video processing algorithms for subjective quality improvement, such as adaptive image enhancement, frame rate conversion— Experience with multimedia packaging formats such as HLS, DASH, and encryption We offer — Work with team of professional developers— A product for real users, not ghostly tasks— Established development processes— Good salary— Profile courses, trainings,conferences— Medical Insurance— Flexible schedule Responsibilities — Together with team working on a new product using video cameras for security purposes in combination with traditional security sensors— Develop engineering practices and improve code quality— Participate in discussions of new product functionality and be responsible for product development as part of the system Project description Our partner is a full-cycle company from idea generation and R&D to mass production and sales. They produce physical devices (the system includes many different sensors and hubs), writing firmware for them, develop the server part and release mobile applications.We are looking for engineers to develop a new product related to video streaming",dou,2020-11-12,Senior Video Processing Software Engineer@Data Science UA,,,"{""Required skills"": [""Experience with streaming video from several years"", ""5+ working Experience of Embedded development"", ""Good knowledge of C, C ++, Python"", ""Expert knowledge of Linux"", ""Free navigation in open source libraries, from ffmpeg to OpenCV"", ""Expert level in working with signaling and streaming protocols (RTSP)""], ""As a plus"": [""Experience in IoT product solutions"", ""Experience of ice team development"", ""Good understanding of ARM architectures (v7, v8)"", ""Experience of working with mission-critical video applications and standards"", ""Practical experience of using and integrating with relevant open source technologiesExperience with pre-processing and video processing algorithms for subjective quality improvement, such as adaptive image enhancement, frame rate conversion"", ""Experience with multimedia packaging formats such as HLS, DASH, and encryption""], ""We offer"": [""Work with team of professional developers"", ""A product for real users, not ghostly tasks"", ""Established development processes"", ""Good salary"", ""Profile courses, trainings,conferences"", ""Medical Insurance"", ""Flexible schedule""], ""Responsibilities"": [""Together with team working on a new product using video cameras for security purposes in combination with traditional security sensors"", ""Develop engineering practices and improve code quality"", ""Participate in discussions of new product functionality and be responsible for product development as part of the system""], ""Project description"": [""Our partner is a full-cycle company from idea generation and R&D to mass production and sales. They produce physical devices (the system includes many different sensors and hubs), writing firmware for them, develop the server part and release mobile applications.We are looking for engineers to develop a new product related to video streaming""]}"
Grid Dynamics,https://jobs.dou.ua/companies/grid-dynamics/,Senior/Lead Data Scientist,https://jobs.dou.ua/companies/grid-dynamics/vacancies/136152/," Kyiv, Kharkiv, Lviv",23 October 2020,,"As a leader of the Data Science team, you will run the development of next-generation platforms for customers micro-segmentation, clusterization, behavior analysis, and prediction as well as building up new improved recommendation systems for the customers.You will use your expertise to design data science models and frameworks, data processing pipelines, and lead efforts of the ML models productization. You will use project management skills to manage the deliverables and ensure all stakeholder work is complete on time. You will also manage stakeholder communications and present your project across a wide spectrum of management levels. Lead Data Scientist is supposed to handle complex problems independently and demonstrate analytical thinking. At this position, you should be able to make judgments and recommendations based on the analysis and interpretation of data. The position requires excellent communication skills and experience working directly with technical teams as well as with business stakeholders. Being able to present findings in a meaningful and clear way is a must. Responsibilities:— Lead a team of skilled data scientists and data engineers; — Drive the collection, cleaning, processing, and analysis of new and existing data sources;— Communicate with business stakeholders to clarify their requirements and present the teamwork results;— Learn & stay current on developments in one or more analytics domains: Optimization, Machine Learning, Deep Learning / AI, Simulation, etc. Requirements:— Data scientist with 5+ years of experience;— Solid understanding of Statistics, Machine Learning and Deep Learning;— Hands-on experience in Python;— Experience with Recommender Systems (Content-Based, Collaborative Filtering, Hybrid, Market Basket Analysis, Repeat Purchase);— Experience with look-alike modeling and sequential-input models, RNNs (LSTM, GRU, etc.);— Expertise in building, productionising and scaling analytics solutions for big data problems;— Experience with SQL databases;— Familiarity with cloud ML Platforms (Google AI Platform, Amazon Sagemaker, MS Azure AI Platform) is a plus;— Hands-on experience with data preparation, cleansing, feature engineering, and visualization;— Great communication and presentation skills. Proven ability to present progress made by the team to senior business management and the project stakeholders;— Understanding of and experience with customer intelligence & marketing domains a plus;— Experience in working across different global cultures a plus We offer:— Opportunity to work on bleeding-edge projects;— Work with a highly motivated and dedicated team;— Flexible schedule;— Medical insurance;— Benefits program;— Corporate social events;— Professional development opportunities. About us:Grid Dynamics is the engineering services company known for transformative, mission-critical cloud solutions for retail, finance and technology sectors. We architected some of the busiest e-commerce services on the Internet and have never had an outage during the peak season. Founded in 2006 and headquartered in San Ramon, California with offices throughout the US and Eastern Europe, we focus on big data analytics, scalable omnichannel services, DevOps, and cloud enablement.",dou,2020-11-12,Senior/Lead Data Scientist@Grid Dynamics,,,{}
SoftServe,https://jobs.dou.ua/companies/softserve/,"Business Analyst (Data Analysis Experience), ID 57540",https://jobs.dou.ua/companies/softserve/vacancies/136150/, Kyiv,23 October 2020,,"WE ARE Our client is a healthcare company with 3000 hospitals around the world. The company initiatives related to web development are in progress. The organization is focusing on replacing outdated products. Currently, the company is in a state of a migration of a legacy system to a new event-driven architecture based on reactive microservices. SoftServe is participating in a program to transform microservices-based solution to use Google Cloud Services YOU ARE A specialist who will be able to figure out Patterns of Data Structure and to specify rules that match attributes of the same data pattern and how they are spread on timelines. A professional having the above-mentioned skills and experience more than 4 years in business/data analysis.• Hands-on experience with data warehousing, data lakes, data marts, enterprise reporting, and visualizationsSQL knowledge (a must-have)• Strong analytical skills to critically evaluate the information gathered from multiple sources • Sound problem solving and research skills, system thinking • Good leadership and teambuilding skills • Ability to work effectively under pressure and in different environments YOU WANT TO WORK WITH • Data which is separated across different databases • Descriptive analysis of data and their interpretation • Application design sessions and work with the development team during the development phase • Inspecting/modeling/analyzing Domains data, identifying and describing business rules and restrictions• Assisting in requirements gathering and project leadership; drive the analysis, design, and development of the solution• Engagement in a descriptive analysis of data and their interpretation and work closely with the IT management team TOGETHER WE WILL • Enjoy the ability to offer and implement your own solutions • Work with a wide range of experts — from top Architects to associates of all profiles and levels in the company• Participate in international project activities • Build strong teams and relations with world-class clients• Enable the possibility to work remotely• Have access to robust educational and mentorship programs • Get certifications on cutting-edge technologies • Share with you a package of benefits (medical insurance/additional (paid) vacation, anniversary gifts/corporate) and foreign language classes)",dou,2020-11-12,"Business Analyst (Data Analysis Experience), ID 57540@SoftServe",,,{}
ПриватБанк,https://jobs.dou.ua/companies/privatbank/,Керівник управління валідації моделей,https://jobs.dou.ua/companies/privatbank/vacancies/136146/, Kyiv,23 October 2020,,"Required skills — досвід роботи керування командою Data Science від 1 року;— глибокі знання статистики;— досвід роботи з Python, SQL;— рівень володіння англійською мовою не нижче intermediate. We offer • Работу у найбільш іноваційному банку східної Европи• роботу з великими об’ємами даних;• використання найсучасніших підходів у машинному навчанні;• робота у команді досвідчених Data Scientists;• професійне і кар’єрне зростання;• широкий діапазон задач;• гнучкий графік роботи. Дохід за результатами співбесіди Responsibilities — проводити валідацію широкого спектру моделей, що використовуються в банку (як за оцінкою основних видів ризику (кредитний, ринковий, процентний ризик банківської книги, ліквідності, операційний), так і бізнес-моделей);— збирати і аналізувати дані, необхідні для проведення валідації, а також проведення статистичних тестів;— оцінювати відповідність моделей вимогам Базеля, МСФЗ9, вимогам НБУ і провідним практикам;— оцінювати впровадження і подальше застосування моделей;— готувати рекомендації щодо поліпшення моделей і процесів щодо їх застосування;— готувати та презентувати звіти з результатами валідації;— розробляти і покращувати методологію з проведення валідації;— впроваджувати підходи до оцінки модельного ризику в банку;— автоматизовувати розрахунки тестів.",dou,2020-11-12,Керівник управління валідації моделей@ПриватБанк,,,"{""Required skills"": [""\u0434\u043e\u0441\u0432\u0456\u0434 \u0440\u043e\u0431\u043e\u0442\u0438 \u043a\u0435\u0440\u0443\u0432\u0430\u043d\u043d\u044f \u043a\u043e\u043c\u0430\u043d\u0434\u043e\u044e Data Science \u0432\u0456\u0434 1 \u0440\u043e\u043a\u0443;"", ""\u0433\u043b\u0438\u0431\u043e\u043a\u0456 \u0437\u043d\u0430\u043d\u043d\u044f \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u043a\u0438;"", ""\u0434\u043e\u0441\u0432\u0456\u0434 \u0440\u043e\u0431\u043e\u0442\u0438 \u0437 Python, SQL;"", ""\u0440\u0456\u0432\u0435\u043d\u044c \u0432\u043e\u043b\u043e\u0434\u0456\u043d\u043d\u044f \u0430\u043d\u0433\u043b\u0456\u0439\u0441\u044c\u043a\u043e\u044e \u043c\u043e\u0432\u043e\u044e \u043d\u0435 \u043d\u0438\u0436\u0447\u0435 intermediate.""], ""We offer"": [""\u0420\u0430\u0431\u043e\u0442\u0443 \u0443 \u043d\u0430\u0439\u0431\u0456\u043b\u044c\u0448 \u0456\u043d\u043e\u0432\u0430\u0446\u0456\u0439\u043d\u043e\u043c\u0443 \u0431\u0430\u043d\u043a\u0443 \u0441\u0445\u0456\u0434\u043d\u043e\u0457 \u0415\u0432\u0440\u043e\u043f\u0438"", ""\u0440\u043e\u0431\u043e\u0442\u0443 \u0437 \u0432\u0435\u043b\u0438\u043a\u0438\u043c\u0438 \u043e\u0431\u2019\u0454\u043c\u0430\u043c\u0438 \u0434\u0430\u043d\u0438\u0445;"", ""\u0432\u0438\u043a\u043e\u0440\u0438\u0441\u0442\u0430\u043d\u043d\u044f \u043d\u0430\u0439\u0441\u0443\u0447\u0430\u0441\u043d\u0456\u0448\u0438\u0445 \u043f\u0456\u0434\u0445\u043e\u0434\u0456\u0432 \u0443 \u043c\u0430\u0448\u0438\u043d\u043d\u043e\u043c\u0443 \u043d\u0430\u0432\u0447\u0430\u043d\u043d\u0456;"", ""\u0440\u043e\u0431\u043e\u0442\u0430 \u0443 \u043a\u043e\u043c\u0430\u043d\u0434\u0456 \u0434\u043e\u0441\u0432\u0456\u0434\u0447\u0435\u043d\u0438\u0445 Data Scientists;"", ""\u043f\u0440\u043e\u0444\u0435\u0441\u0456\u0439\u043d\u0435 \u0456 \u043a\u0430\u0440\u2019\u0454\u0440\u043d\u0435 \u0437\u0440\u043e\u0441\u0442\u0430\u043d\u043d\u044f;"", ""\u0448\u0438\u0440\u043e\u043a\u0438\u0439 \u0434\u0456\u0430\u043f\u0430\u0437\u043e\u043d \u0437\u0430\u0434\u0430\u0447;"", ""\u0433\u043d\u0443\u0447\u043a\u0438\u0439 \u0433\u0440\u0430\u0444\u0456\u043a \u0440\u043e\u0431\u043e\u0442\u0438.""], ""Responsibilities"": [""\u0414\u043e\u0445\u0456\u0434 \u0437\u0430 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0430\u043c\u0438 \u0441\u043f\u0456\u0432\u0431\u0435\u0441\u0456\u0434\u0438""]}"
SoftServe,https://jobs.dou.ua/companies/softserve/,"Middle Data Engineer (Java), ID 57371",https://jobs.dou.ua/companies/softserve/vacancies/136145/," Kharkiv, remote",23 October 2020,,"WE ARE Our client is a healthcare company with 3000 hospitals around the world. The company initiatives related to web development are in progress. The organization is focusing on replacing outdated products. Currently, the company is in a state of a migration of a legacy system to a new event-driven architecture based on reactive microservices. SoftServe is participating in a program to transform microservices-based solution to use google cloud services YOU ARE • Experienced in JavaCore, Apache Beam or Airflow• Strong SQL skills (stored procedures, functions)• Demonstrating Google Cloud Platform (Spanner) experience• Knowledgeable about Rest Services on Spring Boot which would be an advantage• Possessing strong problem-solving, analytical skills• A detail-orientated person which is required• Performing data analysis and implement solutions according to the requirements• Demonstrating Intermediate+ English level YOU WANT TO WORK WITH • Discovering data relationships and consistency constraints• Cutting-edge technologies for managing data in the large scale enterprise environment TOGETHER WE WILL • Work with a wide range of experts — from top Architects to associates of all profiles and levels in the company• Participate in international projects activities• Build strong teams and relations with world-class clients",dou,2020-11-12,"Middle Data Engineer (Java), ID 57371@SoftServe",,,{}
ПриватБанк,https://jobs.dou.ua/companies/privatbank/,Data Scientist Управління Data science,https://jobs.dou.ua/companies/privatbank/vacancies/136138/, Dnipro,23 October 2020,,"Required skills • PhD в статистиці, економетрики, фізики, математики, інформатики або суміжних областях;• рівень володіння англійською не нижче pre-intermediate обов’язково;• робота з великими масивами даних As a plus • досвід побудови моделей машинного навчання• досвід роботи у фінансовій сфері• знання SQL We offer • Работу у найбільш іноваційному банку східної Европи• роботу з великими об’ємами даних;• використання найсучасніших підходів у машинному навчанні;• робота у команді досвідчених Data Scientists;• професійне і кар’єрне зростання;• широкий діапазон задач;• гнучкий графік роботи. Дохід за результатами співбесіди Responsibilities • задачі кредитного скоринга;• сегментація клієнтської бази для різних процесів банку;• пошук аномалій у даних;• роботи з прогнозуванням часових рядів;• проведення стрес-тестування;• розробка алгоритмів;• ведення відповідної документації;",dou,2020-11-12,Data Scientist Управління Data science@ПриватБанк,,,"{""Required skills"": [""PhD \u0432 \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u0446\u0456, \u0435\u043a\u043e\u043d\u043e\u043c\u0435\u0442\u0440\u0438\u043a\u0438, \u0444\u0456\u0437\u0438\u043a\u0438, \u043c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u043a\u0438, \u0456\u043d\u0444\u043e\u0440\u043c\u0430\u0442\u0438\u043a\u0438 \u0430\u0431\u043e \u0441\u0443\u043c\u0456\u0436\u043d\u0438\u0445 \u043e\u0431\u043b\u0430\u0441\u0442\u044f\u0445;"", ""\u0440\u0456\u0432\u0435\u043d\u044c \u0432\u043e\u043b\u043e\u0434\u0456\u043d\u043d\u044f \u0430\u043d\u0433\u043b\u0456\u0439\u0441\u044c\u043a\u043e\u044e \u043d\u0435 \u043d\u0438\u0436\u0447\u0435 pre-intermediate \u043e\u0431\u043e\u0432\u2019\u044f\u0437\u043a\u043e\u0432\u043e;"", ""\u0440\u043e\u0431\u043e\u0442\u0430 \u0437 \u0432\u0435\u043b\u0438\u043a\u0438\u043c\u0438 \u043c\u0430\u0441\u0438\u0432\u0430\u043c\u0438 \u0434\u0430\u043d\u0438\u0445""], ""As a plus"": [""\u0434\u043e\u0441\u0432\u0456\u0434 \u043f\u043e\u0431\u0443\u0434\u043e\u0432\u0438 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u043c\u0430\u0448\u0438\u043d\u043d\u043e\u0433\u043e \u043d\u0430\u0432\u0447\u0430\u043d\u043d\u044f"", ""\u0434\u043e\u0441\u0432\u0456\u0434 \u0440\u043e\u0431\u043e\u0442\u0438 \u0443 \u0444\u0456\u043d\u0430\u043d\u0441\u043e\u0432\u0456\u0439 \u0441\u0444\u0435\u0440\u0456"", ""\u0437\u043d\u0430\u043d\u043d\u044f SQL""], ""We offer"": [""\u0420\u0430\u0431\u043e\u0442\u0443 \u0443 \u043d\u0430\u0439\u0431\u0456\u043b\u044c\u0448 \u0456\u043d\u043e\u0432\u0430\u0446\u0456\u0439\u043d\u043e\u043c\u0443 \u0431\u0430\u043d\u043a\u0443 \u0441\u0445\u0456\u0434\u043d\u043e\u0457 \u0415\u0432\u0440\u043e\u043f\u0438"", ""\u0440\u043e\u0431\u043e\u0442\u0443 \u0437 \u0432\u0435\u043b\u0438\u043a\u0438\u043c\u0438 \u043e\u0431\u2019\u0454\u043c\u0430\u043c\u0438 \u0434\u0430\u043d\u0438\u0445;"", ""\u0432\u0438\u043a\u043e\u0440\u0438\u0441\u0442\u0430\u043d\u043d\u044f \u043d\u0430\u0439\u0441\u0443\u0447\u0430\u0441\u043d\u0456\u0448\u0438\u0445 \u043f\u0456\u0434\u0445\u043e\u0434\u0456\u0432 \u0443 \u043c\u0430\u0448\u0438\u043d\u043d\u043e\u043c\u0443 \u043d\u0430\u0432\u0447\u0430\u043d\u043d\u0456;"", ""\u0440\u043e\u0431\u043e\u0442\u0430 \u0443 \u043a\u043e\u043c\u0430\u043d\u0434\u0456 \u0434\u043e\u0441\u0432\u0456\u0434\u0447\u0435\u043d\u0438\u0445 Data Scientists;"", ""\u043f\u0440\u043e\u0444\u0435\u0441\u0456\u0439\u043d\u0435 \u0456 \u043a\u0430\u0440\u2019\u0454\u0440\u043d\u0435 \u0437\u0440\u043e\u0441\u0442\u0430\u043d\u043d\u044f;"", ""\u0448\u0438\u0440\u043e\u043a\u0438\u0439 \u0434\u0456\u0430\u043f\u0430\u0437\u043e\u043d \u0437\u0430\u0434\u0430\u0447;"", ""\u0433\u043d\u0443\u0447\u043a\u0438\u0439 \u0433\u0440\u0430\u0444\u0456\u043a \u0440\u043e\u0431\u043e\u0442\u0438.""], ""Responsibilities"": [""\u0414\u043e\u0445\u0456\u0434 \u0437\u0430 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0430\u043c\u0438 \u0441\u043f\u0456\u0432\u0431\u0435\u0441\u0456\u0434\u0438""]}"
Matic,https://jobs.dou.ua/companies/matic/,Data QA Engineer (Product),https://jobs.dou.ua/companies/matic/vacancies/136046/, Lviv,23 October 2020,,"Every day our insurance products are generated thousands of records which are then transformed into analytical insights and drive business outcomes. Decisions made for the development of our company depend on the quality of these data, so we are looking for a Data Quality specialist who will help us keep our data accurate. Our data team consists of 7 people (5 Data Analysts, Data Engineer and Data Scientist) and despite the difference in location and time zones, we support and help each other. — Analyzing and testing of data for our data warehouse— Collaborate with other data professionals, engineers, and business operation experts to understand needs— Write and execute test cases, create automation scripts, evaluate compared to expected results as needed, and communicating relevant insights to the team— Develop high-level test strategies and plans for all projects by analyzing project documentation— Produce the appropriate documentation and test evidence — We do not limit the choice of tools and welcome experiments— Opportunity to try yourself in different parts of working with data, we support the development of skills — Proficient in SQL— Experience developing test plans— Demonstrated ability to implement and maintain QA processes— A passion for technology — we are looking for someone who is keen to leverage their existing skills and seek out new skills and solutions— Upper Intermediate+ level of English (our Data Team is distributed)— Strongly details oriented — Experience with business intelligence tools (Metabase, Tableau)— Experience with Snowflake — Snowflake as a Data Lake storage— Products Data is collected in the Data Lake using ETL (Fivetran, Stitch), and User engagement captured via Segment— Data Aggregated by DBT (SQL models)— Data Visualized using Tableau and Metabase",dou,2020-11-12,Data QA Engineer (Product)@Matic,,,{}
R&D Center,https://jobs.dou.ua/companies/rnd-center/,Computer Vision Research Engineer,https://jobs.dou.ua/companies/rnd-center/vacancies/118715/, Kyiv,23 October 2020,,"As a member of our Research Team, you will be engaged in the iterated process of prototyping and/or implementation of CV algorithms, their performance measurement, data preparation and deploying your solution together with the engineering team. You will work closely with members of our Research and Engineering teams to improve models and bring products to the market based on your research. — At least 3 years of software engineering and research experience— 2+ years in machine learning and/or computer vision domain— Experience with C++ and deep understanding of C++ concepts— Knowledge of different deep learning architectures and frameworks (Tensorflow, Pytorch)— Experience with Python including multithreading and multiprocessing— Knowledge of classical algorithms and data structures— Basic understanding of Big Data, understanding of the difference between MapReduce and in-memory processing— Good written and spoken English — Knowledge of classic computer vision algorithms (aka. Mathematical Morphology, Foreground extraction, etc.)— Experience with digital image processing— Cross-platform development, code optimization skills— Expertise in Linux environment— Docker — Working on the world’s most impactful security products. More than that, an opportunity to get some of those for personal usage — Competitive salary and perks— PE accounting and support— WFH and remote working mode possibility. Partial furniture compensation— Social package, including medical insurance available from the start date and sports compensation after the trial period— 18 paid vacation days per year, paid public holidays according to the Ukrainian legislation— Educational possibilities like corporate courses, knowledge hubs, and free English classes. Semiannual performance review— Free meals, fruits, and snacks when working in the office.",dou,2020-11-12,Computer Vision Research Engineer@R&D Center,,,{}
EPAM,https://jobs.dou.ua/companies/epam-systems/,"Data Analyst / Report Engineer (PowerBI / Tableau) [Kharkiv, Dnipro]",https://jobs.dou.ua/companies/epam-systems/vacancies/136107/," Kharkiv, Dnipro, remote",23 October 2020,,"Striving for excellence is in our DNA. Since 1993, we have been helping the world’s leading companies imagine, design, engineer, and deliver software and digital experiences that change the world. We are more than just specialists, we are experts. EPAM is committed to providing our global team of 36,700+ EPAMers with inspiring careers from day one. EPAMers lead with passion and honesty and think creatively. Our people are the source of our success and we value collaboration, try to always understand our customers’ business, and strive for the highest standards of excellence. In today’s new market conditions, we continue to support operations for hundreds of clients around the world remotely, with the vast majority of our teams working from home. No matter where you are located, you’ll join a dedicated, diverse community that will help you discover your fullest potential. DESCRIPTION REQUIREMENTS RESPONSIBILITIES BENEFITS EVEN MORE EPAM BENEFITS ABOUT EPAM",dou,2020-11-12,"Data Analyst / Report Engineer (PowerBI / Tableau) [Kharkiv, Dnipro]@EPAM",,,"{""DESCRIPTION"": [""Striving for excellence is in our DNA. Since 1993, we have been helping the world\u2019s leading companies imagine, design, engineer, and deliver software and digital experiences that change the world. We are more than just specialists, we are experts.""], ""REQUIREMENTS"": [""EPAM is committed to providing our global team of 36,700+ EPAMers with inspiring careers from day one. EPAMers lead with passion and honesty and think creatively. Our people are the source of our success and we value collaboration, try to always understand our customers\u2019 business, and strive for the highest standards of excellence. In today\u2019s new market conditions, we continue to support operations for hundreds of clients around the world remotely, with the vast majority of our teams working from home. No matter where you are located, you\u2019ll join a dedicated, diverse community that will help you discover your fullest potential.""]}"
All company jobs,https://jobs.dou.ua/companies/js-technology-solutions-inc/vacancies/,Python Data Scraping Engineer,https://jobs.dou.ua/companies/js-technology-solutions-inc/vacancies/136092/, remote,23 October 2020,$1000–2500,"Required skills * Python* Javascript* Selenium As a plus * Windows tasks, PowerShell* ChromeDriver* LAMP stack* DevOps and production support We offer * Flexible hours* Interesting and challenging projects* Great teammates and clients Responsibilities * Maintain and enhance existing scripts to extract data from applications and websites* Develop new scripts to support updated needs from users* Work with team members and users to troubleshoot issues and develop solutions* Propose new ideas for improving architecture of data extraction scripts Project description One of our closest clients has a SAAS product that is rapidly gaining traction. One of the most critical elements of the software is its ability to extract data in a reliable way from a range of applications and back-end services. This role is a key component of enhancing the product and ensuring new customers can be rapidly recruited. The role will involve daily collaboration with other members of the dev team, our client, and end users.",dou,2020-11-12,Python Data Scraping Engineer@All company jobs,,,"{""Required skills"": [""* Python* Javascript* Selenium""], ""As a plus"": [""* Windows tasks, PowerShell* ChromeDriver* LAMP stack* DevOps and production support""], ""We offer"": [""* Flexible hours* Interesting and challenging projects* Great teammates and clients""], ""Responsibilities"": [""* Maintain and enhance existing scripts to extract data from applications and websites* Develop new scripts to support updated needs from users* Work with team members and users to troubleshoot issues and develop solutions* Propose new ideas for improving architecture of data extraction scripts""], ""Project description"": [""One of our closest clients has a SAAS product that is rapidly gaining traction. One of the most critical elements of the software is its ability to extract data in a reliable way from a range of applications and back-end services. This role is a key component of enhancing the product and ensuring new customers can be rapidly recruited. The role will involve daily collaboration with other members of the dev team, our client, and end users.""]}"
Manpower Ukraine,https://jobs.dou.ua/companies/manpower-ukraine/,Data Scientist,https://jobs.dou.ua/companies/manpower-ukraine/vacancies/136074/, Kharkiv,23 October 2020,,"Required skills * Expertise in computer vision technologies and computer graphics (e.g., deep convolutional neural networks, object detection, tracking, segmentation, AR/VR, image and video processing, 3D geometry, SLAM, multi-view geometry, simulation, graphics rendering)* Skilled in explaining and presenting analyses and computer vision and machine learning concepts to a broad technical audience* Experience in TensorFlow / Keras / PyTorch* Hands-on experience in Python As a plus * Ph.D. in Math or Physics is a plus* Participation in programming contests and Data Science competitions is a plus We offer * Good salary according to the experience* Friendly team.* medical insurance * convenient office with cookies and fruits Responsibilities *Responsible for the scientific part of the challenge*Develop AI powered Computer Vision models and software solutions *Source of Data Science related expertise to the rest of the team Project description А world leader and innovator in pioneering high technology is looking for Data Scientist",dou,2020-11-12,Data Scientist@Manpower Ukraine,,,"{""Required skills"": [""* Expertise in computer vision technologies and computer graphics (e.g., deep convolutional neural networks, object detection, tracking, segmentation, AR/VR, image and video processing, 3D geometry, SLAM, multi-view geometry, simulation, graphics rendering)* Skilled in explaining and presenting analyses and computer vision and machine learning concepts to a broad technical audience* Experience in TensorFlow / Keras / PyTorch* Hands-on experience in Python""], ""As a plus"": [""* Ph.D. in Math or Physics is a plus* Participation in programming contests and Data Science competitions is a plus""], ""We offer"": [""* Good salary according to the experience* Friendly team.* medical insurance * convenient office with cookies and fruits""], ""Responsibilities"": [""*Responsible for the scientific part of the challenge*Develop AI powered Computer Vision models and software solutions *Source of Data Science related expertise to the rest of the team""], ""Project description"": [""\u0410 world leader and innovator in pioneering high technology is looking for Data Scientist""]}"
R&D Center,https://jobs.dou.ua/companies/rnd-center/,Lead Research Engineer,https://jobs.dou.ua/companies/rnd-center/vacancies/123434/, Kyiv,23 October 2020,,"We are looking for a team leader/software development manager (SDM) in our Research Organization. As SDM you will be responsible for one or several computer vision features starting from translating high-level product vision to a feasible solution with clear research stages, milestones, and timelines. Your main responsibilities will be technical leadership for the team as well as people management and cross-department communication with Engineering, Data, Product, and TPM organizations. — Leading the team of ~4-8 engineers and researchers to advance the algorithmic performance using a mix of state of the art or “classical” techniques.— Owning one or several CV features end-to-end starting from non-technical product vision from product team till deploying it to production. — Coordinate with data team to select and label relevant data to improve our algorithms and develop metrics by which the feature is evaluated — 1+ years experience of a leading research group or a team on a data science project.— 3+ years experience in research or data analysis.— Degree in Math, Computer Science, Robotics or a related field— Experienced software engineer working on problems with advanced algorithms or researcher with a track record of real-world applications involving solid software engineering— Managed or tech lead small teams— Strong understanding of deep learning, machine learning, and data analysis concepts. — Strong theoretical and practical applied knowledge of deep learning and computer vision algorithms.— Good knowledge of CS fundamentals, algorithms and data structures— Good written and spoken English— Strong experience in Python — Experience with DSP, NLP, GAN, Object Classification, detection, segmentation, tracking, Metric Learning, Tracking, Kalman filters.— Experience in C++; if the candidate is not already an expert in C++, a willingness to gain stronger C++ expertise is required.— AWS, Docker — Working on the world’s most impactful security products. More than that, an opportunity to get some of those for personal usage — Competitive salary and perks— PE accounting and support— WFH and remote working mode possibility. Partial furniture compensation— Social package, including medical insurance available from the start date and sports compensation after the trial period— 18 paid vacation days per year, paid public holidays according to the Ukrainian legislation— Educational possibilities like corporate courses, knowledge hubs, and free English classes. Semiannual performance review— Free meals, fruits, and snacks when working in the office",dou,2020-11-12,Lead Research Engineer@R&D Center,,,{}
International Development Resources Ltd.,https://jobs.dou.ua/companies/international-development-resources/,Senior Software Engineer/Data Streaming,https://jobs.dou.ua/companies/international-development-resources/vacancies/136021/, Kyiv,22 October 2020,,"Required skills • Bachelor’s degree in the field of Science, Technology, Engineering, Math, or equivalent practical experience in technical support, professional services, software development, or product operations management.• Extensive experience in pub-sub messaging frameworks (commercial, open-source) like Kafka and RabbitMQ• Experience developing, reading and debugging code (i.e. one or more of the following: Java/Spring, C, C++, Python) As a plus • Experience with Kafka and designing and implementing event-driven architectures• Experience with schema registry and schema evolution• Expertise in developing data producers and consumer API’s • Kafka Connect a plus• Experience in reactive programming• Experience with Elasticsearch a plus• Experience in RESTful API design We offer • Great office location 3 minutes from Pecherska metro station and great office conditions;• Flexible working hours;• Comfortable working place;• Medical insurance after the trial period ending;• Team-building activities;• 24 paid vacation days per year;• 5 paid sick leave days per year. Responsibilities We’re expanding our data product capabilities and we are in need of someone to anchor our data integration efforts. We’re taking event-driven architecture to heart and want someone passionate about the system to system data streaming. Project description From the way we communicate to the way we travel and shop, the smartphone has facilitated one of the greatest shifts in human behavior since the invention of the internet. Our company is at the forefront of understanding that shift in mobile research. Through our 4.5 stars rated app we have built the largest, highest rated, and fastest-growing smartphone consumer panel in America. Our mission is to combine online/offline, validated behavior with opinions from real shoppers to create the most comprehensive platform for understanding consumer behavior ever created. Simply put, we provide a better way to understand consumers.",dou,2020-11-12,Senior Software Engineer/Data Streaming@International Development Resources Ltd.,,,"{""Required skills"": [""Bachelor\u2019s degree in the field of Science, Technology, Engineering, Math, or equivalent practical experience in technical support, professional services, software development, or product operations management."", ""Extensive experience in pub-sub messaging frameworks (commercial, open-source) like Kafka and RabbitMQ"", ""Experience developing, reading and debugging code (i.e. one or more of the following: Java/Spring, C, C++, Python)""], ""As a plus"": [""Experience with Kafka and designing and implementing event-driven architectures"", ""Experience with schema registry and schema evolution"", ""Expertise in developing data producers and consumer API\u2019s"", ""Kafka Connect a plus"", ""Experience in reactive programming"", ""Experience with Elasticsearch a plus"", ""Experience in RESTful API design""], ""We offer"": [""Great office location 3 minutes from Pecherska metro station and great office conditions;"", ""Flexible working hours;"", ""Comfortable working place;"", ""Medical insurance after the trial period ending;"", ""Team-building activities;"", ""24 paid vacation days per year;"", ""5 paid sick leave days per year.""], ""Responsibilities"": [""We\u2019re expanding our data product capabilities and we are in need of someone to anchor our data integration efforts. We\u2019re taking event-driven architecture to heart and want someone passionate about the system to system data streaming.""], ""Project description"": [""From the way we communicate to the way we travel and shop, the smartphone has facilitated one of the greatest shifts in human behavior since the invention of the internet. Our company is at the forefront of understanding that shift in mobile research. Through our 4.5 stars rated app we have built the largest, highest rated, and fastest-growing smartphone consumer panel in America.""]}"
BetterMe,https://jobs.dou.ua/companies/betterme/,Data Engineer,https://jobs.dou.ua/companies/betterme/vacancies/136020/, Kyiv,22 October 2020,,"ABOUT US: BetterMe is a #1 Health&Fitness publisher in the world in terms of installs and grossing for 2019 according to Appannie. For the last 3 years BetterMe mobile apps ecosystem has been downloaded more than 80 million times (iOS+Android). We have 7.5 million followers on our social media platforms — more than any other competitor has. This all became possible through a team of world-class talented professionals in composition 100+ people in Kyiv. We’re one of the largest partners of Facebook / Google / Snapchat / Twitter from CEE. Our mission is creating happiness within since it’s vital to have your mind and body in harmony. There are 500 million people in the world who value a healthy lifestyle. We believe that every one of those people should be a BetterMe user. We plan to capture the growth of the Global Health Market, and our ideal candidate will focus on building the largest Health company in the world. We are looking for data engineer to join our analytics team and help it to be more efficient with data warehouse. ABOUT YOU:— 3+ years of experience in Data Engineering / Data Warehousing roles;— Expert knowledge of SQL / NoSQL databases: MySQL, MongoDB;— Practical experience of Big Data stack operation: Hadoop, Hive, Kafka, Spark, Cassandra;— Working experience on AWS: EC2, Redshift, S3, VPC, VPN etc;— Extreme attention to detail;— Passion for data. YOUR IMPACT:— Set up data warehouse in the company from scratch;— Define, design, model, implement, and operate large, evolving, structured and unstructured datasets;— Identify ways to automate data pipelines and dashboards;— Work with engineering and business stakeholders to understand data requirements. Our company is built on the ability to find the best people and provide them with everything needed to stay focused on what’s important to make our users even healthier, sportier, happier and better! We create a business environment that brings up best in everyone. We have no bureaucracy, and we give our colleagues complete freedom to make decisions and achieve brilliant results. Competitive salary. Compensation that will help you focus on your projects and personal development.Professional Growth. We offer a possibility to attend internal, external courses, seminars and access to a corporate library. You will be working with a team of professionals to get insights and discuss ideas.Comfortable working environment. BetterMe team is located in a spacious office in the Astarta business center within 10 minutes walk from Kontraktova metro station. We serve complimentary breakfasts, lunches, snacks, fruits and all necessary equipment for your role.Health&Fitness. We provide employees with 20 days of paid vacation, medical insurance and a variety of sports activities available for employees inside and outside the office.Rest. We organize team buildings, parties and various team activities to boost our collaboration.",dou,2020-11-12,Data Engineer@BetterMe,,,{}
Squro,https://jobs.dou.ua/companies/squro/,Senior Data Engineer,https://jobs.dou.ua/companies/squro/vacancies/109919/, Kyiv,22 October 2020,,"Required skills — Work experience as a Developer for at least 4 years;— Knowledge of the principles of building DWH data warehouses;— Knowledge of ETL building approaches;— Database Experience (SQL / NoSQL);— Python experience or other programming language;— Working experience with RabbitMQ / Kafka;— Knowledge of web application architecture;— Knowledge of software design patterns, basic algorithms, data structures;— Good knowledge and experience in applying the SOLID principles;— Working experience with version control systems;— Working experience in integrating with third-party services;— Working experience in communicating with the customer and collecting the requirements;— English Intermediate. As a plus — Working experience with Hadoop;— Working experience with Apache Spark. We offer — Friendly working environment;— Possibility of personal and professional growth;— Compensation of gym and language courses and professional conferences;— Cozy office in the city center (1 minute from Pecherskaya metro station);— Regular corporate events and other company benefits;— Competitive salary;— Flexible schedule. Responsibilities — Collecting all the requirements directly from business;— Application development and design of the data warehouse;— Integration and support of developed applications;— Research and analysis of third-party solutions and components;— Work using the best practices: refactoring, code review, continuous delivery, scrum. Project description We focus on developing financial high secured web applications, which include a big amount of different transactions, currencies, balances and integrated with other financial services. Now we are looking for a Senior Data Engineer to work on our new project. You will have an opportunity to influence the architecture of the project choosing all the necessary instruments yourself in collaboration with our Team Lead.",dou,2020-11-12,Senior Data Engineer@Squro,,,"{""Required skills"": [""Work experience as a Developer for at least 4 years;"", ""Knowledge of the principles of building DWH data warehouses;"", ""Knowledge of ETL building approaches;"", ""Database Experience (SQL / NoSQL);"", ""Python experience or other programming language;"", ""Working experience with RabbitMQ / Kafka;"", ""Knowledge of web application architecture;"", ""Knowledge of software design patterns, basic algorithms, data structures;"", ""Good knowledge and experience in applying the SOLID principles;"", ""Working experience with version control systems;"", ""Working experience in integrating with third-party services;"", ""Working experience in communicating with the customer and collecting the requirements;"", ""English Intermediate.""], ""As a plus"": [""Working experience with Hadoop;"", ""Working experience with Apache Spark.""], ""We offer"": [""Friendly working environment;"", ""Possibility of personal and professional growth;"", ""Compensation of gym and language courses and professional conferences;"", ""Cozy office in the city center (1 minute from Pecherskaya metro station);"", ""Regular corporate events and other company benefits;"", ""Competitive salary;"", ""Flexible schedule.""], ""Responsibilities"": [""Collecting all the requirements directly from business;"", ""Application development and design of the data warehouse;"", ""Integration and support of developed applications;"", ""Research and analysis of third-party solutions and components;"", ""Work using the best practices: refactoring, code review, continuous delivery, scrum.""], ""Project description"": [""We focus on developing financial high secured web applications, which include a big amount of different transactions, currencies, balances and integrated with other financial services. Now we are looking for a Senior Data Engineer to work on our new project. You will have an opportunity to influence the architecture of the project choosing all the necessary instruments yourself in collaboration with our Team Lead.""]}"
Newxel,https://jobs.dou.ua/companies/newxel/,Data Engineer for International gaming company,https://jobs.dou.ua/companies/newxel/vacancies/122126/, Kyiv,22 October 2020,,"Required skills — 3+ years of experience with Python or Scala or Java— Experience in AWS and/or GCP— Experience with BigQuery, Airflow, Docker, and streaming process— At least an Intermediate level of English As a plus — Experience with Java, ETL, Pandas We offer — Adequate management;— Competitive salary;— Flexible working schedule;— Modern and comfortable office near the Vystavkovyi center. Responsibilities — Own projects from initial discussions to release, including data exploration, architecture design, the benchmark of new technologies and product feedback;— Leverage our data pipelines and create new ones, making sure the architecture will support the business requirements;— Manage the development of distributed data pipelines and complex software designs to support high data rates using cloud-based tools & Python;— Make data exploration in order to gather insights as part of the data processing development;— Work closely with game analysts, data scientists and other key roles on the various data processes;— Develop unit and integration test procedures. Project description Crazy Labs is a top 10 mobile games publisher and with over 3 billion downloads to date, we are now a worldwide leader in casual games development, distribution and innovation, looking for an excellent Mobile Developer, to join our best of breed framework Development Team.",dou,2020-11-12,Data Engineer for International gaming company@Newxel,,,"{""Required skills"": [""3+ years of experience with Python or Scala or Java"", ""Experience in AWS and/or GCP"", ""Experience with BigQuery, Airflow, Docker, and streaming process"", ""At least an Intermediate level of English""], ""As a plus"": [""Experience with Java, ETL, Pandas""], ""We offer"": [""Adequate management;"", ""Competitive salary;"", ""Flexible working schedule;"", ""Modern and comfortable office near the Vystavkovyi center.""], ""Responsibilities"": [""Own projects from initial discussions to release, including data exploration, architecture design, the benchmark of new technologies and product feedback;"", ""Leverage our data pipelines and create new ones, making sure the architecture will support the business requirements;"", ""Manage the development of distributed data pipelines and complex software designs to support high data rates using cloud-based tools & Python;"", ""Make data exploration in order to gather insights as part of the data processing development;"", ""Work closely with game analysts, data scientists and other key roles on the various data processes;"", ""Develop unit and integration test procedures.""], ""Project description"": [""Crazy Labs is a top 10 mobile games publisher and with over 3 billion downloads to date, we are now a worldwide leader in casual games development, distribution and innovation, looking for an excellent Mobile Developer, to join our best of breed framework Development Team.""]}"
Lohika,https://jobs.dou.ua/companies/lohika-systems/,TL BigData Engineer for healthcare project (#7682),https://jobs.dou.ua/companies/lohika-systems/vacancies/88629/, Kyiv,22 October 2020,,"Purpose of the job:We are a healthcare company totally build on top of cutting-edge technologies. We care about creating a culture of diversity, openness, and transparency, while engaging our intellectual curiosity, and problem-solving skills. In this role, you will be part of a team rebuilding and enhancing our platform. As an engineering team, we believe good code is easy to read and understand. We expect all of our engineers to continually teach as well as learn. We’ve got a ton of interesting problems to solve around distributed systems, data pipelines, data analytics and predictions, system reliability, security, privacy, and more. If you’re passionate about tackling hard problems while making a real difference in the world, we’d love to talk!MAIN TASKS AND RESPONSIBILITIES: Lead the team according to established processes while tightly collaborating with the stakeholders on client side and locally.Continue to evolve our data platform, data models, and analytics.Partner with Data Scientists, ML Engineers, Analysts and Product teams to achieve company goals.Take part in current system audit to identify bottlenecks and more priority tasks.Take part in requirements analysis, tasks planning, estimation, coding and work distribution between the team members.Suggest technical and functional improvements to add value to the product.Research and evaluate technical options to implement business requirements.EDUCATION, SKILLS AND EXPERIENCE: MUST HAVE: 5+ years of commercial SW development experience.2+ years of experience in a Team Lead role.Experience developing in one or more general purpose programming languages, including but not limited to: Java, Scala, GoLang or Python.Knowledge of data structures, algorithms, distributed systems, and information retrieval.Familiarity with Airflow or Glue.Familiarity with Presto or Athena.Familiarity with Looker.Familiarity with Spark, Databricks, AWS services, IAM.Upper-Intermediate English.Balances getting things done with making things perfect.WOULD BE A PLUS: BS\MS in Computer science or related field.",dou,2020-11-12,TL BigData Engineer for healthcare project (#7682)@Lohika,,,{}
UnderDefense,https://jobs.dou.ua/companies/underdefense/,Strong Junior Front end Developer in Network Data Analysis product,https://jobs.dou.ua/companies/underdefense/vacancies/135969/, Lviv,22 October 2020,,"Required skills Ability to work independentlyGood communication skills (at least Upper-Intermediate English level) Experience in working with modern JS frameworks (preferably MVC/MVVM-based, like Ember/Angular/Vue)Knowledge of design patterns and object oriented programmingKnowledge of CSS and responsive designExperience in working with REST APIKnowledge of NoSQL databases As a plus Knowledge of TypeScriptExperience in working with SQLExperience in creating data visualizations (AMCharts, ECharts, D3)Knowledge of UX principles We offer Official employment, 18 days vacation, paid sick leaveCompetitive salaryFlexible work scheduleFree English lessonsComfortable officeExciting work in a dynamic teamBusiness trips to Canada Responsibilities Participate in the development of our real-time network sniffer, collaborate with UI offices, implement state-of-the-art interfaces. Be proactive in terms of interface design. Project description Come and participate in the design of the European product that competes with the American giants thanks to teams of talents in many fields. How does it work ? We place our sniffer in our clients’ infrastructure and it produces performance analysis to the backend. Then we provide an HMI and APIs to analyze the data received, process it and provide a real-time analysis of the performance of infrastructures and applications!",dou,2020-11-12,Strong Junior Front end Developer in Network Data Analysis product@UnderDefense,,,"{""Required skills"": [""Ability to work independentlyGood communication skills (at least Upper-Intermediate English level) Experience in working with modern JS frameworks (preferably MVC/MVVM-based, like Ember/Angular/Vue)Knowledge of design patterns and object oriented programmingKnowledge of CSS and responsive designExperience in working with REST APIKnowledge of NoSQL databases""], ""As a plus"": [""Knowledge of TypeScriptExperience in working with SQLExperience in creating data visualizations (AMCharts, ECharts, D3)Knowledge of UX principles""], ""We offer"": [""Official employment, 18 days vacation, paid sick leaveCompetitive salaryFlexible work scheduleFree English lessonsComfortable officeExciting work in a dynamic teamBusiness trips to Canada""], ""Responsibilities"": [""Participate in the development of our real-time network sniffer, collaborate with UI offices, implement state-of-the-art interfaces. Be proactive in terms of interface design.""], ""Project description"": [""Come and participate in the design of the European product that competes with the American giants thanks to teams of talents in many fields.""]}"
SoftServe,https://jobs.dou.ua/companies/softserve/,"Senior Big Data Engineer (Java+GCP), ID 57519",https://jobs.dou.ua/companies/softserve/vacancies/135966/," Lviv, remote",22 October 2020,,"WE ARE The largest Big Data & AI Center of Excellence in Eastern Europe, our expertise in Machine Learning, AI, Big Data, IoT, Cloud technologies recognized by Google and Amazon.We are transforming the way thousands of global organizations do business by creating the most innovative solutions using proven architecture design methodologies, innovation frameworks and experience design approaches. Our group successfully delivered 50+ discovery & consulting projects and 75+ implementation projects in 2019. Together with the SEI, and Carnegie Mellon University, we invented Smart Decisions game, in order to promote architecture design methodologies, intended to facilitate the design of complex architectures.Please read more at smartdecisionsgame.com YOU ARE An expert demonstrating • Excellent understanding of distributed computing technologies, approaches and patterns• Proficiency in one of the following programming languages: Java, Scala or Python• Cloud expertise, which is a big plus point: AWS, GCP or Azure• Hands-on experience with Hadoop, NoSQL, MPP technologies, processing frameworks, or other Big Data technology areas: data ingestion, consolidation, streaming or batching• Background of at least one of the processing and computation frameworks: Kafka Streams, Storm, Spark, Flink, Beam/DataFlow, Akka, etc.• Practice with at least one of the RDBMS or NoSQL engines: PostgreSQL, MySQL, Cassandra, HBase, Elasticsearch, Redis, MongoDB, Impala, Kudu, etc.• A skill in implementing Data Lakes, Data Warehousing, or analytics systems, which is a big advantage YOU WANT TO WORK WITH Opportunity to • Create sustainable Big Data and AI solutions• Evaluate cutting-edge Big Data technologies, implement PoCs and MVPs• Learn new technologies and obtain certifies• Learn how to design architectures using proven methodologies by SEI, Carnegie Mellon• Become a top expert, or a certified Architect• Gain deep expertise in one of the clouds or multiple clouds TOGETHER WE WILL • Deliver discovery and consulting projects, such as solution design, technology assessment and architecture evaluation together with CoE Lead Architects• Implement full-scale high-performance Data Platforms and decision-support systems• Utilize rapid prototyping techniques to accelerate implementation of new technical solutions• Adopt cutting-edge technologies on a challenging project• Provide high-value services to a different range of companies: from startups to Fortune 100• Engage new clients and work closely with Sales teamOur benefits • Assimilate best practices from experts, working in the team of top-notch Architects• Work closely or be a part of Google or Amazon professional services• Being a part of Center of Excellence, you will have a chance to address different business and technology challenges, drive multiple projects and initiatives• Boost your communication and leadership skills by obtaining experience on different projects• Attend and speak at international events",dou,2020-11-12,"Senior Big Data Engineer (Java+GCP), ID 57519@SoftServe",,,{}
Proxify,https://jobs.dou.ua/companies/proxify/,Azure Data Factory Engineer,https://jobs.dou.ua/companies/proxify/vacancies/132506/, remote,22 October 2020,,"Proxify is a Sweden IT company, experiencing intense growth. We match remote IT professionals with IT companies in Sweden and abroad. The difference with us is that we like to make sure that the remote workers we present are the very best in their field. We like to make sure we get it the right first time, every time! We are growing fast and currently looking for an Azure Data Factory Engineer to join our team.Our client is is a Swedish multinational home appliance manufacturer, headquartered in Stockholm. We are looking for Azure Data Factory Engineer to join us on a remote basis, who will be working in cross-functional teams but deployed to the recently established Data Science department. Requirements:—You have +0,5 years of experience with Azure Data Factory;—You have +1 years of experience with Azure in general;—You follow best practices and conventions;—Responsible and able to work with minimal supervision;—Upper-intermediate English level;—You can communicate well with both technical and non-technical clients. Nice-to-have: —Experience with Microsoft Power BI—Timezone: CET (+/- 3 hours) Responsibilities:—Build data pipelines using Azure Data Factory —Work along-side with Project managers, stakeholders, and Data Engineers/ScientistsAzure Data Factory Engineer will be responsible for building data pipelines in Azure Data Factory for projects across a wide variety of areas in the company: Finance, IoT, Marketing, Operations, etc. What we offer💻100% remote work (work from where you want);💪We pay for overtime (over 8 hours);🏄🏻‍♂️Business trips to Sweden at company expense (if and when necessary);👌🏻The ability to change the project to another one;💵Competitive salary/hour with a potential bonus scheme;🧘🏻‍♂️Very flexible working schedule;🚀Opportunities for professional development and personal growth;🐕Pet-friendly office, if you need to work from the office, you can come with your little friend.",dou,2020-11-12,Azure Data Factory Engineer@Proxify,,,{}
Geomotiv,https://jobs.dou.ua/companies/geomotiv/,Data Test Engineer (Data Quality),https://jobs.dou.ua/companies/geomotiv/vacancies/135936/," Kyiv, remote",22 October 2020,от $3000,"Required skills QUALITIES / EXPERIENCE WE’RE SEEKING: 5+ years of Quality Assurance/Testing experience;3+ years of Data Quality experience, or QA experience with a focus on data, data warehousing, reporting, etc;Strong SQL experience, with knowledge of AWS Redshift, Snowflake, or columnar databases. Knowledge of analytical functions is a plus;Experience with reporting or analytics tools like Tableau or Mode;Experience working with Amazon Web Services, querying and working with data in various AWS services;Programming experience in a language such as Python, Java, etc. for the purposes of parsing files and running queries;Experience with analytics implementations (network events, ad beacons, user action events, etc.) in a web or mobile application;Functional Testing on Native Apps, OTT (Over-the-top), web platforms, a BIG plus;3+ years of testing experience working within an Agile environment, and with Agile Management tools such as JIRA;Speaking English B1+. We offer BENEFITS: Professional self-realization;Competitive compensation;Flexible working hours;Remote work;25 days annual leave;5 sick-days;English lessons. Responsibilities SUMMARY At the project, we approach testing differently — we are testing, and breaking, code constantly, but we help rebuild it better. Data Test Engineers (DTE) test and verify the streaming service’s analytics events across all applications. This role is a DTE with a focus on Data Quality. In this role you will apply your QA, Data Quality, and SQL experience to manually test the firing of analytics events and verify those events in downstream databases or files. This DTE will work with Business Intelligence analysts and developers to make sure data quality and integrity is maintained as new events are added to applications. The position requires strong QA experience with a focus on data, knowledge of data pipelines from raw data to reporting, and demonstrable SQL skills. The DTE will also act as a representative of the Software Test Engineering Team in scrum meetings, and work alongside product management to address how to provide better quality coverage for the applications supported. RESPONSIBILITIES: Work with project teams implementing analytics features into client applications and verify the firing of those analytics events by manually working with devices or initiating scripts;Verify applications and tools developed on data, data warehousing & AWS Redshift, Snowflake or columnar databases;Verify capturing of analytics events in related file systems or databases through SQL, or a scripting language (Python, Java, shell scripting, etc.);Work with Business Intelligence and Product Management to create test strategies, plans and cases that provide acceptable coverage for a given data pipeline, from event creation to reporting;Perform data profiling on downstream data for the purposes of finding field anomalies and possible data quality issues;Work in an Agile Software Delivery methodology, highly focused in creating data validation tests based on requirements;Work with other members of Data Test Engineering to compile a regression test suite that can be executed quickly;Help create a manageable, repeatable strategy for manual test cases;Provide a risk assessment on the defects identified and set the correct priority and severity. Project description ABOUT THE PROJECT It is a leading free streaming television service in America, delivering 100+ live and original channels and thousands of on-demand movies in partnership with major TV networks, movie studios, publishers, and digital media companies. Millions of viewers tune in each month to watch premium news, TV shows, movies, sports, lifestyle, and trending digital series. The service is available on all mobile, web, and connected TV streaming devices.",dou,2020-11-12,Data Test Engineer (Data Quality)@Geomotiv,,,"{""Required skills"": [""QUALITIES / EXPERIENCE WE\u2019RE SEEKING:""], ""We offer"": [""5+ years of Quality Assurance/Testing experience;3+ years of Data Quality experience, or QA experience with a focus on data, data warehousing, reporting, etc;Strong SQL experience, with knowledge of AWS Redshift, Snowflake, or columnar databases. Knowledge of analytical functions is a plus;Experience with reporting or analytics tools like Tableau or Mode;Experience working with Amazon Web Services, querying and working with data in various AWS services;Programming experience in a language such as Python, Java, etc. for the purposes of parsing files and running queries;Experience with analytics implementations (network events, ad beacons, user action events, etc.) in a web or mobile application;Functional Testing on Native Apps, OTT (Over-the-top), web platforms, a BIG plus;3+ years of testing experience working within an Agile environment, and with Agile Management tools such as JIRA;Speaking English B1+.""], ""Responsibilities"": [""BENEFITS:""], ""Project description"": [""Professional self-realization;Competitive compensation;Flexible working hours;Remote work;25 days annual leave;5 sick-days;English lessons.""]}"
MGID,https://jobs.dou.ua/companies/mgid/,Data Product Manager,https://jobs.dou.ua/companies/mgid/vacancies/132486/, Kyiv,22 October 2020,,"MGID was founded in 2008 and is one of the leading companies in native advertising. We enable our media partners to monetize their audience and help brands to promote their services and goods effectively. MGID offers a range of integrated solutions covering the promotion process every step of the way; we offer services ranging from planning out the marketing strategy to its thoughtful implementation and optimization. Our clients include major international brands like Renault, Domino’s, airbnb, PizzaHut, Qatar Airlines, and many others, including media organizations and web agencies. MGID is:— One of the largest MarTech-companies in the Ukrainian market;— A proprietary Highload service that delivers 185 billion advertisements to 850 million unique users in more than 70 languages;— Handling over 20 billion queries per day (more than 3X the query volume of search globally), our platform operates at an unprecedented scale;— The winner of multiple AdTech awards for innovation and product quality;— A workforce of 600+ employees operating from offices in the US, Europe and Asia;— A passion for cutting-edge technologies and a seamless vertical structure that allows the regional teams to exchange skills and development practices. Primary requirements: — 2+ years in Product Management;— Data-driven decision-making;— Startup mindset (Fail Fast, Learn Fast);— Advanced analytical skills;— Experience in A/B testing;— Familiarity with a data ecosystem including publicly available data, 3rd party data providers, and 1st and 2nd party sources;— Expertise with analytics tools (such as SQL, Tableau, or similar);— Good sense of data privacy and data security;— Ability to organize work with customers and manage relationships with them;— Ad-tech ecosystem understanding;— Comprehensive understanding and application of marketing data and metrics;— Market and competitors research skills;— Advanced English. Will be an advantage: — Biz Dev experience;— Experience in digital advertising landscape (ad servers, RTB, DSPs, DMPs);— Experience in building a DMP. Responsibilities: In this role, you will own the roadmap and execution for various audiences and data products. This can include UI/API experiences for building and managing audiences; data visualizations of audience attributes; data science models and pipelines for data discoverability, data quality, and audience creation; and workflows for activating these audiences across the MGID platform. As a Product Manager for data products, you will be working closely with stakeholders across our UX/UI, Data Partnerships, Data Science, and Engineering Teams — helping think through tradeoffs in workflow implementations, visualizations, model approaches, and customer impact. You will work on researching and integrating data from different data sources. Your main responsibilities will be: — Build and maintain relationships with key partners in the advertising ecosystem;— Manage joint product development and rollout with partners;— Conducting research to deeply understand our customers’ needs;— Working with the technical department to implement confirmed hypotheses;— Building strong relationships with media, insights & analytics partners / key vendors across audience building, activation and technology to bring innovations that align with market business initiatives;— Understanding of how evolving data privacy legislation impacts our business work with our data partners to operate in a compliant manner;— Work with the Data Science team to transform insights into meaningful product improvements. What we offer: —Friendly team, opportunities to share your knowledge and experience;—The newest office in the business center “Marmalade”;—English courses with a native speaker;—Flexible approach to the schedule;—Corporate participation in sports, eco-and social projects. How it happens — here www.facebook.com/MGID.inside;—""Premium"" health insurance package.",dou,2020-11-12,Data Product Manager@MGID,,,{}
ZONE3000,https://jobs.dou.ua/companies/zone3000/,Data Analyst (Marketing),https://jobs.dou.ua/companies/zone3000/vacancies/126184/, Kharkiv,22 October 2020,,"Required skills — Degree in Economics / Statistics / Mathematics— Strong SQL knowledge— Good theoretical background in math and statistic— Understanding predictive analytics— Experience building visualizations in Tableau or other similar tools— Experience working with data received from Google Analytics, Adwords, Facebook, etc.— Understanding of LTV / CAC, Conversion Rate, Cohort analysis, Retention Rate— Understanding of digital marketing, especially PPC and SEO fields— Analytical mindset— Upper-intermediate English (written and oral) We offer — High & competitive salary— Challenging work in an international professional environment— Opportunity to influence software development process, to be the owner of the product in your field of expertise— Opportunity to apply SAFe methodology— Flexible management— Flexible / Casual Leave— Relocation Bonus when moving from a different city / country— Full benefits package: paid vacation and sick leave— Continuous professional development (free internal and external professional trainings)— Free English classes in the company office— Free use of the services provided by Namecheap (for non-commercial purposes)— Quarterly teambuilding activities and company corporate events — RDX gym membership — Coffee, tea, fruits Responsibilities — Provide Marketing department with deep insights regarding all marketing activities— Support in decision-making— Building forecasts and predictive customer behavioral models— Data analysis to detect the reasons for deviations in the main and secondary metrics— Help maximize revenue by bringing early attention issues or changes in trends related to our products, business practice, processes, finances, and accounting— Ensure that data is tracked in the proper manner— Looking for alternative data sources and tools— Initiate deep digging analysis— Create visualizations and presenting them to the stakeholders Project description Data Analyst will become a member of a professional and motivated team. Our main goal is to transform data into knowledge. Data Analyst will work closely with the Marketing department and will help with data analysis, finding patterns, looking for hidden correlations.",dou,2020-11-12,Data Analyst (Marketing)@ZONE3000,,,"{""Required skills"": [""Degree in Economics / Statistics / Mathematics"", ""Strong SQL knowledge"", ""Good theoretical background in math and statistic"", ""Understanding predictive analytics"", ""Experience building visualizations in Tableau or other similar tools"", ""Experience working with data received from Google Analytics, Adwords, Facebook, etc."", ""Understanding of LTV / CAC, Conversion Rate, Cohort analysis, Retention Rate"", ""Understanding of digital marketing, especially PPC and SEO fields"", ""Analytical mindset"", ""Upper-intermediate English (written and oral)""], ""We offer"": [""High & competitive salary"", ""Challenging work in an international professional environment"", ""Opportunity to influence software development process, to be the owner of the product in your field of expertise"", ""Opportunity to apply SAFe methodology"", ""Flexible management"", ""Flexible / Casual Leave"", ""Relocation Bonus when moving from a different city / country"", ""Full benefits package: paid vacation and sick leave"", ""Continuous professional development (free internal and external professional trainings)"", ""Free English classes in the company office"", ""Free use of the services provided by Namecheap (for non-commercial purposes)"", ""Quarterly teambuilding activities and company corporate events"", ""RDX gym membership"", ""Coffee, tea, fruits""], ""Responsibilities"": [""Provide Marketing department with deep insights regarding all marketing activities"", ""Support in decision-making"", ""Building forecasts and predictive customer behavioral models"", ""Data analysis to detect the reasons for deviations in the main and secondary metrics"", ""Help maximize revenue by bringing early attention issues or changes in trends related to our products, business practice, processes, finances, and accounting"", ""Ensure that data is tracked in the proper manner"", ""Looking for alternative data sources and tools"", ""Initiate deep digging analysis"", ""Create visualizations and presenting them to the stakeholders""], ""Project description"": [""Data Analyst will become a member of a professional and motivated team. Our main goal is to transform data into knowledge. Data Analyst will work closely with the Marketing department and will help with data analysis, finding patterns, looking for hidden correlations.""]}"
eTeam,https://jobs.dou.ua/companies/eteam/,Senior Data Engineer,https://jobs.dou.ua/companies/eteam/vacancies/128967/, Kyiv,22 October 2020,,"Required skills — 5+ years’ experience working with data— Perfect understanding of data normalization, relational databases, data cleansing, data pipelines, BI/ETL tools— Strong proficiency in writing SQL queries— Experience setting up and maintaining data pipelines— Programming skills in Python— Familiarity with Spark, Kafka, BigQuery — Good communication skills (written and spoken English) As a plus — Practical experience with Google Cloud/AWS, Docker, Kubernetes We offer — Grow professionally with subsidized certifications, courses, and conferences— Improve your English with conversation clubs and direct client communication— Enjoy our extra perks: unlimited vacation, flexible schedule, bonuses for important life events, fresh food & snacks in the office— Let loose with fun parties, BBQs, online activities and off-sites Responsibilities — Data warehouse configuration and management— ETL process implementation and constant improvements— BigQuery queries creation and optimization Project description We’re looking for a data engineer to join our data science team to help with the ETL process, data gathering, and pipeline setup. You’ll own a data warehouse and work closely with data scientists to extract valuable information for our customer.",dou,2020-11-12,Senior Data Engineer@eTeam,,,"{""Required skills"": [""5+ years\u2019 experience working with data"", ""Perfect understanding of data normalization, relational databases, data cleansing, data pipelines, BI/ETL tools"", ""Strong proficiency in writing SQL queries"", ""Experience setting up and maintaining data pipelines"", ""Programming skills in Python"", ""Familiarity with Spark, Kafka, BigQuery"", ""Good communication skills (written and spoken English)""], ""As a plus"": [""Practical experience with Google Cloud/AWS, Docker, Kubernetes""], ""We offer"": [""Grow professionally with subsidized certifications, courses, and conferences"", ""Improve your English with conversation clubs and direct client communication"", ""Enjoy our extra perks: unlimited vacation, flexible schedule, bonuses for important life events, fresh food & snacks in the office"", ""Let loose with fun parties, BBQs, online activities and off-sites""], ""Responsibilities"": [""Data warehouse configuration and management"", ""ETL process implementation and constant improvements"", ""BigQuery queries creation and optimization""], ""Project description"": [""We\u2019re looking for a data engineer to join our data science team to help with the ETL process, data gathering, and pipeline setup.""]}"
IntelliSoft,https://jobs.dou.ua/companies/intellisoft/,Data and Configuration Engineer,https://jobs.dou.ua/companies/intellisoft/vacancies/125808/," Kyiv, remote",21 October 2020,$1000–2000,"Required skills We are looking for a dedicated person with IT background who as a data & configuration engineer can contribute to our international team which develops an award winning medical software product. The product is saving many lives daily in most of the hospitals in Denmark, Sweden and Iceland.The product has many different features to help doctors and nurses, including data acquisition from medical devices, advanced data handling algorithms and statistics. Successful candidates should possess all of the following skills\qualifications: Proven 2+ years experience as a data engineer or tester or technical writer or business analyst or localization manager in a company related to software development;Splendid communication skills;Excellent information decomposition skills;Ability to structure information well;Strong analytical skills;Understanding of software development life cycle and main principles;Acquainted with main software development methodologies;Good English skills; As a plus Expertise in software testing;Awareness of testing methodologies, types, and approaches;Any experience with testing & bug tracking tools. We offer Remote work (all equipment for home office is provided by us) for the time of COVID-19 epidemic, then work from our Kyiv office Be a part of the bright international teamWork on industry leading and award winning product which saves livesProfessional and career growth opportunities50% compensation for learning materials, courses, rantings. and tech. eventsGood working conditions in Podil, 2 minutes from Tarasa Schevchenko metro station.Free fruits, snacks, and good coffeeCompetitive salaryBusiness trips to EU4 calendar weeks of paid annual vacationPaid sick leaveCorporate Medical Insurance programs Responsibilities As a data & configuration engineer in this team, you will work closely with other team members such as product and project managers, business analysts, software developers, quality assurance engineers, technical writers in order to produce a product of very high quality, that makes doctors happy. Your main duties will be: Creating and maintaining configurations of our clinical IT systemsAnalyze possibilities for implementing clinical needs content. Collaborate with software developers on how to solve problems. Manage requirements from various customers. Manage requirements from suppliers of data. Keep informed about updates from suppliers of data. Advising others on what is possible. Documenting planned work and work that has been carried out. Managing translation files during internationalization and localization of our products. Work on improving/automating some processes, to have them automated with help from the software development team. Creating test data for test cases and sales demonstrations etc.Assisting the Quality Assurance team Project description Cambio CIS — the best way to help clinical staff save more lives is by providing them with a comprehensive, interactive and dynamic patient-centric data overview. A holistic view with easy access to information, in any required resolution; viewing and updating what is needed in any given patient situation including relevant and customized clinical decision tools.",dou,2020-11-12,Data and Configuration Engineer@IntelliSoft,,,"{""Required skills"": [""We are looking for a dedicated person with IT background who as a data & configuration engineer can contribute to our international team which develops an award winning medical software product. The product is saving many lives daily in most of the hospitals in Denmark, Sweden and Iceland.The product has many different features to help doctors and nurses, including data acquisition from medical devices, advanced data handling algorithms and statistics.""], ""As a plus"": [""Successful candidates should possess all of the following skills\\qualifications:""], ""We offer"": [""Proven 2+ years experience as a data engineer or tester or technical writer or business analyst or localization manager in a company related to software development;Splendid communication skills;Excellent information decomposition skills;Ability to structure information well;Strong analytical skills;Understanding of software development life cycle and main principles;Acquainted with main software development methodologies;Good English skills;""], ""Responsibilities"": [""Expertise in software testing;Awareness of testing methodologies, types, and approaches;Any experience with testing & bug tracking tools.""], ""Project description"": [""Remote work (all equipment for home office is provided by us) for the time of COVID-19 epidemic, then work from our Kyiv office Be a part of the bright international teamWork on industry leading and award winning product which saves livesProfessional and career growth opportunities50% compensation for learning materials, courses, rantings. and tech. eventsGood working conditions in Podil, 2 minutes from Tarasa Schevchenko metro station.Free fruits, snacks, and good coffeeCompetitive salaryBusiness trips to EU4 calendar weeks of paid annual vacationPaid sick leaveCorporate Medical Insurance programs""]}"
Synergetica,https://jobs.dou.ua/companies/synergetica/,Middle QA 2+ (Data Science Platform),https://jobs.dou.ua/companies/synergetica/vacancies/135851/, Kyiv,21 October 2020,,"Required skills — Experience working with Linux commands and basic shell scripting;— Knowledge and Experience working with Kubernetes and Docker environments;— Excellent oral and written skills in English;— Creative talents and ability to solve problems;— Ability to handle pressure and meet deadlines;— Ability to prioritizing and triaging obligations and attention to details. As a plus — Knowledge of cloud computing;— Knowledge of performance and stress testing; — Knowledge of Python or Rails. We offer — Competitive compensation depending on experience and skills;— Opportunities for self-realization, professional and career growth;— Office near the Teatral’na metro station (Bohdana Khmelnytskogo Str.);— Compensation package (paid vacation, sick leaves), flexible working hours;— English classes, yoga, tech training, and conference participation. Responsibilities — Write and execute high-level test cases;— Verify and go over automation daily reports every day;— Support testing features with Cnvrg CLI and SDK;— Verify production bug fixes. Project description The project is a full-stack data science platform that helps enterprises manage and scale AI. Its collaborative end-to-end solution enables companies to accelerate innovation and build high impact machine, learning models. From Fortune 500 companies to startups, this solution helps data scientists solve complex problems, by building intelligent machines. The platform is used across industries by leading companies in finance, gaming, BI, automotive, manufacturing, e-commerce, and more. We are looking for a Detailed oriented Middle QA engineer, with a big emphasis on functionality experience in testing client/server complex web applications. Ability to work independently as well as part of a team with the desire to make an effect and build a cutting edge system.",dou,2020-11-12,Middle QA 2+ (Data Science Platform)@Synergetica,,,"{""Required skills"": [""Experience working with Linux commands and basic shell scripting;"", ""Knowledge and Experience working with Kubernetes and Docker environments;"", ""Excellent oral and written skills in English;"", ""Creative talents and ability to solve problems;"", ""Ability to handle pressure and meet deadlines;"", ""Ability to prioritizing and triaging obligations and attention to details.""], ""As a plus"": [""Knowledge of cloud computing;"", ""Knowledge of performance and stress testing;"", ""Knowledge of Python or Rails.""], ""We offer"": [""Competitive compensation depending on experience and skills;"", ""Opportunities for self-realization, professional and career growth;"", ""Office near the Teatral\u2019na metro station (Bohdana Khmelnytskogo Str.);"", ""Compensation package (paid vacation, sick leaves), flexible working hours;"", ""English classes, yoga, tech training, and conference participation.""], ""Responsibilities"": [""Write and execute high-level test cases;"", ""Verify and go over automation daily reports every day;"", ""Support testing features with Cnvrg CLI and SDK;"", ""Verify production bug fixes.""], ""Project description"": [""The project is a full-stack data science platform that helps enterprises manage and scale AI. Its collaborative end-to-end solution enables companies to accelerate innovation and build high impact machine, learning models. From Fortune 500 companies to startups, this solution helps data scientists solve complex problems, by building intelligent machines. The platform is used across industries by leading companies in finance, gaming, BI, automotive, manufacturing, e-commerce, and more. We are looking for a Detailed oriented Middle QA engineer, with a big emphasis on functionality experience in testing client/server complex web applications. Ability to work independently as well as part of a team with the desire to make an effect and build a cutting edge system.""]}"
Grid Dynamics,https://jobs.dou.ua/companies/grid-dynamics/,Senior BigData Developer with Python / Spark / ETL,https://jobs.dou.ua/companies/grid-dynamics/vacancies/135845/," Kyiv, Kharkiv, Lviv",21 October 2020,,"We are looking for a talented Data Engineer to work on wide range of data projects in close collaboration with other data engineers, data scientists, and data analysts in an integrated Data Team. We use a modern all-cloud data stack including Airflow, Docker, DBT, Python, Snowflake, Tableau, Sigma, and our old friend SQL. Our client is the leading marketplace for investing in single-family rental homes that cash flow day one. With over $2B in transactions, their mission is to make real estate investing radically accessible, cost effective and simple. What you will do:— Design, implement and deploy, scalable, fault-tolerant pipelines that ingest, and refine large diverse (structured, semi-structured and unstructured datasets) into simplified, accessible data models in production— Built departmental data-marts for supporting analytics across the company;— Collaborate with cross-functional teams to understand data flows and processes to enable design and creation of the best possible solutions;— Provide quality data solutions in a timely manner and be responsible for data governance and integrity while meeting objectives and maintaining SLAs;— Build tools and fundamental data sets that encourage self-service;— Improve and maintain the data infrastructure. Requirements:— 3+ years professional experience in writing production Python code, shell scripts, and complex SQL;— 1+ year building and deploying data-related infrastructure (messaging, storage, compute, transform, execution via docker, and/or CI/CD pipelines across dev/stage/prod;— Strong communication and interpersonal skills. Nice to have:— Experience with Airflow (or similar tools), AWS, Azure and DBT is a plus;— Experience in data warehousing and dimensional data modeling. We offer:— Opportunity to work on bleeding-edge projects;— Work with a highly motivated and dedicated team;— Competitive salary;— Flexible schedule;— Medical insurance;— Benefits program;— Corporate social events. About us:Grid Dynamics is the engineering services company known for transformative, mission-critical cloud solutions for retail, finance and technology sectors. We architected some of the busiest e-commerce services on the Internet and have never had an outage during the peak season. Founded in 2006 and headquartered in San Ramon, California with offices throughout the US and Eastern Europe, we focus on big data analytics, scalable omnichannel services, DevOps, and cloud enablement.",dou,2020-11-12,Senior BigData Developer with Python / Spark / ETL@Grid Dynamics,,,{}
Grid Dynamics,https://jobs.dou.ua/companies/grid-dynamics/,BigData Developer with Python / Spark / NoSQL / Cloud,https://jobs.dou.ua/companies/grid-dynamics/vacancies/135842/," Kyiv, Kharkiv, Lviv",21 October 2020,,"A project for a top American multinational food, snack and beverage corporation with interests in the manufacturing, marketing, and distribution of grain-based snack foods, beverages, and other products. We are building a Data Analytics and Management platform to provide NextGen level of business operations efficiency on the global market. Responsibilities:— Design, implement and deploy, scalable, fault-tolerant pipelines that ingest, and refine large diverse (structured, semi-structured and unstructured datasets) into simplified, accessible data models in production;— Provide quality data solutions in a timely manner and be responsible for data governance and integrity while meeting objectives and maintaining SLAs;— Improve and maintain the data infrastructure;— Work closely with data scientists and analysts to create and deploy new features;— Built departmental data-marts for supporting analytics across the company;— Monitor and plan out core infrastructure enhancements;— Contribute to and promote good software engineering practices across the team;— Mentor and educate team members to adopt best practices in writing and maintaining production code; — Communicate clearly and effectively to technical and non-technical audiences. Required skills:— 2+ years of solid experience working with Python;— In-depth knowledge of Spark;— Experience working with Cloud solutions;— Experience working with a variety of relational SQL and NoSQL databases;— Working experience with performance tuning on any RDBMS. Nice to have:— Experience on Azure platform, Data Lake, Blob, HiveSQL, Data Bricks, CI/CD pipeline;— Good experience in Hive;— Unix shell scripting experience;— Experience on Teradata;— Ability to work independently and prioritize work across multiple projects;— Experience at data analysis and profiling. We offer:— Opportunity to work on bleeding-edge projects;— Work with a highly motivated and dedicated team;— Competitive salary;— Flexible schedule;— Medical insurance;— Benefits program;— Corporate social events. About us:Grid Dynamics is the engineering services company known for transformative, mission-critical cloud solutions for retail, finance and technology sectors. We architected some of the busiest e-commerce services on the Internet and have never had an outage during the peak season. Founded in 2006 and headquartered in San Ramon, California with offices throughout the US and Eastern Europe, we focus on big data analytics, scalable omnichannel services, DevOps, and cloud enablement.",dou,2020-11-12,BigData Developer with Python / Spark / NoSQL / Cloud@Grid Dynamics,,,{}
SoftServe,https://jobs.dou.ua/companies/softserve/,Senior Data Science Engineer (ID 57612),https://jobs.dou.ua/companies/softserve/vacancies/135841/," Ivano-Frankivsk, remote",21 October 2020,,"WE ARE Transforming the way thousands of global organizations do business by developing the most innovative technologies and processes in Big Data, Internet of Things (IoT), Data Science, and experience design.We are one of the best and oldest Data Science teams in Ukraine and you will get tons of experience working with the best talents in the field.We are a Data Science Center of Excellence and you will have a chance to contribute to the wide range of projects from different areas and technologies. We’re looking for you, a person who is inspired by data, analytics, and AI as we do, and who wants to grow with us! YOU ARE A Data Scientist who will help us discover the information hidden in vast amounts of data and make smarter decisions to deliver even better products.Strongly competent in Applied Statistical Data Analysis, competent in solving of Data Mining and Machine Learning problems with application to the Predictive and Prescriptive analytics techniques and qualified in Deep Learning techniques application.Having knowledge of all mandatory technical and logical areas of the practice, understanding of all technical and logical areas of the practice and have their own researches and publications.You possess such qualification as • PhD/ Master’s Degree in a quantitative field (Maths, Statistics, Computer Science, Engineering, Data Science, Operations Research, etc.)• Extensive knowledge and practical experience in Applied Statistics (Exploratory Data Analysis and Distributions Fitting, Inference on Populations, Regression Analysis, Factor Analysis, and Dimensional Reduction) and Data Mining (Clustering, Frequent Pattern Mining, Outliers Detection)• Vast concept and practical expertise in Advanced Data Analysis (Casuality, Data Analysis in Social Science, Network Analysis) and Predictive analytics (Regression Models, Time-series analysis and forecasting, Survival or duration analysis)• Strong knowledge about Machine learning applications and Deep Learning Experience working on projects of complex scope where independent judgment is used within a broad range of defined procedures and practices• Understanding of software development company functioning Understanding of software development life cycles Upper-Intermediate English level or higher (oral/written)Your extra power reveals in • Good understanding of Digital Signal Processing and Prescriptive Analytics (what-if analysis, Decision-tree based model, fuzzy inference systems)• Knowledge about Big Data solutions and advanced data mining tools, Experience in Data Science Specializations (Natural Language Processing, Recommender Systems)• Experience with Machine Learning/Advanced analytics Cloud Platforms Understanding of machine learning productionalization processes Advanced proficiency in statistical programming software, (e.g, R, Python, SAS, Matlab, etc.)• Data Warehousing, Collection and Transformation (Databases, database systems, SQL and NoSQL)Development Tools (SVN, Git) YOU WANT WORK WITH • Full-stack Data Analysis, Data Mining, Predictive analytics, Machine Learning models pipeline, and deep analysis of the collected customer data. It includes making decisions regarding relevant computational tools for study, experiment, or trial research objectives• Data Science related problems for different business domains, implement and support predictive models, apply data mining techniques, provide analytical support in the pipeline of Machine Learning models productionalization process, communicate with the world-leading companies from our logos portfolio• The production of clear, concise, well-organized, and error-free computer programs, statistical reporting with the appropriate technological stack Compiling and interpreting analysis results as well as contributing to the reports delivered to the customer• The application of appropriate Data Mining and Predictive analytics methods TOGETHER WE WILL • Support your technical and personal growth; we have a dedicated career plan for all roles in our company• Process dynamic projects and still have so much stable place to work• Enjoy flexible working hours; whether you are a morning bird or a sleepy-head, it is ok with us• Take part in internal and external events where you can build and promote your personal brand• Give you access to experienced specialists willing to share their knowledge• Show you our casual atmosphere no dress code• Care about your individual initiatives — we are open for them, just come and share your ideas• Share many other advantages with you such as attractive salary, modern office, a package of benefits, language classes",dou,2020-11-12,Senior Data Science Engineer (ID 57612)@SoftServe,,,{}
Ciklum,https://jobs.dou.ua/companies/ciklum/,Cloud Architect for Ciklum Digital,https://jobs.dou.ua/companies/ciklum/vacancies/135839/, Kyiv,21 October 2020,,"On behalf of Ciklum Digital, Ciklum is looking for a Cloud Architect to join our team on a full-time basis.You will join a highly motivated team and will be working on a modern solution for our existing client. We are looking for technology experts who want to make an impact on new business by applying best practices and taking ownership. About Project:Ciklum is building the collaborative sportswear design and creation platform for the TOP-5 multinational company of the sportswear market. Scalable solution that will be used by famous worldwide sportswear designers to create and bring to market new sports fashion collections. Part of the larger Product Lifecycle Management ecosystem the platform delivered within the current project will become a market differentiator and key comparative advantage for a multi-billion business. The project will use proven and edge technologies and concepts, such as SAFe, Behaviour-Driven Development, DevOps, Event-Driven architecture.",dou,2020-11-12,Cloud Architect for Ciklum Digital@Ciklum,,,{}
Grid Dynamics,https://jobs.dou.ua/companies/grid-dynamics/,Architect / Lead BigData Developer,https://jobs.dou.ua/companies/grid-dynamics/vacancies/135834/," Kyiv, Kharkiv, Lviv",21 October 2020,,"We are looking for a seasoned Big Data Developer/Architect to kick off the development of the Enterprise Data Platform for one of our major customers. The platform is intended to serve several key divisions with the ultimate goal to become the enterprise level system. The scope of work we are involved in includes: initial assessment, design of the reference architecture for the short and mid-term platform evolution, execution of the platform development. Further the candidate will be responsible for the architecture design and technical leadership of the development team. Responsibilities:— Evaluate current state architecture and provide recommendations for the next transition state that focuses on enabling teams to work in the platform without requiring the platform team to do the work;— Define the 1 year roadmap, including set of governance rules and policies to build the business outcome oriented solution, including: core data and ML platform capabilities, data injection from various internal systems of records and data sources, implementation of data engineering, data transformation, and data consumption use cases;— Define recommended technology stack based on the business requirements provided by several departments. Should consider existing technologies in the environment and a cost comparison and justification for changing technologies;— Create data platform reference architecture to cover various capabilities including: data ingestion for batch and real-time stream processing (structured and unstructured data); data engineering: cleaning, deduplication, transformation, aggregation; data governance: catalog, lineage, quality, semantic layer, etc.;— Cover various capabilities including: data science and machine learning platform for end-to-end data science workflow; DevOps for data engineering, ML Ops, continuous delivery, and testing; patterns and standards for transactional systems architecture (microservices) for unified data platform use cases;— Provide a long-term recommendation for data platform evolution to support enterprise goals;— Define recommended team structure, tracks, and skills for the Platform Development effort;— Act as a Principal Big Data Developer and Tech Lead during the platform development effort. Required skills:— Solid working experience with Python/Scala;— Profound working experience with AWS Cloud: EMR, S3, Glue;— Extensive working experience with Kinesis, Kafka, HDFS, YARN, Spark; — Experience in building Data Platforms in public clouds, especially in AWS and Azure;— Proficient expertise of in-depth Big Data technologies;— Perfect communication skills;— Advanced+ level of English. We offer:— Opportunity to work on bleeding-edge projects;— Work with a highly motivated and dedicated team;— Competitive salary;— Flexible schedule;— Medical insurance;— Benefits program;— Corporate social events. About us:Grid Dynamics is the engineering services company known for transformative, mission-critical cloud solutions for retail, finance and technology sectors. We architected some of the busiest e-commerce services on the Internet and have never had an outage during the peak season. Founded in 2006 and headquartered in San Ramon, California with offices throughout the US and Eastern Europe, we focus on big data analytics, scalable omnichannel services, DevOps, and cloud enablement.",dou,2020-11-12,Architect / Lead BigData Developer@Grid Dynamics,,,{}
LITSLINK,https://jobs.dou.ua/companies/litslink/,Middle ML Engineer (NLP),https://jobs.dou.ua/companies/litslink/vacancies/135795/," Kyiv, Kharkiv, remote",21 October 2020,,"Required skills — 3+ year commercial experience with hands-on Python development and NLP— Solid practical and theoretical knowledge of Statistics, Machine Learning and Deep Learning— Theoretical understanding, practical implementation of classical NLP and Neutral Network driven NLP— Familiarity with cloud ML Platforms is a plus.— Experience with DNN frameworks as PyTorch, or TensorFlow— Upper-Intermediate + English level We offer — Great opportunity to work in a strong professional environment in one of the fastest-growing Ukrainian IT company;— Competitive salary;— Flexible working schedule. We respect your personal time. So you can start any time till 11 a.m. and finish accordingly till 8 p.m.;— Paid vacation and sick leaves, corporate events;— Comfortable office in the Kyiv city center (near Lvivska square, BC Kiyanovskiy). Responsibilities — Fully cover (develop, maintain and monitor) the entire lifecycle of created models.— Create production-ready solutions, launch MVPs, help DevOps Team to deploy them.— Work with NLP algorithms and data structures— Mentor and guide other NLP team members if necessary",dou,2020-11-12,Middle ML Engineer (NLP)@LITSLINK,,,"{""Required skills"": [""3+ year commercial experience with hands-on Python development and NLP"", ""Solid practical and theoretical knowledge of Statistics, Machine Learning and Deep Learning"", ""Theoretical understanding, practical implementation of classical NLP and Neutral Network driven NLP"", ""Familiarity with cloud ML Platforms is a plus."", ""Experience with DNN frameworks as PyTorch, or TensorFlow"", ""Upper-Intermediate + English level""], ""We offer"": [""Great opportunity to work in a strong professional environment in one of the fastest-growing Ukrainian IT company;"", ""Competitive salary;"", ""Flexible working schedule. We respect your personal time. So you can start any time till 11 a.m. and finish accordingly till 8 p.m.;"", ""Paid vacation and sick leaves, corporate events;"", ""Comfortable office in the Kyiv city center (near Lvivska square, BC Kiyanovskiy).""], ""Responsibilities"": [""Fully cover (develop, maintain and monitor) the entire lifecycle of created models."", ""Create production-ready solutions, launch MVPs, help DevOps Team to deploy them."", ""Work with NLP algorithms and data structures"", ""Mentor and guide other NLP team members if necessary""]}"
AgileEngine,https://jobs.dou.ua/companies/agileengine/,Senior/Lead Data Engineer,https://jobs.dou.ua/companies/agileengine/vacancies/135786/," Kyiv, Kharkiv, Odesa",21 October 2020,,"What is required?● 4+ years experience in software development● Practical experience in building the data collection, validation and normalization● Practical experience building ETL/ELT data pipelines from scratch or using existing frameworks like StreamSets DataCollector (SDC), Fivetran, Apache NiFi● Knowledge of SQL Server or other relational database management systems (Oracle, PostgreSQL, MySQL)● Practical experience of using Data Warehouse(s) with Snowflake or other DW-oriented databases (Google BigQuery, Redshift, etc.)● Practical experience with one of the following languages: Python, Java, Scala● Intermediate English What will be a plus?● AWS basics● DevOps practice basics● Practical experience in using BI tools such as Microsoft Power BI, Pentaho, GoodData What will you do?● Build data pipelines(ETL flows) to connect different sources of data together● Designing data structures for analytics● Identification of performance bottlenecks● Serve as a go-to Data Engineering consultant for other team members● Optimize SQL queries● Communication with Engineering & Product management● Alignment with existing development teams What We offer?● A friendly and a very skilled team with great corporate culture and mentorship (visit us and see it yourself)● Interesting and challenging tasks● Flexible work schedule● Zero bureaucracy● US democratic management style● Opportunities for self-realization, professional and career growth● Cool events and team activities● Professional workshops and training, a great engineering culture What About the project? Quanterix is a company that’s digitizing biomarker analysis with the goal of advancing the science of precision health. The company’s digital health solution, Simoa, has the potential tochange the way in which healthcare is provided today by giving researchers the ability to closely examine the continuum from health to disease. Quanterix’ technology is designed to enable much earlier disease detection, better prognoses and enhanced treatment methods to improve the quality of life and longevity of the population for generations to come. The technology is currently being used for research applications in several therapeutic areas, including oncology, neurology, cardiology, inflammation and infectious disease. The company was established in 2007 and is located in Billerica, Massachusetts. For additional Information, please visit www.quanterix.com. What Quanterix is going to build ARC: Standalone software application to provide researchers with a tool that automatically summarizes study data from a series of reports into a single, collated documentedBenefits: ● Offers users improved efficiency over manual reporting where users manually collate experiments results into a single report by copying & pasting results into Excel/Word/PowerPoint● Moving from a manual to an automated collation process improves data integrity by reducing risk of human error in transposing data from individual run reports into consolidated summary● Tool features can be expanded to capture additional data elements to support GLP (Good Laboratory Practice) and data integrity standards— GLP: reagent lot/expiry reporting, user records, run QC and validity (curve, control and precision flagging), qualitative result assignment— Data integrity: validated compiler, audit logs, electronic signatures The challenge for the data engineer● Each of the platforms puts the data in its own database● It will be some sort of normalizing the data to allow and we’ll be able to pull from each of those databases into the Reporting tool● Reporting tool must be able generate the same set of reports regardless of which database the data was pulled from● And that can be done independent from the existing software",dou,2020-11-12,Senior/Lead Data Engineer@AgileEngine,,,{}
Ciklum,https://jobs.dou.ua/companies/ciklum/,Middle Data Engineer for Ciklum Digital,https://jobs.dou.ua/companies/ciklum/vacancies/135775/, Kyiv,21 October 2020,,"On behalf of Ciklum Digital, Ciklum is looking for a Middle Data Engineer to join the UA team on a full-time basis. You will join a highly motivated team and will be working on a modern solution for our existing client. We are looking for technology experts who want to make an impact on new business by applying best practices and taking ownership. About Project: Tetrisoft is an in-house developed project management application dedicated to the Tétris Design and Build Business. It enables the daily online management of projects, budgets, quotations, sales orders, purchase orders, and invoices for a $700 million business. The application serves about 700 end users from Project Managers to Finance and Compliance teams in 12 countries. Technology Stack: MS SQL Server with SSRS, and ASP.NET",dou,2020-11-12,Middle Data Engineer for Ciklum Digital@Ciklum,,,{}
iDeals Solutions,https://jobs.dou.ua/companies/ideals-solutions/,Data Analyst & Process Automation Specialist (Sales Operations Team),https://jobs.dou.ua/companies/ideals-solutions/vacancies/132096/," Kyiv, remote",21 October 2020,,"Currently we are expanding our Sales Operations team and we are looking for the Sales Operations Tech Specialist, who will be responsible for supporting, improving, and customizing our sales tech stack (CRM and other tech solutions). The mission of this role is to make the sales-flow easier and more comfortable for all the participants and increase the efficiency of the process through the automatization of the routine tasks. WHAT YOU WILL DO: SKILLS AND COMPETENCIES: WHAT WE OFFER: iDeals has been delivering the best customer experience since 2008. We were able to make a competitive product and build a leading business in a crowded and highly segmented market. We have decided to make a product of our dreams that leave competitors far behind. More than 2500 clients spread around the globe, among which there are LG, PWC, HP, EY, Deloitte, Barclays, Rothschild, inspiring us to deliver the best product in the market. We are an international team of 170 people who speak ten languages. We share transparency, teamwork, data-driven decision-making, responsibility, critical thinking, and focusing on exceeding clients’ expectations as our primary common values. iDeals is made up of people from a wide variety of backgrounds and lifestyles. We embrace diversity and invite applications from people from all walks of life. We don’t discriminate against employees or applicants based on gender identity or expression, sexual orientation, race, religion, age, national origin, citizenship.",dou,2020-11-12,Data Analyst & Process Automation Specialist (Sales Operations Team)@iDeals Solutions,,,"{""WHAT YOU WILL\u00a0DO:"": [""Currently we are expanding our Sales Operations team and we are looking for the Sales Operations Tech Specialist, who will be responsible for supporting, improving, and customizing our sales tech stack (CRM and other tech solutions).""], ""SKILLS AND\u00a0COMPETENCIES:"": [""The mission of this role is to make the sales-flow easier and more comfortable for all the participants and increase the efficiency of the process through the automatization of the routine tasks.""], ""WHAT WE\u00a0OFFER:"": [""iDeals has been delivering the best customer experience since 2008. We were able to make a competitive product and build a leading business in a crowded and highly segmented market. We have decided to make a product of our dreams that leave competitors far behind. More than 2500 clients spread around the globe, among which there are LG, PWC, HP, EY, Deloitte, Barclays, Rothschild, inspiring us to deliver the best product in the market.""]}"
SocialTech,https://jobs.dou.ua/companies/socialtech/,Product Data Analyst (Mobile App),https://jobs.dou.ua/companies/socialtech/vacancies/135769/, Kyiv,21 October 2020,,"Plich — глобальная IT-компания в индустрии Social Discovery, часть холдинга SocialTech.Мы создаем и развиваем собственные продукты с нуля, которыми пользуются миллионы людей по всему миру.Сейчас мы ищем продуктового аналитика, который сможет возглавить аналитику бизнес юнита мобильных приложений. Это 3 продукта с суммарным скачиванием более 2 млн на iOS/Android платформах.Чем мы можем быть интересны:● Сложные аналитические вызовы, работа с прогнозными моделями на волатильных выборках.● Возможности тестировать свои гипотезы на миллионной аудитории.● Работа в кросс функциональной команде senior специалистов.● Минимум бюрократии и гибкость в выборе инструментов.Тебя ждут задачи в 3 направлениях:● Продуктовая аналитика. Анализировать поведение пользователей, формировать гипотезы для продуктовых улучшений и оценивать А / В тесты.● Маркетинговая аналитика. Оценивать эффективность маркетинговых затрат с учетом Remarketing и Repurchase, адаптация и применение существующих моделей скоринга источников трафика.● Инфраструктура данных. Помогать поддерживать специфические для приложения пайплайны выгрузки данных и выступать стейкхолдером в инфраструктурных задачах.Если у тебя есть:● 1-2 года опыта работы на позиции Product Analyst / Data Analyst.● Умение работать с цифрами и способность системно мыслить.● Готовность закрыть под ключ зону аналитики.● Понимание как оценивать A/B тесты.● Уверенное владение SQL.● Опыт работы с Python (pandas, seaborn) / R (tidyverse, ggplot).Плюсом будет:● Опыт с Tableau / PowerBI / QlikView● Опыт в сфере мобильных приложенийЖдем тебя! P.S. И ждем твое резюме;)",dou,2020-11-12,Product Data Analyst (Mobile App)@SocialTech,,,{}
"Svitla Systems, Inc.",https://jobs.dou.ua/companies/svitla-systems-inc/,Senior Data Engineer,https://jobs.dou.ua/companies/svitla-systems-inc/vacancies/135761/," Kyiv, Kharkiv, Lviv, remote",21 October 2020,,"Svitla Systems Inc. is looking for Senior Data Engineer for a full-time position (40 hours per week) in Ukraine. Our client is reimagining how the world moves freight. They are a fast-growing tech startup based in Pittsburgh that is working on building the next era of fleet management, dispatch, and infrastructure technologies for the transportation and service industries. In a few short years, they’ve grown from an idea to create smart driver safety tracking to becoming a full-blown fleet management and telematics company that has successfully deployed to thousands of drivers across the country with some of the nation’s top 100 fleets. They provide mission critical software and applications that power the transportation industry 24/7. This means client has a huge responsibility to create rock solid technology and products that are fault tolerant and tested extremely thoroughly. 5+ years of experience working as Data Engineer.Experience in creating Data oriented/Big data architectures from scratch.Working experience with one of script languages for Data Pipelines development: Node.js, Python. (preferably with both).Experience with Data oriented Amazon Cloud Services: S3, Dynamo DB, Kinesis, EMR + Spark, Redshift (as a plus).Experience with ETL modelling for data pipelines. Healthy, Diverse Teams Breed Innovation. Our client is proud to be an equal opportunity employer. We deeply believe that diverse backgrounds and experiences make better products, and we seek to attract talent from all walks of life.The team is smart, friendly and passionate, and we value a healthy work environment to foster personal development and opportunities to move within our small, but quickly growing organization. 1. Experience in development Streaming Data Pipelines.2. Experience in AWS Serverless stack with data pipelines — Lambda, Step Functions. Competitive compensation plan that takes skills and experience into consideration.Annual performance appraisals.Possibility to choose your workspace either remote or combination of your home and one of our development offices. Flexible working hours and adjustable work/life balance. Projects that use advanced, cutting-edge technologies.Competitive bonuses for a personal recommendation of new employees.Vacation time, sick-leaves, national holidays, family supplementary days off.Comprehensive medical insurance including dental services, massages, and sports activities.Support for a healthy lifestyle, compensation of running events.A personal loan budget is available for long-term personnel.Partial compensation of conferences, courses and English classes.Free meetups, webinars, and conferences organized by Svitla.Birthday presents for personnel and New Year gifts for children.Fun summer and winter corporate parties and memorable anniversary presents.",dou,2020-11-12,"Senior Data Engineer@Svitla Systems, Inc.",,,{}
N-iX,https://jobs.dou.ua/companies/n-ix/,GCP Data Engineer,https://jobs.dou.ua/companies/n-ix/vacancies/135752/," Kyiv, Lviv, remote",21 October 2020,,"N-iX is looking for a passionate and motivated Data Engineer to join our team. Our customer is the multi-cloud solutions expert. They combine their expertise with the world’s leading technologies- across applications, data, and security- to deliver end-to-end multi-cloud solutions. As a global, multi-cloud technology services pioneer, they deliver the innovative capabilities of the cloud to help customers build new revenue streams, increase efficiency, and create incredible experiences. Responsibilities:• Building Data Lake using Google Cloud Platform• Develop end-to-end data solutions for structured and unstructured data including, but not limited to, ingestion, parsing, integration, auditing, logging, aggregation, and error handling Requirements:• Strong experience in building cloud-native data engineering solutions using GCP or AWS platforms• 3+ years of development experience with Python at an advanced level including concurrent and metaprogramming• Prior experience in building data ingestion pipelines of telemetry data in GCP/AWS including app monitoring, performance monitoring, and network monitoring logs• Deep background in building data integration applications using Spark or MapReduce frameworks• Track record of producing software artifacts of exceptional quality by adhering to coding standards, design patterns, and best practices• Well versed in SQL / NoSQL technologies and deep familiarity with dimensional modeling and Data warehousing concepts• Expertise in workflow scheduling and orchestration frameworks such as Airflow or Oozie• Prefer someone who has worked with GCP products such as BigQuery, Cloud Composer, Data Fusion, GCS and GKE or corresponding technologies on the AWS platform• High proficiency in working with Git, automated build, and CI/CD pipelines• Upper-Intermediate level of English We offer:• Flexible working hours• A competitive salary and good compensation package• Best hardware• A masseur and a corporate doctor• Healthcare & sport benefits• An inspiring, comfy, clean, and safe office Professional growth:• Challenging tasks and innovative projects• Meetups and events for professional development• An individual development plan• Mentorship program Fun:• Corporate events and outstanding parties• Exciting team buildings• Memorable anniversary presents• A fun zone where you can play video games, foosball, ping pong, and more.",dou,2020-11-12,GCP Data Engineer@N-iX,,,{}
Phase One Karma,https://jobs.dou.ua/companies/p1k/,Data Annotation Specialist,https://jobs.dou.ua/companies/p1k/vacancies/135750/, Kyiv,21 October 2020,,"Required skills — 2+ year of relevant experience;— Upper-intermediate level of English (technical);— Confident use of Mac OS;— Experience in using MS Office. As a plus — Some technical background;— Previous experience working with legal documents;— Previous experience working with annotation tools. We offer — Engagement into meaningful and ambitious projects that have a significant impact;— Comfortable compensation rate — we respect the value of human work;— Flexible work schedule — we expect a full-time commitment but do not track your working hours;— Flat hierarchy without micromanagement — our doors are open, and all teammates are approachable;— Creative autonomy and freedom to apply your vision;— A learning environment that promotes growth and professional improvement;— Medical insurance for employees and the possibility of training employees at the expense of the company;— Inclusive corporate culture — we embrace diversity and individuality;— Energized and empathetic team of experts who enjoy working and hanging out together;— 20 days of paid vacation and 10 sick days — we value rest and recreation. We also comply with thenational holidays and offer 2 remote workdays per month;— Modern lofty office with high ceilings, spacious workplace, and brand-new work equipment. Responsibilities — Analyzing legal contracts;— Identifying entities in contracts according to annotation guidelines;— Updating and creating annotation instructions;— Verifying annotation quality;— Generating reports about performed tasks. Project description You will join an ambitious AI-powered legal tech product on its early stages. As Loio is about to be released, you’ll be able to apply your unique vision and contribute to a high-quality disruptive tool aimed to conquer the international market.",dou,2020-11-12,Data Annotation Specialist@Phase One Karma,,,"{""Required skills"": [""2+ year of relevant experience;"", ""Upper-intermediate level of English (technical);"", ""Confident use of Mac OS;"", ""Experience in using MS Office.""], ""As a plus"": [""Some technical background;"", ""Previous experience working with legal documents;"", ""Previous experience working with annotation tools.""], ""We offer"": [""Engagement into meaningful and ambitious projects that have a significant impact;"", ""Comfortable compensation rate"", ""we respect the value of human work;"", ""Flexible work schedule"", ""we expect a full-time commitment but do not track your working hours;"", ""Flat hierarchy without micromanagement"", ""our doors are open, and all teammates are approachable;"", ""Creative autonomy and freedom to apply your vision;"", ""A learning environment that promotes growth and professional improvement;"", ""Medical insurance for employees and the possibility of training employees at the expense of the company;"", ""Inclusive corporate culture"", ""we embrace diversity and individuality;"", ""Energized and empathetic team of experts who enjoy working and hanging out together;"", ""20 days of paid vacation and 10 sick days"", ""we value rest and recreation. We also comply with thenational holidays and offer 2 remote workdays per month;"", ""Modern lofty office with high ceilings, spacious workplace, and brand-new work equipment.""], ""Responsibilities"": [""Analyzing legal contracts;"", ""Identifying entities in contracts according to annotation guidelines;"", ""Updating and creating annotation instructions;"", ""Verifying annotation quality;"", ""Generating reports about performed tasks.""], ""Project description"": [""You will join an ambitious AI-powered legal tech product on its early stages. As Loio is about to be released, you\u2019ll be able to apply your unique vision and contribute to a high-quality disruptive tool aimed to conquer the international market.""]}"
Brainstack_,https://jobs.dou.ua/companies/brainstack/,Junior+ Data Analyst (Python or/and R),https://jobs.dou.ua/companies/brainstack/vacancies/135745/, Kyiv,21 October 2020,,"Required skills — Техническое или экономическое образование.— 1+ лет профессионального опыта в роли аналитика. — Хорошие знания Python/R, SQL (MySQL, PostgreSQL).— Хороший теоретический бекграунд в статистике, тестировании гипотез и знания в регрессионном анализе (Logistic Regression, и т.п.)— Опыт работы с хранилищами данных и инструментами визуализации.— Знание web аналитики— Высокий уровень владения Excel (сводные таблицы, подключение к внешним источникам).— Аналитическое мышление, способность находить закономерность в изученных наборах данных, причины отклонений показателей, описывающих бизнес-процессы, кратко и точно акцентировать внимание на основных причинах отклонений.— Знание английского на уровне Pre-Intermediate или выше As a plus — Хорошее понимание digital маркетинга, понимание онлайн бизнеса, знание его специфических показателей— Опыт в прогнозной аналитике (анализ и прогнозирование временных рядов, анализ выживаемости и т. д.)— Опыт с сервисами облачных вычислений (GCP, etc.)— Опыт работы с платежными сервисами / электронной коммерцией / банками / провайдерами платежей We offer — Много возможностей для профессиональной реализации и развития.— Дружественная и теплая атмосфера в коллективе.— Оформление через ФОП.— 24 календарных дня отпуска.— Медицинская страховка после выхода на работу.— Компенсация изучения английского языка.— Оплата больничных.— Драйвовые корпоративы и много-много других плюшек. Responsibilities — Разработка Business Intelligent (создание автоматизированной отчетности в Google Data Studio и Tableau, работа с дата-инженерами для разработки схемы DWH и процессов ETL). — Анализ данных для выявления причин отклонений бизнес-метрик, а также оптимизации деятельности.— Оценка результатов экспериментов (A/B-тесты).— Прогнозирование финансовых результатов.— Участие в проектах по оптимизации бизнеса. Project description Brainstack_ is a team of intelligent, fun-loving people working on their own products that truly have value. Some of our products have already made their name In US, Europe, Central and South America, and we keep catching up with new products conquering new territories. We think that creating great products requires absolute teamwork, no matter which position you occupy: every task, every pixel, every code line makes a difference.",dou,2020-11-12,Junior+ Data Analyst (Python or/and R)@Brainstack_,,,"{""Required skills"": [""\u0422\u0435\u0445\u043d\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u0438\u043b\u0438 \u044d\u043a\u043e\u043d\u043e\u043c\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u0435."", ""1+ \u043b\u0435\u0442 \u043f\u0440\u043e\u0444\u0435\u0441\u0441\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e\u0433\u043e \u043e\u043f\u044b\u0442\u0430 \u0432 \u0440\u043e\u043b\u0438 \u0430\u043d\u0430\u043b\u0438\u0442\u0438\u043a\u0430."", ""\u0425\u043e\u0440\u043e\u0448\u0438\u0435 \u0437\u043d\u0430\u043d\u0438\u044f Python/R, SQL (MySQL, PostgreSQL)."", ""\u0425\u043e\u0440\u043e\u0448\u0438\u0439 \u0442\u0435\u043e\u0440\u0435\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u0431\u0435\u043a\u0433\u0440\u0430\u0443\u043d\u0434 \u0432 \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u043a\u0435, \u0442\u0435\u0441\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0438 \u0433\u0438\u043f\u043e\u0442\u0435\u0437 \u0438 \u0437\u043d\u0430\u043d\u0438\u044f \u0432 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u043e\u043d\u043d\u043e\u043c \u0430\u043d\u0430\u043b\u0438\u0437\u0435 (Logistic Regression, \u0438 \u0442.\u043f.)"", ""\u041e\u043f\u044b\u0442 \u0440\u0430\u0431\u043e\u0442\u044b \u0441 \u0445\u0440\u0430\u043d\u0438\u043b\u0438\u0449\u0430\u043c\u0438 \u0434\u0430\u043d\u043d\u044b\u0445 \u0438 \u0438\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442\u0430\u043c\u0438 \u0432\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438."", ""\u0417\u043d\u0430\u043d\u0438\u0435 web \u0430\u043d\u0430\u043b\u0438\u0442\u0438\u043a\u0438"", ""\u0412\u044b\u0441\u043e\u043a\u0438\u0439 \u0443\u0440\u043e\u0432\u0435\u043d\u044c \u0432\u043b\u0430\u0434\u0435\u043d\u0438\u044f Excel (\u0441\u0432\u043e\u0434\u043d\u044b\u0435 \u0442\u0430\u0431\u043b\u0438\u0446\u044b, \u043f\u043e\u0434\u043a\u043b\u044e\u0447\u0435\u043d\u0438\u0435 \u043a \u0432\u043d\u0435\u0448\u043d\u0438\u043c \u0438\u0441\u0442\u043e\u0447\u043d\u0438\u043a\u0430\u043c)."", ""\u0410\u043d\u0430\u043b\u0438\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u043c\u044b\u0448\u043b\u0435\u043d\u0438\u0435, \u0441\u043f\u043e\u0441\u043e\u0431\u043d\u043e\u0441\u0442\u044c \u043d\u0430\u0445\u043e\u0434\u0438\u0442\u044c \u0437\u0430\u043a\u043e\u043d\u043e\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u044c \u0432 \u0438\u0437\u0443\u0447\u0435\u043d\u043d\u044b\u0445 \u043d\u0430\u0431\u043e\u0440\u0430\u0445 \u0434\u0430\u043d\u043d\u044b\u0445, \u043f\u0440\u0438\u0447\u0438\u043d\u044b \u043e\u0442\u043a\u043b\u043e\u043d\u0435\u043d\u0438\u0439 \u043f\u043e\u043a\u0430\u0437\u0430\u0442\u0435\u043b\u0435\u0439, \u043e\u043f\u0438\u0441\u044b\u0432\u0430\u044e\u0449\u0438\u0445 \u0431\u0438\u0437\u043d\u0435\u0441-\u043f\u0440\u043e\u0446\u0435\u0441\u0441\u044b, \u043a\u0440\u0430\u0442\u043a\u043e \u0438 \u0442\u043e\u0447\u043d\u043e \u0430\u043a\u0446\u0435\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0432\u043d\u0438\u043c\u0430\u043d\u0438\u0435 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u043d\u044b\u0445 \u043f\u0440\u0438\u0447\u0438\u043d\u0430\u0445 \u043e\u0442\u043a\u043b\u043e\u043d\u0435\u043d\u0438\u0439."", ""\u0417\u043d\u0430\u043d\u0438\u0435 \u0430\u043d\u0433\u043b\u0438\u0439\u0441\u043a\u043e\u0433\u043e \u043d\u0430 \u0443\u0440\u043e\u0432\u043d\u0435 Pre-Intermediate \u0438\u043b\u0438 \u0432\u044b\u0448\u0435""], ""As a plus"": [""\u0425\u043e\u0440\u043e\u0448\u0435\u0435 \u043f\u043e\u043d\u0438\u043c\u0430\u043d\u0438\u0435 digital \u043c\u0430\u0440\u043a\u0435\u0442\u0438\u043d\u0433\u0430, \u043f\u043e\u043d\u0438\u043c\u0430\u043d\u0438\u0435 \u043e\u043d\u043b\u0430\u0439\u043d \u0431\u0438\u0437\u043d\u0435\u0441\u0430, \u0437\u043d\u0430\u043d\u0438\u0435 \u0435\u0433\u043e \u0441\u043f\u0435\u0446\u0438\u0444\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u043f\u043e\u043a\u0430\u0437\u0430\u0442\u0435\u043b\u0435\u0439"", ""\u041e\u043f\u044b\u0442 \u0432 \u043f\u0440\u043e\u0433\u043d\u043e\u0437\u043d\u043e\u0439 \u0430\u043d\u0430\u043b\u0438\u0442\u0438\u043a\u0435 (\u0430\u043d\u0430\u043b\u0438\u0437 \u0438 \u043f\u0440\u043e\u0433\u043d\u043e\u0437\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0445 \u0440\u044f\u0434\u043e\u0432, \u0430\u043d\u0430\u043b\u0438\u0437 \u0432\u044b\u0436\u0438\u0432\u0430\u0435\u043c\u043e\u0441\u0442\u0438 \u0438 \u0442. \u0434.)"", ""\u041e\u043f\u044b\u0442 \u0441 \u0441\u0435\u0440\u0432\u0438\u0441\u0430\u043c\u0438 \u043e\u0431\u043b\u0430\u0447\u043d\u044b\u0445 \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u0439 (GCP, etc.)"", ""\u041e\u043f\u044b\u0442 \u0440\u0430\u0431\u043e\u0442\u044b \u0441 \u043f\u043b\u0430\u0442\u0435\u0436\u043d\u044b\u043c\u0438 \u0441\u0435\u0440\u0432\u0438\u0441\u0430\u043c\u0438 / \u044d\u043b\u0435\u043a\u0442\u0440\u043e\u043d\u043d\u043e\u0439 \u043a\u043e\u043c\u043c\u0435\u0440\u0446\u0438\u0435\u0439 / \u0431\u0430\u043d\u043a\u0430\u043c\u0438 / \u043f\u0440\u043e\u0432\u0430\u0439\u0434\u0435\u0440\u0430\u043c\u0438 \u043f\u043b\u0430\u0442\u0435\u0436\u0435\u0439""], ""We offer"": [""\u041c\u043d\u043e\u0433\u043e \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e\u0441\u0442\u0435\u0439 \u0434\u043b\u044f \u043f\u0440\u043e\u0444\u0435\u0441\u0441\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e\u0439 \u0440\u0435\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 \u0438 \u0440\u0430\u0437\u0432\u0438\u0442\u0438\u044f."", ""\u0414\u0440\u0443\u0436\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u0430\u044f \u0438 \u0442\u0435\u043f\u043b\u0430\u044f \u0430\u0442\u043c\u043e\u0441\u0444\u0435\u0440\u0430 \u0432 \u043a\u043e\u043b\u043b\u0435\u043a\u0442\u0438\u0432\u0435."", ""\u041e\u0444\u043e\u0440\u043c\u043b\u0435\u043d\u0438\u0435 \u0447\u0435\u0440\u0435\u0437 \u0424\u041e\u041f."", ""24 \u043a\u0430\u043b\u0435\u043d\u0434\u0430\u0440\u043d\u044b\u0445 \u0434\u043d\u044f \u043e\u0442\u043f\u0443\u0441\u043a\u0430."", ""\u041c\u0435\u0434\u0438\u0446\u0438\u043d\u0441\u043a\u0430\u044f \u0441\u0442\u0440\u0430\u0445\u043e\u0432\u043a\u0430 \u043f\u043e\u0441\u043b\u0435 \u0432\u044b\u0445\u043e\u0434\u0430 \u043d\u0430 \u0440\u0430\u0431\u043e\u0442\u0443."", ""\u041a\u043e\u043c\u043f\u0435\u043d\u0441\u0430\u0446\u0438\u044f \u0438\u0437\u0443\u0447\u0435\u043d\u0438\u044f \u0430\u043d\u0433\u043b\u0438\u0439\u0441\u043a\u043e\u0433\u043e \u044f\u0437\u044b\u043a\u0430."", ""\u041e\u043f\u043b\u0430\u0442\u0430 \u0431\u043e\u043b\u044c\u043d\u0438\u0447\u043d\u044b\u0445."", ""\u0414\u0440\u0430\u0439\u0432\u043e\u0432\u044b\u0435 \u043a\u043e\u0440\u043f\u043e\u0440\u0430\u0442\u0438\u0432\u044b \u0438 \u043c\u043d\u043e\u0433\u043e-\u043c\u043d\u043e\u0433\u043e \u0434\u0440\u0443\u0433\u0438\u0445 \u043f\u043b\u044e\u0448\u0435\u043a.""], ""Responsibilities"": [""\u0420\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u043a\u0430 Business Intelligent (\u0441\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u0430\u0432\u0442\u043e\u043c\u0430\u0442\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u043e\u0439 \u043e\u0442\u0447\u0435\u0442\u043d\u043e\u0441\u0442\u0438 \u0432 Google Data Studio \u0438 Tableau, \u0440\u0430\u0431\u043e\u0442\u0430 \u0441 \u0434\u0430\u0442\u0430-\u0438\u043d\u0436\u0435\u043d\u0435\u0440\u0430\u043c\u0438 \u0434\u043b\u044f \u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u0441\u0445\u0435\u043c\u044b DWH \u0438 \u043f\u0440\u043e\u0446\u0435\u0441\u0441\u043e\u0432 ETL)."", ""\u0410\u043d\u0430\u043b\u0438\u0437 \u0434\u0430\u043d\u043d\u044b\u0445 \u0434\u043b\u044f \u0432\u044b\u044f\u0432\u043b\u0435\u043d\u0438\u044f \u043f\u0440\u0438\u0447\u0438\u043d \u043e\u0442\u043a\u043b\u043e\u043d\u0435\u043d\u0438\u0439 \u0431\u0438\u0437\u043d\u0435\u0441-\u043c\u0435\u0442\u0440\u0438\u043a, \u0430 \u0442\u0430\u043a\u0436\u0435 \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u0438 \u0434\u0435\u044f\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438."", ""\u041e\u0446\u0435\u043d\u043a\u0430 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432 \u044d\u043a\u0441\u043f\u0435\u0440\u0438\u043c\u0435\u043d\u0442\u043e\u0432 (A/B-\u0442\u0435\u0441\u0442\u044b)."", ""\u041f\u0440\u043e\u0433\u043d\u043e\u0437\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u0444\u0438\u043d\u0430\u043d\u0441\u043e\u0432\u044b\u0445 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432."", ""\u0423\u0447\u0430\u0441\u0442\u0438\u0435 \u0432 \u043f\u0440\u043e\u0435\u043a\u0442\u0430\u0445 \u043f\u043e \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u0438 \u0431\u0438\u0437\u043d\u0435\u0441\u0430.""], ""Project description"": [""Brainstack_ is a team of intelligent, fun-loving people working on their own products that truly have value. Some of our products have already made their name In US, Europe, Central and South America, and we keep catching up with new products conquering new territories. We think that creating great products requires absolute teamwork, no matter which position you occupy: every task, every pixel, every code line makes a difference.""]}"
Ciklum,https://jobs.dou.ua/companies/ciklum/,Data Analyst for Similarweb,https://jobs.dou.ua/companies/ciklum/vacancies/135737/, Kyiv,21 October 2020,,"On behalf of SimilarWeb, Ciklum is looking for a talented and dedicated analyst with research/business mindsets to join our Data Solutions team as a Data Analyst. As part of this role you will help us scale and stabilize our new rocketing Digital Insights (DI) reporting products at SimilarWeb. We are looking for a talented and dedicated analyst with research/business mindsets to join our Data Solutions team as an Analyst. As part of this role you will help us scale and stabilize our new rocketing Digital Insights (DI) reporting products at SimilarWeb. We believe that building a great product and a great company starts with finding amazing people and helping them grow and develop professionally and personally. At SimilarWeb, you’ll be surrounded by the most talented professionals and you’ll work across departments gaining skills and driving impact. Our Values:",dou,2020-11-12,Data Analyst for Similarweb@Ciklum,,,{}
VisiQuate,https://jobs.dou.ua/companies/visiquate/,Data Reporting Analyst,https://jobs.dou.ua/companies/visiquate/vacancies/38768/," Kharkiv, Lviv",21 October 2020,,"Required skills ● 2+ years experience as a business analyst, data analyst or reporting analyst; ● English — upper intermediate level at least; ● Proficiency in MS Excel (creating reports, formulas, macros, pivot tables); ● 1+ year experience in SQL language; ● 1+ year MS Access reporting and development experience As a plus ● High-level understanding of major Healthcare data sets: Accounts, Transactions, Charges● 1+ years Healthcare data experience● Basic proficiency in desktop applications such as MS Word and MS PowerPoint● Knowledge/Experience of working with BI tools We offer ● Work in a team of smart and talented people;● Be a part of a fast-growing product company;● Comfortable office in the city center (Rymarska Str 32);● Holidays per Ukrainian calendar;● Flexible work schedule within Ukrainian business hours and a friendly atmosphere in the office;● Free English classes. Responsibilities This person needs to interpret data, analyze results and provide ongoing reports, generally using MS Excel and MS SQL as database. Project description VisiQuate is a daily SaaS-based service for healthcare and enterprise clients that makes the volume, variety, and velocity of Big Data simple and actionable. VisiQuate employes data scientists, visual scientists, and subject matter experts who integrate terabytes of incompatible Big Data onto a single platform.We’re searching for amazing people to join our team in relation to the expansion of one of the long term and important projects.",dou,2020-11-12,Data Reporting Analyst@VisiQuate,,,"{""Required skills"": [""\u25cf 2+ years experience as a business analyst, data analyst or reporting analyst; \u25cf English"", ""upper intermediate level at least; \u25cf Proficiency in MS Excel (creating reports, formulas, macros, pivot tables); \u25cf 1+ year experience in SQL language; \u25cf 1+ year MS Access reporting and development experience""], ""As a plus"": [""\u25cf High-level understanding of major Healthcare data sets: Accounts, Transactions, Charges\u25cf 1+ years Healthcare data experience\u25cf Basic proficiency in desktop applications such as MS Word and MS PowerPoint\u25cf Knowledge/Experience of working with BI tools""], ""We offer"": [""\u25cf Work in a team of smart and talented people;\u25cf Be a part of a fast-growing product company;\u25cf Comfortable office in the city center (Rymarska Str 32);\u25cf Holidays per Ukrainian calendar;\u25cf Flexible work schedule within Ukrainian business hours and a friendly atmosphere in the office;\u25cf Free English classes.""], ""Responsibilities"": [""This person needs to interpret data, analyze results and provide ongoing reports, generally using MS Excel and MS SQL as database.""], ""Project description"": [""VisiQuate is a daily SaaS-based service for healthcare and enterprise clients that makes the volume, variety, and velocity of Big Data simple and actionable. VisiQuate employes data scientists, visual scientists, and subject matter experts who integrate terabytes of incompatible Big Data onto a single platform.We\u2019re searching for amazing people to join our team in relation to the expansion of one of the long term and important projects.""]}"
Brightgrove,https://jobs.dou.ua/companies/brightgrove/,Senior Data Test Engineer for Online TV Streaming Giant,https://jobs.dou.ua/companies/brightgrove/vacancies/135705/," Kyiv, Kharkiv",20 October 2020,,"Required skills • 5+ years of Quality Assurance/Testing experience.• 3+ years of Data Quality experience, or SDET experience with a focus on data, data warehousing, reporting, etc.• 3+ years of testing experience working within an Agile environment, and with Agile Management tools such as JIRA.• Experience with Automation Framework development using Java.• Experience with Performance Test Design, Development and load testing execution.• Design, create and maintain assets used to execute performance tests and contribute to the execution and monitoring of performance test executions using ApacheJMeter, LoadRunner or similar tools.• Working knowledge of JAVA, JVM, Spring Boot , data warehouse, data integration, sql server, apache kafka, data streaming, big data, mongoDB, SQL, Web Services, microservices, ETL, change data capture (CDC), DevOps.• Strong SQL experience. Knowledge of analytical functions is a plus.• Experience working with Amazon Web Services, querying and working with data in various AWS services like Redshift, Athena, ElasticSearch.• Programming experience in a language such as Python, Java, etc. for the purposes of parsing files and running queries. As a plus • Experience with analytics implementations (network events, ad beacons, user action events, etc.) in a web or mobile application.• Experience with reporting or analytics tools like Tableau or Mode. We offer • Very warm and friendly working environment• Professional and career growth• No corporate BS — we’re moving too fast for that• Competitive compensation depending on experience and skills• Opportunities to travel international and between our offices• Working with the latest technologies• Excellent opportunities to work with remarkable teams from all over the world• Flexible working hours — as long as you get the work done• Comfortable and cozy office in the city center• Awesome corporate events At Brightgrove, developers are integrated, involved, and essential to each part of our company. As with wine, great software is created by great people. Responsibilities • Work with project teams implementing analytics features into client applications and verify the firing of those analytics events by manually working with devices or initiating scripts.• Verify applications and tools developed on data, data warehousing & AWS Redshift , Snowflake or columnar databases.• Verify capturing of analytics events in related file systems or databases through SQL, or a scripting language (Python, Java, shell scripting, etc.).• Work with Business Intelligence and Product Management to create test strategies, plans and cases that provide acceptable coverage for a given data pipeline, from event creation to reporting.• Perform data profiling on downstream data for the purposes of finding field anomalies and possible data quality issues.• Work in an Agile Software Delivery methodology, highly focused in creating data validation tests based on requirements.• Work with other members of Data Test Engineering to compile a regression test suite that can be executed quickly.• Help create a manageable, repeatable strategy for manual test cases.• Provide a risk assessment on the defects identified and set the correct priority and severity. Project description About the Client:Founded in 2013, our client is a hugely popular, cutting-edge online streaming television company from California. This free broadcasting service provides millions of viewers with more than 100 exclusive live and original channels that users won’t find on local TV. The content is full of TV shows of all kinds, premium news, on-demand movies, cartoons, viral videos, sports, lifestyle and trending digital series. In a nutshell, there is literally everything you’d like to see on your screen — hand-picked by people who know and love the entertainment industry. More so, they offer a free app that runs seamlessly on multiple mobile, web and various connected TV streaming devices. Well-known online giants The Verge, Venture Beat, and Vibe declared our customer as ‘awesome’. So, we can say for sure that our client is redefining the future of television. About the Project:One of the hottest startups in California is led by a team of highly-skilled and passionate entrepreneurs, alongside with the TV and technology executives. There is nothing but the high energy culture and the top-notch development team. Only the best of the best in the industry work for the platform’s owners. The project has investors from US Venture Partners, Chicago Ventures, Universal Music Group, and more. Millions of users across multiple platforms interact with their app on a daily basis. Famous magazines like NY Times, Forbes, TechCrunch, Variety, USA Today, CNN and Mashable never miss the opportunity to write about one of the leading video streaming platforms in the industry. As of right now, the goal of the project is to transform the future of television by building the largest online TV service in the world, while continuing to stay free. About the Team:Every team member is passionate about the project and committed to a common goal. You’ll be led by some of the best media and technology entrepreneurs. For instance, the Executive Chairman was the Chief Content Officer of Spotify, and principal architect built the first Roku for Netflix. It’s a world-class startup team that runs many experiments and solves real problems — you can be sure of that. Here no successful products are built without thorough tests. The development process is lean, agile, organic, non-GMO, all that stuff. You’d be not just an engineer but an entrepreneur with the autonomy when it comes to the problem-solving process.",dou,2020-11-12,Senior Data Test Engineer for Online TV Streaming Giant@Brightgrove,,,"{""Required skills"": [""5+ years of Quality Assurance/Testing experience."", ""3+ years of Data Quality experience, or SDET experience with a focus on data, data warehousing, reporting, etc."", ""3+ years of testing experience working within an Agile environment, and with Agile Management tools such as JIRA."", ""Experience with Automation Framework development using Java."", ""Experience with Performance Test Design, Development and load testing execution."", ""Design, create and maintain assets used to execute performance tests and contribute to the execution and monitoring of performance test executions using ApacheJMeter, LoadRunner or similar tools."", ""Working knowledge of JAVA, JVM, Spring Boot , data warehouse, data integration, sql server, apache kafka, data streaming, big data, mongoDB, SQL, Web Services, microservices, ETL, change data capture (CDC), DevOps."", ""Strong SQL experience. Knowledge of analytical functions is a plus."", ""Experience working with Amazon Web Services, querying and working with data in various AWS services like Redshift, Athena, ElasticSearch."", ""Programming experience in a language such as Python, Java, etc. for the purposes of parsing files and running queries.""], ""As a plus"": [""Experience with analytics implementations (network events, ad beacons, user action events, etc.) in a web or mobile application."", ""Experience with reporting or analytics tools like Tableau or Mode.""], ""We offer"": [""Very warm and friendly working environment"", ""Professional and career growth"", ""No corporate BS"", ""we\u2019re moving too fast for that"", ""Competitive compensation depending on experience and skills"", ""Opportunities to travel international and between our offices"", ""Working with the latest technologies"", ""Excellent opportunities to work with remarkable teams from all over the world"", ""Flexible working hours"", ""as long as you get the work done"", ""Comfortable and cozy office in the city center"", ""Awesome corporate events""], ""Responsibilities"": [""At Brightgrove, developers are integrated, involved, and essential to each part of our company. As with wine, great software is created by great people.""], ""Project description"": [""Work with project teams implementing analytics features into client applications and verify the firing of those analytics events by manually working with devices or initiating scripts."", ""Verify applications and tools developed on data, data warehousing & AWS Redshift , Snowflake or columnar databases."", ""Verify capturing of analytics events in related file systems or databases through SQL, or a scripting language (Python, Java, shell scripting, etc.)."", ""Work with Business Intelligence and Product Management to create test strategies, plans and cases that provide acceptable coverage for a given data pipeline, from event creation to reporting."", ""Perform data profiling on downstream data for the purposes of finding field anomalies and possible data quality issues."", ""Work in an Agile Software Delivery methodology, highly focused in creating data validation tests based on requirements."", ""Work with other members of Data Test Engineering to compile a regression test suite that can be executed quickly."", ""Help create a manageable, repeatable strategy for manual test cases."", ""Provide a risk assessment on the defects identified and set the correct priority and severity.""]}"
"ООО ""Новая Почта""",https://jobs.dou.ua/companies/novaposhta/,Керівник білінгового супроводу клієнтів і фізичних осіб,https://jobs.dou.ua/companies/novaposhta/vacancies/135695/, Kyiv,20 October 2020,до $4000,"Required skills Вища освіта (технічна);Аналітичні навички;Досвід роботи з великими об’ємами даних. We offer Повні соціальні гарантії з (у тому числі офіційний дохід, оплачувані відпустки — 26 к.д., лікарняні);Медичне страхування за корпоративною програмою;Амбіційний молодий колектив професіоналів, який завжди допомагає в досягненні результатів;Віддалена робота та на офісі за адресою: Столичне шосе 103, БЦ «Європа», м. Видубичі;Корпоративний транспорт до офісу з різних р-нів Києва,Безкоштовний підземний паркінг;Корпоративну з кавою та чаєм, гейм-зони на території офісу. Responsibilities Створення бази фізичних осіб з усіма транзакціями компанії, з усіма перевагами, зв’язками між собою;Створення єдиної бази клієнтів з усіма вхідними в них фізичними особами, з усіма їх статусами як юр. осіб (холдинги, групи компаній і т.д.), взаємини їх між собою, взаємини їх з фіз. особами;Білінг як в розрізі клієнтів, так і в розрізі фіз. осіб;Збір всіх транзакцій в розрізі клієнтів і в розрізі фіз. осіб;Створення база бонусних рахунків (бонусний білінг);Створення бази умов роботи з усіма клієнтами компанії.",dou,2020-11-12,"Керівник білінгового супроводу клієнтів і фізичних осіб@ООО ""Новая Почта""",,,"{""Required skills"": [""\u0412\u0438\u0449\u0430 \u043e\u0441\u0432\u0456\u0442\u0430 (\u0442\u0435\u0445\u043d\u0456\u0447\u043d\u0430);\u0410\u043d\u0430\u043b\u0456\u0442\u0438\u0447\u043d\u0456 \u043d\u0430\u0432\u0438\u0447\u043a\u0438;\u0414\u043e\u0441\u0432\u0456\u0434 \u0440\u043e\u0431\u043e\u0442\u0438 \u0437 \u0432\u0435\u043b\u0438\u043a\u0438\u043c\u0438 \u043e\u0431\u2019\u0454\u043c\u0430\u043c\u0438 \u0434\u0430\u043d\u0438\u0445.""], ""We offer"": [""\u041f\u043e\u0432\u043d\u0456 \u0441\u043e\u0446\u0456\u0430\u043b\u044c\u043d\u0456 \u0433\u0430\u0440\u0430\u043d\u0442\u0456\u0457 \u0437 (\u0443 \u0442\u043e\u043c\u0443 \u0447\u0438\u0441\u043b\u0456 \u043e\u0444\u0456\u0446\u0456\u0439\u043d\u0438\u0439 \u0434\u043e\u0445\u0456\u0434, \u043e\u043f\u043b\u0430\u0447\u0443\u0432\u0430\u043d\u0456 \u0432\u0456\u0434\u043f\u0443\u0441\u0442\u043a\u0438"", ""26 \u043a.\u0434., \u043b\u0456\u043a\u0430\u0440\u043d\u044f\u043d\u0456);\u041c\u0435\u0434\u0438\u0447\u043d\u0435 \u0441\u0442\u0440\u0430\u0445\u0443\u0432\u0430\u043d\u043d\u044f \u0437\u0430 \u043a\u043e\u0440\u043f\u043e\u0440\u0430\u0442\u0438\u0432\u043d\u043e\u044e \u043f\u0440\u043e\u0433\u0440\u0430\u043c\u043e\u044e;\u0410\u043c\u0431\u0456\u0446\u0456\u0439\u043d\u0438\u0439 \u043c\u043e\u043b\u043e\u0434\u0438\u0439 \u043a\u043e\u043b\u0435\u043a\u0442\u0438\u0432 \u043f\u0440\u043e\u0444\u0435\u0441\u0456\u043e\u043d\u0430\u043b\u0456\u0432, \u044f\u043a\u0438\u0439 \u0437\u0430\u0432\u0436\u0434\u0438 \u0434\u043e\u043f\u043e\u043c\u0430\u0433\u0430\u0454 \u0432 \u0434\u043e\u0441\u044f\u0433\u043d\u0435\u043d\u043d\u0456 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0456\u0432;\u0412\u0456\u0434\u0434\u0430\u043b\u0435\u043d\u0430 \u0440\u043e\u0431\u043e\u0442\u0430 \u0442\u0430 \u043d\u0430 \u043e\u0444\u0456\u0441\u0456 \u0437\u0430 \u0430\u0434\u0440\u0435\u0441\u043e\u044e: \u0421\u0442\u043e\u043b\u0438\u0447\u043d\u0435 \u0448\u043e\u0441\u0435 103, \u0411\u0426 \u00ab\u0404\u0432\u0440\u043e\u043f\u0430\u00bb, \u043c. \u0412\u0438\u0434\u0443\u0431\u0438\u0447\u0456;\u041a\u043e\u0440\u043f\u043e\u0440\u0430\u0442\u0438\u0432\u043d\u0438\u0439 \u0442\u0440\u0430\u043d\u0441\u043f\u043e\u0440\u0442 \u0434\u043e \u043e\u0444\u0456\u0441\u0443 \u0437 \u0440\u0456\u0437\u043d\u0438\u0445 \u0440-\u043d\u0456\u0432 \u041a\u0438\u0454\u0432\u0430,\u0411\u0435\u0437\u043a\u043e\u0448\u0442\u043e\u0432\u043d\u0438\u0439 \u043f\u0456\u0434\u0437\u0435\u043c\u043d\u0438\u0439 \u043f\u0430\u0440\u043a\u0456\u043d\u0433;\u041a\u043e\u0440\u043f\u043e\u0440\u0430\u0442\u0438\u0432\u043d\u0443 \u0437 \u043a\u0430\u0432\u043e\u044e \u0442\u0430 \u0447\u0430\u0454\u043c, \u0433\u0435\u0439\u043c-\u0437\u043e\u043d\u0438 \u043d\u0430 \u0442\u0435\u0440\u0438\u0442\u043e\u0440\u0456\u0457 \u043e\u0444\u0456\u0441\u0443.""], ""Responsibilities"": [""\u0421\u0442\u0432\u043e\u0440\u0435\u043d\u043d\u044f \u0431\u0430\u0437\u0438 \u0444\u0456\u0437\u0438\u0447\u043d\u0438\u0445 \u043e\u0441\u0456\u0431 \u0437 \u0443\u0441\u0456\u043c\u0430 \u0442\u0440\u0430\u043d\u0437\u0430\u043a\u0446\u0456\u044f\u043c\u0438 \u043a\u043e\u043c\u043f\u0430\u043d\u0456\u0457, \u0437 \u0443\u0441\u0456\u043c\u0430 \u043f\u0435\u0440\u0435\u0432\u0430\u0433\u0430\u043c\u0438, \u0437\u0432\u2019\u044f\u0437\u043a\u0430\u043c\u0438 \u043c\u0456\u0436 \u0441\u043e\u0431\u043e\u044e;\u0421\u0442\u0432\u043e\u0440\u0435\u043d\u043d\u044f \u0454\u0434\u0438\u043d\u043e\u0457 \u0431\u0430\u0437\u0438 \u043a\u043b\u0456\u0454\u043d\u0442\u0456\u0432 \u0437 \u0443\u0441\u0456\u043c\u0430 \u0432\u0445\u0456\u0434\u043d\u0438\u043c\u0438 \u0432 \u043d\u0438\u0445 \u0444\u0456\u0437\u0438\u0447\u043d\u0438\u043c\u0438 \u043e\u0441\u043e\u0431\u0430\u043c\u0438, \u0437 \u0443\u0441\u0456\u043c\u0430 \u0457\u0445 \u0441\u0442\u0430\u0442\u0443\u0441\u0430\u043c\u0438 \u044f\u043a \u044e\u0440. \u043e\u0441\u0456\u0431 (\u0445\u043e\u043b\u0434\u0438\u043d\u0433\u0438, \u0433\u0440\u0443\u043f\u0438 \u043a\u043e\u043c\u043f\u0430\u043d\u0456\u0439 \u0456 \u0442.\u0434.), \u0432\u0437\u0430\u0454\u043c\u0438\u043d\u0438 \u0457\u0445 \u043c\u0456\u0436 \u0441\u043e\u0431\u043e\u044e, \u0432\u0437\u0430\u0454\u043c\u0438\u043d\u0438 \u0457\u0445 \u0437 \u0444\u0456\u0437. \u043e\u0441\u043e\u0431\u0430\u043c\u0438;\u0411\u0456\u043b\u0456\u043d\u0433 \u044f\u043a \u0432 \u0440\u043e\u0437\u0440\u0456\u0437\u0456 \u043a\u043b\u0456\u0454\u043d\u0442\u0456\u0432, \u0442\u0430\u043a \u0456 \u0432 \u0440\u043e\u0437\u0440\u0456\u0437\u0456 \u0444\u0456\u0437. \u043e\u0441\u0456\u0431;\u0417\u0431\u0456\u0440 \u0432\u0441\u0456\u0445 \u0442\u0440\u0430\u043d\u0437\u0430\u043a\u0446\u0456\u0439 \u0432 \u0440\u043e\u0437\u0440\u0456\u0437\u0456 \u043a\u043b\u0456\u0454\u043d\u0442\u0456\u0432 \u0456 \u0432 \u0440\u043e\u0437\u0440\u0456\u0437\u0456 \u0444\u0456\u0437. \u043e\u0441\u0456\u0431;\u0421\u0442\u0432\u043e\u0440\u0435\u043d\u043d\u044f \u0431\u0430\u0437\u0430 \u0431\u043e\u043d\u0443\u0441\u043d\u0438\u0445 \u0440\u0430\u0445\u0443\u043d\u043a\u0456\u0432 (\u0431\u043e\u043d\u0443\u0441\u043d\u0438\u0439 \u0431\u0456\u043b\u0456\u043d\u0433);\u0421\u0442\u0432\u043e\u0440\u0435\u043d\u043d\u044f \u0431\u0430\u0437\u0438 \u0443\u043c\u043e\u0432 \u0440\u043e\u0431\u043e\u0442\u0438 \u0437 \u0443\u0441\u0456\u043c\u0430 \u043a\u043b\u0456\u0454\u043d\u0442\u0430\u043c\u0438 \u043a\u043e\u043c\u043f\u0430\u043d\u0456\u0457.""]}"
INSART,https://jobs.dou.ua/companies/insart/,"Data Scientist (AI, ML)",https://jobs.dou.ua/companies/insart/vacancies/135683/," Kyiv, Kharkiv, remote",20 October 2020,,"Required skills — 3+ years working in data science and machine learning— Fluent English— Proficient using statistics and learning tools— Experience working in the learning field, recommendations and personalization— Content analysis, rate, lead scoring— Have experience building production-ready systems that capture and utilize large data sets in order to improve products performance— Experienced in computer science fundamentals such as object-oriented design, data structures and algorithms— Excellent communication skills. Communicate with project and product management on customer side directly on daily basis— Be ready for R&D environment. Bring new ideas and be ready to implement them. Work on POCs— Communicate closely with product team— Expertise in Python and SQL. As a plus — Java development experience— Experience in NLP area We offer ● Full-time position● Work in friendly and professional team● Comfortable office located in the center of Kharkiv (Pushkinskaya metro station)● Paid sick-list● 4 weeks of paid vacation● Medical insurance● Corporate library● English classes● Corporate club (for some relax during workday)● Corporate events (birthdays, holidays, etc.)● Tea, coffee, milk, juice, fruits, snacks and cookies =) Responsibilities — Research and experiment with different machine learning algorithms and techniques to solve business problems — Data exploration and its quality evaluation — Feature engineering and data fusion — Perform statistical analysis — Train, fine-tuning and evaluation of ML models — Work with engineers to deploy ready ML models — Proposing new ideas In future we plan to work with text processing like sentiment analysis and text enhancement Project description Salsa Labs is a premier SaaS technology company providing online organizing tools to nonprofits. The tools they create allow thousands to organize and create change across the country and across the globe. Their greatest strength lies not only within the technology they produce, but also within the greatness of their people. They pride themselves on their team of talented, driven, entrepreneurial individuals and in having a workplace that values work/life balance, creativity, and innovation.",dou,2020-11-12,"Data Scientist (AI, ML)@INSART",,,"{""Required skills"": [""3+ years working in data science and machine learning"", ""Fluent English"", ""Proficient using statistics and learning tools"", ""Experience working in the learning field, recommendations and personalization"", ""Content analysis, rate, lead scoring"", ""Have experience building production-ready systems that capture and utilize large data sets in order to improve products performance"", ""Experienced in computer science fundamentals such as object-oriented design, data structures and algorithms"", ""Excellent communication skills. Communicate with project and product management on customer side directly on daily basis"", ""Be ready for R&D environment. Bring new ideas and be ready to implement them. Work on POCs"", ""Communicate closely with product team"", ""Expertise in Python and SQL.""], ""As a plus"": [""Java development experience"", ""Experience in NLP area""], ""We offer"": [""\u25cf Full-time position\u25cf Work in friendly and professional team\u25cf Comfortable office located in the center of Kharkiv (Pushkinskaya metro station)\u25cf Paid sick-list\u25cf 4 weeks of paid vacation\u25cf Medical insurance\u25cf Corporate library\u25cf English classes\u25cf Corporate club (for some relax during workday)\u25cf Corporate events (birthdays, holidays, etc.)\u25cf Tea, coffee, milk, juice, fruits, snacks and cookies =)""], ""Responsibilities"": [""Research and experiment with different machine learning algorithms and techniques to solve business problems"", ""Data exploration and its quality evaluation"", ""Feature engineering and data fusion"", ""Perform statistical analysis"", ""Train, fine-tuning and evaluation of ML models"", ""Work with engineers to deploy ready ML models"", ""Proposing new ideas""], ""Project description"": [""In future we plan to work with text processing like sentiment analysis and text enhancement""]}"
Parimatch Tech,https://jobs.dou.ua/companies/parimatch-tech/,Data and Analytics Technical Lead,https://jobs.dou.ua/companies/parimatch-tech/vacancies/130597/, Kyiv,20 October 2020,,"The ideal candidate for the Data and Analytics Stream Technical Lead is a senior-level leader who has significant depth in the design/build and management of big data solutions. He will be responsible for the support and maintenance of current technology solutions as well as the design/build/transition to data platforms that align with new and growing business needs. — 5+ years of Data, analytics, solutions and digital transformation experience, preferably in a high growth company;— 3+ years’ experience as a senior leader, growing and managing high performing technology teams;— Extensive experience leading large complex projects, including strategic and tactical planning and implementation, analysis, design, technology selection and deployment;— Hands on experience deploying data warehouses and advanced analytical solutions;— Integrating structured and unstructured data from hundreds of source systems;— Experience with PgSQL, Kafka, Airflow, Nifi;— Ability to do technical Project Management;— Advanced knowledge of data modeling and relational database design. — Lead Data technology team to ensure quality, completeness and timeliness of solution delivery;— Develop technical aspects of the stream strategy to ensure alignment with its business goals;— Support business development efforts within the data platforms sector. This includes conceptual design, sizing and development of Enterprise Data Warehouse and data lake;— Consistently evaluating and enhancing technical efficiency;— Inspire, manage, and continuously develop a world-class team of technology professionals and their careers;— Ensure a continuous improvement, high quality and innovation;— Communicate effectively with business stakeholders;— Synthesize relevant information on key milestones, success criteria and risks. — Have the ability to manage multiple, high priority efforts effectively under significant deadline pressure;— Ability to communicate advanced technical concepts to non-technical audiences;— Team and process-oriented spirit;— Data Quality mindset;— Leadership and organizational abilities;— Strategic thinking.",dou,2020-11-12,Data and Analytics Technical Lead@Parimatch Tech,,,{}
MyCredit,https://jobs.dou.ua/companies/mycredit/,Senior Data Science,https://jobs.dou.ua/companies/mycredit/vacancies/135670/, Kyiv,20 October 2020,,"Required skills — 4+ years in Machine learning;— strong knowledge of SQL/NoSQL;— be responsive to deadlines;— experience in deploying models to the production. As a plus — experience with deep learning;— work experience in the financial sector;— experience with data engineering and schema design in Data Warehouses, Data Lakes. We offer — competitive salary;— professional development: training and continuing education courses at the expense of the company;— the ability to realize your ideas in the product);— paid vacation and sick leave;— Scrum, a lot of Scrum in work with excellent team and top-hardware in the workplace from the first day of work; — performance review every six months;— office in the very heart of Kiev, 1 minute from Arsenalnaya metro station;— we have thematic corporate parties 3 times a year! Also, we conduct teambuildings and board games;— The Company partially compensates sport costs; Some of our employees participating Race Nation competition;— we conduct book challenges, secret Santa, Good Day, Valentine’s Day and many other activities;— we are socially responsible and helping children from orphanages.You can meet our team now by following the link: www.instagram.com/mycredit_livemycredit.tilda.ws Are you interested to joing our team? We are waiting for your CV — send it right now! Responsibilities — creating a bunch of credit scoring algorithms;— recreating current infrastructure to a new flexible approach. When each algorithm works independently;— feature engineering, building and optimizing classifiers;— creating a monitoring system for all machine learning algorithms;— mentoring team members for professional growth. Project description MyCredit (Mycredit.ua) — продуктовая IT компания и основная наша задача — ежедневное совершенствование собственного финансового сервиса, внедрение нового и поддержка текущего функционала. Узнай о нас больше: mycredit.tilda.ws FinTech — актуальная и прогрессивная отрасль, за 4 года работы на рынке микрокредитования среди 820 конкурентов мы заняли уверенную позицию в тройке лучших проектов страны. За прошлый год количество людей в компании выросло в 3 раза, все это заслуги нашей команды, сильного менеджмента и работы, ориентированной на результат. Наша миссия: Делаем доступными деньги! Наши ценности: Клиент, Команда, Развитие, Ответственность и Честность, определяют наши подходы к работе и отношение к друг другу. У нас два офиса: В Харькове (отдел IT разработки и маркетинга) и в Киеве (операционная часть). О нас:— У нашего продукта современная архитектура, он построен на технологии микросервисов;— Различные компоненты системы используют реляционные базы данных, базы no SQL;— Продукт является высоконагруженной системой, где используются современные методы горизонтального масштабирования;— Мы работаем с последними стабильными версиями .Net и .Net Core для back-end, а также Angular и AngularJS со стороны front-end;— Продукт быстро развивается, разрабатываются новые компоненты и у вас есть возможность принимать активное участие в обсуждении и выборе архитектурных решений. Почему стоит идти к нам:— Став членом команды разработки, вы узнаете как действительно должен работать SCRUM и сможете прокачать свою результативность;— Работая с нами вы сможете стать частью команды специалистов, у которых есть чему поучиться;— Все наши сотрудники имеют возможность посещать тренинги и курсы за счет компании;— Для каждого сотрудника мы готовим индивидуальный план профессионального и карьерного развития, раз в полгода проводим Performance review;— У нас просторный, светлый и теплый офис, расположенный в 10-ти минутах от ст. м. пл. Конституции (Харьков) и возле ст. м. Арсенальная (Киев);— Мы поощряем инициативу наших сотрудников, работая с нами вы получите возможность реализовать свои идеи в продукте, которые помогут сделать продукт лучше;— Индивидуально под каждого нового сотрудника мы укомплектовываем рабочее место новым оборудованием;— У нас оплачиваемые отпуска и больничные. 4 Факта о компании MyCredit:1. MyCredit — компания, которая создана на внутреннем рынке и выпускает собственный продукт2. В компании сильная команда профессионалов и широкий стек новых технологий3. Более 3/4 команды работают с нами с момента основании компании4. Наша компания очень дружная, поэтому очень часто мы собираемся на совместные корпоративы, а также любим устраивать велозаезды и турниры по боулингу. Мы гордимся нашей командой профессионалов и приглашаем вас стать ее частью!Направляйте ваши резюме!",dou,2020-11-12,Senior Data Science@MyCredit,,,"{""Required skills"": [""4+ years in Machine learning;"", ""strong knowledge of SQL/NoSQL;"", ""be responsive to deadlines;"", ""experience in deploying models to the production.""], ""As a plus"": [""experience with deep learning;"", ""work experience in the financial sector;"", ""experience with data engineering and schema design in Data Warehouses, Data Lakes.""], ""We offer"": [""competitive salary;"", ""professional development: training and continuing education courses at the expense of the company;"", ""the ability to realize your ideas in the product);"", ""paid vacation and sick leave;"", ""Scrum, a lot of Scrum in work with excellent team and top-hardware in the workplace from the first day of work;"", ""performance review every six months;"", ""office in the very heart of Kiev, 1 minute from Arsenalnaya metro station;"", ""we have thematic corporate parties 3 times a year! Also, we conduct teambuildings and board games;"", ""The Company partially compensates sport costs; Some of our employees participating Race Nation competition;"", ""we conduct book challenges, secret Santa, Good Day, Valentine\u2019s Day and many other activities;"", ""we are socially responsible and helping children from orphanages.You can meet our team now by following the link: www.instagram.com/mycredit_livemycredit.tilda.ws""], ""Responsibilities"": [""Are you interested to joing our team? We are waiting for your CV"", ""send it right now!""], ""Project description"": [""creating a bunch of credit scoring algorithms;"", ""recreating current infrastructure to a new flexible approach. When each algorithm works independently;"", ""feature engineering, building and optimizing classifiers;"", ""creating a monitoring system for all machine learning algorithms;"", ""mentoring team members for professional growth.""]}"
Vodafone Україна,https://jobs.dou.ua/companies/vodafone-ukraine/,Data Analyst (Python),https://jobs.dou.ua/companies/vodafone-ukraine/vacancies/126151/, Kyiv,20 October 2020,,Обов’язки: Вимоги: Ми пропонуємо:,dou,2020-11-12,Data Analyst (Python)@Vodafone Україна,,,{}
Vodafone Україна,https://jobs.dou.ua/companies/vodafone-ukraine/,Data Scientist,https://jobs.dou.ua/companies/vodafone-ukraine/vacancies/50036/, Kyiv,20 October 2020,,"Необхідні навички Буде плюсом З чим доведеться мати справу: Ми віримо, що ти зможеш:",dou,2020-11-12,Data Scientist@Vodafone Україна,,,{}
Newfire Global Partners,https://jobs.dou.ua/companies/newfire-partners/,Lead/Senior Data Engineer,https://jobs.dou.ua/companies/newfire-partners/vacancies/135635/," Lviv, Ivano-Frankivsk",20 October 2020,,"Required skills 5+ years of commercial experience with hand-on Data Architecting, Modeling and Data Warehousing stack including AWS Redshift and other equivalent modern technologiesExperience with AWS solutionsExperienced with Python programming languageFamiliarity with GIT and release engineering strategiesDeep experience with SQL design and architecture best-practices (i.e. third normal form, index management, constraints, and foreign key relationships, etc.).Expertise in query tuning and performance optimization.Experienced in creating ERD diagramsStrong SQL abilities and experience with massive relational database systems.Strong knowledge of all traditional Data Warehouse-related components (Sourcing, ETL, Data Modeling, Infrastructure, BI, Reporting) and the modern tools to support those componentsAbility to clearly explain and justify ideas when faced with competing alternativesTrack record of working in Scrum / Agile software teamsProficient in spoken and written English As a plus Experienced in the healthcare domain (FHIR/HL7) is preferredBachelor’s degree or higher in a technical field of studyExperience with Postgres, Aurora (MySQL), DynamoDBFamiliarity with the MPP Databases, Hadoop ecosystemFamiliarity with continuous delivery and DevOps, understanding of common database administration tasks and concepts such as backup & restore, recovery models, integrity checks, and replication methods.Flexibility and creativity in solution design — including leveraging emerging technologies We offer Favorite Perks:The clear growth path for every role and regular performance reviewsMeaningful projects with an impact on real-world problemsLots of senior developers and mentorsHigh-trust environment: no time-trackers and flexible schedules Other Benefits:Very little bureaucracyPersonal development compensation: from English classes to Udemy courses15 PTO days (20 PTO days after a year in our team) + 8 paid sick daysRelocation support for Lviv & Ivano-Frankivsk officesQuarantine-friendly cozy officesSnack-stocked kitchen and $50 monthly lunch compensationParties, team buildings, and weekly pizza/sushi days Responsibilities Work as part of multifunctional teams to own the design and architecture of the logical entity model of the client’s suite of productsConvert logical models into physical data models employing sound database normalization techniquesCreate physical database objects like tables and views with appropriate data types, foreign keys, constraints, and upfront design and maintenance of proper indexesCreate and maintain easy to follow technical documentation of data modelsServe as a go-to resource for questions related to existing and proposed logical and physical data modelsPerform SQL code reviews and ensure that new database code meets company standards for readability, reliability, and performanceAssist with resolving the performance of poorly executing stored procedures and queriesSupport team initiatives by developing tools and identifying opportunities for process automation; assist in evaluation and selection of standard tools for the departmentSupport building and deploying the infrastructure for ingesting high-volume data from various sourcesSupport developing and maintaining the data-related scripting for build/test/deployment automationResearch individually and in collaboration with other teams on how to solve problemsPartner with team and research, design, test, and evaluate new technologies and services as it applies to data warehousing and architectingPartner with the team to maintain an organization-wide view of current and future strategy and approach as it applies to data warehousing and architectingIdentify and resolve bottlenecks and bugs Project description Our client is a ​healthcare​ company ​focused on healthcare data. They are a ​well-funded organization ​that has reputable clients and respected investors.They are a ​US-based company​ headquartered in Boston, MA. This client brings information together so that consumers can make better health choices and physicians can make more informed treatment decisions through AI-powered insights.",dou,2020-11-12,Lead/Senior Data Engineer@Newfire Global Partners,,,"{""Required skills"": [""5+ years of commercial experience with hand-on Data Architecting, Modeling and Data Warehousing stack including AWS Redshift and other equivalent modern technologiesExperience with AWS solutionsExperienced with Python programming languageFamiliarity with GIT and release engineering strategiesDeep experience with SQL design and architecture best-practices (i.e. third normal form, index management, constraints, and foreign key relationships, etc.).Expertise in query tuning and performance optimization.Experienced in creating ERD diagramsStrong SQL abilities and experience with massive relational database systems.Strong knowledge of all traditional Data Warehouse-related components (Sourcing, ETL, Data Modeling, Infrastructure, BI, Reporting) and the modern tools to support those componentsAbility to clearly explain and justify ideas when faced with competing alternativesTrack record of working in Scrum / Agile software teamsProficient in spoken and written English""], ""As a plus"": [""Experienced in the healthcare domain (FHIR/HL7) is preferredBachelor\u2019s degree or higher in a technical field of studyExperience with Postgres, Aurora (MySQL), DynamoDBFamiliarity with the MPP Databases, Hadoop ecosystemFamiliarity with continuous delivery and DevOps, understanding of common database administration tasks and concepts such as backup & restore, recovery models, integrity checks, and replication methods.Flexibility and creativity in solution design"", ""including leveraging emerging technologies""], ""We offer"": [""Favorite Perks:The clear growth path for every role and regular performance reviewsMeaningful projects with an impact on real-world problemsLots of senior developers and mentorsHigh-trust environment: no time-trackers and flexible schedules""], ""Responsibilities"": [""Other Benefits:Very little bureaucracyPersonal development compensation: from English classes to Udemy courses15 PTO days (20 PTO days after a year in our team) + 8 paid sick daysRelocation support for Lviv & Ivano-Frankivsk officesQuarantine-friendly cozy officesSnack-stocked kitchen and $50 monthly lunch compensationParties, team buildings, and weekly pizza/sushi days""], ""Project description"": [""Work as part of multifunctional teams to own the design and architecture of the logical entity model of the client\u2019s suite of productsConvert logical models into physical data models employing sound database normalization techniquesCreate physical database objects like tables and views with appropriate data types, foreign keys, constraints, and upfront design and maintenance of proper indexesCreate and maintain easy to follow technical documentation of data modelsServe as a go-to resource for questions related to existing and proposed logical and physical data modelsPerform SQL code reviews and ensure that new database code meets company standards for readability, reliability, and performanceAssist with resolving the performance of poorly executing stored procedures and queriesSupport team initiatives by developing tools and identifying opportunities for process automation; assist in evaluation and selection of standard tools for the departmentSupport building and deploying the infrastructure for ingesting high-volume data from various sourcesSupport developing and maintaining the data-related scripting for build/test/deployment automationResearch individually and in collaboration with other teams on how to solve problemsPartner with team and research, design, test, and evaluate new technologies and services as it applies to data warehousing and architectingPartner with the team to maintain an organization-wide view of current and future strategy and approach as it applies to data warehousing and architectingIdentify and resolve bottlenecks and bugs""]}"
DEVrepublik,https://jobs.dou.ua/companies/devrepublik/,Machine Learning (Python) Course Instructor,https://jobs.dou.ua/companies/devrepublik/vacancies/104384/, remote,20 October 2020,,"Required skills We’re looking for a data science expert to join us for our exciting course program. Skills:3+ years of hands-on Data Science industry experienceDemonstrated subject matter expert in stats and probability, programming in Python, Python data science toolkit (comprised of Jupyter notebooks, Pandas, sci-kit-learn), A/B testing, supervised and unsupervised machine learning.Collaborative. You enjoy partnering with people and have excellent project management skills and follow through with excellent teaching skills. You’ve got a gift for explaining complicated concepts in a beginner-friendly way. You are fond of teaching and explaining different stuff to other people. We believe this is a win-win situation for us and the instructor since by teaching others you will get a lot of other perks such as public speaking skills, reorganizing your own knowledge and skill set, implementation of the organized skills at work, and of course overall development of soft skills. Sounds like you? Write to us :) We offer If you feel willing to work with us, but do not have all the requirements, this is not a problem! Write to us :) We will consider all applicants. Project description IT bootcamps are a unique solution to the problem of providing top-quality training in a short amount of time. Based on a military model, IT bootcamp has different goals but similar methods in creating an intense, low-distraction setting for learning.",dou,2020-11-12,Machine Learning (Python) Course Instructor@DEVrepublik,,,"{""Required skills"": [""We\u2019re looking for a data science expert to join us for our exciting course program. Skills:3+ years of hands-on Data Science industry experienceDemonstrated subject matter expert in stats and probability, programming in Python, Python data science toolkit (comprised of Jupyter notebooks, Pandas, sci-kit-learn), A/B testing, supervised and unsupervised machine learning.Collaborative. You enjoy partnering with people and have excellent project management skills and follow through with excellent teaching skills. You\u2019ve got a gift for explaining complicated concepts in a beginner-friendly way. You are fond of teaching and explaining different stuff to other people.""], ""We offer"": [""We believe this is a win-win situation for us and the instructor since by teaching others you will get a lot of other perks such as public speaking skills, reorganizing your own knowledge and skill set, implementation of the organized skills at work, and of course overall development of soft skills.""], ""Project description"": [""Sounds like you? Write to us :)""]}"
Very Good Security,https://jobs.dou.ua/companies/very-good-group/,Data Analyst,https://jobs.dou.ua/companies/very-good-group/vacancies/135621/," Kyiv, Lviv",20 October 2020,,"Required skills — 5+ years of work experience as a Data Analyst or in a related field;— Familiarity with data storage and ETL pipelines;— Knowledge and experience with SQL and hands-on experience with different analytics tools;— Experience of working with different warehouses;— Excellent knowledge of building correct and understanding reporting;— Ability to analyze existing tools and databases and provide software solution recommendations;— Ability to translate business requirements into non-technical, lay terms;— High-level experience in methodologies and processes for managing large scale databases;— Demonstrated experience in handling large data sets and relational databases;— High-level written and verbal communication skills;— Upper-intermediate or higher English level. We offer — Silicon Valley Experience;— 3 weeks of paid vacation and 2 weeks of days off+sick leaves;— Hackers’ days;— Corporate retreats;— Paid lunches and parking;— Covering professional learning: conferences, trainings, and other events;— Sports activities compensation;— English Speaking Club with native speakers;— Medical insurance;— VGS stock options. Responsibilities — Collect and interpret data, implement interactive dashboards;— Create reports for internal teams and external clients;— Use graphs, infographics and other methods to visualize data;— Manage and design the reporting environment, including data sources, and security;— Support the data warehouse in identifying and revising reporting requirements;— Generate reports from single or multiple systems;— Troubleshoot the reporting database environment and reports. Project description At Very Good Security (“VGS”) we are on a mission to protect the world’s sensitive data — and we’d love to have you along for this journey. VGS was founded by highly successful repeat entrepreneurs and is backed by world-class investors like Goldman Sachs, Andreessen Horowitz, and Visa. We are building an amazing global team spread across four cities. As a young and growing company, we are laser-focused on delighting our customers and hiring talented and entrepreneurial-minded individuals. We’re looking for a Data Analyst with an equal flair for creative problem solving, new technologies enthusiasm, and desire to contribute to product development.",dou,2020-11-12,Data Analyst@Very Good Security,,,"{""Required skills"": [""5+ years of work experience as a Data Analyst or in a related field;"", ""Familiarity with data storage and ETL pipelines;"", ""Knowledge and experience with SQL and hands-on experience with different analytics tools;"", ""Experience of working with different warehouses;"", ""Excellent knowledge of building correct and understanding reporting;"", ""Ability to analyze existing tools and databases and provide software solution recommendations;"", ""Ability to translate business requirements into non-technical, lay terms;"", ""High-level experience in methodologies and processes for managing large scale databases;"", ""Demonstrated experience in handling large data sets and relational databases;"", ""High-level written and verbal communication skills;"", ""Upper-intermediate or higher English level.""], ""We offer"": [""Silicon Valley Experience;"", ""3 weeks of paid vacation and 2 weeks of days off+sick leaves;"", ""Hackers\u2019 days;"", ""Corporate retreats;"", ""Paid lunches and parking;"", ""Covering professional learning: conferences, trainings, and other events;"", ""Sports activities compensation;"", ""English Speaking Club with native speakers;"", ""Medical insurance;"", ""VGS stock options.""], ""Responsibilities"": [""Collect and interpret data, implement interactive dashboards;"", ""Create reports for internal teams and external clients;"", ""Use graphs, infographics and other methods to visualize data;"", ""Manage and design the reporting environment, including data sources, and security;"", ""Support the data warehouse in identifying and revising reporting requirements;"", ""Generate reports from single or multiple systems;"", ""Troubleshoot the reporting database environment and reports.""], ""Project description"": [""At Very Good Security (\u201cVGS\u201d) we are on a mission to protect the world\u2019s sensitive data"", ""and we\u2019d love to have you along for this journey. VGS was founded by highly successful repeat entrepreneurs and is backed by world-class investors like Goldman Sachs, Andreessen Horowitz, and Visa. We are building an amazing global team spread across four cities. As a young and growing company, we are laser-focused on delighting our customers and hiring talented and entrepreneurial-minded individuals.""]}"
Brightgrove,https://jobs.dou.ua/companies/brightgrove/,Senior Data Base Administrator (DB2) for Global Hotel Solutions Provider,https://jobs.dou.ua/companies/brightgrove/vacancies/135613/, Kyiv,20 October 2020,,"Required skills • 5+ years experience as a DB administrator (DB2, SQL) • Expertise system monitoring, maintenance, tuning and troubleshooting • Process-oriented work and analytical thinking • Good communication skills, ability to work in a team • Ability to work in a highly critical production environment • Willingness to take over responsibility and to gain expertise on new technologies • Spoken and written English We offer • Very warm and friendly working environment• Professional and career growth• No corporate BS — we’re moving too fast for that• Competitive compensation depending on experience and skills• Opportunities to travel international and between our offices• Working with the latest technologies• Excellent opportunities to work with remarkable teams from all over the world• Flexible working hours — as long as you get the work done• Comfortable and cozy office in the city center• Awesome corporate events At Brightgrove, developers are integrated, involved, and essential to each part of our company. As with wine, great software is created by great people. Responsibilities • Installation and maintenance of the DB2 databases • Support of the DB2 application developers • System performance analysis and tuning • Database high availability operation (DB2-HADR) • DB2 SQL replication support and maintenance • Error debugging and managing software issues with vendors • DB encryption and securing • Capacity and risk management • Regular documentation • On-call support for production environments Project description About the Client:Our partner is a well-established, leading hospitality solutions brand from Germany with a worldwide network. Being one of the global market giants in the business travel industry, the company provides various top-grade hotel management portals. Their solutions simplify the booking and management processes equally for corporates, travel managers, and travelers. About the Project:We are looking for a Database Administrator (m/f/d) to support HRS Group in maintaining and developing the future of our DB2 environment. About the Team:You will be part of a global distributed team based in Cologne, Berlin, Warshaw and Kyiv.",dou,2020-11-12,Senior Data Base Administrator (DB2) for Global Hotel Solutions Provider@Brightgrove,,,"{""Required skills"": [""5+ years experience as a DB administrator (DB2, SQL)"", ""Expertise system monitoring, maintenance, tuning and troubleshooting"", ""Process-oriented work and analytical thinking"", ""Good communication skills, ability to work in a team"", ""Ability to work in a highly critical production environment"", ""Willingness to take over responsibility and to gain expertise on new technologies"", ""Spoken and written English""], ""We offer"": [""Very warm and friendly working environment"", ""Professional and career growth"", ""No corporate BS"", ""we\u2019re moving too fast for that"", ""Competitive compensation depending on experience and skills"", ""Opportunities to travel international and between our offices"", ""Working with the latest technologies"", ""Excellent opportunities to work with remarkable teams from all over the world"", ""Flexible working hours"", ""as long as you get the work done"", ""Comfortable and cozy office in the city center"", ""Awesome corporate events""], ""Responsibilities"": [""At Brightgrove, developers are integrated, involved, and essential to each part of our company. As with wine, great software is created by great people.""], ""Project description"": [""Installation and maintenance of the DB2 databases"", ""Support of the DB2 application developers"", ""System performance analysis and tuning"", ""Database high availability operation (DB2-HADR)"", ""DB2 SQL replication support and maintenance"", ""Error debugging and managing software issues with vendors"", ""DB encryption and securing"", ""Capacity and risk management"", ""Regular documentation"", ""On-call support for production environments""]}"
Xenoss,https://jobs.dou.ua/companies/xenoss/,AI/ML Engineer (JS/Python),https://jobs.dou.ua/companies/xenoss/vacancies/135609/," Kyiv, Kharkiv, Lviv, Dnipro, Odesa, Vinnytsia, Rivne, Uzhhorod, Chernivtsi, remote",20 October 2020,,"Required skills — Strong understanding of data structures & algorithms— Proficiency in JS and/or Python — Experience of building applications/services based on open source ML models— Goal orientation & Problem-solving— Working proficiency in English Do not hesitate to apply if you are missing some specific experience. We will be happy to help you with gaining one. As a plus — Node.js— React.js We offer — Great experience and professional growth by working on challenging projects— Highly skilled & friendly team with an excellent work ethic— Flexible working hours— Unlimited work-from-home option— Various snacks, tea, coffee and fresh fruits in the office that’s open 24/7 (in case you’d like to hit up)— Trips to London (optional)— Paid vacation and sick-period Responsibilities — You’ll be designing and building new AI/ML-based creative automation services and ensuring their smooth integration into the solution. — You’ll be actively communicating with the product team on the feature requirements.— You’ll be transforming product requirements into reliable, scalable and maintainable solution. Project description The solution helps big global brands to automate the creation of marketing creatives with help of AI/ML, configure advertising campaigns in buying platforms (Google, Facebook and more), analyze performance data and discover insights, optimize performance, grow revenue & increase customer satisfaction. Through AI, creative can be reformatted at speed and scale, and automatically edited to suit the location and audience it is intended for, saving production costs and ensuring its highly targeted. Building technology for smart advertising solutions is one of the biggest modern challenges that currently exist. Making ads to be more efficient, more relevant & less intrusive will benefit both advertisers and users. By collecting and analyzing (big) data on user reaction to the creatives platform learns & provides the most relevant creatives for a particular audience. Tech stack includes Node.js, Express.js, React.js, Redux, Rematch, Typescript, AWS, Docker, MongoDB, Elasticsearch The product is used by Nestle, Adidas, Uber, Virgin, HSBC, J&J, and other top brands.",dou,2020-11-12,AI/ML Engineer (JS/Python)@Xenoss,,,"{""Required skills"": [""Strong understanding of data structures & algorithms"", ""Proficiency in JS and/or Python"", ""Experience of building applications/services based on open source ML models"", ""Goal orientation & Problem-solving"", ""Working proficiency in English""], ""As a plus"": [""Do not hesitate to apply if you are missing some specific experience. We will be happy to help you with gaining one.""], ""We offer"": [""Node.js"", ""React.js""], ""Responsibilities"": [""Great experience and professional growth by working on challenging projects"", ""Highly skilled & friendly team with an excellent work ethic"", ""Flexible working hours"", ""Unlimited work-from-home option"", ""Various snacks, tea, coffee and fresh fruits in the office that\u2019s open 24/7 (in case you\u2019d like to hit up)"", ""Trips to London (optional)"", ""Paid vacation and sick-period""], ""Project description"": [""You\u2019ll be designing and building new AI/ML-based creative automation services and ensuring their smooth integration into the solution."", ""You\u2019ll be actively communicating with the product team on the feature requirements."", ""You\u2019ll be transforming product requirements into reliable, scalable and maintainable solution.""]}"
Newxel,https://jobs.dou.ua/companies/newxel/,SQL/DB developer for Top-10 Mobile Games Publisher,https://jobs.dou.ua/companies/newxel/vacancies/131731/, Kyiv,20 October 2020,,"Required skills — 3+ years of experience with DB`s (SQL and/or NoSql)— Hands-on experience with Python (Pandas or similar) and ETL stuff— At least an Intermediate level of English As a plus — Experience with RedShift, BigQuery, Airflow, Streaming process We offer — Adequate management;— Competitive salary;— Flexible working schedule;— Modern and comfortable office near the Vystavkovyi center. Responsibilities — Own projects from initial discussions to release, including data exploration, architecture design, the benchmark of new technologies and product feedback;— Leverage our data pipelines and create new ones, making sure the architecture will support the business requirements;— Manage the development of distributed data pipelines and complex software designs to support high data rates using cloud-based tools & Python;— Make data exploration in order to gather insights as part of the data processing development;— Work closely with game analysts, data scientists and other key roles on the various data processes;— Develop unit and integration test procedures. Project description Crazy Labs is a top 10 mobile games publisher and with over 3 billion downloads to date, we are now a worldwide leader in casual games development, distribution and innovation, looking for an excellent Mobile Developer, to join our best of breed framework Development Team.",dou,2020-11-12,SQL/DB developer for Top-10 Mobile Games Publisher@Newxel,,,"{""Required skills"": [""3+ years of experience with DB`s (SQL and/or NoSql)"", ""Hands-on experience with Python (Pandas or similar) and ETL stuff"", ""At least an Intermediate level of English""], ""As a plus"": [""Experience with RedShift, BigQuery, Airflow, Streaming process""], ""We offer"": [""Adequate management;"", ""Competitive salary;"", ""Flexible working schedule;"", ""Modern and comfortable office near the Vystavkovyi center.""], ""Responsibilities"": [""Own projects from initial discussions to release, including data exploration, architecture design, the benchmark of new technologies and product feedback;"", ""Leverage our data pipelines and create new ones, making sure the architecture will support the business requirements;"", ""Manage the development of distributed data pipelines and complex software designs to support high data rates using cloud-based tools & Python;"", ""Make data exploration in order to gather insights as part of the data processing development;"", ""Work closely with game analysts, data scientists and other key roles on the various data processes;"", ""Develop unit and integration test procedures.""], ""Project description"": [""Crazy Labs is a top 10 mobile games publisher and with over 3 billion downloads to date, we are now a worldwide leader in casual games development, distribution and innovation, looking for an excellent Mobile Developer, to join our best of breed framework Development Team.""]}"
Data Science UA,https://jobs.dou.ua/companies/data-science-ua/,BigData Developer with Python or Scala (Kyiv or remotely),https://jobs.dou.ua/companies/data-science-ua/vacancies/121044/," Kyiv, remote",20 October 2020,,"Required skills Requirements: — 3+ years of experience of developing in Python to transform large datasets on distributed and cluster infrastructure— 5+ years of experience in engineering ETL data pipelines for Big Data Systems— Proficient in SQL. Have some experience performing data transformations and data analysis using SQL— Comfortable in juggling multiple technologies and high priority tasks As a plus Nice to have:— 5+ years of Object Oriented Programming experience in any of languages such as Java, Scala, C++— Prior experience of designing and building ETL infrastructure involving streaming systems such as Kafka, Spark, AWS Kinesis We offer — Work with a highly motivated and dedicated team— Competitive salary— Flexible schedule— Benefits program— Social package — medical insurance, sports— Corporate social events— Professional development opportunities— Opportunity for long business trips to the US and possibility for relocation Responsibilities Responsibilities: — Develop ETL (Extract, Transform and Load) Data pipelines in Spark, Kinesis, Kafka, custom Python apps to transfer massive amounts of data (over 20TB/ month) most efficiently between systems— Engineer complex and efficient and distributed data transformation solutions using Python, Java, Scala, SQL— Productionalize Machine Learning models efficiently utilizing resources in clustered environment— Research, plan, design, develop, document, test, implement and support proprietary software applications— Analytical data validation for accuracy and completeness of reported business metrics— Open to taking on, learn and implement engineering projects outside of core competency— Understand the business problem and engineer/architect/build an efficient, cost-effective and scalable technology infrastructure solution— Monitor system performance after implementation and iteratively devise solutions to improve performance and user experience— Research and innovate new data product ideas to grow client’s revenue opportunities and contribute to company’s intellectual property Project description You are going to build innovative data pipelines for processing and analyzing client’s large user datasets (250 billion + events per month). A unique challenge with the role is being comfortable in developing varied technologies like custom transformation/integration apps in Python and Java, and pipelines in Spark, Kafka, Kinesis, transforming and analyzing in SQL.",dou,2020-11-12,BigData Developer with Python or Scala (Kyiv or remotely)@Data Science UA,,,"{""Required skills"": [""Requirements:""], ""As a plus"": [""3+ years of experience of developing in Python to transform large datasets on distributed and cluster infrastructure"", ""5+ years of experience in engineering ETL data pipelines for Big Data Systems"", ""Proficient in SQL. Have some experience performing data transformations and data analysis using SQL"", ""Comfortable in juggling multiple technologies and high priority tasks""], ""We offer"": [""Nice to have:"", ""5+ years of Object Oriented Programming experience in any of languages such as Java, Scala, C++"", ""Prior experience of designing and building ETL infrastructure involving streaming systems such as Kafka, Spark, AWS Kinesis""], ""Responsibilities"": [""Work with a highly motivated and dedicated team"", ""Competitive salary"", ""Flexible schedule"", ""Benefits program"", ""Social package"", ""medical insurance, sports"", ""Corporate social events"", ""Professional development opportunities"", ""Opportunity for long business trips to the US and possibility for relocation""], ""Project description"": [""Responsibilities:""]}"
Data Science UA,https://jobs.dou.ua/companies/data-science-ua/,Senior ML/3D engineer,https://jobs.dou.ua/companies/data-science-ua/vacancies/132078/, remote,20 October 2020,,"Required skills — Solid mathematical background.— 5+ years of software engineering experience.— Experience researching and developing 3d algorithms.— 3+ years of experience with Python.— Practical experience with at least one deep learning frameworks (Tensorflow, PyTorch).— Practical experience in developing classical optimization methods solutions. As a plus — Practical experience with differentiable rendering or multiview problems is a big plus. We offer — Stock option plan;— Investment in your growth and self-development;— Competitive compensation;— 20 working days of paid vacations and paid sick leaves;— 10 remote days each month and one remote month from another country per year;— Foreign language classes inhouse and communication with native speakers;— Online fitness with a corporate trainer;— Modern and conveniently located offices with good working conditions;— Corporate, social and cultural events. Responsibilities — Entire pipeline improvement that corresponds to human 3d models, from raw scan to the parametric space(registration, rigging, etc.)— Analyzing existing features, reveal the hidden problems, etc.— Proposing new ideas. Project description Our partner is on a mission to bring deeper levels of personalization to the consumer shopping experience by turning body data into business intelligence and remove the challenge of fit communication between consumers and the places they love to shop. We are result-oriented enthusiasts who work tirelessly to build technology that has the potential to transform the fashion industry and how we shop online. We combine expertise and experience and work together to deliver outstanding results for our clients. This is a great opportunity for you to shape the growth, development and culture of an exciting and very fast-growing company in the retail market. As a 3D Engineer, you will work on our 3d models, from raw scan to the parametric space(registration, rigging, etc.). If you want to be a part of the company that will change the online shopping market, we’d love to meet you!",dou,2020-11-12,Senior ML/3D engineer@Data Science UA,,,"{""Required skills"": [""Solid mathematical background."", ""5+ years of software engineering experience."", ""Experience researching and developing 3d algorithms."", ""3+ years of experience with Python."", ""Practical experience with at least one deep learning frameworks (Tensorflow, PyTorch)."", ""Practical experience in developing classical optimization methods solutions.""], ""As a plus"": [""Practical experience with differentiable rendering or multiview problems is a big plus.""], ""We offer"": [""Stock option plan;"", ""Investment in your growth and self-development;"", ""Competitive compensation;"", ""20 working days of paid vacations and paid sick leaves;"", ""10 remote days each month and one remote month from another country per year;"", ""Foreign language classes inhouse and communication with native speakers;"", ""Online fitness with a corporate trainer;"", ""Modern and conveniently located offices with good working conditions;"", ""Corporate, social and cultural events.""], ""Responsibilities"": [""Entire pipeline improvement that corresponds to human 3d models, from raw scan to the parametric space(registration, rigging, etc.)"", ""Analyzing existing features, reveal the hidden problems, etc."", ""Proposing new ideas.""], ""Project description"": [""Our partner is on a mission to bring deeper levels of personalization to the consumer shopping experience by turning body data into business intelligence and remove the challenge of fit communication between consumers and the places they love to shop.""]}"
N-iX,https://jobs.dou.ua/companies/n-ix/,Senior BigData Engineer,https://jobs.dou.ua/companies/n-ix/vacancies/135558/," Kyiv, Lviv, remote",19 October 2020,,"Required skills Requirements: • Experience in Python or Scala• Experience in SQL• Experience in Linux• Good understanding of software development methodologies• Being result-oriented, ability to make things done in a highly dynamic and stressful environment• Good communication skills and good English• Being a good team player As a plus Nice to have: • Apache Spark including an understanding of optimization techniques• Knowledge of Hadoop architecture• Experience in working with AWS (S3, EMR cluster, Lambda, Kinesis, DynamoDB, etc.) We offer We offer: • Flexible working hours• A competitive salary and good compensation package• Best hardware• A masseur and a corporate doctor• Healthcare & sport benefits• An inspiring and comfy office Professional growth: • Challenging tasks and innovative projects• Meetups and events for professional development• An individual development plan• Mentorship program Fun: • Corporate events and outstanding parties• Exciting team buildings• Memorable anniversary presents• A fun zone where you can play video games, foosball, ping pong, and more. Responsibilities Responsibilities and job specifics: • Develop and support existing end-to-end data AWS based Big Data solutions for structured and unstructured data including, but not limited to, ingestion, parsing, integration, auditing, logging, aggregation, normalization, and error handling• Create jobs and queries to perform auditing and error handling• Develop PySpark and Scala Spark pipelines Project description We are looking for a Senior BigData Engineer to join our team. Our client is a US company that is specialized in delivering reliable Internet connectivity and entertainment multimedia to aircrafts worldwide, enhancing the experience for both passengers and crew. We are cooperating with the Business Intelligence, Data Analysis and BigData directions and looking for talents who can contribute to the complex data management and analysis projects.",dou,2020-11-12,Senior BigData Engineer@N-iX,,,"{""Required skills"": [""Requirements:""], ""As a plus"": [""Experience in Python or Scala"", ""Experience in SQL"", ""Experience in Linux"", ""Good understanding of software development methodologies"", ""Being result-oriented, ability to make things done in a highly dynamic and stressful environment"", ""Good communication skills and good English"", ""Being a good team player""], ""We offer"": [""Nice to have:""], ""Responsibilities"": [""Apache Spark including an understanding of optimization techniques"", ""Knowledge of Hadoop architecture"", ""Experience in working with AWS (S3, EMR cluster, Lambda, Kinesis, DynamoDB, etc.)""], ""Project description"": [""We offer:""]}"
Playtika,https://jobs.dou.ua/companies/playtika-ua/,Java Developer (Data),https://jobs.dou.ua/companies/playtika-ua/vacancies/135552/, Kyiv,19 October 2020,,"Required skills ● Deep Java knowledge;● Deep Spark knowledge;● Ability to work in Agile/Scrum environment;● Team Player● Strong SQL knowledge;● Multithreading;● Strong awareness of current web development standards and frameworks (e.g. Spring Framework);● Desire to write tests;● Strong sense of responsibility and initiative. As a plus ● Spring Boot 2.0, Spring Cloud;● Docker, Kubernetes;● Messaging systems (Kafka);● High loaded systems experience;● Agile environment experience;● HDFS● Spark SQL● Hive We offer ● Competitive salary, performance-based bonuses and flexible working hours;● Training programs, certifications, conferences including international events; ● Social package, including gym membership compensation, medical insurance; ● Paid vacation and sick leaves;● Corporate English classes;● Referral program;● Corporate celebrations, team buildings, and fun activities;● Free meals twice a day, refreshments, happy hours;● Technical library with option to order books;● Coaching and reviews to support your career development;● Comfortable office near the ’Osokorki’ metro station with a magnificent view at the whole right bank of the Dnieper, an article about it here;● Working for one of the top social gaming companies in the world;● Cute and funny corporate events — some of them definitely unforgettable. We have fun like that;● By the way, you can feel us here. Responsibilities ● Develop and implement new features;● Maintain existed microservices infrastructure, monitor production incidents, trace and debug business transactions;● Design new technical solutions, improve performance and optimize current data processing algorithms;● Build data transferring pipelines with Spark;● Transform existing data pipelines from SQL to Spark;● Collaborate with other teams and game studios inside Playtika in order to provide fast integration with their services. Project description Playtika is looking for a Java/Data Developer to join the Kyiv office on a full- time basis. Playtika — the company of storytellers and coders, artists and data-scientists, explorers and strategists. Since 2010, Playtika has been a pioneer in the social games industry. Playtika doesn’t just build games. It brings them to life. Fueled by over six terabytes of data daily, our games are continuously evolving journeys, personalized to deliver new challenges and surprising thrills, at every twist and every turn.We were the first to introduce free-to-play casino-style games to social networks. We are successfully applying our intuitive understanding of what players want. That’s why we have 30 million players and 17 offices around the world. Data department works with all Playtika games to provide services and tools for monetization and acquisition based on data analysis. The team is focused on tools for media buyers. We have a startup spirit, so we are working in touch with business, propose our solution for business needs, and trying new approaches that can make the business better. We are proud of how fast we are moving new services to prod and update them according to feedback. Also, each team member takes a part of creating a solution, from requirements discussion to create the architecture of new feature/service.In this role you will work with modern technology stack: Java 8, Spring Boot, Spark, Microservice architecture, Activity, Kafka, AeroSpike, PostgreSQL, Vertica, MemSQL, Docker, etc. to build new services and features.",dou,2020-11-12,Java Developer (Data)@Playtika,,,"{""Required skills"": [""\u25cf Deep Java knowledge;\u25cf Deep Spark knowledge;\u25cf Ability to work in Agile/Scrum environment;\u25cf Team Player\u25cf Strong SQL knowledge;\u25cf Multithreading;\u25cf Strong awareness of current web development standards and frameworks (e.g. Spring Framework);\u25cf Desire to write tests;\u25cf Strong sense of responsibility and initiative.""], ""As a plus"": [""\u25cf Spring Boot 2.0, Spring Cloud;\u25cf Docker, Kubernetes;\u25cf Messaging systems (Kafka);\u25cf High loaded systems experience;\u25cf Agile environment experience;\u25cf HDFS\u25cf Spark SQL\u25cf Hive""], ""We offer"": [""\u25cf Competitive salary, performance-based bonuses and flexible working hours;\u25cf Training programs, certifications, conferences including international events; \u25cf Social package, including gym membership compensation, medical insurance; \u25cf Paid vacation and sick leaves;\u25cf Corporate English classes;\u25cf Referral program;\u25cf Corporate celebrations, team buildings, and fun activities;\u25cf Free meals twice a day, refreshments, happy hours;\u25cf Technical library with option to order books;\u25cf Coaching and reviews to support your career development;\u25cf Comfortable office near the \u2019Osokorki\u2019 metro station with a magnificent view at the whole right bank of the Dnieper, an article about it here;\u25cf Working for one of the top social gaming companies in the world;\u25cf Cute and funny corporate events"", ""some of them definitely unforgettable. We have fun like that;\u25cf By the way, you can feel us here.""], ""Responsibilities"": [""\u25cf Develop and implement new features;\u25cf Maintain existed microservices infrastructure, monitor production incidents, trace and debug business transactions;\u25cf Design new technical solutions, improve performance and optimize current data processing algorithms;\u25cf Build data transferring pipelines with Spark;\u25cf Transform existing data pipelines from SQL to Spark;\u25cf Collaborate with other teams and game studios inside Playtika in order to provide fast integration with their services.""], ""Project description"": [""Playtika is looking for a Java/Data Developer to join the Kyiv office on a full- time basis.""]}"
B2B Soft,https://jobs.dou.ua/companies/b2b-soft/,Manual QA Engineer (Data Team),https://jobs.dou.ua/companies/b2b-soft/vacancies/135548/, Kyiv,19 October 2020,,"Required skills • Experience in QA 2+ years;• Experience of testing WEB apps;• Good understanding of QA methodologies & practices;• Strong knowledge with SQL manager (queries, tracing, exec stored procedure);• Experience with log management systems (e.g. GrayLog)• Experience with CI/CD infrastructure (e.g. TeamCity, Octopus)• Experience of testing client-server applications;• Experience of testing web-services: SoapUI, Postman, Swagger, Fiddler, etc;• Good written and spoken English skills (intermediate +) . Soft skills needed:• Mature, self-organized and responsible person;• Cooperative and flexible team player; • Analytical skills • Proactive and result oriented person As a plus Would be a plus:• Experience in Mobile testing • Experience in DWH testing • Technical Education; We offer Why you should choose us:• We are not outsourcing company, you will have a great opportunity to work with advanced and progressive products;• Our position — great opportunity to implement your ideas!• We are focused on growth and innovation;• No bureaucracy. Compensation & Benefits:• Ability to work in successful U.S. IT company with international team;• Participation in challenging project that is famous and successful in the U.S. market;• Career and proficiency development plans;• Competitive salary;• Paid vacation and sick leave;• Health insurance;• Free English courses with native speakers;• Referral program;• Great working conditions;• Friendly atmosphere and corporate events;• Comfortable office near downtown close to the metro station. Responsibilities Responsibilities:• Provide functional, regression, smoke manual testing of B2B Soft Reporting products (general reports, BI reports, APIs, subscription services) and data analyzing;• Investigation of complex solution and finding the root cause;• Creation and maintenance of testing documentation;• Participate in the analysis and estimation of product requirements; • Full coverage of QA processes in Scrum-based process environment;• Demo conduction for PO• reporting to QA Lead/ Delivery manager Project description About the company:B2B Soft is a world-leading SaaS solutions provider primarily focused on the Telecom and General Retail Industries. Our innovative product suite delivers Omnichannel software and hardware solutions, which leverage Business Intelligence, Artificial Intelligence, and Machine learning.Widely recognized as an industry innovator, we service some of the leading worldwide carriers including Altice, AT&T, Boost, GCI, T-Mobile, Tracfone, Verizon & XplornetFounder led and operated for 15+ years, B2B Soft is headquartered in New York City and currently employs over 150 associates worldwide. We’re seeking talented, creative individuals to join our fast-paced, rapidly expanding, and dynamic team. If you’re looking for a new challenge with tremendous opportunity for upward mobility at an industry-leading tech company, you’ve found your perfect match!Project Description:Wireless Standard POS (Point-Of-Sales) (www.b2bsoft.com) is our retail management solution for the Telecom Market.It provides thousands of retailers with features and functionalities they need to run their businesses effectively with full visibility and control into every aspect of sales and operations. It is simple to learn, easy to use and as operation grows, more features can be added on.Our system can optimize and simplify all processes related to retail in this business area. Few things that our product can do:•Robust Online Reporting•Repair Management Software•3rd Party Integrations•Customer Connect Marketing•Time and Attendance•Carrier Commission Reconciliation As a member of B2B Soft’s QA Team, you will collaborate closely with our great Development teams and work with several different testing directions including embedded, web, desktop, and mobile. You will have the opportunity to learn and help improve the quality of our deep and complex product architecture by diving into customer needs, working with our Product, Business analytics, and Architecture teams, and communicating with Customer representatives.From a technology standpoint you will use and have a chance to work with the following technology stack:•Manual QA Stack: Fiddler, Swagger, Zephyr (a corporate platform for managing tests) and SQL, Postman, REST, SOAP.•Automation QA Stack on Python: IDE: PyCharm, Git (Version Control System), Python v.3.6 and higher, knowledge and experience with test frameworks: pytest, behave, selenium, HTML, CSS, SQL, JavaScript.•Automation QA Stack on Java: JUnit, Maven, Allure reporting, Rest Assured",dou,2020-11-12,Manual QA Engineer (Data Team)@B2B Soft,,,"{""Required skills"": [""Experience in QA 2+ years;"", ""Experience of testing WEB apps;"", ""Good understanding of QA methodologies & practices;"", ""Strong knowledge with SQL manager (queries, tracing, exec stored procedure);"", ""Experience with log management systems (e.g. GrayLog)"", ""Experience with CI/CD infrastructure (e.g. TeamCity, Octopus)"", ""Experience of testing client-server applications;"", ""Experience of testing web-services: SoapUI, Postman, Swagger, Fiddler, etc;"", ""Good written and spoken English skills (intermediate +) .""], ""As a plus"": [""Soft skills needed:"", ""Mature, self-organized and responsible person;"", ""Cooperative and flexible team player;"", ""Analytical skills"", ""Proactive and result oriented person""], ""We offer"": [""Would be a plus:"", ""Experience in Mobile testing"", ""Experience in DWH testing"", ""Technical Education;""], ""Responsibilities"": [""Why you should choose us:"", ""We are not outsourcing company, you will have a great opportunity to work with advanced and progressive products;"", ""Our position"", ""great opportunity to implement your ideas!"", ""We are focused on growth and innovation;"", ""No bureaucracy.""], ""Project description"": [""Compensation & Benefits:"", ""Ability to work in successful U.S. IT company with international team;"", ""Participation in challenging project that is famous and successful in the U.S. market;"", ""Career and proficiency development plans;"", ""Competitive salary;"", ""Paid vacation and sick leave;"", ""Health insurance;"", ""Free English courses with native speakers;"", ""Referral program;"", ""Great working conditions;"", ""Friendly atmosphere and corporate events;"", ""Comfortable office near downtown close to the metro station.""]}"
N-iX,https://jobs.dou.ua/companies/n-ix/,Trainee Scala Engineer (with BigData stack),https://jobs.dou.ua/companies/n-ix/vacancies/135541/?from=first-job," Kyiv, Lviv",19 October 2020,,"We are looking for the Trainee Scala Engineer (with BigData stack) to join our new team. The client and your role:You will be working as a Trainee Specialist for a Data department of one of the biggest Jordan bank. This is the opportunity to utilize your skills at the top influential plane of the company and express your knowledge and rich experience to a wide audience that will affect many products and their end users. Requirements:• Good knowledge of Scala/Java or Python• Experience (or ready to learn) with Hadoop ecosystem technology stacks as HDFS, HBase, Hive, Spark (Scala Spark), Sqoop, Cloudera etc.• Experience in using Jenkins, GIT• Entry level experience with Kafka• Experience with SQL knowledge• Basic understanding of algorithms• Good English knowledge verbal and written communication skill Responsibilities:• Write production-ready code and unit tests that meet both system and business requirements• Respond to feature requests, bug reports, performance issues and ad-hoc questions• Work collaboratively within the team to deliver quality software We offer:• Flexible working hours• A competitive salary and good compensation package• Best hardware• A masseur and a corporate doctor• Healthcare & sport benefits• An inspiring and comfy office Professional growth:• Challenging tasks and innovative projects• Meetups and events for professional development• An individual development plan• Mentorship program Fun:• Corporate events and outstanding parties• Exciting team buildings• Memorable anniversary presents• A fun zone where you can play video games, foosball, ping pong, and more",dou,2020-11-12,Trainee Scala Engineer (with BigData stack)@N-iX,,,{}
Railsware,https://jobs.dou.ua/companies/railsware/,Data Analyst,https://jobs.dou.ua/companies/railsware/vacancies/88796/," Kyiv, Krakow (Poland), remote",19 October 2020,,"Required skills 🔹You have at least 2+ years of experience as a Data Analyst working with small-to-medium datasets on a wide range of contexts🔹Extensive knowledge of SQL + Python / R🔹Strong background in Statistics and Probability theory🔹Hands-on experience in data visualization via reports and dashboards (e.g., Data Studio, Tableau, Power BI, etc.)🔹Skills in tuning data flows and setting up automations between various data sources 🔹Good skills in management of own tasks and projects independently, writing notes, project scope🔹You are always curious and ready to do deep research, interview the teams, detect and understand their needs for analysis🔹You can deliver own solutions that will bring value to projects you’ll work on based on the business objectives🔹You value quality and simplicity and know when to simplify your solutions, or even make them self supportable🔹Proactiveness — this skill is must-have to work at Railsware :) You are an easy-going, fast learner who is always ready to experiment with new concepts and tools🔹Fluent English verbal and written communication skills — you must be able to explain data models, describe processes, and deliver data in an easy to understand manner for non-data savvy users As a plus 🔹Software product analysis experience would be a great plus🔹Javascript knowledge🔹GitHub usage experience🔹Data infrastructure organization, monitoring and maintenance: managing ETL, writing tests for data analytics code, maintaining smart seed data and monitoring data consistency🔹Experience with Jupyter Notebook🔹Experience with Google AppsScript🔹Experience in GSheets or Excel, e.g. multiline array formulas, Query function🔹Experience in Machine Learning We offer 👨‍🎓Outstanding development culture: bit.ly/rwdouculture🚩Collaborate with us remotely from any location or in one of our offices: bit.ly/rwdouremote⏰ We offer flexible hours💸 Get competitive compensation, yearly bonus, access to savings program and microcredits💻Thanks to our hardware policy, we use the best equipment and can regularly update it🌴 34 days a year as a paid time off (24 standard days + 10 more to cover public holidays)🏃Health policy budget will cover your private sports and healthcare expenses🤝Participate in local and international conferences✈️Every year we go for a 7-day company trip with our families. We’ve already visited Austrian Alps, Crete, Italy, and Croatia together🏢Our offices are equipped with modern ergonomic chairs and standing desks🍒You can always find fresh food and drinks in our kitchen 🔥See all Railsware benefits here: l.rw.rw/benefits Responsibilities The major area of responsibility will be related to the data of our own products, company operations, and consultancy. The insights to be provided will cover various parts of the business including forecasting, team efficiency, user engagement, project progress, etc. Some examples of the results we would like to see:🔸A system to keep the right balance between profits and losses (collect and process data from various sources, spot outliers in expenses, etc.)🔸Railsware products metrics monitor (MRR, LTV, Stickiness, Churn, etc.)🔸Research of sales and marketing: leads quality, geography, sources, conversion rates, etc🔸Qualification of product early adopters to distinguish between one-person companies and huge businesses Project description We’re looking for a true Data Analysis geek to supercharge Railsware in making smart data-driven decisions for the business. The key aim of the position is to gather a lot of data from different sources, process it masterfully, and convert it to simple and useful insights for the team. We can promise that you won’t be bored here. The results of your work will be a valuable contribution to the development of our own products, clients’ solutions, and improvement of the company’s internal operations (like recruitment). We would really like to combine your strong expertise and freedom in coming up with ideas and their implementation. That’s how we do it here, at Railsware. Railsware is a product studio. Since 2007, we have shaped our own “know-how” in product creation. Railsware helped many US and EU startups to turn into multi-million-dollar companies. We have built our own products, that now have 600K+ of excited users and generate $1M+ a year.We look for people with high potential ready to evolve in multiple directions. The right hires shape a team of A-players to learn from and evolve together. We support Railswarians with outstanding benefits and remote culture. Ready to become the next Railswarian? Apply today! 🔹Join our team: railsware.com/careers🔹Products we’ve built: railsware.com/case-studies🔹Blog: railsware.com/blog🔹YouTube: youtube.com/c/railsware 🔹Clients about us: youtu.be/EIEFStPmmz8",dou,2020-11-12,Data Analyst@Railsware,,,"{""Required skills"": [""\ud83d\udd39You have at least 2+ years of experience as a Data Analyst working with small-to-medium datasets on a wide range of contexts\ud83d\udd39Extensive knowledge of SQL + Python / R\ud83d\udd39Strong background in Statistics and Probability theory\ud83d\udd39Hands-on experience in data visualization via reports and dashboards (e.g., Data Studio, Tableau, Power BI, etc.)\ud83d\udd39Skills in tuning data flows and setting up automations between various data sources \ud83d\udd39Good skills in management of own tasks and projects independently, writing notes, project scope\ud83d\udd39You are always curious and ready to do deep research, interview the teams, detect and understand their needs for analysis\ud83d\udd39You can deliver own solutions that will bring value to projects you\u2019ll work on based on the business objectives\ud83d\udd39You value quality and simplicity and know when to simplify your solutions, or even make them self supportable\ud83d\udd39Proactiveness"", ""this skill is must-have to work at Railsware :) You are an easy-going, fast learner who is always ready to experiment with new concepts and tools\ud83d\udd39Fluent English verbal and written communication skills"", ""you must be able to explain data models, describe processes, and deliver data in an easy to understand manner for non-data savvy users""], ""As a plus"": [""\ud83d\udd39Software product analysis experience would be a great plus\ud83d\udd39Javascript knowledge\ud83d\udd39GitHub usage experience\ud83d\udd39Data infrastructure organization, monitoring and maintenance: managing ETL, writing tests for data analytics code, maintaining smart seed data and monitoring data consistency\ud83d\udd39Experience with Jupyter Notebook\ud83d\udd39Experience with Google AppsScript\ud83d\udd39Experience in GSheets or Excel, e.g. multiline array formulas, Query function\ud83d\udd39Experience in Machine Learning""], ""We offer"": [""\ud83d\udc68\u200d\ud83c\udf93Outstanding development culture: bit.ly/rwdouculture\ud83d\udea9Collaborate with us remotely from any location or in one of our offices: bit.ly/rwdouremote\u23f0 We offer flexible hours\ud83d\udcb8 Get competitive compensation, yearly bonus, access to savings program and microcredits\ud83d\udcbbThanks to our hardware policy, we use the best equipment and can regularly update it\ud83c\udf34 34 days a year as a paid time off (24 standard days + 10 more to cover public holidays)\ud83c\udfc3Health policy budget will cover your private sports and healthcare expenses\ud83e\udd1dParticipate in local and international conferences\u2708\ufe0fEvery year we go for a 7-day company trip with our families. We\u2019ve already visited Austrian Alps, Crete, Italy, and Croatia together\ud83c\udfe2Our offices are equipped with modern ergonomic chairs and standing desks\ud83c\udf52You can always find fresh food and drinks in our kitchen""], ""Responsibilities"": [""\ud83d\udd25See all Railsware benefits here: l.rw.rw/benefits""], ""Project description"": []}"
Newxel,https://jobs.dou.ua/companies/newxel/,Data engineering specialist for Amazon sellers product,https://jobs.dou.ua/companies/newxel/vacancies/89361/, Kyiv,19 October 2020,,"Required skills — 2+ years of experience in Scala server-side development with cloud services (preferably AWS) and building distributed systems.— Computer science background or university degree in related fields.— Hands-on experience with data pipeline and data processing technologies (ex Apache Spark, Kafka).— Experience with SQL-databases and BigData DBs — Experience in Agile methodologies and working in distributed teams— Good English communication skills — spoken and written As a plus — Experience with CI/CD.— Knowledge of Python We offer — Competitive salary— Modern technical stack (Spark 3, EMR 6, etc.)— Team buildings— Free meals, fruits, sweets, and cookies— Long-term employment with 20 working-days paid vacation— Flexible working schedule— Medical insurance— Paid sick leaves (10 per year)— Modern and comfortable office near the Vystavkovyi center Responsibilities In this role, you will— Create data and algorithmic processes with a cutting-edge technology model as a SaaS.— Collaborate with stakeholders and implement software development needs.— Design and develop server-side software, including documentation and coding. Coordinate server-side development and UI development matching.— Design and build data pipelines based on third-party integrations. Project description Are you buying on Amazon? There’s a good chance what you paid was calculated by our technology and created by our Top-Notch engineers. Feedvisor is the ’AI-First‘ optimization and intelligence platform for large sellers and brands on Amazon — Feedvisor uses Big Data, Machine Learning and AI Algorithms to facilitate automatic business-critical processes and decisions for online retailers, helping them grow their business and win the competition. At Feedvisor, our technology makes a real and lasting impact on our customers’ businesses. We are seeking a passionate and talented Engineer to join our Top-Notch Engineering team. At Feedvisor we care a great deal about our company culture. You will be a fit if:You are taking extreme ownershipYou seek to understand the world through other people’s eyesYou are constantly asking “How can I help?”You see opportunity in demanding situationsYou are trustful: you walk your talkYou are a proactive problem-solver who gets stuff doneYou are humble, radically candid, and open to feedback",dou,2020-11-12,Data engineering specialist for Amazon sellers product@Newxel,,,"{""Required skills"": [""2+ years of experience in Scala server-side development with cloud services (preferably AWS) and building distributed systems."", ""Computer science background or university degree in related fields."", ""Hands-on experience with data pipeline and data processing technologies (ex Apache Spark, Kafka)."", ""Experience with SQL-databases and BigData DBs"", ""Experience in Agile methodologies and working in distributed teams"", ""Good English communication skills"", ""spoken and written""], ""As a plus"": [""Experience with CI/CD."", ""Knowledge of Python""], ""We offer"": [""Competitive salary"", ""Modern technical stack (Spark 3, EMR 6, etc.)"", ""Team buildings"", ""Free meals, fruits, sweets, and cookies"", ""Long-term employment with 20 working-days paid vacation"", ""Flexible working schedule"", ""Medical insurance"", ""Paid sick leaves (10 per year)"", ""Modern and comfortable office near the Vystavkovyi center""], ""Responsibilities"": [""In this role, you will"", ""Create data and algorithmic processes with a cutting-edge technology model as a SaaS."", ""Collaborate with stakeholders and implement software development needs."", ""Design and develop server-side software, including documentation and coding. Coordinate server-side development and UI development matching."", ""Design and build data pipelines based on third-party integrations.""], ""Project description"": [""Are you buying on Amazon? There\u2019s a good chance what you paid was calculated by our technology and created by our Top-Notch engineers.""]}"
ZONE3000,https://jobs.dou.ua/companies/zone3000/,Business Intelligence Data Specialist,https://jobs.dou.ua/companies/zone3000/vacancies/132391/, Kharkiv,19 October 2020,,"Required skills — Bachelor’s degree in Computer Science, information management or similar field— At least 2 years business intelligence experience— Experience with SQL databases— Experience gathering and analyzing system requirements — Strong problem-solving skills and an analytical mind— Highly accurate and thorough in all tasks— Able to thrive in a fast-paced, deadline-driven environment— Collaborative team player— English level — upper-intermediate As a plus — Experience with MongoDB, Hadoop, Apache Spark or any other database modeling and management tools— Experience with C4 model or any other visualizing tool We offer — High & competitive salary— Challenging work in an international professional environment— Opportunity to influence software development process, to be the owner of the product in your field of expertise— Opportunity to apply SAFe methodology— Flexible management— Flexible / Casual Leave— Relocation Bonus when moving from a different city / country— Full benefits package: paid vacation and sick leave— Continuous professional development (free internal and external professional trainings)— Free English classes in the company office— Free use of the services provided by Namecheap (for non-commercial purposes)— Quarterly teambuilding activities and company corporate events — RDX gym membership — Coffee, tea, fruits Responsibilities — Creating and implementing tracking documents to meet stated requirements for metadata management, operational data stores and Extract Transform Load environments— Design conceptual and logical data models and flowcharts— Assisting with developing database solutions to store and retrieve company information— Collaborate with internal customers and IT partners, including system architects, software developers, database administrators, design analysts and information modeling experts to determine project requirements and capabilities, and strategize development and implementation timelines— Performing regular data integrity and quality audits, collating and verifying data from multiple sources — Research new technologies, data modeling methods and information management systems todetermine which ones should be incorporated into company data architectures, and develop implementation timelines and milestones Project description Looking for a Business Intelligence Data Specialist.Zone3000 works with Namecheap project (www.namecheap.com).Namecheap was founded in 2000 on the idea that all people deserve value-priced domains delivered through stellar service. Today, Namecheap is a leading ICANN-accredited domain name registrar and web hosting company with over two million customers and ten million domains under management — and we’re just getting started.Our culture is built on the values that we live every day; the way we work, the way we collaborate with our global network of colleagues and the way we relentlessly innovate solutions that meet the emerging needs of our customers.",dou,2020-11-12,Business Intelligence Data Specialist@ZONE3000,,,"{""Required skills"": [""Bachelor\u2019s degree in Computer Science, information management or similar field"", ""At least 2 years business intelligence experience"", ""Experience with SQL databases"", ""Experience gathering and analyzing system requirements"", ""Strong problem-solving skills and an analytical mind"", ""Highly accurate and thorough in all tasks"", ""Able to thrive in a fast-paced, deadline-driven environment"", ""Collaborative team player"", ""English level"", ""upper-intermediate""], ""As a plus"": [""Experience with MongoDB, Hadoop, Apache Spark or any other database modeling and management tools"", ""Experience with C4 model or any other visualizing tool""], ""We offer"": [""High & competitive salary"", ""Challenging work in an international professional environment"", ""Opportunity to influence software development process, to be the owner of the product in your field of expertise"", ""Opportunity to apply SAFe methodology"", ""Flexible management"", ""Flexible / Casual Leave"", ""Relocation Bonus when moving from a different city / country"", ""Full benefits package: paid vacation and sick leave"", ""Continuous professional development (free internal and external professional trainings)"", ""Free English classes in the company office"", ""Free use of the services provided by Namecheap (for non-commercial purposes)"", ""Quarterly teambuilding activities and company corporate events"", ""RDX gym membership"", ""Coffee, tea, fruits""], ""Responsibilities"": [""Creating and implementing tracking documents to meet stated requirements for metadata management, operational data stores and Extract Transform Load environments"", ""Design conceptual and logical data models and flowcharts"", ""Assisting with developing database solutions to store and retrieve company information"", ""Collaborate with internal customers and IT partners, including system architects, software developers, database administrators, design analysts and information modeling experts to determine project requirements and capabilities, and strategize development and implementation timelines"", ""Performing regular data integrity and quality audits, collating and verifying data from multiple sources"", ""Research new technologies, data modeling methods and information management systems todetermine which ones should be incorporated into company data architectures, and develop implementation timelines and milestones""], ""Project description"": [""Looking for a Business Intelligence Data Specialist.Zone3000 works with Namecheap project (www.namecheap.com).Namecheap was founded in 2000 on the idea that all people deserve value-priced domains delivered through stellar service. Today, Namecheap is a leading ICANN-accredited domain name registrar and web hosting company with over two million customers and ten million domains under management"", ""and we\u2019re just getting started.Our culture is built on the values that we live every day; the way we work, the way we collaborate with our global network of colleagues and the way we relentlessly innovate solutions that meet the emerging needs of our customers.""]}"
WeRush,https://jobs.dou.ua/companies/werush/,Data Scientist / ML Engineer,https://jobs.dou.ua/companies/werush/vacancies/132352/, Kyiv,19 October 2020,,"Required skills — Experience in one of Python, Java or Scala— Experience in implementation of Machine Learning— Experience with developing dashboards and reports— Strong SQL skills (MySQL, PostgreSQL)— Knowledge of working with large datasets— Strong technical, analytical, and mathematical skills— Intermediate English or higher As a plus — Experience in igaming We offer — Competitive compensation— Full time job with flexible schedule— Paid sick leave, vacation and holidays— Opportunity of career growth— Free English language courses in our office— Positive corporate environment— Team-buildings and corporate events Responsibilities — Deploying models and systems to production by applying data-driven Machine Learning and AI solutions end to end, starting from research and design to development and testing.— Perform research & analysis to solve complex problems using advanced AI technologies (deep learning, re-enforcement learning, time-series, NLP...)— Work closely with product, data engineers, and the R&D teams to deliver the best solution to the business.— Run research and POCs for new technologies and tools as part of our continuous improvement life cycle.— Make sure the solution is stable, scalable, and provides accurate results at all times. Project description WeRush, international company founded in 2017, with offices in Europe, with products focusing on the following markets — Sweden, Finland, Norway, Canada, New Zealand, UK showing extensive growth and is gaining on strong popularity among customers.We are looking for an experienced and passionate Machine Learning Engineer to join our team! Do not miss your chance! We look forward to welcoming you to our team!",dou,2020-11-12,Data Scientist / ML Engineer@WeRush,,,"{""Required skills"": [""Experience in one of Python, Java or Scala"", ""Experience in implementation of Machine Learning"", ""Experience with developing dashboards and reports"", ""Strong SQL skills (MySQL, PostgreSQL)"", ""Knowledge of working with large datasets"", ""Strong technical, analytical, and mathematical skills"", ""Intermediate English or higher""], ""As a plus"": [""Experience in igaming""], ""We offer"": [""Competitive compensation"", ""Full time job with flexible schedule"", ""Paid sick leave, vacation and holidays"", ""Opportunity of career growth"", ""Free English language courses in our office"", ""Positive corporate environment"", ""Team-buildings and corporate events""], ""Responsibilities"": [""Deploying models and systems to production by applying data-driven Machine Learning and AI solutions end to end, starting from research and design to development and testing."", ""Perform research & analysis to solve complex problems using advanced AI technologies (deep learning, re-enforcement learning, time-series, NLP...)"", ""Work closely with product, data engineers, and the R&D teams to deliver the best solution to the business."", ""Run research and POCs for new technologies and tools as part of our continuous improvement life cycle."", ""Make sure the solution is stable, scalable, and provides accurate results at all times.""], ""Project description"": [""WeRush, international company founded in 2017, with offices in Europe, with products focusing on the following markets"", ""Sweden, Finland, Norway, Canada, New Zealand, UK showing extensive growth and is gaining on strong popularity among customers.We are looking for an experienced and passionate Machine Learning Engineer to join our team!""]}"
Avenga,https://jobs.dou.ua/companies/avenga/,Scala/Big Data Engineer,https://jobs.dou.ua/companies/avenga/vacancies/135383/," Kyiv, Lviv, Vinnytsia, Ivano-Frankivsk, Lutsk, Poltava, Khmelnytskyi, Cherkasy",16 October 2020,,"Required skills — At least 2 years of a relevant experience for Middle role and 4 years for Senior;— Good knowledge and Hands-on Scala Programming;— Proficiency in Spark Scala for technical development and implementation;— Data pipeline using Spark Scala;— Load disparate data sets by leveraging Kafka consumers;— Good Understanding of Big Data technologies like Hadoop, Spark, Scala, Hive, HBase, Pig, Cascading;— Ability to utilize Hive, Spark, Cassandra, Mesos and Kafka;— Good knowledge on Hive and HiveQL;— Good Hands on in Hadoop stack (MapReduce, Hive, Sqoop, Oozie);— Experience with AWS components and services, particularly EMR, S3, and Lambda;— Good understanding of file formats including JSON, Parquet, Avro, and others;— Experience with open source technologies such as Cassandra;— Experience with messaging and complex event-processing systems such as Kafka and Storm;— Extensive hands on Data frame and Dataset operations of Spark;— Excellent in writing shell scripts;— Experience with application architecture in a big data environment;— Experience with AWS technologies or equivalent cloud stack As a hands-on engineer, influence all architecture decisions; — Build reusable code, with the ability to scale with very large data volumes;— English — Upper-Intermediate. As a plus — Knowledge on Cassandra Architecture and CQL;— Knowledge on Java Programming;— Data Migration/ETL knowledge. We offer — New office with areas for recreation, well-staffed kitchen facilities, shower, covered bike parking;— Compensation of medical service;— Sport compensation: gym-membership reimbursement, small gym on-site;— Language school;— Compensation of training: our employees can participate in the training given by external and internal experts;— PE accounting and support;— Free fruits and sweets. Responsibilities — Programs, tests, and documents systems in accordance with programming standards and validation procedures;— For more senior candidate — to serve as technical lead on a project;— Plans and coordinates the complex design, development, implementation, maintenance, and level 4 support of associated systems;— Provides software, system and/or programming solutions across projects and/or technologies to meet internal and external needs;— Engineer systems and/or software, and/or transfer data for internal and external clients;— Plans and coordinates peer review of work products such as code, designs, and test plans produced by other team members;— Interact with corporate and project team members to negotiate timelines, responsibilities, and deliverables;— Provides expert technical advice and solutions for internal and external clients;— Provides expert review of database development work with regard to programming standards and validation procedures;— Provides long term evaluations of systems, company wide;— Meets personal and professional milestones as assigned, and interacts with the project team to organize timelines, responsibilities and deliverables;— Performs other duties as directed. Project description Avenga is a global IT and digital transformation champion. Together, we are more than 2500 professionals with over 20 years of experience. We deliver strategy, customer experience, solution engineering, managed services and software products. Our mission is to shake up the conventional IT market.Now, as we grow fast and smart, Avenga is looking for a Middle or Senior Scala/Big Data Software Engineer to join our team on a full-time basis. Our client is one among the largest Healthcare companies in the USA. We expect the successful candidate to demonstrate high motivation and ability to solve complex problems and achieve set goals.",dou,2020-11-12,Scala/Big Data Engineer@Avenga,,,"{""Required skills"": [""At least 2 years of a relevant experience for Middle role and 4 years for Senior;"", ""Good knowledge and Hands-on Scala Programming;"", ""Proficiency in Spark Scala for technical development and implementation;"", ""Data pipeline using Spark Scala;"", ""Load disparate data sets by leveraging Kafka consumers;"", ""Good Understanding of Big Data technologies like Hadoop, Spark, Scala, Hive, HBase, Pig, Cascading;"", ""Ability to utilize Hive, Spark, Cassandra, Mesos and Kafka;"", ""Good knowledge on Hive and HiveQL;"", ""Good Hands on in Hadoop stack (MapReduce, Hive, Sqoop, Oozie);"", ""Experience with AWS components and services, particularly EMR, S3, and Lambda;"", ""Good understanding of file formats including JSON, Parquet, Avro, and others;"", ""Experience with open source technologies such as Cassandra;"", ""Experience with messaging and complex event-processing systems such as Kafka and Storm;"", ""Extensive hands on Data frame and Dataset operations of Spark;"", ""Excellent in writing shell scripts;"", ""Experience with application architecture in a big data environment;"", ""Experience with AWS technologies or equivalent cloud stack As a hands-on engineer, influence all architecture decisions;"", ""Build reusable code, with the ability to scale with very large data volumes;"", ""English"", ""Upper-Intermediate.""], ""As a plus"": [""Knowledge on Cassandra Architecture and CQL;"", ""Knowledge on Java Programming;"", ""Data Migration/ETL knowledge.""], ""We offer"": [""New office with areas for recreation, well-staffed kitchen facilities, shower, covered bike parking;"", ""Compensation of medical service;"", ""Sport compensation: gym-membership reimbursement, small gym on-site;"", ""Language school;"", ""Compensation of training: our employees can participate in the training given by external and internal experts;"", ""PE accounting and support;"", ""Free fruits and sweets.""], ""Responsibilities"": [""Programs, tests, and documents systems in accordance with programming standards and validation procedures;"", ""For more senior candidate"", ""to serve as technical lead on a project;"", ""Plans and coordinates the complex design, development, implementation, maintenance, and level 4 support of associated systems;"", ""Provides software, system and/or programming solutions across projects and/or technologies to meet internal and external needs;"", ""Engineer systems and/or software, and/or transfer data for internal and external clients;"", ""Plans and coordinates peer review of work products such as code, designs, and test plans produced by other team members;"", ""Interact with corporate and project team members to negotiate timelines, responsibilities, and deliverables;"", ""Provides expert technical advice and solutions for internal and external clients;"", ""Provides expert review of database development work with regard to programming standards and validation procedures;"", ""Provides long term evaluations of systems, company wide;"", ""Meets personal and professional milestones as assigned, and interacts with the project team to organize timelines, responsibilities and deliverables;"", ""Performs other duties as directed.""], ""Project description"": [""Avenga is a global IT and digital transformation champion. Together, we are more than 2500 professionals with over 20 years of experience. We deliver strategy, customer experience, solution engineering, managed services and software products. Our mission is to shake up the conventional IT market.Now, as we grow fast and smart, Avenga is looking for a Middle or Senior Scala/Big Data Software Engineer to join our team on a full-time basis. Our client is one among the largest Healthcare companies in the USA. We expect the successful candidate to demonstrate high motivation and ability to solve complex problems and achieve set goals.""]}"
EPAM,https://jobs.dou.ua/companies/epam-systems/,"Senior Big Data Engineer [Odesa, Kherson, Mykolaiv]",https://jobs.dou.ua/companies/epam-systems/vacancies/135353/," Odesa, Kherson, Mykolaiv, remote",16 October 2020,,"Striving for excellence is in our DNA. Since 1993, we have been helping the world’s leading companies imagine, design, engineer, and deliver software and digital experiences that change the world. We are more than just specialists, we are experts. EPAM is committed to providing our global team of 36,700+ EPAMers with inspiring careers from day one. EPAMers lead with passion and honesty and think creatively. Our people are the source of our success and we value collaboration, try to always understand our customers’ business, and strive for the highest standards of excellence. In today’s new market conditions, we continue to support operations for hundreds of clients around the world remotely, with the vast majority of our teams working from home. No matter where you are located, you’ll join a dedicated, diverse community that will help you discover your fullest potential. DESCRIPTION Our customer is a leader in global prestige retail, teaching and inspiring clients to play in a world of beauty. The company has earned its reputation all over the world as a beauty trailblazer with its expertise, innovation, and entrepreneurial spirit. Our project is about developing a full-stack e-Commerce application for the world’s biggest cosmetics retailer chain of stores. PROJECT TECHNOLOGIES AND TOOLS REQUIREMENTS BENEFITS EVEN MORE EPAM BENEFITS ABOUT EPAM",dou,2020-11-12,"Senior Big Data Engineer [Odesa, Kherson, Mykolaiv]@EPAM",,,"{""DESCRIPTION"": [""Striving for excellence is in our DNA. Since 1993, we have been helping the world\u2019s leading companies imagine, design, engineer, and deliver software and digital experiences that change the world. We are more than just specialists, we are experts.""], ""PROJECT TECHNOLOGIES AND\u00a0TOOLS"": [""EPAM is committed to providing our global team of 36,700+ EPAMers with inspiring careers from day one. EPAMers lead with passion and honesty and think creatively. Our people are the source of our success and we value collaboration, try to always understand our customers\u2019 business, and strive for the highest standards of excellence. In today\u2019s new market conditions, we continue to support operations for hundreds of clients around the world remotely, with the vast majority of our teams working from home. No matter where you are located, you\u2019ll join a dedicated, diverse community that will help you discover your fullest potential.""], ""REQUIREMENTS"": [""Our customer is a leader in global prestige retail, teaching and inspiring clients to play in a world of beauty. The company has earned its reputation all over the world as a beauty trailblazer with its expertise, innovation, and entrepreneurial spirit. Our project is about developing a full-stack e-Commerce application for the world\u2019s biggest cosmetics retailer chain of stores.""]}"
Star,https://jobs.dou.ua/companies/star/,Senior Data Analyst / Data Scientist,https://jobs.dou.ua/companies/star/vacancies/135343/, Kyiv,16 October 2020,,"THE COMPANYStar is a global consultancy that connects strategy, design, engineering and marketing services into a seamless workflow devised to support our clients every step of the way — no matter how long or complex the technology-based business journey. Our strategists, designers and engineers create useful, scalable technology products and solutions. We are 750 strategists, designers, engineers and marketers in 12 locations around the world, and we are here to make every great idea, every great person and every great company shine. That is why we’re called Star. THE OPPORTUNITYYou will be an essential part of the Star Data Science practice and your major tasks will be to participate in delivering data driven projects, perform data analysis and model data using statistical models. You will work together with our global team and support more junior team members. You will have an opportunity to contribute your knowledge to end-to-end data projects hosted in the cloud. THE PERSONMust have:— The individual will be an independent-thinking technical generalist with 5+ years of professional experience in data analysis— 2+ years of strong client-facing experience and the ability to understand client challenges and articulate solutions.— 4+ years of commercial experience with Python (Jupyter, pandas, numpy)— Experience with classic machine learning toolset (linear models and clustering)— Experience with BI tools (Tableau, QuickSight)— Great data visualization skills— Excellent SQL skills— Strong mathematical and statistical skills Nice to have experience with:— Cloud services (AWS, GCP)— ETL pipelines (Glue, NiFi, Airflow)— BigData (EMR, Hadoop, Spark)— Deep learning (pytorch, keras) COMPENSATION AND BENEFITSStar offers a competitive salary and benefits package, as well as an intellectually and creatively stimulating work environment, flexible working hours, and unique international travel opportunities.",dou,2020-11-12,Senior Data Analyst / Data Scientist@Star,,,{}
R&D Center,https://jobs.dou.ua/companies/rnd-center/,Data Analyst,https://jobs.dou.ua/companies/rnd-center/vacancies/119506/, Kyiv,16 October 2020,,"We are looking for a passionate and experienced Data Analyst to join our Monitoring team. In this role, you will get the exciting opportunity to work with product managers to understand their business requirements and deliver solutions and actionable insight throughout the product development lifecycle. Our team is serious about developing great iOS/Android/Web user experiences and hardware products with a scalable data-focused approach. A successful candidate will be a person who enjoys diving deep into data analysis, discovering root causes, and implementing long-term solutions. You will help customize business policies and processes while developing new metrics and functionality. — 3+ years of experience as BI/Data Analyst— Significant experience using SQL for statistical analysis involving large data sets, solid SQL optimization skills— Experience using Tableau— Significant experience creating and maintaining ETLs and databases in a business environment with large-scale, complex datasets— Experience creating and maintaining a data model.— Experienced AWS user with an in-depth understanding of AWS technology stack, experience spinning up and managing AWS services.— Experience in logs analysis (iOS/Android/Web)— English level — B2 — Electrical engineering PCB Component/firmware comprehension— Linux application log comprehension— IoT device setup/management comprehension — Experienced Python/R developer— Docker and NoSQL knowledge, hands-on experience with DataDog, Kibana, MixPanel, Heap, Google Analytics, Firebase, Fabric, Quicksight — Working on the world’s most impactful security products. More than that, an opportunity to get some of those for personal usage — Competitive salary and perks— PE accounting and support— WFH and remote working mode possibility. Partial furniture compensation— Social package, including medical insurance available from the start date and sports compensation after the trial period— 18 paid vacation days per year, paid public holidays according to the Ukrainian legislation— Educational possibilities like corporate courses, knowledge hubs, and free English classes. Semiannual performance review— Free meals, fruits, and snacks when working in the office.",dou,2020-11-12,Data Analyst@R&D Center,,,{}
​Finsight Group Inc.,https://jobs.dou.ua/companies/finsight-group-inc/,Junior Data Entry Specialist (evening time shift vacancy),https://jobs.dou.ua/companies/finsight-group-inc/vacancies/135303/, Kyiv,16 October 2020,,"Required skills The Role:As a Data Entry Specialist, you will work directly with Chief Data Officer (based in New York City) to clean, organize and manage our various client-facing databases and ensure timely and accurate entry of new data. It’s the evening shift vacancy. The working day is from 15:00. Key requirements:● Intermediate or higher writing and speaking English level● Strong knowledge of Excel (vlookup, countif, concatenate and other various excel formulas in order to manipulate and organize the data)● Ability to work with a large amount of information;● Proficiency with online research● Interest with cleaning, organizing and maintaining large complex data sets;● High attention to detail ● Self-organization● Self-motivation. As a plus Would be a plus: ● Experience with data analysis, data quality, and data entry● Knowledge specific of capital markets. We offer What do we offer:● Great opportunities for further professional development● Competitive compensation based on experience and skills● A comfortable and modern office in the city center (business center “Panorama”, Zoloti Vorota metro station)● Professional growth and learning● Participation in seminars, training, and conferences● Friendly team, where everyone is a key team player and can influence on the project● We provide corporate MacBooks● Tea / Coffee and biscuits in the office● English lessons (during quarantine — online. Responsibilities Key responsibilities: ● Working closely with our Chief Data Officer (based in New York City) and ensuring all tasks assigned are completed in a timely manner● Organizing and maintaining an Excel and application database of transactions● Monitoring data feeds and ensuring timely data entry with high attention to detail ● Performing data verification on existing transactions to ensure records are accurate and complete● Researching and managing additional information and data sets that complement our existing and future services. Project description FinSight Group Inc. is a New York City-based product company that offers a highly accessible, cost-effective and frictionless environment to research, prospect, structure, market and monitor the new issues and secondary market securities. We develop four leading platforms in US Fixed Income: Finsight, Deal Roadshow, 17g5.com, and DealVDR. Every month, over $30 billion dollars of financing is exclusively marketed through our software across all major Wall Street banks and hundreds of the largest corporations in the world.We are a friendly, close-knit team of motivated and open-minded individuals committed to personal development, solving real problems and delivering excellent products and services to our clients.",dou,2020-11-12,Junior Data Entry Specialist (evening time shift vacancy)@​Finsight Group Inc.,,,"{""Required skills"": [""The Role:As a Data Entry Specialist, you will work directly with Chief Data Officer (based in New York City) to clean, organize and manage our various client-facing databases and ensure timely and accurate entry of new data.""], ""As a plus"": [""It\u2019s the evening shift vacancy. The working day is from 15:00.""], ""We offer"": [""Key requirements:\u25cf Intermediate or higher writing and speaking English level\u25cf Strong knowledge of Excel (vlookup, countif, concatenate and other various excel formulas in order to manipulate and organize the data)\u25cf Ability to work with a large amount of information;\u25cf Proficiency with online research\u25cf Interest with cleaning, organizing and maintaining large complex data sets;\u25cf High attention to detail \u25cf Self-organization\u25cf Self-motivation.""], ""Responsibilities"": [""Would be a plus: \u25cf Experience with data analysis, data quality, and data entry\u25cf Knowledge specific of capital markets.""], ""Project description"": [""What do we offer:\u25cf Great opportunities for further professional development\u25cf Competitive compensation based on experience and skills\u25cf A comfortable and modern office in the city center (business center \u201cPanorama\u201d, Zoloti Vorota metro station)\u25cf Professional growth and learning\u25cf Participation in seminars, training, and conferences\u25cf Friendly team, where everyone is a key team player and can influence on the project\u25cf We provide corporate MacBooks\u25cf Tea / Coffee and biscuits in the office\u25cf English lessons (during quarantine"", ""online.""]}"
DataRobot,https://jobs.dou.ua/companies/datarobot/,"Ingest (Data) Backend Engineer, Data Management",https://jobs.dou.ua/companies/datarobot/vacancies/125355/," Kyiv, Lviv",16 October 2020,,"Required skills Strong experience in Python and either Java or ScalaExperience with large applications and systems developmentStrong experience using JDBC for database accessStrong database and blob storage experienceExperience integrating applications with cloud data storage systems like S3, GCS, Azure Blob Storage, etc.Experience with authentication methods like Kerberos, SSO, Active Directory or similarFamiliarity with HTTP and experience using tools such as curl or Postman to test HTTP APIsAbility and willingness to quickly learn about new technologies As a plus Experience with Kubernetes ecosystem or Hadoop ecosystemExperience developing applications using DockerExperience with Gradle or other build systemsExperience with functional programming Responsibilities As an Ingest Backend Engineer, you will develop ingest systems to integrate DataRobot with databases, cloud storage systems, SaaS products, and other sources of data, and you will help enhance and maintain these systems as they evolve to meet future connectivity and performance needs. You will contribute to the design of new frameworks to support reading, writing, authentication, failure handling, and more with high performance and reliability. These ingest systems are a key component of the overall architecture of DataRobot.Ideally, a candidate can bring new ideas from concept to implementation, write quality, testable code, and participate in design/development discussions. They also have a can-do attitude and highly value the customer experience above all else, and embrace maintaining and firefighting systems when necessary. The ability and willingness to quickly learn new technologies are essential for this role. Project description The Global Data Domain at DataRobot helps make AI incredibly easy to build by surfacing and connecting users to all enterprise data, and enabling data prep at modeling and prediction time. In this role, you’ll help enable our users to easily interact with the data required to train models and generate predictions. Additionally, you’ll help design solutions that foster collaboration amongst distributed teams of engineers, data scientists, and beyond.",dou,2020-11-12,"Ingest (Data) Backend Engineer, Data Management@DataRobot",,,"{""Required skills"": [""Strong experience in Python and either Java or ScalaExperience with large applications and systems developmentStrong experience using JDBC for database accessStrong database and blob storage experienceExperience integrating applications with cloud data storage systems like S3, GCS, Azure Blob Storage, etc.Experience with authentication methods like Kerberos, SSO, Active Directory or similarFamiliarity with HTTP and experience using tools such as curl or Postman to test HTTP APIsAbility and willingness to quickly learn about new technologies""], ""As a plus"": [""Experience with Kubernetes ecosystem or Hadoop ecosystemExperience developing applications using DockerExperience with Gradle or other build systemsExperience with functional programming""], ""Responsibilities"": [""As an Ingest Backend Engineer, you will develop ingest systems to integrate DataRobot with databases, cloud storage systems, SaaS products, and other sources of data, and you will help enhance and maintain these systems as they evolve to meet future connectivity and performance needs. You will contribute to the design of new frameworks to support reading, writing, authentication, failure handling, and more with high performance and reliability. These ingest systems are a key component of the overall architecture of DataRobot.Ideally, a candidate can bring new ideas from concept to implementation, write quality, testable code, and participate in design/development discussions. They also have a can-do attitude and highly value the customer experience above all else, and embrace maintaining and firefighting systems when necessary. The ability and willingness to quickly learn new technologies are essential for this role.""], ""Project description"": [""The Global Data Domain at DataRobot helps make AI incredibly easy to build by surfacing and connecting users to all enterprise data, and enabling data prep at modeling and prediction time. In this role, you\u2019ll help enable our users to easily interact with the data required to train models and generate predictions. Additionally, you\u2019ll help design solutions that foster collaboration amongst distributed teams of engineers, data scientists, and beyond.""]}"
SoftConstruct Ukraine,https://jobs.dou.ua/companies/softconstruct-ukraine/,Data Analyst (Junior Data Engineer),https://jobs.dou.ua/companies/softconstruct-ukraine/vacancies/135282/, Kyiv,16 October 2020,,"SoftConstruct conducts basic and applied research in four key areas: data science, computer vision, big data, real-time processing. Our experience is extremely wide: from working with complex computer and engineering systems, programming for data science — to developing and putting into practice innovative solutions in the field of sports, eSports, and security. Project: Data Platform.Data warehouse that delivers the performance, simplicity, concurrency, and affordability for data collecting, rapid analytics, and extracting data-driven insights for business users.Lambda architecture, designed to handle massive quantities of data by taking advantage of both batch and stream-processing methods. This approach to architecture attempts to balance latency, throughput, and fault-tolerance by using batch processing to provide comprehensive and accurate views of batch data, while simultaneously using real-time stream processing to provide views of online data.In many organizations, data consumption processes for timely reporting or crucial analytical requirements are hindered by both delays in query execution and information presentation. The main challenge is to minimize decision delay with help of flexible solutions with open and simplified architecture and high performance based on future-oriented technologies using AI models and forecasting analytics. In numbers:Up to 1 PB of data from 500+ partners in long-term storage30+ M new records dailyLatency for insight based on data delivery time <5 secTechnologies: SQL, Google BigQuery, Google Colab, Metabase. Requirements:— 1+ years of experience as BI/Data Analyst;— Hands-on experience with data visualization tools;— Solid data visualization principles understanding;— Business analysis: Marketing, Operations, Finance areas— Data analysis: SQL, basic statistics, EDA, anomaly detection, clustering problems etc.— VCS: git, Bitbucket;— OS: Linux. Nice to have:— Domain in Gambling area (SportsBook or casino);— R or Python;— Familiarity with data storage and ETL pipelines;— Hands-on experience with Google Cloud Platform: Cloud Storage, BigQuery etc. Responsibilities:— Data collection and preparation;— Building analytic dashboards (mostly using Metabase, Grafana, Kibana);— Collaboration with the team Backend, Data Engineers, FrontEnd, Data Science, Design;— Creation and support of project documentation;— Corporate DWH and data science workbench integration. We offer:— Interesting and challenging work in a product and data-driven company;— Plenty of opportunities to learn, grow and progress in your career;— We keep work-life balance;— Health insurance;— Free English classes;— Paid vacation 24 days and paid sick leave;— Silent office near Lukyanivska metro station;— Participate in R&D projects.",dou,2020-11-12,Data Analyst (Junior Data Engineer)@SoftConstruct Ukraine,,,{}
Clario,https://jobs.dou.ua/companies/clario/,Product Analyst\Data Analyst,https://jobs.dou.ua/companies/clario/vacancies/131038/, Kyiv,15 October 2020,,"Hi, We are Clario, a consumer-focused cybersecurity company on a mission to change an industry. Over 800 professionals, including 600 digital security experts, with one common goal — supporting everyone’s right to a digital life, secured. We’re here to create a next-generation digital security solution with a human touch. Join us and help people take back control of their digital privacy and security. Clario Team is looking for a Product Analyst for full-time work to help build the Clario brand that aims to become the disruptive global leader in digital privacy and security. Our product, with its intuitive and engaging UX design, has been specifically created to promote better digital safety through enhanced ease of use. It gives real-time protection against digital threats, money loss, breaches of personal information and identity theft, device security, data privacy, and more. It will initially be focused on meeting the needs of Apple customers through apps designed for both macOS and iOS. Later, it plans to support other platforms including Windows and Android.What we’re offering goes beyond software. We combine the latest advances in digital security with tech experts on hand 24/7 to help our customers where technology fails. • Research and find insights based on user behavior patterns• Build automated dashboards for visualizing key metrics and trends to support Product and Marketing functions (MySQL, HIVE, Salesforce, Tableau)• Analyze A/B tests • Communicate with stakeholders highlighting problems and opportunities, suggest actions to be taken based on analysis • Superior analytical and problem-solving abilities with keen attention to details• Self-starter with ability to work in a fast-paced environment and manage tight deadlines• Strong communication and presentation skills• Strong SQL knowledge• Experience of building reports via Tableau• 3+ years of experience working as a product analyst in a product company (IT technical background)• Ability to work with a large amount of data and figures, IT platforms, systems, tools, and databases• Knowledge of Hadoop, Hive, and MapReduce would be an advantage• English level — Upper Intermediate +• University degree in Math, Applied Mathematics, Computer Science or similar beneficial but not mandatory• Knowledge of R / Python desirable but not mandatory We are not just a company, we are Clario! We put the customer at the heart of all that we do, we achieve our best together, take responsibility, and challenge our limits to create a difference! To apply for this position, please send your CV (in English only) with a detailed description of your career, experience, skills, and projects. We guarantee the privacy of any information received",dou,2020-11-12,Product Analyst\Data Analyst@Clario,,,{}
All company jobs,https://jobs.dou.ua/companies/1648-factory/vacancies/,Data Engineer,https://jobs.dou.ua/companies/1648-factory/vacancies/135238/," Lviv, remote",15 October 2020,$3000–3800,"Required skills Qualifications / Skills:— Knowledge of best practices and IT operations in an always-up, always-available service— Excellent problem solving and troubleshooting skills— Process oriented with great documentation skills— Excellent oral and written English communication skills with a keen sense of customer service— Team player with a sense of humor who is characterized by pronounced analytical talent, high performance motivation, curiosity, persuasiveness and customer orientation— Experience with or knowledge of Agile Software Development methodologies is a plus Education, Experience, and Licensing Requirements:— BS or MS degree in Computer Science or a related technical field— 3+ years coding experience in Python/ pySpark— Experienced in Analysis Services.— 2+ years’ experience with Data Visualization tool — Power BI. Other BI tools will be considered a plus (Qlik Sense, Tableau or DOMO)— Experience with Airflow is a plus— 2+ years of experience with schema design and dimensional data modeling— Experience in cloud data platforms — Azure is a plus, developing queries and consuming data from Azure, actively working with the data in PowerBI We offer We Offer— Innovative solutions delivery to the world’s digital changes— Experience exchange with colleagues all around the world— Opportunities for self-realization— Friendly team and enjoyable working environment— Engineering, corporate and social events— Social package: professional & soft skills trainings, medical & family care programs, sports— Flexible working schedule Responsibilities Job Responsibilities:— Collaborates with analytics and business teams to improve data models, increasing data accessibility and fostering data-driven decision making.— Responsible for designing data engineering structures necessary to support BI initiatives— Suggests processes and systems to monitor data quality, ensuring production data is always accurate.— Contributes to engineering wiki, and documents work.— Performs data analysis required to troubleshoot data related issues and assist in the resolution of data issues.— Works closely with a team of frontend and backend engineers, product managers, and analysts. Project description One of Europe’s leading providers of management consulting for financial services providers is currently looking for a Data Engineer to join their strong team. With more than 950 employees, the company offers services in a number of fields, including Strategy & Organization, Finance & Risk and IT.",dou,2020-11-12,Data Engineer@All company jobs,,,"{""Required skills"": [""Qualifications / Skills:"", ""Knowledge of best practices and IT operations in an always-up, always-available service"", ""Excellent problem solving and troubleshooting skills"", ""Process oriented with great documentation skills"", ""Excellent oral and written English communication skills with a keen sense of customer service"", ""Team player with a sense of humor who is characterized by pronounced analytical talent, high performance motivation, curiosity, persuasiveness and customer orientation"", ""Experience with or knowledge of Agile Software Development methodologies is a plus""], ""We offer"": [""Education, Experience, and Licensing Requirements:"", ""BS or MS degree in Computer Science or a related technical field"", ""3+ years coding experience in Python/ pySpark"", ""Experienced in Analysis Services."", ""2+ years\u2019 experience with Data Visualization tool"", ""Power BI. Other BI tools will be considered a plus (Qlik Sense, Tableau or DOMO)"", ""Experience with Airflow is a plus"", ""2+ years of experience with schema design and dimensional data modeling"", ""Experience in cloud data platforms"", ""Azure is a plus, developing queries and consuming data from Azure, actively working with the data in PowerBI""], ""Responsibilities"": [""We Offer"", ""Innovative solutions delivery to the world\u2019s digital changes"", ""Experience exchange with colleagues all around the world"", ""Opportunities for self-realization"", ""Friendly team and enjoyable working environment"", ""Engineering, corporate and social events"", ""Social package: professional & soft skills trainings, medical & family care programs, sports"", ""Flexible working schedule""], ""Project description"": [""Job Responsibilities:"", ""Collaborates with analytics and business teams to improve data models, increasing data accessibility and fostering data-driven decision making."", ""Responsible for designing data engineering structures necessary to support BI initiatives"", ""Suggests processes and systems to monitor data quality, ensuring production data is always accurate."", ""Contributes to engineering wiki, and documents work."", ""Performs data analysis required to troubleshoot data related issues and assist in the resolution of data issues."", ""Works closely with a team of frontend and backend engineers, product managers, and analysts.""]}"
Base B,https://jobs.dou.ua/companies/base-b/,Senior Database Performance Engineer,https://jobs.dou.ua/companies/base-b/vacancies/135223/," Lviv, remote",15 October 2020,$4200–5000,"Required skills — optimize the existing data platform for both performance and reliability— optimize SQL/NoSQL database queries, stored procedures, triggers, user functions, analytic functions, etc— read and Optimize Query Plans We offer Strong Candidates will Have:PythonHasuraPostgreSQLGraphQLBigQueryKubernetesHands-on experience developing APIs and SDKs Responsibilities — experience with both large scale transaction and analytical database technologies— experience with service-oriented architecture and a good understanding of distributed systems, data stores, data modeling, and indexing (especially with Event Sourcing and/or CQRS)— hands-on experience with SQL and large-scale distributed storage and database systems— ability to deal with ambiguity and communicate well with both technical and non-technical teams Project description Our client is a US company, the leader in real estate data and insight. It provides investors, insurers, brokers and other large enterprises with a platform to collect, resolve, and augment real estate data from thousands of public, private, and internal sources. The platform combines transaction and analytical data and makes it available through multiple interfaces and APIs. We need an expert who can improve query performance and contribute towards architectural decisions, taking into consideration the constraints created by the different use cases for data and the complexities introduced by Microservices, containers and container orchestration.",dou,2020-11-12,Senior Database Performance Engineer@Base B,,,"{""Required skills"": [""optimize the existing data platform for both performance and reliability"", ""optimize SQL/NoSQL database queries, stored procedures, triggers, user functions, analytic functions, etc"", ""read and Optimize Query Plans""], ""We offer"": [""Strong Candidates will Have:PythonHasuraPostgreSQLGraphQLBigQueryKubernetesHands-on experience developing APIs and SDKs""], ""Responsibilities"": [""experience with both large scale transaction and analytical database technologies"", ""experience with service-oriented architecture and a good understanding of distributed systems, data stores, data modeling, and indexing (especially with Event Sourcing and/or CQRS)"", ""hands-on experience with SQL and large-scale distributed storage and database systems"", ""ability to deal with ambiguity and communicate well with both technical and non-technical teams""], ""Project description"": [""Our client is a US company, the leader in real estate data and insight. It provides investors, insurers, brokers and other large enterprises with a platform to collect, resolve, and augment real estate data from thousands of public, private, and internal sources.""]}"
Zoral,https://jobs.dou.ua/companies/zoral/,Senior Data Reporting Engineer,https://jobs.dou.ua/companies/zoral/vacancies/116859/, Kyiv,15 October 2020,,"Required skills Zoral, a leading provider of research and development to the software industry, is searching for an experienced Senior Data Reporting Engineer to be part of Zoral software teams delivering Big Data and FinTech (financial technology) solutions to financial and telecom businesses across the world. Required skills:• Strong skills in Python and SQL• Experience in Data Analysis• Experience in communication with English speaking technical and business stakeholders to discuss requirements and design• Understanding of databases and data structures — envision solutions — provide options back for enhancements• Familiarity with AWS services that cater to ETL (I.e. Glue, Batch, Airflow)• Strong documentation skills• Open to working closely to EST hours As a plus • Experience with BI systems: Tableau is preferred; Excel as a BI tool, including integration with external data sources• Experience with different SQL dialects and relation database systems (MS SQL Server, PostgreSQL, etc), including stored procedures and triggers• Cloud experience: Google Cloud Platform (BigQuery, Data Studio, Dataprep, Storage, etc• Unix/Linux experience• R knowledge• Data Mining, Machine Learning experience We offer • Attractive salary and good opportunities for career and personal development• Compensation package• Good opportunities for career and personal development• Office in a picturesque place Responsibilities • Maintain existing reports• Design new reports• Data analysis• Data quality assessment• Close work with business representatives to understand their needs Project description We specialize in advanced software fields such as BI, Data Mining, Artificial Intelligence, Machine Learning (AI/ML), High Speed Computing, Cloud Computing, BIG Data Predictive Analytics, Unstructured Data processing, Finance, Risk Management and Security.We create extensible decision engine services, data analysis and management solutions, real-time automatic data processing applications.We are looking for the software engineers to design, build and implement large, scalable web service architecture with decision engine used as its base. If you are excited about development of artificial intellect, behavior analysis data solutions, big data approach then we can give you an opportunity to reveal your talents.",dou,2020-11-12,Senior Data Reporting Engineer@Zoral,,,"{""Required skills"": [""Zoral, a leading provider of research and development to the software industry, is searching for an experienced Senior Data Reporting Engineer to be part of Zoral software teams delivering Big Data and FinTech (financial technology) solutions to financial and telecom businesses across the world.""], ""As a plus"": [""Required skills:"", ""Strong skills in Python and SQL"", ""Experience in Data Analysis"", ""Experience in communication with English speaking technical and business stakeholders to discuss requirements and design"", ""Understanding of databases and data structures"", ""envision solutions"", ""provide options back for enhancements"", ""Familiarity with AWS services that cater to ETL (I.e. Glue, Batch, Airflow)"", ""Strong documentation skills"", ""Open to working closely to EST hours""], ""We offer"": [""Experience with BI systems: Tableau is preferred; Excel as a BI tool, including integration with external data sources"", ""Experience with different SQL dialects and relation database systems (MS SQL Server, PostgreSQL, etc), including stored procedures and triggers"", ""Cloud experience: Google Cloud Platform (BigQuery, Data Studio, Dataprep, Storage, etc"", ""Unix/Linux experience"", ""R knowledge"", ""Data Mining, Machine Learning experience""], ""Responsibilities"": [""Attractive salary and good opportunities for career and personal development"", ""Compensation package"", ""Good opportunities for career and personal development"", ""Office in a picturesque place""], ""Project description"": [""Maintain existing reports"", ""Design new reports"", ""Data analysis"", ""Data quality assessment"", ""Close work with business representatives to understand their needs""]}"
SoftServe,https://jobs.dou.ua/companies/softserve/,Data Engineer (ID 57539),https://jobs.dou.ua/companies/softserve/vacancies/135209/," Kyiv, remote",15 October 2020,,"WE ARE Our client is a healthcare company with 3000 hospitals around the world. The company initiatives related to web development are in progress. The organization is focusing on replacing outdated products.Currently, the company is in a state of a migration of a legacy system to a new event-driven architecture based on reactive microservices. SoftServe is participating in a program to transform microservices-based solution to use google cloud services YOU ARE An expert with • Experience in Apach Beam or Airflow• Strong SQL skills (stored procedures, functions)• Google Cloud Platform (Spanner) expertise• Knowledge of Rest Services on Spring Boot, which would be an advantage• Strong problem-solving, analytical skills• Detail orientation which is required to perform data analysis and implement solutions according to the requirements• Intermediate+ English level YOU WANT TO WORK WITH • Discovering data relationships and consistency constraints• Cutting-edge technologies for managing data in the large scale enterprise environment TOGETHER WE WILL • Work with a wide range of experts — from top Architects to associates of all profiles and levels in the company• Participate in international project activities• Build strong teams and relations with world-class clients",dou,2020-11-12,Data Engineer (ID 57539)@SoftServe,,,{}
SciForce,https://jobs.dou.ua/companies/sciforce/,Speech Processing Engineer,https://jobs.dou.ua/companies/sciforce/vacancies/44028/," Kharkiv, Lviv",15 October 2020,,"Required skills Research mindset and math-oriented intelligence;Strong technical skills regarding data analysis, statistics and programming;Strong working knowledge of Python;Knowledge of machine learning, deep learning technologies and frameworks;Ability to read, feel, understand and implement innovative ideas and algorithms from journals and papers. As a plus Experience with audio and digital signal processing for speech;Ability to work on an AWS stack;Familiarity with Linux/Unix/Shell environments;Curious about new technologies and passionate about exploring new ways of technology improvement. We offer Transparency in communication between the сompany and an employee;Paid vacation (20 business days) and paid sick leave;Accounting as a service;Flexible working hours;Competitive salary;Friendly working environment;Language classes;Great working conditions and comfortable office. Project description At SciForce we’re building an advanced speech processing platform for language learning. You will be responsible for developing proof-of-concept prototypes for a world-class language-learning product. You will be a part of a cross-functional team focused on product innovations and new technologies.",dou,2020-11-12,Speech Processing Engineer@SciForce,,,"{""Required skills"": [""Research mindset and math-oriented intelligence;Strong technical skills regarding data analysis, statistics and programming;Strong working knowledge of Python;Knowledge of machine learning, deep learning technologies and frameworks;Ability to read, feel, understand and implement innovative ideas and algorithms from journals and papers.""], ""As a plus"": [""Experience with audio and digital signal processing for speech;Ability to work on an AWS stack;Familiarity with Linux/Unix/Shell environments;Curious about new technologies and passionate about exploring new ways of technology improvement.""], ""We offer"": [""Transparency in communication between the \u0441ompany and an employee;Paid vacation (20 business days) and paid sick leave;Accounting as a service;Flexible working hours;Competitive salary;Friendly working environment;Language classes;Great working conditions and comfortable office.""], ""Project description"": [""At SciForce we\u2019re building an advanced speech processing platform for language learning. You will be responsible for developing proof-of-concept prototypes for a world-class language-learning product. You will be a part of a cross-functional team focused on product innovations and new technologies.""]}"
N-iX,https://jobs.dou.ua/companies/n-ix/,Junior BigData Engineer,https://jobs.dou.ua/companies/n-ix/vacancies/135172/, Lviv,15 October 2020,,"We are looking for a Junior BigData Engineer , who will join our engineering team and participate in delivering products with high quality. About Client: Our client is a US company that is specialized in delivering reliable Internet connectivity and entertainment multimedia to aircrafts worldwide, enhancing the experience for both passengers and crew. We are cooperating with the Business Intelligence, Data Analysis and BigData directions and looking for talents who can contribute to the complex data management and analysis projects. We are looking for engineer to join our BigData Operational team and improve his/her AWS BigData skills. Responsibilities: • Develop and support existing end-to-end data AWS based Big Data solutions for structured and unstructured data including, but not limited to, ingestion, parsing, integration, auditing, logging, aggregation, normalization, and error handling• Create jobs and queries to perform auditing and error handling• Develop PySpark and Scala Spark pipelines Requirements: • Experience in Python or Scala• Experience in SQL• Experience in Linux• Good understanding of software development methodologies• Being result-oriented, ability to make things done in a highly dynamic and stressful environment• Good communication skills and good English• Being a good team player Will be a plus: • Apache Spark including an understanding of optimization techniques• Knowledge of Hadoop architecture• Experience in working with AWS (S3, EMR cluster, Lambda, Kinesis, DynamoDB, etc.) We offer: • Flexible working hours• A competitive salary and good compensation package• Possibility of partial remote work• Best hardware• A masseur and a corporate doctor• Healthcare & sport benefits• An inspiring and comfy office Professional growth: • Challenging tasks and innovative projects• Meetups and events for professional development• An individual development plan• Mentorship program Fun: • Corporate events and outstanding parties• Exciting team buildings• Memorable anniversary presents• A fun zone where you can play video games, foosball, ping pong, and more",dou,2020-11-12,Junior BigData Engineer@N-iX,,,{}
Esports Charts,https://jobs.dou.ua/companies/esports-charts/,Data Analyst,https://jobs.dou.ua/companies/esports-charts/vacancies/135161/, Kyiv,15 October 2020,,"Required skills — Опыт аналитики данных от двух лет— Глубокое понимание SQL-подобных баз данных— Опыт с R/Python более года— Сильные аналитические навыки — предстоит анализировать связи и зависимости, искать причины корреляции данных As a plus — Диплом магистра технической специальности или экономической кибернетики— Опыт работы с Jupyter или подобными средами— Опыт с аналитическими и/или ML библиотеками — NumPy, Pandas, Scikit-learn, TensorFlow— Бекграунд в геймдеве и/или аналитике просмотров Responsibilities — Готовить отчёты для партнёров на основе уже готовых запросов в базы данных,— Аппроксимировать необходимые величины,— Дополнять недостающие данных,— Тесно работать с командой разработки над скреппингом новых данных, пониманием уже имеющихся. Project description Esports Charts — украинская ИТ-компания, разрабатывающая сервисы в сфере киберспорта. Мы собираем статистику просмотров всех живых трансляций в мире и создаем на основе этого уникальную аналитику. Несколько фактов о нас:— Предоставляем статистику командам, турнирным агрегаторам и игровым издателям по всей планете— Мы работаем с таблицами до 50 миллиардов строк, наполняя их данными с 29 платформ и делаем это быстро— Большинство детальной статистики о стриминге в интернете появляется в нашем офисе Мы ищем Data аналитика, который поможет нам улучшить нашу работу с данными, сделает наши метрики еще более точными. Cейчас у нас много больших таблиц (>10 миллиардов строк, до 50), которые показывают статистику просмотров онлайн-трансляций, активности в чате и всего, что мы можем собрать. Более того, у нас есть инструменты для быстрой работы с данными, веб-интерфейс для обычных задач. На основе этой статистики мы создаем множество бизнес-метрик, которые затем анализируем. Хотя сейчас мы делаем много прогнозов, мы хотим увеличивать их дальность и точность. Примеры того, что мы хотим выяснить — кто станет более известными в Китае через год — OG или Liquid, насколько популярным будет турнир через полгода, если приглашенные команды нам уже известны.",dou,2020-11-12,Data Analyst@Esports Charts,,,"{""Required skills"": [""\u041e\u043f\u044b\u0442 \u0430\u043d\u0430\u043b\u0438\u0442\u0438\u043a\u0438 \u0434\u0430\u043d\u043d\u044b\u0445 \u043e\u0442 \u0434\u0432\u0443\u0445 \u043b\u0435\u0442"", ""\u0413\u043b\u0443\u0431\u043e\u043a\u043e\u0435 \u043f\u043e\u043d\u0438\u043c\u0430\u043d\u0438\u0435 SQL-\u043f\u043e\u0434\u043e\u0431\u043d\u044b\u0445 \u0431\u0430\u0437 \u0434\u0430\u043d\u043d\u044b\u0445"", ""\u041e\u043f\u044b\u0442 \u0441 R/Python \u0431\u043e\u043b\u0435\u0435 \u0433\u043e\u0434\u0430"", ""\u0421\u0438\u043b\u044c\u043d\u044b\u0435 \u0430\u043d\u0430\u043b\u0438\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u043d\u0430\u0432\u044b\u043a\u0438"", ""\u043f\u0440\u0435\u0434\u0441\u0442\u043e\u0438\u0442 \u0430\u043d\u0430\u043b\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0441\u0432\u044f\u0437\u0438 \u0438 \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u0438, \u0438\u0441\u043a\u0430\u0442\u044c \u043f\u0440\u0438\u0447\u0438\u043d\u044b \u043a\u043e\u0440\u0440\u0435\u043b\u044f\u0446\u0438\u0438 \u0434\u0430\u043d\u043d\u044b\u0445""], ""As a plus"": [""\u0414\u0438\u043f\u043b\u043e\u043c \u043c\u0430\u0433\u0438\u0441\u0442\u0440\u0430 \u0442\u0435\u0445\u043d\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u0441\u043f\u0435\u0446\u0438\u0430\u043b\u044c\u043d\u043e\u0441\u0442\u0438 \u0438\u043b\u0438 \u044d\u043a\u043e\u043d\u043e\u043c\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u043a\u0438\u0431\u0435\u0440\u043d\u0435\u0442\u0438\u043a\u0438"", ""\u041e\u043f\u044b\u0442 \u0440\u0430\u0431\u043e\u0442\u044b \u0441 Jupyter \u0438\u043b\u0438 \u043f\u043e\u0434\u043e\u0431\u043d\u044b\u043c\u0438 \u0441\u0440\u0435\u0434\u0430\u043c\u0438"", ""\u041e\u043f\u044b\u0442 \u0441 \u0430\u043d\u0430\u043b\u0438\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u043c\u0438 \u0438/\u0438\u043b\u0438 ML \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0430\u043c\u0438"", ""NumPy, Pandas, Scikit-learn, TensorFlow"", ""\u0411\u0435\u043a\u0433\u0440\u0430\u0443\u043d\u0434 \u0432 \u0433\u0435\u0439\u043c\u0434\u0435\u0432\u0435 \u0438/\u0438\u043b\u0438 \u0430\u043d\u0430\u043b\u0438\u0442\u0438\u043a\u0435 \u043f\u0440\u043e\u0441\u043c\u043e\u0442\u0440\u043e\u0432""], ""Responsibilities"": [""\u0413\u043e\u0442\u043e\u0432\u0438\u0442\u044c \u043e\u0442\u0447\u0451\u0442\u044b \u0434\u043b\u044f \u043f\u0430\u0440\u0442\u043d\u0451\u0440\u043e\u0432 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u0443\u0436\u0435 \u0433\u043e\u0442\u043e\u0432\u044b\u0445 \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u0432 \u0432 \u0431\u0430\u0437\u044b \u0434\u0430\u043d\u043d\u044b\u0445,"", ""\u0410\u043f\u043f\u0440\u043e\u043a\u0441\u0438\u043c\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u044b\u0435 \u0432\u0435\u043b\u0438\u0447\u0438\u043d\u044b,"", ""\u0414\u043e\u043f\u043e\u043b\u043d\u044f\u0442\u044c \u043d\u0435\u0434\u043e\u0441\u0442\u0430\u044e\u0449\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0445,"", ""\u0422\u0435\u0441\u043d\u043e \u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c \u0441 \u043a\u043e\u043c\u0430\u043d\u0434\u043e\u0439 \u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u043d\u0430\u0434 \u0441\u043a\u0440\u0435\u043f\u043f\u0438\u043d\u0433\u043e\u043c \u043d\u043e\u0432\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445, \u043f\u043e\u043d\u0438\u043c\u0430\u043d\u0438\u0435\u043c \u0443\u0436\u0435 \u0438\u043c\u0435\u044e\u0449\u0438\u0445\u0441\u044f.""], ""Project description"": [""Esports Charts"", ""\u0443\u043a\u0440\u0430\u0438\u043d\u0441\u043a\u0430\u044f \u0418\u0422-\u043a\u043e\u043c\u043f\u0430\u043d\u0438\u044f, \u0440\u0430\u0437\u0440\u0430\u0431\u0430\u0442\u044b\u0432\u0430\u044e\u0449\u0430\u044f \u0441\u0435\u0440\u0432\u0438\u0441\u044b \u0432 \u0441\u0444\u0435\u0440\u0435 \u043a\u0438\u0431\u0435\u0440\u0441\u043f\u043e\u0440\u0442\u0430.""]}"
Allset,https://jobs.dou.ua/companies/allset/,Data Entry Specialist,https://jobs.dou.ua/companies/allset/vacancies/103574/, Kyiv,15 October 2020,,"We’re looking for talented and enthusiastic individuals who want to learn and grow with the company! The Data Entry Specialist is responsible for information input, verification of information input, and the visual clarity of product images. Qualifications— At least Upper-Intermediate English level;— Typing speed and accuracy;— Excellent knowledge of correct spelling, grammar, and punctuation;— Attention to details;— Familiarity with spreadsheets and online forms;— Ability to problem-solve and utilize tools and resources;— Passionate about technology and startup culture;— Passionate about food and restaurants;— Willingness to take flexible shifts. Nice to have— Data entry work experience at least 1 year;— Knowledgeable about different types of cuisines;— Basic literacy in Microsoft Office. Desirable— Additional computer training or certification will be an asset;— IELTS Certificate (International English Language Testing System) 6 or higher. Responsibilities— Data entry;— Menu uploading;— Review, editing, and upload of quality photos of menu items;— Quality assurance of uploaded content;— Phone calls to restaurants. We offer— Great office in the heart of Kyiv;— Working in the successful US startup backed by top investors;— Stock Option Plan;— Paid lunches at restaurants;— Gym memberships;— Educational possibilities;— Compensation of English Classes;— Salary review. Allset (www.allsetnow.com) is the leading platform for restaurant reservations & preordering available at 2,000+ restaurants in 12 major cities across the US. The company is headquartered in San Francisco and was founded by a team of technology experts and forward thinkers with a focus on improving the restaurant dining experience. Currently available in NYC, LA, SF, Chicago, Houston, Boston, Seattle, Austin, San Jose, Las Vegas, and coming to more cities soon. Find out what Forbes, TechCrunch, Business Insider, and more have been writing about Allset.",dou,2020-11-12,Data Entry Specialist@Allset,,,{}
Ciklum,https://jobs.dou.ua/companies/ciklum/,Data Architect for Ciklum Digital,https://jobs.dou.ua/companies/ciklum/vacancies/135113/," Kyiv, Lviv, Odesa",15 October 2020,,"On behalf of Ciklum Digital, Ciklum is looking for a Data Architect to join our Ukrainian team (Kyiv, Lviv, Odesa) on full-time basis to create innovative solutions that exceed the needs of our customers. About the Project:Data architecture redesign and optimisation for Product Availability service in a world leader travelling company. AWS Aurora is used as CQRS storage for micro-services applications, which make reservations, check product availability and prices for company’s services. The goal of the project would be to provide series of PoC in order to understand performance bottlenecks in current architecture, create base line and redesign the architecture, create a performance monitoring tool and implement improvements.",dou,2020-11-12,Data Architect for Ciklum Digital@Ciklum,,,{}
Fractal Analytics,https://jobs.dou.ua/companies/fractal-analytics/,ML Engineer (Azure),https://jobs.dou.ua/companies/fractal-analytics/vacancies/135106/, Kyiv,15 October 2020,,"Overview: Fractal is looking for people who are passionate around solving business problems through innovation and engineering practices. You’ll be required to apply your depth of knowledge and expertise to all aspects of the analytical problem solution lifecycle, as well as partner continuously with your many stakeholders on a daily basis to stay focused on common goals. We embrace a culture of experimentation and constantly strive for improvement and learning. You’ll work in a collaborative, trusting, thought-provoking environment-one that encourages diversity of thought and creative solutions that are in the best interests of our customers globally. As an ML Engineer, you will work collaboratively with Data Scientists and Data engineers to deploy and operate systems. You’ll help automate and streamline our operations and processes. You’ll build and maintain tools for deployment, monitoring, and operations. You’ll also troubleshoot and resolve issues in development, testing, and production environments. Responsibilities:• Operate and maintain systems supporting the provisioning of new clients, applications, and features.• Day-to-day monitoring of the Production service delivery environment to ensure all services and applications are operating optimally and SLAs are met.• Software deployment and configuration management in both QA and Production environments.• Collaborate with Data Scientists and Data Engineers on feature development teams to containerize and build out deployment pipelines for new modules• Design, build and optimize applications’ containerization and orchestration with Docker and Kubernetes and AWS or Azure• Automate applications and infrastructure deployments. • Produce build and deployment automation scripts to integrate between services• Be a subject matter expert on DevOps practices, CI/CD and Configuration Management with assigned engineering team• Experience with cloud computing platforms: Google Cloud, Amazon Web Service, Azure, Kubernetes. • Experience in Azure cloud services (especially Azure Devops) and deploying production code in the same • Experience in MLFlow, Qubeflow, MLTracking, MLExperiments• Experience in big data technologies preferred: Hadoop, Hive, Spark, Kafka. • Knowledge of machine learning frameworks: Tensorflow, Caffe/Caffe2, Pytorch, Keras, MXNet, Scikit-Learn. Skills:• At least 3 years’ experience working with cloud-base services and DevOps concepts, tools and practices• Extensive experience with Unix/AIX/Linux environments• Experience with Kubernetes or Docker Swarm• Experience working in cross-functional Agile engineering teams• Expertise in standard concepts and technologies used in CI/CD build, deployment pipelines• Experience with scripting and coding using Python, Shell• Experience in Azure ecosystem — Azure data bricks, Azure Devops and Azure ML Services would be preferred.• Experience with configuration using tools such as Chef, Ansible• Experience with automation servers such as Jenkins, CloudBees, Travis• Experience with logging tools such as Splunk, ElasticSearch, Kibana, Logstash• Experience with monitoring tools such as Munin, Prometheus, Grafana, AlertManager, PagerDuty• Big data technical stack experience is a plus such as HDFS, Spark, Ambari, ZooKeeper, Kafka• Excellent Written and Verbal Communication Skills• Ability to collaborate effectively with highly technical resources in a fast-paced environment• Ability to solve complex challenges/problems and rapidly deliver innovative solutions We offer:Best team: multicultural team of bright specialists and friendly, helping people (data scientists, ML/AI specialists, data engineers, developers, business analysts, QA team, as well as internal support staff);Challenge: plenty of complex and exciting projects from international clients;Income: competitive salary and end-year bonuses;Travelling: possibility to travel onshore and clients’ side;Learning and professional development: access to company learning platforms with free courses, external certifications and learning programs;Being healthy: free insurance after probation period; paid vacation;Communication: company parties, celebrations, workshops in different locations, cross-locations and cross-projects exchange programs.",dou,2020-11-12,ML Engineer (Azure)@Fractal Analytics,,,{}
Windsor.ai,https://jobs.dou.ua/companies/windsor-ai/,"Sales and account manager, Web Data Analytics, SAAS",https://jobs.dou.ua/companies/windsor-ai/vacancies/135104/," Kyiv, remote",14 October 2020,$1500–3500,Required skills -SAAS-Sales-Analytics-Data Studio-SQL As a plus University DegreeSAAS Experience We offer The opportunity to be part of a fast-growing high-performing team and shape the industry. Flexible working hoursBonus schemeProfit sharing/share optionsAdditional holidaysStock options Responsibilities Sales and marketingSetup sales meetings and demo’s Project description We are a SAAS analytics and data company. We are looking for a sales and account manager to increase sales for our company. You would have experience in:-web-analytics -Sales You like data and analytics and are a high performer who can help us grow in the US.,dou,2020-11-12,"Sales and account manager, Web Data Analytics, SAAS@Windsor.ai",,,"{""Required skills"": [""-SAAS-Sales-Analytics-Data Studio-SQL""], ""As a plus"": [""University DegreeSAAS Experience""], ""We offer"": [""The opportunity to be part of a fast-growing high-performing team and shape the industry.""], ""Responsibilities"": [""Flexible working hoursBonus schemeProfit sharing/share optionsAdditional holidaysStock options""], ""Project description"": [""Sales and marketingSetup sales meetings and demo\u2019s""]}"
N-iX,https://jobs.dou.ua/companies/n-ix/,Lead Azure Data Engineer,https://jobs.dou.ua/companies/n-ix/vacancies/135084/," Kyiv, Lviv, remote",14 October 2020,,"Seeking a Lead Azure Data Engineer that has proven Azure Cloud skills based around the design and development of multiple data pipelines going between legacy on premise and cloud environments. They will leverage their coding skills to enable multiple sources to move data through an Azure Data Lake Storage Gen2, Azure Synapse Analytics environments using tools such as Talend cloud and more. Responsibilities:• Uses Talend cloud to create multiple complex pipelines and activities using both Azure and On-Prem data stores for full and incremental data loads into a Azure Synapse Analytics• Establishes data pipelines with automated batch, micro-batch, and incremental data refreshes• Migrates on premise applications/solutions into the cloud • Provisions, deploys, and scales Azure resources• Creates technical design documentation in adherence to business, source system, target system, and solution architecture requirements• Leads a team Requirements:• 5+ years recent experience developing and implementing Data cloud-based solutions in a Microsoft Azure environment• Experience managing Azure Data Lakes (ADLS) and Azure Synapse Analytics and an understanding of how to integrate with other Azure Services.• Experience with Python or Spark• Experience with JSON, API calls• Experience with Data warehousing technologies such Azure Synapse Analytics/Azure SQL DW, or Redshift/Snowflake• Experience using SQL, T-SQL• Experience with Azure DevOps, Azure DevOps GitEx Nice to have:Experience with message brokers such as Event Hubs or KafkaExperience with Azure Stream Analytics, Azure Functions, Azure Data Factory We offer:• ​Flexible working hours​• A competitive salary and good compensation package​• Best hardware​• A masseur and a corporate doctor​• Healthcare & sport benefits​• An inspiring and comfy office Professional growth:• Challenging tasks and innovative projects​• Meetups and events for professional development​• An individual development plan​• Mentorship program Fun:​• Corporate events and outstanding parties​• Exciting team buildings​• Memorable anniversary presents​• A fun zone where you can play video games, foosball, ping pong, and more",dou,2020-11-12,Lead Azure Data Engineer@N-iX,,,{}
VisiQuate,https://jobs.dou.ua/companies/visiquate/,Business Data Analyst,https://jobs.dou.ua/companies/visiquate/vacancies/122236/, Kharkiv,14 October 2020,,"Required skills • At least Upper-intermediate level of English;• Able to create high value and high readability requirements specification;• Able to support development team with domain knowledge;• Demonstrate the ability to compile functional and non-functional requirements;• Experienced in working on data warehouse/BI projects.• Knowledge in SQL/Excel/Access to be able to analyze data;• UX/Usability principles understanding;• Knowledge of Agile/Scrum;• EDI format knowledge; As a plus • US Healthcare finance knowledge. We offer • Comfortable office in the city center;• 24 calendar days of paid vacation, paid sick leaves;• Holidays per Ukrainian calendar;• Flexible work schedule and a friendly atmosphere in the office;• Free English classes. Responsibilities • Drive the process from idea to implementation;• Requirements elicitation/clarification;• Write specifications;• Update system documentation;• Clarifying priorities;• Communicating status and blockers;• Tight collaboration with the development and design teams;• Organizing and performing grooming sessions, supporting planning meetings;• Performing acceptance testing — ensuring the provided functionality satisfies customer needs/requirements;• Making recommended go/no go decision. Project description We’re a fast growing California-based Big Data company that is focused on disrupting business intelligence and analytics as currently defined. Our solutions make a difference because we’re different. Instead of typical BI that delivers static dashboards based on lagging indicators, we give our clients streaming intelligence. Your job will be to help design our vision of delivering Insights as a Service, Prediction as a Service, even Prevention as a Service: All the things that let us describe ourselves as the first SaaS2 company.",dou,2020-11-12,Business Data Analyst@VisiQuate,,,"{""Required skills"": [""At least Upper-intermediate level of English;"", ""Able to create high value and high readability requirements specification;"", ""Able to support development team with domain knowledge;"", ""Demonstrate the ability to compile functional and non-functional requirements;"", ""Experienced in working on data warehouse/BI projects."", ""Knowledge in SQL/Excel/Access to be able to analyze data;"", ""UX/Usability principles understanding;"", ""Knowledge of Agile/Scrum;"", ""EDI format knowledge;""], ""As a plus"": [""US Healthcare finance knowledge.""], ""We offer"": [""Comfortable office in the city center;"", ""24 calendar days of paid vacation, paid sick leaves;"", ""Holidays per Ukrainian calendar;"", ""Flexible work schedule and a friendly atmosphere in the office;"", ""Free English classes.""], ""Responsibilities"": [""Drive the process from idea to implementation;"", ""Requirements elicitation/clarification;"", ""Write specifications;"", ""Update system documentation;"", ""Clarifying priorities;"", ""Communicating status and blockers;"", ""Tight collaboration with the development and design teams;"", ""Organizing and performing grooming sessions, supporting planning meetings;"", ""Performing acceptance testing"", ""ensuring the provided functionality satisfies customer needs/requirements;"", ""Making recommended go/no go decision.""], ""Project description"": [""We\u2019re a fast growing California-based Big Data company that is focused on disrupting business intelligence and analytics as currently defined. Our solutions make a difference because we\u2019re different.""]}"
Proxet,https://jobs.dou.ua/companies/proxet/,Data Analyst for the healthcare domain,https://jobs.dou.ua/companies/proxet/vacancies/135049/, remote,13 October 2020,,"Required skills — Experience with SQL databases— Experience working on EDA with Python— Experience in data models and reporting packages;— Ability to analyze large datasets— Ability to write comprehensive reports;— Strong verbal and written communication skills;— An analytical mind and inclination for problem-solving;— Attention to detail;— Statistical Methods, Predictive Modeling (Time series, Linear regression & Other methods) As a plus — Experience with Pandas, Plotly Dash— Experience with building small ETL processes— Experience with some AWS stack tools: S3, Redshift We offer — Challenging work in an international professional environment— Mastering English language with a native speaker— 40-hour working week with flexible working hours— Flexible work-from-home policy— Competitive salary — PE accounting and support— 20 paid vacation days per year— 14 paid sick leaves per year— Medical insurance— Annual 250$ deposit for attending external events (conferences, workshops, etc.)— Long-term employment and real opportunities to change roles and projects within the company— Yoga classes, workout corner— Collaborative friendly team environment— Cozy fully equipped office space in the city center (near “Palats Ukraina” subway station) Responsibilities — Collecting, analyzing, and interpreting existing data;— Help designing questions and answers to be able to extract data: Which type of treatment works better? — — Are the patients getting better or worse?, etc— Reporting the results back to the other business stakeholders— Working alongside teams within the business or the management team to establish business needs;— Defining new data collection and analysis processes; Project description mindyra.com is a behavioral health technology and data analytics company that was developed with one thought in mind: create a streamlined system that can be used to effortlessly, accurately, and systematically move through the phases of mental healthcare; from diagnostic assessment through tracking treatment progress. We are developing the next generation of digital health platform that automates & streamlines workflows for providers; uses AI to more precisely diagnose and coordinate care for individuals affected by emotional and substance abuse disorders. We are building a secure, HIPAA/HITECH compliant platform that includes all of the tools necessary to streamline the mental healthcare process both for patients and providers. At the core of the system is focus on measurement and data collection.",dou,2020-11-12,Data Analyst for the healthcare domain@Proxet,,,"{""Required skills"": [""Experience with SQL databases"", ""Experience working on EDA with Python"", ""Experience in data models and reporting packages;"", ""Ability to analyze large datasets"", ""Ability to write comprehensive reports;"", ""Strong verbal and written communication skills;"", ""An analytical mind and inclination for problem-solving;"", ""Attention to detail;"", ""Statistical Methods, Predictive Modeling (Time series, Linear regression & Other methods)""], ""As a plus"": [""Experience with Pandas, Plotly Dash"", ""Experience with building small ETL processes"", ""Experience with some AWS stack tools: S3, Redshift""], ""We offer"": [""Challenging work in an international professional environment"", ""Mastering English language with a native speaker"", ""40-hour working week with flexible working hours"", ""Flexible work-from-home policy"", ""Competitive salary"", ""PE accounting and support"", ""20 paid vacation days per year"", ""14 paid sick leaves per year"", ""Medical insurance"", ""Annual 250$ deposit for attending external events (conferences, workshops, etc.)"", ""Long-term employment and real opportunities to change roles and projects within the company"", ""Yoga classes, workout corner"", ""Collaborative friendly team environment"", ""Cozy fully equipped office space in the city center (near \u201cPalats Ukraina\u201d subway station)""], ""Responsibilities"": [""Collecting, analyzing, and interpreting existing data;"", ""Help designing questions and answers to be able to extract data: Which type of treatment works better?"", ""Are the patients getting better or worse?, etc"", ""Reporting the results back to the other business stakeholders"", ""Working alongside teams within the business or the management team to establish business needs;"", ""Defining new data collection and analysis processes;""], ""Project description"": [""mindyra.com is a behavioral health technology and data analytics company that was developed with one thought in mind: create a streamlined system that can be used to effortlessly, accurately, and systematically move through the phases of mental healthcare; from diagnostic assessment through tracking treatment progress. We are developing the next generation of digital health platform that automates & streamlines workflows for providers; uses AI to more precisely diagnose and coordinate care for individuals affected by emotional and substance abuse disorders. We are building a secure, HIPAA/HITECH compliant platform that includes all of the tools necessary to streamline the mental healthcare process both for patients and providers. At the core of the system is focus on measurement and data collection.""]}"
Xenoss,https://jobs.dou.ua/companies/xenoss/,Data Scientist / ML Engineer // DTS,https://jobs.dou.ua/companies/xenoss/vacancies/135044/," Kyiv, Kharkiv, Lviv, Dnipro, Odesa, Vinnytsia, Rivne, Uzhhorod, Chernivtsi, remote",13 October 2020,,"Required skills — Degree in applied math, computer science, or equivalent. — 5+ year experience with a SQL-like query language, Python, and Spark— Good communicator who can explain and understand complex problems while dealing with both non- technical and technical teams.— Ability to query large amounts of data, build models, and derive insights for customers.— Ability to structure and solve difficult problems with minimal supervision.— A focus on details and willingness to learn.— Good verbal and written English communication skills (Intermediate level or higher). Do not hesitate to apply if you are missing some specific experience. We will be happy to help you with gaining one As a plus — Experience in online mobile advertising is a plus.— Experience with Zeppelin and AWS Redshift is an advantage. We offer — Great experience and professional growth by working on challenging projects— Flexible working hours— Unlimited work-from-home option— Various snacks, tea, coffee, and fresh fruits in the office— Paid vacation and sick-period Responsibilities The role is very hands-on, and If you are keen to join an early-stage startup within the ad-tech industry and be part of a fantastic team then this is the opportunity for you. Project description ML model building using cutting-edge algorithms (Deep Learning, XGBoost, GBM, etc.) from a proof of concept to productionised models, monitoring the performance of live models bidding in real-time, and producing insight for customers (understand what drives installs/retention behaviours).Continuous research and improvements on productionised models, including ML feature engineering and model training tuning to reflect changes in users’ behaviours and market dynamics.Collaborating closely with the commercial team to identify, develop, and optimise business opportunities. Generating insight from campaign data using Python, Tableau, and other technologies to satisfy customers’ needs.Reviewing, monitoring, and optimising running campaigns and integrations providing advice and troubleshooting for customers.",dou,2020-11-12,Data Scientist / ML Engineer // DTS@Xenoss,,,"{""Required skills"": [""Degree in applied math, computer science, or equivalent."", ""5+ year experience with a SQL-like query language, Python, and Spark"", ""Good communicator who can explain and understand complex problems while dealing with both non- technical and technical teams."", ""Ability to query large amounts of data, build models, and derive insights for customers."", ""Ability to structure and solve difficult problems with minimal supervision."", ""A focus on details and willingness to learn."", ""Good verbal and written English communication skills (Intermediate level or higher).""], ""As a plus"": [""Do not hesitate to apply if you are missing some specific experience. We will be happy to help you with gaining one""], ""We offer"": [""Experience in online mobile advertising is a plus."", ""Experience with Zeppelin and AWS Redshift is an advantage.""], ""Responsibilities"": [""Great experience and professional growth by working on challenging projects"", ""Flexible working hours"", ""Unlimited work-from-home option"", ""Various snacks, tea, coffee, and fresh fruits in the office"", ""Paid vacation and sick-period""], ""Project description"": [""The role is very hands-on, and If you are keen to join an early-stage startup within the ad-tech industry and be part of a fantastic team then this is the opportunity for you.""]}"
MWDN,https://jobs.dou.ua/companies/mwdn/,Senior Data Scientist,https://jobs.dou.ua/companies/mwdn/vacancies/135027/, remote,13 October 2020,,"Required skills Programming experience in Python and .NETExperience in machine learning frameworks such as Pandas, skicit-learn, TensorFlow or Keras2-3 years experience with NLP (Natural Language Processing). Real experience example with NLP projectProficiency with MongoDB and MSSQLFamiliarity with LinuxFamiliarity with Anaconda; RabitMQ; Jupyter NotebookKnowledgeable of core CS concepts such as common data structures and algorithmsExperience with Container-ecosystem (DOCKER, JENKINS)Experience with cloud platforms AWS (S3. ECS, EC2 etс.)English Upper-intermediate As a plus Able to work independently and self-identify tasksAbility to review and maintain existing code and applicationsAbility to research and evaluate new concepts and processes to improve performance We offer People-oriented management without bureaucracyFriendly climate inside the company — previous employees have come back oftenFlexible working hours100% paid sick leavePaid participating in the sports eventsEducational budget Responsibilities Translate business requirements into technical requirements and implement themWork in sprints, know JiraGood documentation skills, organized Project description CaboodleAI is a new must-have software suite for all organisations across any sector that want to generate content for their audience, without the need for employing more staff. They offers any website the opportunity to embed news pages, newsletters, social media and industry directories (all driven by our snazzy Artificial Intelligence wizard) and it will revolutionise business. Imagine not having to employ journalists or marketing executives to source relevant content, imagine being able to become the definitive resource for news in your sector, without hiring more staff, imagine being able to enhance your businesses revenue by offering advertising opportunities for your clients. Imagine all this news being driven by the finest AI engine created!",dou,2020-11-12,Senior Data Scientist@MWDN,,,"{""Required skills"": [""Programming experience in Python and .NETExperience in machine learning frameworks such as Pandas, skicit-learn, TensorFlow or Keras2-3 years experience with NLP (Natural Language Processing). Real experience example with NLP projectProficiency with MongoDB and MSSQLFamiliarity with LinuxFamiliarity with Anaconda; RabitMQ; Jupyter NotebookKnowledgeable of core CS concepts such as common data structures and algorithmsExperience with Container-ecosystem (DOCKER, JENKINS)Experience with cloud platforms AWS (S3. ECS, EC2 et\u0441.)English Upper-intermediate""], ""As a plus"": [""Able to work independently and self-identify tasksAbility to review and maintain existing code and applicationsAbility to research and evaluate new concepts and processes to improve performance""], ""We offer"": [""People-oriented management without bureaucracyFriendly climate inside the company"", ""previous employees have come back oftenFlexible working hours100% paid sick leavePaid participating in the sports eventsEducational budget""], ""Responsibilities"": [""Translate business requirements into technical requirements and implement themWork in sprints, know JiraGood documentation skills, organized""], ""Project description"": [""CaboodleAI is a new must-have software suite for all organisations across any sector that want to generate content for their audience, without the need for employing more staff.""]}"
SWEET.TV,https://jobs.dou.ua/companies/trinity-1/,Data Scientist,https://jobs.dou.ua/companies/trinity-1/vacancies/134980/," Kyiv, Mariupol, remote",13 October 2020,от $3000,"Required skills — математическое высшее образование (системный анализ, экономическая кибернетика, финансы, механико-математический);— опыт работы в data science от 2 лет, знание Python (Pandas, Numpy, Scipy, Scikit-learn, Flask)— глубокие знания принципов и основных алгоритмов машинного обучения, оценок качества моделей;— опыт проведения полного цикла построения модели: подготовка данных, определение значимых параметров, сравнение алгоритмов, оценка точности результата, применение на практике для бизнес-целей. As a plus — навыки работы с базами данных (SQL Server), в том числе способность писать и оптимизировать сложные SQL запросы; We offer -достойная заработная плата;-работа в комфортном офисе;-крутая команда и дружественная атмосфера;-сложные и интересные задачи;-возможность расширения перечня своих профессиональных навыков. Responsibilities — исследование, проектирование, внедрение и развертывание решений для анализа данных и машинного обучения;— построение моделей для предсказания, ключевых событий в жизни пользователя на сайте;— сегментация пользовательской базы;— подготовка данных для построения моделей;— оценка точности модели и анализ эффективности внедрения.— кластеризация аудитории для персонализации рекламы; Project description SWEET.TV — новое телевидение для всей страны! Наше неоспоримое преимущество — собственные разработки, умение работать в высококонкурентной среде. Работаем гибко и с максимальной отдачей, сосредоточены на предоставлении качественного телевидения клиентам в любой точке на карте. Мы растем и развиваемся, именно поэтому ищем в команду активных, амбициозных и увлеченных людей!",dou,2020-11-12,Data Scientist@SWEET.TV,,,"{""Required skills"": [""\u043c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u0432\u044b\u0441\u0448\u0435\u0435 \u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u0435 (\u0441\u0438\u0441\u0442\u0435\u043c\u043d\u044b\u0439 \u0430\u043d\u0430\u043b\u0438\u0437, \u044d\u043a\u043e\u043d\u043e\u043c\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u043a\u0438\u0431\u0435\u0440\u043d\u0435\u0442\u0438\u043a\u0430, \u0444\u0438\u043d\u0430\u043d\u0441\u044b, \u043c\u0435\u0445\u0430\u043d\u0438\u043a\u043e-\u043c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0439);"", ""\u043e\u043f\u044b\u0442 \u0440\u0430\u0431\u043e\u0442\u044b \u0432 data science \u043e\u0442 2 \u043b\u0435\u0442, \u0437\u043d\u0430\u043d\u0438\u0435 Python (Pandas, Numpy, Scipy, Scikit-learn, Flask)"", ""\u0433\u043b\u0443\u0431\u043e\u043a\u0438\u0435 \u0437\u043d\u0430\u043d\u0438\u044f \u043f\u0440\u0438\u043d\u0446\u0438\u043f\u043e\u0432 \u0438 \u043e\u0441\u043d\u043e\u0432\u043d\u044b\u0445 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u043e\u0432 \u043c\u0430\u0448\u0438\u043d\u043d\u043e\u0433\u043e \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f, \u043e\u0446\u0435\u043d\u043e\u043a \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430 \u043c\u043e\u0434\u0435\u043b\u0435\u0439;"", ""\u043e\u043f\u044b\u0442 \u043f\u0440\u043e\u0432\u0435\u0434\u0435\u043d\u0438\u044f \u043f\u043e\u043b\u043d\u043e\u0433\u043e \u0446\u0438\u043a\u043b\u0430 \u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0438: \u043f\u043e\u0434\u0433\u043e\u0442\u043e\u0432\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445, \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0437\u043d\u0430\u0447\u0438\u043c\u044b\u0445 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432, \u0441\u0440\u0430\u0432\u043d\u0435\u043d\u0438\u0435 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u043e\u0432, \u043e\u0446\u0435\u043d\u043a\u0430 \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u0438 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0430, \u043f\u0440\u0438\u043c\u0435\u043d\u0435\u043d\u0438\u0435 \u043d\u0430 \u043f\u0440\u0430\u043a\u0442\u0438\u043a\u0435 \u0434\u043b\u044f \u0431\u0438\u0437\u043d\u0435\u0441-\u0446\u0435\u043b\u0435\u0439.""], ""As a plus"": [""\u043d\u0430\u0432\u044b\u043a\u0438 \u0440\u0430\u0431\u043e\u0442\u044b \u0441 \u0431\u0430\u0437\u0430\u043c\u0438 \u0434\u0430\u043d\u043d\u044b\u0445 (SQL Server), \u0432 \u0442\u043e\u043c \u0447\u0438\u0441\u043b\u0435 \u0441\u043f\u043e\u0441\u043e\u0431\u043d\u043e\u0441\u0442\u044c \u043f\u0438\u0441\u0430\u0442\u044c \u0438 \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0441\u043b\u043e\u0436\u043d\u044b\u0435 SQL \u0437\u0430\u043f\u0440\u043e\u0441\u044b;""], ""We offer"": [""-\u0434\u043e\u0441\u0442\u043e\u0439\u043d\u0430\u044f \u0437\u0430\u0440\u0430\u0431\u043e\u0442\u043d\u0430\u044f \u043f\u043b\u0430\u0442\u0430;-\u0440\u0430\u0431\u043e\u0442\u0430 \u0432 \u043a\u043e\u043c\u0444\u043e\u0440\u0442\u043d\u043e\u043c \u043e\u0444\u0438\u0441\u0435;-\u043a\u0440\u0443\u0442\u0430\u044f \u043a\u043e\u043c\u0430\u043d\u0434\u0430 \u0438 \u0434\u0440\u0443\u0436\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u0430\u044f \u0430\u0442\u043c\u043e\u0441\u0444\u0435\u0440\u0430;-\u0441\u043b\u043e\u0436\u043d\u044b\u0435 \u0438 \u0438\u043d\u0442\u0435\u0440\u0435\u0441\u043d\u044b\u0435 \u0437\u0430\u0434\u0430\u0447\u0438;-\u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e\u0441\u0442\u044c \u0440\u0430\u0441\u0448\u0438\u0440\u0435\u043d\u0438\u044f \u043f\u0435\u0440\u0435\u0447\u043d\u044f \u0441\u0432\u043e\u0438\u0445 \u043f\u0440\u043e\u0444\u0435\u0441\u0441\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u044b\u0445 \u043d\u0430\u0432\u044b\u043a\u043e\u0432.""], ""Responsibilities"": [""\u0438\u0441\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u043d\u0438\u0435, \u043f\u0440\u043e\u0435\u043a\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435, \u0432\u043d\u0435\u0434\u0440\u0435\u043d\u0438\u0435 \u0438 \u0440\u0430\u0437\u0432\u0435\u0440\u0442\u044b\u0432\u0430\u043d\u0438\u0435 \u0440\u0435\u0448\u0435\u043d\u0438\u0439 \u0434\u043b\u044f \u0430\u043d\u0430\u043b\u0438\u0437\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 \u0438 \u043c\u0430\u0448\u0438\u043d\u043d\u043e\u0433\u043e \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f;"", ""\u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u0434\u043b\u044f \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f, \u043a\u043b\u044e\u0447\u0435\u0432\u044b\u0445 \u0441\u043e\u0431\u044b\u0442\u0438\u0439 \u0432 \u0436\u0438\u0437\u043d\u0438 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f \u043d\u0430 \u0441\u0430\u0439\u0442\u0435;"", ""\u0441\u0435\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u044f \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u0441\u043a\u043e\u0439 \u0431\u0430\u0437\u044b;"", ""\u043f\u043e\u0434\u0433\u043e\u0442\u043e\u0432\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 \u0434\u043b\u044f \u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0435\u0439;"", ""\u043e\u0446\u0435\u043d\u043a\u0430 \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u0438 \u043c\u043e\u0434\u0435\u043b\u0438 \u0438 \u0430\u043d\u0430\u043b\u0438\u0437 \u044d\u0444\u0444\u0435\u043a\u0442\u0438\u0432\u043d\u043e\u0441\u0442\u0438 \u0432\u043d\u0435\u0434\u0440\u0435\u043d\u0438\u044f."", ""\u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u044f \u0430\u0443\u0434\u0438\u0442\u043e\u0440\u0438\u0438 \u0434\u043b\u044f \u043f\u0435\u0440\u0441\u043e\u043d\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 \u0440\u0435\u043a\u043b\u0430\u043c\u044b;""], ""Project description"": [""SWEET.TV"", ""\u043d\u043e\u0432\u043e\u0435 \u0442\u0435\u043b\u0435\u0432\u0438\u0434\u0435\u043d\u0438\u0435 \u0434\u043b\u044f \u0432\u0441\u0435\u0439 \u0441\u0442\u0440\u0430\u043d\u044b! \u041d\u0430\u0448\u0435 \u043d\u0435\u043e\u0441\u043f\u043e\u0440\u0438\u043c\u043e\u0435 \u043f\u0440\u0435\u0438\u043c\u0443\u0449\u0435\u0441\u0442\u0432\u043e"", ""\u0441\u043e\u0431\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u0435 \u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u043a\u0438, \u0443\u043c\u0435\u043d\u0438\u0435 \u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c \u0432 \u0432\u044b\u0441\u043e\u043a\u043e\u043a\u043e\u043d\u043a\u0443\u0440\u0435\u043d\u0442\u043d\u043e\u0439 \u0441\u0440\u0435\u0434\u0435. \u0420\u0430\u0431\u043e\u0442\u0430\u0435\u043c \u0433\u0438\u0431\u043a\u043e \u0438 \u0441 \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0439 \u043e\u0442\u0434\u0430\u0447\u0435\u0439, \u0441\u043e\u0441\u0440\u0435\u0434\u043e\u0442\u043e\u0447\u0435\u043d\u044b \u043d\u0430 \u043f\u0440\u0435\u0434\u043e\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u0438 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u0433\u043e \u0442\u0435\u043b\u0435\u0432\u0438\u0434\u0435\u043d\u0438\u044f \u043a\u043b\u0438\u0435\u043d\u0442\u0430\u043c \u0432 \u043b\u044e\u0431\u043e\u0439 \u0442\u043e\u0447\u043a\u0435 \u043d\u0430 \u043a\u0430\u0440\u0442\u0435. \u041c\u044b \u0440\u0430\u0441\u0442\u0435\u043c \u0438 \u0440\u0430\u0437\u0432\u0438\u0432\u0430\u0435\u043c\u0441\u044f, \u0438\u043c\u0435\u043d\u043d\u043e \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u0438\u0449\u0435\u043c \u0432 \u043a\u043e\u043c\u0430\u043d\u0434\u0443 \u0430\u043a\u0442\u0438\u0432\u043d\u044b\u0445, \u0430\u043c\u0431\u0438\u0446\u0438\u043e\u0437\u043d\u044b\u0445 \u0438 \u0443\u0432\u043b\u0435\u0447\u0435\u043d\u043d\u044b\u0445 \u043b\u044e\u0434\u0435\u0439!""]}"
Genesis,https://jobs.dou.ua/companies/genesis-technology-partners/,Data Analyst,https://jobs.dou.ua/companies/genesis-technology-partners/vacancies/131201/, Kyiv,13 October 2020,,"Genesis is one of the largest IT companies in Ukraine with more than 1500 people in 9 countries, who create products for 200 million users monthly. We are the most high-loaded company in the country and one of the largest partner of Facebook, Google, Snapchat and Apple in the CEE region. Our team is one of the best high-tech teams in Eastern Europe. Genesis was recognised by DOU.UA as the Best IT Employer in Ukraine in the category 800 — 1500 employees over the last years. We received very high ratings across all major evaluation categories such as professional growth, compensation, working conditions, communication with management and colleagues, etc. Are you excited about creating technology to be used by millions of people worldwide? Are you passionate about web and programming? Do you like learning new things? Are you willing to take responsibility for a product? Do you like challenging goals and personal growth? Do you want to work in a “flat organization” where voice of each team member is important? If you answered “YES” to all these questions, congratulations! You are a candidate for a new Teleservices team at Genesis! You will be part of a small and high performing team responsible for building out and testing teleservices web application. Your goal will be to integrate analytics from different sources, come up with ideas for product improvement, work with development team to iterate through experiments. You will also be responsible for optimization of user acquisition process. You need to be familiar with modern programming concepts and technologies. Responsibilities:• Modelling of user acquisition process and user acquisition optimization• Sales funnel optimization (idea generation, implementation in cooperation with development team, A/B testing and result measurement)• Quantitative modelling of demand (in response to price and other factors), product price optimization Requirements:• Knowledge of general-purpose programming language (e.g. Python, Java)• Proficiency in SQL and Excel• Academic background in mathematics/ statistics/ computer science• English upper intermediate or higher• Knowledge of economics is a plus• Familiarity with modern Machine Learning techniques a plus Genesis is a unique place for the development and growth with:• Expertise in the development of high-loaded products in international markets;• Unique opportunities for learning including various training and seminars within the company, access to a valuable and extensive book library, English courses and participation in key IT industry events worldwide;• Perfect working conditions: an excellent office in a 5 minutes’ walk from Taras Shevchenko metro station, free food and drinks in the office, medical insurance. The company has professional coaches in football, basketball and running and supports various sports initiatives (swim races, marathons, etc).",dou,2020-11-12,Data Analyst@Genesis,,,{}
GeoZilla,https://jobs.dou.ua/companies/geozilla/,Senior Data Engineer,https://jobs.dou.ua/companies/geozilla/vacancies/134903/, Kyiv,13 October 2020,,"Required skills We are working on a new initiative; building exciting features into our family safety product (trusted by over 1M families globally), that are aimed at saving people’s lives, through deployment of cutting-edge ML based technologies and real time data processing. We are looking for a motivated, enthusiastic and highly skilled data engineer to join the team, and work alongside data science and backend engineers to bring to life our product vision. · 4+ years of commercial experience in software development· Self-starter, proactive and ability to work to deadlines· Strong attention to detail with excellent quantitative and qualitative abilities.· Excellent code quality standards and a full understanding of writing unit tested code. Use of Git and familiar with code review practices and pull/merge requests· The ability to turn prototype and proof of concept code into robust production grade systems· Experience with AWS data products including DataPipeline, EMR and S3 repositories· Understanding of principles of parallel computing· Experience in Scala 2, Spark/PySpark, Spark Core, SparkML, Kafka· Deep understanding of Data Modeling· Experience with ETL optimization· Understanding of ML principles· Solid experience with Spark on YARN, Hadoop and Map/Reduce· Linux, Jenkins, SQL, Maven As a plus · Experience with geospatial data sets· Understanding of main principles of NoSQL databases· Understanding of Python at a production level· Experience of AWS infrastructure services such as EMR and Athena· Data engineering experience, working with data scientists · Experience with data streaming, real time data processing at high load· R — ability to understand and convert code where required· Experience of scalable data processing jobs on high volume data We offer — Competitive salary — Possibility of remote work— Medical insurance— In-house English courses— Great office location— Fun working environment Responsibilities · Working productively within in an AGILE team environment· As part of the development team, assist in planning and prioritising jobs· Build data systems and pipelines· Writing, testing and debugging code for new and existing software products· Responding promptly to bug reports and production issues· Develop tools and applications by producing clean, efficient code· Review and debug code· Collaborate with internal teams to fix and improve products· Collaborate with data scientists and architects· Document code and assist in testing and QA of systems.· Ensure software is up-to-date with latest technologies Project description The people here at GeoZilla don’t just create apps — they create the kind of wonder that offers peace of mind to millions of families worldwide. It’s the team’s synergy and its vision that inspire the innovation that runs through everything we do, from amazing technology to partnerships with the industry-leading IoT. Join GeoZilla, and help us make families worldwide feel safer. With over four million registered users on iOS and Android, backed by five years of R&D, GeoZilla is working to make millions of families worldwide safer.More information about GeoZilla can be found on the site www.geozilla.com.",dou,2020-11-12,Senior Data Engineer@GeoZilla,,,"{""Required skills"": [""We are working on a new initiative; building exciting features into our family safety product (trusted by over 1M families globally), that are aimed at saving people\u2019s lives, through deployment of cutting-edge ML based technologies and real time data processing.""], ""As a plus"": [""We are looking for a motivated, enthusiastic and highly skilled data engineer to join the team, and work alongside data science and backend engineers to bring to life our product vision.""], ""We offer"": [""\u00b7 4+ years of commercial experience in software development\u00b7 Self-starter, proactive and ability to work to deadlines\u00b7 Strong attention to detail with excellent quantitative and qualitative abilities.\u00b7 Excellent code quality standards and a full understanding of writing unit tested code. Use of Git and familiar with code review practices and pull/merge requests\u00b7 The ability to turn prototype and proof of concept code into robust production grade systems\u00b7 Experience with AWS data products including DataPipeline, EMR and S3 repositories\u00b7 Understanding of principles of parallel computing\u00b7 Experience in Scala 2, Spark/PySpark, Spark Core, SparkML, Kafka\u00b7 Deep understanding of Data Modeling\u00b7 Experience with ETL optimization\u00b7 Understanding of ML principles\u00b7 Solid experience with Spark on YARN, Hadoop and Map/Reduce\u00b7 Linux, Jenkins, SQL, Maven""], ""Responsibilities"": [""\u00b7 Experience with geospatial data sets\u00b7 Understanding of main principles of NoSQL databases\u00b7 Understanding of Python at a production level\u00b7 Experience of AWS infrastructure services such as EMR and Athena\u00b7 Data engineering experience, working with data scientists \u00b7 Experience with data streaming, real time data processing at high load\u00b7 R"", ""ability to understand and convert code where required\u00b7 Experience of scalable data processing jobs on high volume data""], ""Project description"": [""Competitive salary"", ""Possibility of remote work"", ""Medical insurance"", ""In-house English courses"", ""Great office location"", ""Fun working environment""]}"
SD Solutions,https://jobs.dou.ua/companies/sd-solutions/,Python (BigData) Developer (ThoughtLeaders),https://jobs.dou.ua/companies/sd-solutions/vacancies/134880/," Kyiv, remote",13 October 2020,,"Required skills — 5+ years in software development— Significant experience working with Python (Django 2.2)— Background in Big Data or Data Science — Experience with database systems such as PostgreSQL— Experience working with AWS, Heroku— Experience with ELK is a MUST— Good written and spoken English As a plus — Experience with Docker— Strong OO design skills— Teamwork skills with a problem-solving attitude We offer We genuinely care about our employees’ happiness and engagement.Office in the historical Kyiv centerCompetitive compensationFully stocked kitchen (when in the office).Best working equipment of your choiceChallenging problems to solve and an awesome team to collaborate with every single day.Work culture focused on innovation and the creation of lasting value for our clients and employees. Responsibilities — Engineer complex and efficient and distributed data transformation solutions using Python— Research, plan, design, develop, document, test, implement and support proprietary software applications— Analytical data validation for accuracy and completeness of reported business metrics— Understand the business problem and engineer/architect/build an efficient, cost-effective and scalable technology infrastructure solution— Monitor system performance after implementation and iteratively devise solutions to improve performance and user experience— Research and innovate new data product ideas to grow client’s revenue opportunities and contribute to company’s intellectual property Project description ThoughtLeaders enables brands and agencies to discover, purchase and manage branded content endorsements. The mission is to incentivize high-quality, long-form content and rid the Internet of advertisements that hurt the user experience.ThoughtLeaders help brands reach the perfect audience using native content on a variety of channels, including YouTube, podcasts, email newsletters, and blogs. Brands use our proprietary software to track their competitors and discover, manage and scale what used to be thought of as ""non-scalable""​ advertising campaigns.Website: thoughtleaders.io",dou,2020-11-12,Python (BigData) Developer (ThoughtLeaders)@SD Solutions,,,"{""Required skills"": [""5+ years in software development"", ""Significant experience working with Python (Django 2.2)"", ""Background in Big Data or Data Science"", ""Experience with database systems such as PostgreSQL"", ""Experience working with AWS, Heroku"", ""Experience with ELK is a MUST"", ""Good written and spoken English""], ""As a plus"": [""Experience with Docker"", ""Strong OO design skills"", ""Teamwork skills with a problem-solving attitude""], ""We offer"": [""We genuinely care about our employees\u2019 happiness and engagement.Office in the historical Kyiv centerCompetitive compensationFully stocked kitchen (when in the office).Best working equipment of your choiceChallenging problems to solve and an awesome team to collaborate with every single day.Work culture focused on innovation and the creation of lasting value for our clients and employees.""], ""Responsibilities"": [""Engineer complex and efficient and distributed data transformation solutions using Python"", ""Research, plan, design, develop, document, test, implement and support proprietary software applications"", ""Analytical data validation for accuracy and completeness of reported business metrics"", ""Understand the business problem and engineer/architect/build an efficient, cost-effective and scalable technology infrastructure solution"", ""Monitor system performance after implementation and iteratively devise solutions to improve performance and user experience"", ""Research and innovate new data product ideas to grow client\u2019s revenue opportunities and contribute to company\u2019s intellectual property""], ""Project description"": [""ThoughtLeaders enables brands and agencies to discover, purchase and manage branded content endorsements. The mission is to incentivize high-quality, long-form content and rid the Internet of advertisements that hurt the user experience.ThoughtLeaders help brands reach the perfect audience using native content on a variety of channels, including YouTube, podcasts, email newsletters, and blogs. Brands use our proprietary software to track their competitors and discover, manage and scale what used to be thought of as \""non-scalable\""\u200b advertising campaigns.Website: thoughtleaders.io""]}"
Perion,https://jobs.dou.ua/companies/septa-digital-agency/,Data Analyst,https://jobs.dou.ua/companies/septa-digital-agency/vacancies/130872/, Kyiv,12 October 2020,,"Required skills • 3+ year of a full-time data analytics experience• Expert SQL coding skills against large data sets• Strong analytical skills, including the ability to mine data in order to draw meaningful conclusions• Strong oral and written communications skills• Knowledge of Business Intelligence tools such as Tableau• Spoken English As a plus Experience, ideally in an Ad-tech company We offer • Competitive salary;• Career and professional growth;• Possibility to work on the interesting international project;• Long-term employment with paid vacation and other social benefits;• Opportunities for professional development and personal growth;• 15.00 — 24.00 working hours;• Convenient modern office in the city center;• Training and mentoring. Responsibilities • Work with business teams to understand their analytical needs, including identifying critical metrics and KPIs• Use analytical and problem-solving skills to deliver actionable insights to relevant decision-makers• Develop rich interactive visualizations integrating various reporting components from multiple data sources• Use SQL, Tableau, and other technologies to pull data from different backend systems and produce meaningful information and visualizations• Take complicated problems and build simple frameworks• Work directly with users and management to gather requirements, provide status updates, and build good relationships and rapport Project description At Undertone, we sit at the intersection of media, creative, and technology. We have formed deep partnerships with the world’s best digital media properties, developed a suite of groundbreaking multi-screen creative formats for brands to leverage, and built technology platforms that underpin every aspect of campaign planning, delivery, optimization, and measurement.Undertone’s Data Management Service (UDMS) is a big data, cloud-based data warehouse, dashboard and reporting environment. Are you a data rock star? Do you want to help enable a data-driven organization? This is your opportunity to join a mission-critical team, at an innovative company in an industry just beginning to harness the power of data.As a member of the Undertone’s UDMS Team, the Data Analyst drives value by providing provocative, differentiating analytics and insights. This position will support a wide variety of business intelligence efforts across Undertone while working in a highly collaborative manner within multiple large, cloud-based data sources to identify insights and spearhead our next-generation product offerings. Most importantly, you should “love” the data, working with data, finding the nuance that leads to key differentiation for the business and our customers. This is high visibility analytics and consulting position requiring daily interaction with business users, data scientists, engineers, and key stakeholders.",dou,2020-11-12,Data Analyst@Perion,,,"{""Required skills"": [""3+ year of a full-time data analytics experience"", ""Expert SQL coding skills against large data sets"", ""Strong analytical skills, including the ability to mine data in order to draw meaningful conclusions"", ""Strong oral and written communications skills"", ""Knowledge of Business Intelligence tools such as Tableau"", ""Spoken English""], ""As a plus"": [""Experience, ideally in an Ad-tech company""], ""We offer"": [""Competitive salary;"", ""Career and professional growth;"", ""Possibility to work on the interesting international project;"", ""Long-term employment with paid vacation and other social benefits;"", ""Opportunities for professional development and personal growth;"", ""15.00"", ""24.00 working hours;"", ""Convenient modern office in the city center;"", ""Training and mentoring.""], ""Responsibilities"": [""Work with business teams to understand their analytical needs, including identifying critical metrics and KPIs"", ""Use analytical and problem-solving skills to deliver actionable insights to relevant decision-makers"", ""Develop rich interactive visualizations integrating various reporting components from multiple data sources"", ""Use SQL, Tableau, and other technologies to pull data from different backend systems and produce meaningful information and visualizations"", ""Take complicated problems and build simple frameworks"", ""Work directly with users and management to gather requirements, provide status updates, and build good relationships and rapport""], ""Project description"": [""At Undertone, we sit at the intersection of media, creative, and technology. We have formed deep partnerships with the world\u2019s best digital media properties, developed a suite of groundbreaking multi-screen creative formats for brands to leverage, and built technology platforms that underpin every aspect of campaign planning, delivery, optimization, and measurement.Undertone\u2019s Data Management Service (UDMS) is a big data, cloud-based data warehouse, dashboard and reporting environment. Are you a data rock star? Do you want to help enable a data-driven organization? This is your opportunity to join a mission-critical team, at an innovative company in an industry just beginning to harness the power of data.As a member of the Undertone\u2019s UDMS Team, the Data Analyst drives value by providing provocative, differentiating analytics and insights. This position will support a wide variety of business intelligence efforts across Undertone while working in a highly collaborative manner within multiple large, cloud-based data sources to identify insights and spearhead our next-generation product offerings. Most importantly, you should \u201clove\u201d the data, working with data, finding the nuance that leads to key differentiation for the business and our customers. This is high visibility analytics and consulting position requiring daily interaction with business users, data scientists, engineers, and key stakeholders.""]}"
Perion,https://jobs.dou.ua/companies/septa-digital-agency/,Data scientist,https://jobs.dou.ua/companies/septa-digital-agency/vacancies/131574/, Kyiv,12 October 2020,,"Required skills • MSc degree in Data Science, Statistics, Math, Physics, Engineering, or a similar relevant subject• 3+ years of hands-on experience as Data Scientist• Vast experience in SQL, Python, and spark • statistical modeling experience, analytical skills, and engineering skills• Ability to quickly learn new technologies, frameworks, and algorithms We offer • Competitive salary;• Career and professional growth;• Possibility to work on the interesting international project;• Long-term employment with paid vacation and other social benefits;• Opportunities for professional development and personal growth;• Convenient modern office in the city center;• Training and mentoring. Responsibilities • Deploying models and systems to production by applying data-driven Machine Learning and AI solutions end to end, starting from research and design to development and testing.• Explain results and insights to product & business throughout the process.• Perform research & analysis to solve complex problems using advanced AI technologies (deep learning, re-enforcement learning, time-series, NLP...) • Work closely with product, data engineers, and the R&D teams to deliver the best solution to the business.• Run research and POCs for new technologies and tools as part of our continuous improvement life cycle.• Make sure the solution is stable, scalable, and provides accurate results at all times. Project description Perion is a global technology company that provides digital advertising technology solutions to the biggest brands and publishers around the globe. With our unique data-driven AI/ML based technologies, we deliver and optimize hundreds of terabytes of data and billions of events per day from dozens of sources to provide a superior user experience across screens and platforms, including mobile, video, social and native. We are looking for a highly skilled Data scientist to join our Data Science Group to the journey of leveraging Big Data to revolutionize our offering, business operations and decision making across the company’s ecosystem.",dou,2020-11-12,Data scientist@Perion,,,"{""Required skills"": [""MSc degree in Data Science, Statistics, Math, Physics, Engineering, or a similar relevant subject"", ""3+ years of hands-on experience as Data Scientist"", ""Vast experience in SQL, Python, and spark"", ""statistical modeling experience, analytical skills, and engineering skills"", ""Ability to quickly learn new technologies, frameworks, and algorithms""], ""We offer"": [""Competitive salary;"", ""Career and professional growth;"", ""Possibility to work on the interesting international project;"", ""Long-term employment with paid vacation and other social benefits;"", ""Opportunities for professional development and personal growth;"", ""Convenient modern office in the city center;"", ""Training and mentoring.""], ""Responsibilities"": [""Deploying models and systems to production by applying data-driven Machine Learning and AI solutions end to end, starting from research and design to development and testing."", ""Explain results and insights to product & business throughout the process."", ""Perform research & analysis to solve complex problems using advanced AI technologies (deep learning, re-enforcement learning, time-series, NLP...)"", ""Work closely with product, data engineers, and the R&D teams to deliver the best solution to the business."", ""Run research and POCs for new technologies and tools as part of our continuous improvement life cycle."", ""Make sure the solution is stable, scalable, and provides accurate results at all times.""], ""Project description"": [""Perion is a global technology company that provides digital advertising technology solutions to the biggest brands and publishers around the globe. With our unique data-driven AI/ML based technologies, we deliver and optimize hundreds of terabytes of data and billions of events per day from dozens of sources to provide a superior user experience across screens and platforms, including mobile, video, social and native.""]}"
healthPrecision,https://jobs.dou.ua/companies/healthprecision/,Medical Expert to Data Science team in Cherkassy,https://jobs.dou.ua/companies/healthprecision/vacancies/131413/, Cherkasy,12 October 2020,,"Required skills — Advanced knowledge of English;— Experience at pharmaceutical/biotechnology companies and/or consulting firms a plus but not required;— Strong critical thinking and analytical skills;— Trustworthy — ability to handle confidential information. Responsibilities Medical Expert should also have the ability to gather, compile, and analyze research and program evaluation data and present reports and summaries in written, tabular, and graphic form.Requires a blend of technical aptitude, data management experience and the ability to communicate well with internal and external customers. Project description Medical Brain is a product company. We made the high-end Medical Decision Support solution that successfully supports hospitals and medical doctors in the USA, Canada and all over the world to provide the best service and save lives. Our offices are located in the USA and Ukraine (Kiev, Lvov and Cherkassy). We are the professional medical doctors, passionate IT developers and bright managers. We are enthusiastic and dedicated to our mission — to help bring people to a healthier state and empower them to maintain it. medicalbrain.com",dou,2020-11-12,Medical Expert to Data Science team in Cherkassy@healthPrecision,,,"{""Required skills"": [""Advanced knowledge of English;"", ""Experience at pharmaceutical/biotechnology companies and/or consulting firms a plus but not required;"", ""Strong critical thinking and analytical skills;"", ""Trustworthy"", ""ability to handle confidential information.""], ""Responsibilities"": [""Medical Expert should also have the ability to gather, compile, and analyze research and program evaluation data and present reports and summaries in written, tabular, and graphic form.Requires a blend of technical aptitude, data management experience and the ability to communicate well with internal and external customers.""], ""Project description"": [""Medical Brain is a product company. We made the high-end Medical Decision Support solution that successfully supports hospitals and medical doctors in the USA, Canada and all over the world to provide the best service and save lives.""]}"
GeoZilla,https://jobs.dou.ua/companies/geozilla/,Data Scientist,https://jobs.dou.ua/companies/geozilla/vacancies/128768/, Kyiv,12 October 2020,,"Required skills We are looking for a talented data scientist with strong engineering background to kick start an exciting new direction in our well established consumer app business.You will be responsible for designing and building ML based solution which predict safety hazards before they happen, and work on improving our product quality by optimising the accuracy of our location data. Requirements:— Bachelor’s degree in statistics, applied mathematics, or related discipline;— 4+ years experience in data science;— Proficiency with data mining, mathematics, and statistical analysis;— Advanced pattern recognition and predictive modeling experience; — Excellent analytical and problemsolving skills;— Experience in database interrogation and analysis tools, such asZeppelin, Anaconda, Pandas, ScikitLearn, Jupyter, SQL, Spark;— Experience in programming languages and Python, Scala;— Communication and presentation skills in order to explain your work to people who don’t understand the mechanics behind data analysis;— Effective listening skills in order to understand the requirements of the business;— Drive and the resilience to try new ideas if the first one doesn’t work you’ll be expected to work with minimal supervision, so it’s important that you’re able to motivate yourself;— Planning, time management and organisational skills;— Great attention to detail;— Teamworking skills and a collaborative approach to sharing ideas and finding solutions. We offer — Competitive salary — Possibility of remote work— Medical insurance— In-house English courses— Great office location— Fun working environment Responsibilities — Work with stakeholders throughout the organization to identify opportunities for leveraging company data to drive business solutions;— Mine and analyze data from company databases to drive optimization and improvement of product development, marketing techniques and business strategies;— Assess the effectiveness and accuracy of data sources and data gathering techniques;— Develop custom data models and algorithms to apply to data sets;— Use predictive modeling to increase and optimize customer experiences, revenue generation, ad targeting and other business out comes;— Develop company A/B testing framework and test model quality;— Coordinate with different functional teams to implement models and monitor outcomes;— Develop processes and tools to monitor and analyse model performance and data accuracy. Project description The people here at GeoZilla don’t just create apps — they create the kind of wonder that offers peace of mind to millions of families worldwide. It’s the team’s synergy and its vision that inspire the innovation that runs through everything we do, from amazing technology to partnerships with the industry-leading IoT. Join GeoZilla, and help us make families worldwide feel safer. With over four million registered users on iOS and Android, backed by five years of R&D, GeoZilla is working to make millions of families worldwide safer.More information about GeoZilla can be found on the site www.geozilla.com.",dou,2020-11-12,Data Scientist@GeoZilla,,,"{""Required skills"": [""We are looking for a talented data scientist with strong engineering background to kick start an exciting new direction in our well established consumer app business.You will be responsible for designing and building ML based solution which predict safety hazards before they happen, and work on improving our product quality by optimising the accuracy of our location data.""], ""We offer"": [""Requirements:"", ""Bachelor\u2019s degree in statistics, applied mathematics, or related discipline;"", ""4+ years experience in data science;"", ""Proficiency with data mining, mathematics, and statistical analysis;"", ""Advanced pattern recognition and predictive modeling experience;"", ""Excellent analytical and problemsolving skills;"", ""Experience in database interrogation and analysis tools, such asZeppelin, Anaconda, Pandas, ScikitLearn, Jupyter, SQL, Spark;"", ""Experience in programming languages and Python, Scala;"", ""Communication and presentation skills in order to explain your work to people who don\u2019t understand the mechanics behind data analysis;"", ""Effective listening skills in order to understand the requirements of the business;"", ""Drive and the resilience to try new ideas if the first one doesn\u2019t work you\u2019ll be expected to work with minimal supervision, so it\u2019s important that you\u2019re able to motivate yourself;"", ""Planning, time management and organisational skills;"", ""Great attention to detail;"", ""Teamworking skills and a collaborative approach to sharing ideas and finding solutions.""], ""Responsibilities"": [""Competitive salary"", ""Possibility of remote work"", ""Medical insurance"", ""In-house English courses"", ""Great office location"", ""Fun working environment""], ""Project description"": [""Work with stakeholders throughout the organization to identify opportunities for leveraging company data to drive business solutions;"", ""Mine and analyze data from company databases to drive optimization and improvement of product development, marketing techniques and business strategies;"", ""Assess the effectiveness and accuracy of data sources and data gathering techniques;"", ""Develop custom data models and algorithms to apply to data sets;"", ""Use predictive modeling to increase and optimize customer experiences, revenue generation, ad targeting and other business out comes;"", ""Develop company A/B testing framework and test model quality;"", ""Coordinate with different functional teams to implement models and monitor outcomes;"", ""Develop processes and tools to monitor and analyse model performance and data accuracy.""]}"
Reface,https://jobs.dou.ua/companies/reface/,Machine Learning Engineer,https://jobs.dou.ua/companies/reface/vacancies/134760/, Kyiv,12 October 2020,,"Required skills Machine learning engineer develops deep learning / machine learning models, improves existing solutions,and provides results in a form that can be easily deployed, used and maintained.Machine learning engineer works with a full cycle of ML project development, including searching andpreparing datasets, creating deep learning models, testing and comparing, and preparing packages fordeploying. Specialized knowledge:● Understanding theoretical concepts of machine learning and neural networks.● Understanding theoretical concepts of deep learning architectures.● Understanding how these theoretical concepts could be applied to real-world problems.● Knowledge and hands-on experience with Python or other relevant programming languages.● Knowledge and hands-on experience with ML/DL frameworks (PyTorch, TensorFlow etc.).● Knowledge and hands-on experience with tools for data preprocessing and scraping. Skills and abilities:● Strong English verbal and written communication skills.● Ability to work independently with limited supervision. Experience:● Track record in deep learning, data science, machine learning.● Relevant levels of theoretical knowledge in data science and machine learning. Responsibilities ● Collecting, transforming, and preprocessing raw data to prepare it for modeling.● Building machine learning and deep learning models.● Designing, developing, training, and testing models and algorithms.● Providing comparative research on different algorithms and models.● Implementing the model in a form that can be easily used by engineers, documenting its interfaces.● Delivering the model in a form that can be easily deployable and maintained. Project description Based on different types of data, candidates can create machine learning / deep learning models. Ability to work in short iteration cycles (up to 2 weeks) from initial research to PoC prototype. Ability to work simultaneously on multiple tasks. Ability to work in a solo or a small team and be responsible for the final results. Lots of educational and self-educational activities.",dou,2020-11-12,Machine Learning Engineer@Reface,,,"{""Required skills"": [""Machine learning engineer develops deep learning / machine learning models, improves existing solutions,and provides results in a form that can be easily deployed, used and maintained.Machine learning engineer works with a full cycle of ML project development, including searching andpreparing datasets, creating deep learning models, testing and comparing, and preparing packages fordeploying.""], ""Responsibilities"": [""Specialized knowledge:\u25cf Understanding theoretical concepts of machine learning and neural networks.\u25cf Understanding theoretical concepts of deep learning architectures.\u25cf Understanding how these theoretical concepts could be applied to real-world problems.\u25cf Knowledge and hands-on experience with Python or other relevant programming languages.\u25cf Knowledge and hands-on experience with ML/DL frameworks (PyTorch, TensorFlow etc.).\u25cf Knowledge and hands-on experience with tools for data preprocessing and scraping.""], ""Project description"": [""Skills and abilities:\u25cf Strong English verbal and written communication skills.\u25cf Ability to work independently with limited supervision.""]}"
EOS Data Analytics,https://jobs.dou.ua/companies/eos-data-analytics/,Chief Operating Officer,https://jobs.dou.ua/companies/eos-data-analytics/vacancies/131066/, Kyiv,12 October 2020,,"EOS Data Analytics is developing cloud-based GIS analysis service. Company uses a combination of satellite imagery, geospatial data, customer workflow information, and consumer behaviour principles to make the deepest and the most comprehensive GIS analysis. Aspectum, as a part of EOS Data Analytics, is a Cloud Platform for Location Business Intelligence.Aspectum allows transforming business data into geospatial data, providing high-quality visualization and powerful analytics for converting it into valuable business outcomes. Aspectum is looking for a talented and productive COO to join the team and help improve a product. Responsibilities:• company strategy execution and review, planning operational activities to achieve business goals;• product launch on different markets (WENA focus);• lead marketing activities and sync marketing with sales needs;• combining usage data, market and user research, and industry best practises to understand and identify user problems and breakthrough opportunities;• P&L planning and execution;• measuring the results of changes introduced, deciding and communicating on success or failure, identifying key learnings and integrating them in overall understanding of the owned area. You need to have:• 3+ years of experience in Product Management, CMO, Head Of Sales;• SaaS products launch experience (preferable US market);• leadership skills — you are able to come up with a vision and execute it while leading people who you usually have no authority over;• analytical skills to understand usage data, market and user researches;• proven high level of stress resistance. Company offers:• work with a global brand in dynamic international company;• social guarantees, health insurance and other benefits;• 24-day paid vacation, paid sick leave, medical insurance;• training, conferences, courses;• compensation of foreign language courses and sport sections.",dou,2020-11-12,Chief Operating Officer@EOS Data Analytics,,,{}
Ciklum,https://jobs.dou.ua/companies/ciklum/,Senior Databricks Developer for Ciklum Digital (200004CY),https://jobs.dou.ua/companies/ciklum/vacancies/134754/, Kyiv,12 October 2020,,"On behalf of Ciklum Digital, Ciklum is looking for a Senior Databricks Developer to join the Kyiv team on a full-time basis. Project description: You can name examples of use in different contexts. Are guided by best-practices and specifications of such skills:",dou,2020-11-12,Senior Databricks Developer for Ciklum Digital (200004CY)@Ciklum,,,{}
MGID,https://jobs.dou.ua/companies/mgid/,Data analyst,https://jobs.dou.ua/companies/mgid/vacancies/131432/, Kyiv,12 October 2020,,"MGID — международная компания-лидер на рынке нативной рекламы, основанная в 2008 году. Платформа MGID помогает медиа монетизировать аудиторию, а брендам — донести правильное рекламное сообщение своим потребителям. MGID предлагает комплексные решения: от планирования и разработки рекламной стратегии, до ее внедрения и оптимизации. Наши клиенты — крупные международные бренды: Renault, Domino’s, airbnb, PizzaHut, Qatar Airlines и много других компаний, включая медиахолдинги и сетевые агентства. MGID — это:—Одна из самых больших martech-компаний на украинском рынке;—Собственная Highload платформа, которая предоставляет 185 млрд контентных рекомендаций для 850 млн уникальных посетителей на более чем 70 языках;—Победитель многочисленных премий за инновации и качество продукта в сфере AdTech;—Штат более 600 сотрудников, работающих в офисах в США, Европе и Азии;—Непрерывные инновации, обмен опытом и разработками между региональными командами. Знания и навыки, которые нужны: —Опыт работы на должности Бизнес-аналитика в веб-проектах от 3-х лет;—Знание и наличие опыта работы с Google, Facebook. Оптимизация закупки трафика у них, настройка целей и этапов конверсий;—Умение работать с базами данных (MySQL, Clickhouse);—Знание принципов работы Google Analytics, Яндекс.Метрика;—Высокий уровень организованности и внимательность к деталям;—Знание английского языка не ниже Intermediate. Будет плюсом: —Опыт работы с e-commerce проектами;—Знание Python. Чем предстоит заниматься: —Анализ результатов рекламных кампаний топовых рекламодателей;—Поиск паттернов и на их основании формирование рекомендаций бизнес подразделениям по улучшению методов оптимизации, корректировка стратегии подключения новых клиентов;—Тестирование новых подходов и различных комбинаций настроек;—Тесное сотрудничество с продуктовой командой и командой разработки в рамках решения аналитических задач;—Анализ рынка в целом и конкурентный анализ. Мы предлагаем: —Новый офис в БЦ «Мармелад»;—Курсы английского языка с носителем;—Гибкий подход к графику. Для нас главное — результат и вовлеченность;—Корпоративное участие в спортивных, эко- и социальных проектах. Как это бывает — тут www.facebook.com/MGID.inside—Возможность посещать лекции Академии MGID;—Пакет медицинской страховки уровня «Премиум».",dou,2020-11-12,Data analyst@MGID,,,{}
R&D Center,https://jobs.dou.ua/companies/rnd-center/,Data Moderator (night shifts),https://jobs.dou.ua/companies/rnd-center/vacancies/76959/?from=first-job, Kyiv,12 October 2020,,"We are looking for an enthusiastic person to join our Neighbourhood moderation team. In this role you will be responsible for content moderation of mobile app — you will work with big amounts of videos and comments, posted by our neighbors. Our goal is to be sure that neighbors see only important safety-related posts in their app. Take your chance to work in a passionate and friendly team and make your impact on people’s safety! — Pre-moderate incoming content shared by our neighbors including videos, images, text posts, and comments— Collect and analyze data for the other departments— Identify ways to improve the workflows and suggest solutions— Deliver instant crime and safety alerts to our neighbors and play your part in reducing crime! — Upper-intermediate/Advanced English (written and spoken)— Willingness to learn and adapt to changes— Basic understanding of IT terminology — Experience with PC based computer systems or junior administrator experience (MacOS is preferable)— Knowledge of bug tracking systems like Jira and etc. Working schedule: mixed shifts, two-night shifts/two days off. — Working on the world’s most impactful security products. More than that, an opportunity to get some of those for personal usage — Competitive salary and perks— PE accounting and support— WFH and remote working mode possibility. Partial furniture compensation— Social package, including medical insurance available from the start date and sports compensation after the trial period— 18 paid vacation days per year, paid public holidays according to the Ukrainian legislation— Educational possibilities like corporate courses, knowledge hubs, and free English classes. Semiannual performance review— Free meals, fruits, and snacks when working in the office",dou,2020-11-12,Data Moderator (night shifts)@R&D Center,,,{}
THREDUP Inc.,https://jobs.dou.ua/companies/thredup-inc/,Lead Data Platform Engineer,https://jobs.dou.ua/companies/thredup-inc/vacancies/131333/, Kyiv,12 October 2020,,"Required skills 8+ years of experience in building scalable data platforms and tools.Demonstrated experience in implementing Spark for data processing.Demonstrated experience in implementing Kafka for real-time data processing.Demonstrated experience providing rest endpoints to expose data to other applications.Experience integrating and supporting Databricks.Working with noSQL stores like HBASE/DynamoDB.Experience providing solutions to handle data privacy.Experience in implementing machine learning platforms like MLFlow, Tensorflow, Sagemake.Experience in deploying Airflow as a scheduling tool.Experience in implementing CI/CD pipelines using Jenkins.Expert level using Python and PySpark.Understanding with AWS Data technologies (such as Redshift, S3, Glue)Experience with repositories like Git, maven, jfrog. As a plus Experience working with cloud databases like Snowflake/Redshift/BigQuery.Experience migrating legacy data platforms to cloud-native solutions.Experience implementing Delta-Lake.Experience working with machine learning models.Experience with Java and building microservices. We offer High competitive top market salaryThe opportunity to make a massive impact & influence outcomes for our business and customers alongside passionate coworkersAutonomy. The ability to make, own, and carry out decisionsWorking within a modern tech stackFlexible working hours (possibility to work from home on Tuesdays and Thursdays)Uncounted paid vacationsFull-coverage medical insurance, free lunches on Wednesdays, English classes, etc.Relocation assistance and cost coverage program for candidates from other countries and cities Responsibilities Ideate and build the next generation product roadmap for the data platform team.Own Airflow deployments, integration with Datadog and CI/CD.Own the Mlflow machine learning platform and find alternatives or enhance it to support input data versioning and model deployment in A/B testing mode.Build and evangelize reusable components for data pipelines.Implement a solution to capture metadata and lineage.Own the enterprise event bus solution.Own the databricks environment.Own and shape the feature store infrastructure and roadmap. Project description At thredUP, we live a true data-driven culture with an ever-growing appetite for data and a mindset to generate insights and make informed business decisions. In this role, you will shape the roadmap for the next generation of data platform that is easy to use, elastic, and cost-efficient. This platform should make it easy for data engineers to build batch and streaming pipelines as well as make it easy to interact with the data for the end consumers. Build a robust machine-learning platform to develop, test, and deploy machine learning models deployed for both batch and real-time use cases. Additionally, support the current technology stack and evaluate newer tools/technologies that reduce the support cost and provide improved productivity and experience. You will interact in a highly collaborative environment of data engineers, data scientists, product managers, domain experts, and business leaders to deliver data solutions.",dou,2020-11-12,Lead Data Platform Engineer@THREDUP Inc.,,,"{""Required skills"": [""8+ years of experience in building scalable data platforms and tools.Demonstrated experience in implementing Spark for data processing.Demonstrated experience in implementing Kafka for real-time data processing.Demonstrated experience providing rest endpoints to expose data to other applications.Experience integrating and supporting Databricks.Working with noSQL stores like HBASE/DynamoDB.Experience providing solutions to handle data privacy.Experience in implementing machine learning platforms like MLFlow, Tensorflow, Sagemake.Experience in deploying Airflow as a scheduling tool.Experience in implementing CI/CD pipelines using Jenkins.Expert level using Python and PySpark.Understanding with AWS Data technologies (such as Redshift, S3, Glue)Experience with repositories like Git, maven, jfrog.""], ""As a plus"": [""Experience working with cloud databases like Snowflake/Redshift/BigQuery.Experience migrating legacy data platforms to cloud-native solutions.Experience implementing Delta-Lake.Experience working with machine learning models.Experience with Java and building microservices.""], ""We offer"": [""High competitive top market salaryThe opportunity to make a massive impact & influence outcomes for our business and customers alongside passionate coworkersAutonomy. The ability to make, own, and carry out decisionsWorking within a modern tech stackFlexible working hours (possibility to work from home on Tuesdays and Thursdays)Uncounted paid vacationsFull-coverage medical insurance, free lunches on Wednesdays, English classes, etc.Relocation assistance and cost coverage program for candidates from other countries and cities""], ""Responsibilities"": [""Ideate and build the next generation product roadmap for the data platform team.Own Airflow deployments, integration with Datadog and CI/CD.Own the Mlflow machine learning platform and find alternatives or enhance it to support input data versioning and model deployment in A/B testing mode.Build and evangelize reusable components for data pipelines.Implement a solution to capture metadata and lineage.Own the enterprise event bus solution.Own the databricks environment.Own and shape the feature store infrastructure and roadmap.""], ""Project description"": [""At thredUP, we live a true data-driven culture with an ever-growing appetite for data and a mindset to generate insights and make informed business decisions. In this role, you will shape the roadmap for the next generation of data platform that is easy to use, elastic, and cost-efficient. This platform should make it easy for data engineers to build batch and streaming pipelines as well as make it easy to interact with the data for the end consumers. Build a robust machine-learning platform to develop, test, and deploy machine learning models deployed for both batch and real-time use cases. Additionally, support the current technology stack and evaluate newer tools/technologies that reduce the support cost and provide improved productivity and experience.""]}"
